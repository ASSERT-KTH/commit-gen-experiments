BLEU SCORE: 0.020980574531482755

TEST MSG: ( Hadoop ) ensure that Cluster instances are always closed
GENERATED MSG: update CqlRecordWriter interface

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 5edad20 . . 81ceb25 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 2 . 2 . 4 <nl> + * ( Hadoop ) ensure that Cluster instances are always closed ( CASSANDRA - 10058 ) <nl> * ( cqlsh ) show partial trace if incomplete after max _ trace _ wait ( CASSANDRA - 7645 ) <nl> * Use most up - to - date version of schema for system tables ( CASSANDRA - 10652 ) <nl> * Deprecate memory _ allocator in cassandra . yaml ( CASSANDRA - 10581 , 10628 ) <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> index e531ad1 . . d687183 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> @ @ - 24 , 6 + 24 , 7 @ @ import java . util . concurrent . * ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> + import com . datastax . driver . core . Cluster ; <nl> import com . datastax . driver . core . Host ; <nl> import com . datastax . driver . core . Metadata ; <nl> import com . datastax . driver . core . ResultSet ; <nl> @ @ - 58 , 7 + 59 , 6 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < <nl> private String keyspace ; <nl> private String cfName ; <nl> private IPartitioner partitioner ; <nl> - private Session session ; <nl> <nl> protected void validateConfiguration ( Configuration conf ) <nl> { <nl> @ @ - 90 , 36 + 90 , 36 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < <nl> ExecutorService executor = new ThreadPoolExecutor ( 0 , 128 , 60L , TimeUnit . SECONDS , new LinkedBlockingQueue < Runnable > ( ) ) ; <nl> List < InputSplit > splits = new ArrayList < > ( ) ; <nl> <nl> - try <nl> + List < Future < List < InputSplit > > > splitfutures = new ArrayList < > ( ) ; <nl> + KeyRange jobKeyRange = ConfigHelper . getInputKeyRange ( conf ) ; <nl> + Range < Token > jobRange = null ; <nl> + if ( jobKeyRange ! = null ) <nl> { <nl> - List < Future < List < InputSplit > > > splitfutures = new ArrayList < > ( ) ; <nl> - KeyRange jobKeyRange = ConfigHelper . getInputKeyRange ( conf ) ; <nl> - Range < Token > jobRange = null ; <nl> - if ( jobKeyRange ! = null ) <nl> + if ( jobKeyRange . start _ key ! = null ) <nl> { <nl> - if ( jobKeyRange . start _ key ! = null ) <nl> - { <nl> - if ( ! partitioner . preservesOrder ( ) ) <nl> - throw new UnsupportedOperationException ( " KeyRange based on keys can only be used with a order preserving partitioner " ) ; <nl> - if ( jobKeyRange . start _ token ! = null ) <nl> - throw new IllegalArgumentException ( " only start _ key supported " ) ; <nl> - if ( jobKeyRange . end _ token ! = null ) <nl> - throw new IllegalArgumentException ( " only start _ key supported " ) ; <nl> - jobRange = new Range < > ( partitioner . getToken ( jobKeyRange . start _ key ) , <nl> - partitioner . getToken ( jobKeyRange . end _ key ) ) ; <nl> - } <nl> - else if ( jobKeyRange . start _ token ! = null ) <nl> - { <nl> - jobRange = new Range < > ( partitioner . getTokenFactory ( ) . fromString ( jobKeyRange . start _ token ) , <nl> - partitioner . getTokenFactory ( ) . fromString ( jobKeyRange . end _ token ) ) ; <nl> - } <nl> - else <nl> - { <nl> - logger . warn ( " ignoring jobKeyRange specified without start _ key or start _ token " ) ; <nl> - } <nl> + if ( ! partitioner . preservesOrder ( ) ) <nl> + throw new UnsupportedOperationException ( " KeyRange based on keys can only be used with a order preserving partitioner " ) ; <nl> + if ( jobKeyRange . start _ token ! = null ) <nl> + throw new IllegalArgumentException ( " only start _ key supported " ) ; <nl> + if ( jobKeyRange . end _ token ! = null ) <nl> + throw new IllegalArgumentException ( " only start _ key supported " ) ; <nl> + jobRange = new Range < > ( partitioner . getToken ( jobKeyRange . start _ key ) , <nl> + partitioner . getToken ( jobKeyRange . end _ key ) ) ; <nl> } <nl> + else if ( jobKeyRange . start _ token ! = null ) <nl> + { <nl> + jobRange = new Range < > ( partitioner . getTokenFactory ( ) . fromString ( jobKeyRange . start _ token ) , <nl> + partitioner . getTokenFactory ( ) . fromString ( jobKeyRange . end _ token ) ) ; <nl> + } <nl> + else <nl> + { <nl> + logger . warn ( " ignoring jobKeyRange specified without start _ key or start _ token " ) ; <nl> + } <nl> + } <nl> <nl> - session = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) . connect ( ) ; <nl> + try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) ) <nl> + { <nl> + Session session = cluster . connect ( ) ; <nl> Metadata metadata = session . getCluster ( ) . getMetadata ( ) ; <nl> <nl> for ( TokenRange range : masterRangeNodes . keySet ( ) ) <nl> @ @ - 127 , 7 + 127 , 7 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < <nl> if ( jobRange = = null ) <nl> { <nl> / / for each tokenRange , pick a live owner and ask it to compute bite - sized splits <nl> - splitfutures . add ( executor . submit ( new SplitCallable ( range , masterRangeNodes . get ( range ) , conf ) ) ) ; <nl> + splitfutures . add ( executor . submit ( new SplitCallable ( range , masterRangeNodes . get ( range ) , conf , session ) ) ) ; <nl> } <nl> else <nl> { <nl> @ @ - 137 , 7 + 137 , 7 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < <nl> for ( TokenRange intersection : range . intersectWith ( jobTokenRange ) ) <nl> { <nl> / / for each tokenRange , pick a live owner and ask it to compute bite - sized splits <nl> - splitfutures . add ( executor . submit ( new SplitCallable ( intersection , masterRangeNodes . get ( range ) , conf ) ) ) ; <nl> + splitfutures . add ( executor . submit ( new SplitCallable ( intersection , masterRangeNodes . get ( range ) , conf , session ) ) ) ; <nl> } <nl> } <nl> } <nl> @ @ - 182 , 19 + 182 , 21 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < <nl> private final TokenRange tokenRange ; <nl> private final Set < Host > hosts ; <nl> private final Configuration conf ; <nl> + private final Session session ; <nl> <nl> - public SplitCallable ( TokenRange tr , Set < Host > hosts , Configuration conf ) <nl> + public SplitCallable ( TokenRange tr , Set < Host > hosts , Configuration conf , Session session ) <nl> { <nl> this . tokenRange = tr ; <nl> this . hosts = hosts ; <nl> this . conf = conf ; <nl> + this . session = session ; <nl> } <nl> <nl> public List < InputSplit > call ( ) throws Exception <nl> { <nl> ArrayList < InputSplit > splits = new ArrayList < > ( ) ; <nl> Map < TokenRange , Long > subSplits ; <nl> - subSplits = getSubSplits ( keyspace , cfName , tokenRange , conf ) ; <nl> + subSplits = getSubSplits ( keyspace , cfName , tokenRange , conf , session ) ; <nl> / / turn the sub - ranges into InputSplits <nl> String [ ] endpoints = new String [ hosts . size ( ) ] ; <nl> <nl> @ @ - 225 , 12 + 227 , 12 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < <nl> } <nl> } <nl> <nl> - private Map < TokenRange , Long > getSubSplits ( String keyspace , String cfName , TokenRange range , Configuration conf ) throws IOException <nl> + private Map < TokenRange , Long > getSubSplits ( String keyspace , String cfName , TokenRange range , Configuration conf , Session session ) throws IOException <nl> { <nl> int splitSize = ConfigHelper . getInputSplitSize ( conf ) ; <nl> try <nl> { <nl> - return describeSplits ( keyspace , cfName , range , splitSize ) ; <nl> + return describeSplits ( keyspace , cfName , range , splitSize , session ) ; <nl> } <nl> catch ( Exception e ) <nl> { <nl> @ @ - 240 , 17 + 242 , 17 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < <nl> <nl> private Map < TokenRange , Set < Host > > getRangeMap ( Configuration conf , String keyspace ) <nl> { <nl> - try ( Session session = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) . connect ( ) ) <nl> + try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) ) <nl> { <nl> Map < TokenRange , Set < Host > > map = new HashMap < > ( ) ; <nl> - Metadata metadata = session . getCluster ( ) . getMetadata ( ) ; <nl> + Metadata metadata = cluster . connect ( ) . getCluster ( ) . getMetadata ( ) ; <nl> for ( TokenRange tokenRange : metadata . getTokenRanges ( ) ) <nl> map . put ( tokenRange , metadata . getReplicas ( ' " ' + keyspace + ' " ' , tokenRange ) ) ; <nl> return map ; <nl> } <nl> } <nl> <nl> - private Map < TokenRange , Long > describeSplits ( String keyspace , String table , TokenRange tokenRange , int splitSize ) <nl> + private Map < TokenRange , Long > describeSplits ( String keyspace , String table , TokenRange tokenRange , int splitSize , Session session ) <nl> { <nl> String query = String . format ( " SELECT mean _ partition _ size , partitions _ count " + <nl> " FROM % s . % s " + <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> index 6e8ffd9 . . 14e24fb 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> @ @ - 113 , 27 + 113 , 25 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> batchThreshold = conf . getLong ( ColumnFamilyOutputFormat . BATCH _ THRESHOLD , 32 ) ; <nl> this . clients = new HashMap < > ( ) ; <nl> <nl> - try <nl> + try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ) <nl> { <nl> String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; <nl> - try ( Session client = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) . connect ( keyspace ) ) <nl> + Session client = cluster . connect ( keyspace ) ; <nl> + ringCache = new NativeRingCache ( conf ) ; <nl> + if ( client ! = null ) <nl> { <nl> - ringCache = new NativeRingCache ( conf ) ; <nl> - if ( client ! = null ) <nl> - { <nl> - TableMetadata tableMetadata = client . getCluster ( ) . getMetadata ( ) . getKeyspace ( client . getLoggedKeyspace ( ) ) . getTable ( ConfigHelper . getOutputColumnFamily ( conf ) ) ; <nl> - clusterColumns = tableMetadata . getClusteringColumns ( ) ; <nl> - partitionKeyColumns = tableMetadata . getPartitionKey ( ) ; <nl> - <nl> - String cqlQuery = CqlConfigHelper . getOutputCql ( conf ) . trim ( ) ; <nl> - if ( cqlQuery . toLowerCase ( ) . startsWith ( " insert " ) ) <nl> - throw new UnsupportedOperationException ( " INSERT with CqlRecordWriter is not supported , please use UPDATE / DELETE statement " ) ; <nl> - cql = appendKeyWhereClauses ( cqlQuery ) ; <nl> - } <nl> - else <nl> - { <nl> - throw new IllegalArgumentException ( " Invalid configuration specified " + conf ) ; <nl> - } <nl> + TableMetadata tableMetadata = client . getCluster ( ) . getMetadata ( ) . getKeyspace ( client . getLoggedKeyspace ( ) ) . getTable ( ConfigHelper . getOutputColumnFamily ( conf ) ) ; <nl> + clusterColumns = tableMetadata . getClusteringColumns ( ) ; <nl> + partitionKeyColumns = tableMetadata . getPartitionKey ( ) ; <nl> + <nl> + String cqlQuery = CqlConfigHelper . getOutputCql ( conf ) . trim ( ) ; <nl> + if ( cqlQuery . toLowerCase ( ) . startsWith ( " insert " ) ) <nl> + throw new UnsupportedOperationException ( " INSERT with CqlRecordWriter is not supported , please use UPDATE / DELETE statement " ) ; <nl> + cql = appendKeyWhereClauses ( cqlQuery ) ; <nl> + } <nl> + else <nl> + { <nl> + throw new IllegalArgumentException ( " Invalid configuration specified " + conf ) ; <nl> } <nl> } <nl> catch ( Exception e ) <nl> @ @ - 235 , 7 + 233 , 7 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> { <nl> / / The list of endpoints for this range <nl> protected final List < InetAddress > endpoints ; <nl> - protected Session client ; <nl> + protected Cluster cluster = null ; <nl> / / A bounded queue of incoming mutations for this range <nl> protected final BlockingQueue < List < ByteBuffer > > queue = new ArrayBlockingQueue < List < ByteBuffer > > ( queueSize ) ; <nl> <nl> @ @ - 281 , 6 + 279 , 7 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> * / <nl> public void run ( ) <nl> { <nl> + Session session = null ; <nl> outer : <nl> while ( run | | ! queue . isEmpty ( ) ) <nl> { <nl> @ @ - 299 , 34 + 298 , 37 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> while ( true ) <nl> { <nl> / / send the mutation to the last - used endpoint . first time through , this will NPE harmlessly . <nl> - try <nl> + if ( session ! = null ) <nl> { <nl> - int i = 0 ; <nl> - PreparedStatement statement = preparedStatement ( client ) ; <nl> - while ( bindVariables ! = null ) <nl> + try <nl> { <nl> - BoundStatement boundStatement = new BoundStatement ( statement ) ; <nl> - for ( int columnPosition = 0 ; columnPosition < bindVariables . size ( ) ; columnPosition + + ) <nl> + int i = 0 ; <nl> + PreparedStatement statement = preparedStatement ( session ) ; <nl> + while ( bindVariables ! = null ) <nl> { <nl> - boundStatement . setBytesUnsafe ( columnPosition , bindVariables . get ( columnPosition ) ) ; <nl> + BoundStatement boundStatement = new BoundStatement ( statement ) ; <nl> + for ( int columnPosition = 0 ; columnPosition < bindVariables . size ( ) ; columnPosition + + ) <nl> + { <nl> + boundStatement . setBytesUnsafe ( columnPosition , bindVariables . get ( columnPosition ) ) ; <nl> + } <nl> + session . execute ( boundStatement ) ; <nl> + i + + ; <nl> + <nl> + if ( i > = batchThreshold ) <nl> + break ; <nl> + bindVariables = queue . poll ( ) ; <nl> } <nl> - client . execute ( boundStatement ) ; <nl> - i + + ; <nl> - <nl> - if ( i > = batchThreshold ) <nl> - break ; <nl> - bindVariables = queue . poll ( ) ; <nl> + break ; <nl> } <nl> - break ; <nl> - } <nl> - catch ( Exception e ) <nl> - { <nl> - closeInternal ( ) ; <nl> - if ( ! iter . hasNext ( ) ) <nl> + catch ( Exception e ) <nl> { <nl> - lastException = new IOException ( e ) ; <nl> - break outer ; <nl> - } <nl> + closeInternal ( ) ; <nl> + if ( ! iter . hasNext ( ) ) <nl> + { <nl> + lastException = new IOException ( e ) ; <nl> + break outer ; <nl> + } <nl> + } <nl> } <nl> <nl> / / attempt to connect to a different endpoint <nl> @ @ - 334 , 7 + 336 , 8 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> { <nl> InetAddress address = iter . next ( ) ; <nl> String host = address . getHostName ( ) ; <nl> - client = CqlConfigHelper . getOutputCluster ( host , conf ) . connect ( ) ; <nl> + cluster = CqlConfigHelper . getOutputCluster ( host , conf ) ; <nl> + session = cluster . connect ( ) ; <nl> } <nl> catch ( Exception e ) <nl> { <nl> @ @ - 404 , 9 + 407 , 9 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> <nl> protected void closeInternal ( ) <nl> { <nl> - if ( client ! = null ) <nl> + if ( cluster ! = null ) <nl> { <nl> - client . close ( ) ; <nl> + cluster . close ( ) ; <nl> } <nl> } <nl> <nl> @ @ - 486 , 15 + 489 , 14 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> private void refreshEndpointMap ( ) <nl> { <nl> String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; <nl> - try ( Session session = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) . connect ( keyspace ) ) <nl> + try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ) <nl> { <nl> + Session session = cluster . connect ( keyspace ) ; <nl> rangeMap = new HashMap < > ( ) ; <nl> metadata = session . getCluster ( ) . getMetadata ( ) ; <nl> Set < TokenRange > ranges = metadata . getTokenRanges ( ) ; <nl> for ( TokenRange range : ranges ) <nl> - { <nl> rangeMap . put ( range , metadata . getReplicas ( keyspace , range ) ) ; <nl> - } <nl> } <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java <nl> index ba0a37d . . 74058b1 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java <nl> @ @ - 28 , 6 + 28 , 7 @ @ import java . net . URLDecoder ; <nl> import java . nio . ByteBuffer ; <nl> import java . util . * ; <nl> <nl> + import com . datastax . driver . core . Cluster ; <nl> import com . datastax . driver . core . ColumnMetadata ; <nl> import com . datastax . driver . core . Metadata ; <nl> import com . datastax . driver . core . Row ; <nl> @ @ - 723 , 8 + 724 , 9 @ @ public class CqlNativeStorage extends LoadFunc implements StoreFuncInterface , Lo <nl> / / Only get the schema if we haven ' t already gotten it <nl> if ( ! properties . containsKey ( signature ) ) <nl> { <nl> - try ( Session client = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) , conf ) . connect ( ) ) <nl> + try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) , conf ) ) <nl> { <nl> + Session client = cluster . connect ( ) ; <nl> client . execute ( " USE " + keyspace ) ; <nl> <nl> / / compose the CfDef for the columfamily
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index cda41dc . . 76e710e 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 62 , 6 + 62 , 7 @ @ <nl> 1 . 2 . 6 <nl> * Reduce SSTableLoader memory usage ( CASSANDRA - 5555 ) <nl> * Scale hinted _ handoff _ throttle _ in _ kb to cluster size ( CASSANDRA - 5272 ) <nl> + * ( Hadoop ) Add CQL3 input / output formats ( CASSANDRA - 4421 , 5622 ) <nl> * ( Hadoop ) Fix InputKeyRange in CFIF ( CASSANDRA - 5536 ) <nl> * Fix dealing with ridiculously large max sstable sizes in LCS ( CASSANDRA - 5589 ) <nl> * Ignore pre - truncate hints ( CASSANDRA - 4655 ) <nl> diff - - git a / examples / hadoop _ cql3 _ word _ count / src / WordCount . java b / examples / hadoop _ cql3 _ word _ count / src / WordCount . java <nl> index 611f9c2 . . c92f047 100644 <nl> - - - a / examples / hadoop _ cql3 _ word _ count / src / WordCount . java <nl> + + + b / examples / hadoop _ cql3 _ word _ count / src / WordCount . java <nl> @ @ - 166 , 9 + 166 , 7 @ @ public class WordCount extends Configured implements Tool <nl> private List < ByteBuffer > getBindVariables ( Text word , int sum ) <nl> { <nl> List < ByteBuffer > variables = new ArrayList < ByteBuffer > ( ) ; <nl> - variables . add ( keys . get ( " row _ id1 " ) ) ; <nl> - variables . add ( keys . get ( " row _ id2 " ) ) ; <nl> - variables . add ( ByteBufferUtil . bytes ( word . toString ( ) ) ) ; <nl> + keys . put ( " word " , ByteBufferUtil . bytes ( word . toString ( ) ) ) ; <nl> variables . add ( ByteBufferUtil . bytes ( String . valueOf ( sum ) ) ) ; <nl> return variables ; <nl> } <nl> @ @ - 210 , 9 + 208 , 8 @ @ public class WordCount extends Configured implements Tool <nl> <nl> ConfigHelper . setOutputColumnFamily ( job . getConfiguration ( ) , KEYSPACE , OUTPUT _ COLUMN _ FAMILY ) ; <nl> job . getConfiguration ( ) . set ( PRIMARY _ KEY , " word , sum " ) ; <nl> - String query = " INSERT INTO " + KEYSPACE + " . " + OUTPUT _ COLUMN _ FAMILY + <nl> - " ( row _ id1 , row _ id2 , word , count _ num ) " + <nl> - " values ( ? , ? , ? , ? ) " ; <nl> + String query = " UPDATE " + KEYSPACE + " . " + OUTPUT _ COLUMN _ FAMILY + <nl> + " SET count _ num = ? " ; <nl> CqlConfigHelper . setOutputCql ( job . getConfiguration ( ) , query ) ; <nl> ConfigHelper . setOutputInitialAddress ( job . getConfiguration ( ) , " localhost " ) ; <nl> ConfigHelper . setOutputPartitioner ( job . getConfiguration ( ) , " Murmur3Partitioner " ) ; <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java <nl> index 6428db3 . . 456130d 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java <nl> @ @ - 109 , 7 + 109 , 7 @ @ public abstract class AbstractColumnFamilyRecordWriter < K , Y > extends RecordWrite <nl> / / The list of endpoints for this range <nl> protected final List < InetAddress > endpoints ; <nl> / / A bounded queue of incoming mutations for this range <nl> - protected final BlockingQueue < Pair < ByteBuffer , K > > queue = new ArrayBlockingQueue < Pair < ByteBuffer , K > > ( queueSize ) ; <nl> + protected final BlockingQueue < K > queue = new ArrayBlockingQueue < K > ( queueSize ) ; <nl> <nl> protected volatile boolean run = true ; <nl> / / we want the caller to know if something went wrong , so we record any unrecoverable exception while writing <nl> @ @ - 132 , 7 + 132 , 7 @ @ public abstract class AbstractColumnFamilyRecordWriter < K , Y > extends RecordWrite <nl> / * * <nl> * enqueues the given value to Cassandra <nl> * / <nl> - public void put ( Pair < ByteBuffer , K > value ) throws IOException <nl> + public void put ( K value ) throws IOException <nl> { <nl> while ( true ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java <nl> index 50ec059 . . 6823342 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java <nl> @ @ - 134 , 7 + 134 , 7 @ @ final class ColumnFamilyRecordWriter extends AbstractColumnFamilyRecordWriter < By <nl> * A client that runs in a threadpool and connects to the list of endpoints for a particular <nl> * range . Mutations for keys in that range are sent to this client via a queue . <nl> * / <nl> - public class RangeClient extends AbstractRangeClient < Mutation > <nl> + public class RangeClient extends AbstractRangeClient < Pair < ByteBuffer , Mutation > > <nl> { <nl> public final String columnFamily = ConfigHelper . getOutputColumnFamily ( conf ) ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> index dde6b1f . . 642d8c4 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> @ @ - 20 , 13 + 20 , 12 @ @ package org . apache . cassandra . hadoop . cql3 ; <nl> import java . io . IOException ; <nl> import java . net . InetAddress ; <nl> import java . nio . ByteBuffer ; <nl> - import java . util . HashMap ; <nl> - import java . util . Iterator ; <nl> - import java . util . List ; <nl> - import java . util . Map ; <nl> + import java . util . * ; <nl> import java . util . concurrent . ConcurrentHashMap ; <nl> <nl> - import org . apache . cassandra . thrift . * ; <nl> + import org . slf4j . Logger ; <nl> + import org . slf4j . LoggerFactory ; <nl> + <nl> import org . apache . cassandra . db . marshal . AbstractType ; <nl> import org . apache . cassandra . db . marshal . CompositeType ; <nl> import org . apache . cassandra . db . marshal . LongType ; <nl> @ @ - 38 , 15 + 37 , 13 @ @ import org . apache . cassandra . exceptions . SyntaxException ; <nl> import org . apache . cassandra . hadoop . AbstractColumnFamilyRecordWriter ; <nl> import org . apache . cassandra . hadoop . ConfigHelper ; <nl> import org . apache . cassandra . hadoop . Progressable ; <nl> + import org . apache . cassandra . thrift . * ; <nl> import org . apache . cassandra . utils . ByteBufferUtil ; <nl> import org . apache . cassandra . utils . FBUtilities ; <nl> - import org . apache . cassandra . utils . Pair ; <nl> import org . apache . hadoop . conf . Configuration ; <nl> import org . apache . hadoop . mapreduce . TaskAttemptContext ; <nl> import org . apache . thrift . TException ; <nl> import org . apache . thrift . transport . TTransport ; <nl> - import org . slf4j . Logger ; <nl> - import org . slf4j . LoggerFactory ; <nl> <nl> / * * <nl> * The < code > ColumnFamilyRecordWriter < / code > maps the output & lt ; key , value & gt ; <nl> @ @ - 75 , 7 + 72 , 8 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , <nl> private final String cql ; <nl> <nl> private AbstractType < ? > keyValidator ; <nl> - private String [ ] partitionkeys ; <nl> + private String [ ] partitionKeyColumns ; <nl> + private List < String > clusterColumns ; <nl> <nl> / * * <nl> * Upon construction , obtain the map that this writer will use to collect <nl> @ @ - 96 , 30 + 94 , 30 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , <nl> this . progressable = progressable ; <nl> } <nl> <nl> - CqlRecordWriter ( Configuration conf ) throws IOException <nl> + CqlRecordWriter ( Configuration conf ) <nl> { <nl> super ( conf ) ; <nl> this . clients = new HashMap < Range , RangeClient > ( ) ; <nl> - cql = CqlConfigHelper . getOutputCql ( conf ) ; <nl> <nl> try <nl> { <nl> - String host = getAnyHost ( ) ; <nl> - int port = ConfigHelper . getOutputRpcPort ( conf ) ; <nl> - Cassandra . Client client = CqlOutputFormat . createAuthenticatedClient ( host , port , conf ) ; <nl> + Cassandra . Client client = ConfigHelper . getClientFromOutputAddressList ( conf ) ; <nl> retrievePartitionKeyValidator ( client ) ; <nl> - <nl> + String cqlQuery = CqlConfigHelper . getOutputCql ( conf ) . trim ( ) ; <nl> + if ( cqlQuery . toLowerCase ( ) . startsWith ( " insert " ) ) <nl> + throw new UnsupportedOperationException ( " INSERT with CqlRecordWriter is not supported , please use UPDATE / DELETE statement " ) ; <nl> + cql = appendKeyWhereClauses ( cqlQuery ) ; <nl> + <nl> if ( client ! = null ) <nl> { <nl> TTransport transport = client . getOutputProtocol ( ) . getTransport ( ) ; <nl> if ( transport . isOpen ( ) ) <nl> transport . close ( ) ; <nl> - client = null ; <nl> } <nl> } <nl> catch ( Exception e ) <nl> { <nl> - throw new IOException ( e ) ; <nl> + throw new RuntimeException ( e ) ; <nl> } <nl> } <nl> <nl> @ @ - 161 , 8 + 159 , 7 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , <nl> @ Override <nl> public void write ( Map < String , ByteBuffer > keyColumns , List < ByteBuffer > values ) throws IOException <nl> { <nl> - ByteBuffer rowKey = getRowKey ( keyColumns ) ; <nl> - Range < Token > range = ringCache . getRange ( rowKey ) ; <nl> + Range < Token > range = ringCache . getRange ( getPartitionKey ( keyColumns ) ) ; <nl> <nl> / / get the client for the given range , or create a new one <nl> RangeClient client = clients . get ( range ) ; <nl> @ @ - 174 , 7 + 171 , 14 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , <nl> clients . put ( range , client ) ; <nl> } <nl> <nl> - client . put ( Pair . create ( rowKey , values ) ) ; <nl> + / / add primary key columns to the bind variables <nl> + List < ByteBuffer > allValues = new ArrayList < ByteBuffer > ( values ) ; <nl> + for ( String column : partitionKeyColumns ) <nl> + allValues . add ( keyColumns . get ( column ) ) ; <nl> + for ( String column : clusterColumns ) <nl> + allValues . add ( keyColumns . get ( column ) ) ; <nl> + <nl> + client . put ( allValues ) ; <nl> progressable . progress ( ) ; <nl> } <nl> <nl> @ @ - 201 , 10 + 205 , 10 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , <nl> outer : <nl> while ( run | | ! queue . isEmpty ( ) ) <nl> { <nl> - Pair < ByteBuffer , List < ByteBuffer > > item ; <nl> + List < ByteBuffer > bindVariables ; <nl> try <nl> { <nl> - item = queue . take ( ) ; <nl> + bindVariables = queue . take ( ) ; <nl> } <nl> catch ( InterruptedException e ) <nl> { <nl> @ @ - 220 , 16 + 224 , 15 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , <nl> { <nl> int i = 0 ; <nl> int itemId = preparedStatement ( client ) ; <nl> - while ( item ! = null ) <nl> + while ( bindVariables ! = null ) <nl> { <nl> - List < ByteBuffer > bindVariables = item . right ; <nl> client . execute _ prepared _ cql3 _ query ( itemId , bindVariables , ConsistencyLevel . ONE ) ; <nl> i + + ; <nl> <nl> if ( i > = batchThreshold ) <nl> break ; <nl> <nl> - item = queue . poll ( ) ; <nl> + bindVariables = queue . poll ( ) ; <nl> } <nl> <nl> break ; <nl> @ @ - 294 , 23 + 297 , 22 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , <nl> } <nl> } <nl> <nl> - private ByteBuffer getRowKey ( Map < String , ByteBuffer > keyColumns ) <nl> + private ByteBuffer getPartitionKey ( Map < String , ByteBuffer > keyColumns ) <nl> { <nl> - / / current row key <nl> - ByteBuffer rowKey ; <nl> + ByteBuffer partitionKey ; <nl> if ( keyValidator instanceof CompositeType ) <nl> { <nl> - ByteBuffer [ ] keys = new ByteBuffer [ partitionkeys . length ] ; <nl> + ByteBuffer [ ] keys = new ByteBuffer [ partitionKeyColumns . length ] ; <nl> for ( int i = 0 ; i < keys . length ; i + + ) <nl> - keys [ i ] = keyColumns . get ( partitionkeys [ i ] ) ; <nl> + keys [ i ] = keyColumns . get ( partitionKeyColumns [ i ] ) ; <nl> <nl> - rowKey = ( ( CompositeType ) keyValidator ) . build ( keys ) ; <nl> + partitionKey = ( ( CompositeType ) keyValidator ) . build ( keys ) ; <nl> } <nl> else <nl> { <nl> - rowKey = keyColumns . get ( partitionkeys [ 0 ] ) ; <nl> + partitionKey = keyColumns . get ( partitionKeyColumns [ 0 ] ) ; <nl> } <nl> - return rowKey ; <nl> + return partitionKey ; <nl> } <nl> <nl> / * * retrieve the key validator from system . schema _ columnfamilies table * / <nl> @ @ - 319 , 7 + 321 , 8 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , <nl> String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; <nl> String cfName = ConfigHelper . getOutputColumnFamily ( conf ) ; <nl> String query = " SELECT key _ validator , " + <nl> - 	 	 " key _ aliases " + <nl> + 	 	 " key _ aliases , " + <nl> + 	 	 " column _ aliases " + <nl> " FROM system . schema _ columnfamilies " + <nl> " WHERE keyspace _ name = ' % s ' and columnfamily _ name = ' % s ' " ; <nl> String formatted = String . format ( query , keyspace , cfName ) ; <nl> @ @ - 334 , 16 + 337 , 22 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , <nl> logger . debug ( " partition keys : " + keyString ) ; <nl> <nl> List < String > keys = FBUtilities . fromJsonList ( keyString ) ; <nl> - partitionkeys = new String [ keys . size ( ) ] ; <nl> + partitionKeyColumns = new String [ keys . size ( ) ] ; <nl> int i = 0 ; <nl> for ( String key : keys ) <nl> { <nl> - partitionkeys [ i ] = key ; <nl> + partitionKeyColumns [ i ] = key ; <nl> i + + ; <nl> } <nl> + <nl> + Column rawClusterColumns = result . rows . get ( 0 ) . columns . get ( 2 ) ; <nl> + String clusterColumnString = ByteBufferUtil . string ( ByteBuffer . wrap ( rawClusterColumns . getValue ( ) ) ) ; <nl> + <nl> + logger . debug ( " cluster columns : " + clusterColumnString ) ; <nl> + clusterColumns = FBUtilities . fromJsonList ( clusterColumnString ) ; <nl> } <nl> <nl> - private AbstractType < ? > parseType ( String type ) throws IOException <nl> + private AbstractType < ? > parseType ( String type ) throws ConfigurationException <nl> { <nl> try <nl> { <nl> @ @ - 352 , 32 + 361 , 24 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , <nl> return LongType . instance ; <nl> return TypeParser . parse ( type ) ; <nl> } <nl> - catch ( ConfigurationException e ) <nl> - { <nl> - throw new IOException ( e ) ; <nl> - } <nl> catch ( SyntaxException e ) <nl> { <nl> - throw new IOException ( e ) ; <nl> + throw new ConfigurationException ( e . getMessage ( ) , e ) ; <nl> } <nl> } <nl> - <nl> - private String getAnyHost ( ) throws IOException , InvalidRequestException , TException <nl> + <nl> + / * * <nl> + * add where clauses for partition keys and cluster columns <nl> + * / <nl> + private String appendKeyWhereClauses ( String cqlQuery ) <nl> { <nl> - Cassandra . Client client = ConfigHelper . getClientFromOutputAddressList ( conf ) ; <nl> - List < TokenRange > ring = client . describe _ ring ( ConfigHelper . getOutputKeyspace ( conf ) ) ; <nl> - try <nl> - { <nl> - for ( TokenRange range : ring ) <nl> - return range . endpoints . get ( 0 ) ; <nl> - } <nl> - finally <nl> - { <nl> - TTransport transport = client . getOutputProtocol ( ) . getTransport ( ) ; <nl> - if ( transport . isOpen ( ) ) <nl> - transport . close ( ) ; <nl> - } <nl> - throw new IOException ( " There are no endpoints " ) ; <nl> - } <nl> + String keyWhereClause = " " ; <nl> + <nl> + for ( String partitionKey : partitionKeyColumns ) <nl> + keyWhereClause + = String . format ( " % s = ? " , keyWhereClause . isEmpty ( ) ? partitionKey : ( " AND " + partitionKey ) ) ; <nl> + for ( String clusterColumn : clusterColumns ) <nl> + keyWhereClause + = " AND " + clusterColumn + " = ? " ; <nl> <nl> + return cqlQuery + " WHERE " + keyWhereClause ; <nl> + } <nl> }

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 5edad20 . . 81ceb25 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 2 . 2 . 4 
 + * ( Hadoop ) ensure that Cluster instances are always closed ( CASSANDRA - 10058 ) 
 * ( cqlsh ) show partial trace if incomplete after max _ trace _ wait ( CASSANDRA - 7645 ) 
 * Use most up - to - date version of schema for system tables ( CASSANDRA - 10652 ) 
 * Deprecate memory _ allocator in cassandra . yaml ( CASSANDRA - 10581 , 10628 ) 
 diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 index e531ad1 . . d687183 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 @ @ - 24 , 6 + 24 , 7 @ @ import java . util . concurrent . * ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 + import com . datastax . driver . core . Cluster ; 
 import com . datastax . driver . core . Host ; 
 import com . datastax . driver . core . Metadata ; 
 import com . datastax . driver . core . ResultSet ; 
 @ @ - 58 , 7 + 59 , 6 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < 
 private String keyspace ; 
 private String cfName ; 
 private IPartitioner partitioner ; 
 - private Session session ; 
 
 protected void validateConfiguration ( Configuration conf ) 
 { 
 @ @ - 90 , 36 + 90 , 36 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < 
 ExecutorService executor = new ThreadPoolExecutor ( 0 , 128 , 60L , TimeUnit . SECONDS , new LinkedBlockingQueue < Runnable > ( ) ) ; 
 List < InputSplit > splits = new ArrayList < > ( ) ; 
 
 - try 
 + List < Future < List < InputSplit > > > splitfutures = new ArrayList < > ( ) ; 
 + KeyRange jobKeyRange = ConfigHelper . getInputKeyRange ( conf ) ; 
 + Range < Token > jobRange = null ; 
 + if ( jobKeyRange ! = null ) 
 { 
 - List < Future < List < InputSplit > > > splitfutures = new ArrayList < > ( ) ; 
 - KeyRange jobKeyRange = ConfigHelper . getInputKeyRange ( conf ) ; 
 - Range < Token > jobRange = null ; 
 - if ( jobKeyRange ! = null ) 
 + if ( jobKeyRange . start _ key ! = null ) 
 { 
 - if ( jobKeyRange . start _ key ! = null ) 
 - { 
 - if ( ! partitioner . preservesOrder ( ) ) 
 - throw new UnsupportedOperationException ( " KeyRange based on keys can only be used with a order preserving partitioner " ) ; 
 - if ( jobKeyRange . start _ token ! = null ) 
 - throw new IllegalArgumentException ( " only start _ key supported " ) ; 
 - if ( jobKeyRange . end _ token ! = null ) 
 - throw new IllegalArgumentException ( " only start _ key supported " ) ; 
 - jobRange = new Range < > ( partitioner . getToken ( jobKeyRange . start _ key ) , 
 - partitioner . getToken ( jobKeyRange . end _ key ) ) ; 
 - } 
 - else if ( jobKeyRange . start _ token ! = null ) 
 - { 
 - jobRange = new Range < > ( partitioner . getTokenFactory ( ) . fromString ( jobKeyRange . start _ token ) , 
 - partitioner . getTokenFactory ( ) . fromString ( jobKeyRange . end _ token ) ) ; 
 - } 
 - else 
 - { 
 - logger . warn ( " ignoring jobKeyRange specified without start _ key or start _ token " ) ; 
 - } 
 + if ( ! partitioner . preservesOrder ( ) ) 
 + throw new UnsupportedOperationException ( " KeyRange based on keys can only be used with a order preserving partitioner " ) ; 
 + if ( jobKeyRange . start _ token ! = null ) 
 + throw new IllegalArgumentException ( " only start _ key supported " ) ; 
 + if ( jobKeyRange . end _ token ! = null ) 
 + throw new IllegalArgumentException ( " only start _ key supported " ) ; 
 + jobRange = new Range < > ( partitioner . getToken ( jobKeyRange . start _ key ) , 
 + partitioner . getToken ( jobKeyRange . end _ key ) ) ; 
 } 
 + else if ( jobKeyRange . start _ token ! = null ) 
 + { 
 + jobRange = new Range < > ( partitioner . getTokenFactory ( ) . fromString ( jobKeyRange . start _ token ) , 
 + partitioner . getTokenFactory ( ) . fromString ( jobKeyRange . end _ token ) ) ; 
 + } 
 + else 
 + { 
 + logger . warn ( " ignoring jobKeyRange specified without start _ key or start _ token " ) ; 
 + } 
 + } 
 
 - session = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) . connect ( ) ; 
 + try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) ) 
 + { 
 + Session session = cluster . connect ( ) ; 
 Metadata metadata = session . getCluster ( ) . getMetadata ( ) ; 
 
 for ( TokenRange range : masterRangeNodes . keySet ( ) ) 
 @ @ - 127 , 7 + 127 , 7 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < 
 if ( jobRange = = null ) 
 { 
 / / for each tokenRange , pick a live owner and ask it to compute bite - sized splits 
 - splitfutures . add ( executor . submit ( new SplitCallable ( range , masterRangeNodes . get ( range ) , conf ) ) ) ; 
 + splitfutures . add ( executor . submit ( new SplitCallable ( range , masterRangeNodes . get ( range ) , conf , session ) ) ) ; 
 } 
 else 
 { 
 @ @ - 137 , 7 + 137 , 7 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < 
 for ( TokenRange intersection : range . intersectWith ( jobTokenRange ) ) 
 { 
 / / for each tokenRange , pick a live owner and ask it to compute bite - sized splits 
 - splitfutures . add ( executor . submit ( new SplitCallable ( intersection , masterRangeNodes . get ( range ) , conf ) ) ) ; 
 + splitfutures . add ( executor . submit ( new SplitCallable ( intersection , masterRangeNodes . get ( range ) , conf , session ) ) ) ; 
 } 
 } 
 } 
 @ @ - 182 , 19 + 182 , 21 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < 
 private final TokenRange tokenRange ; 
 private final Set < Host > hosts ; 
 private final Configuration conf ; 
 + private final Session session ; 
 
 - public SplitCallable ( TokenRange tr , Set < Host > hosts , Configuration conf ) 
 + public SplitCallable ( TokenRange tr , Set < Host > hosts , Configuration conf , Session session ) 
 { 
 this . tokenRange = tr ; 
 this . hosts = hosts ; 
 this . conf = conf ; 
 + this . session = session ; 
 } 
 
 public List < InputSplit > call ( ) throws Exception 
 { 
 ArrayList < InputSplit > splits = new ArrayList < > ( ) ; 
 Map < TokenRange , Long > subSplits ; 
 - subSplits = getSubSplits ( keyspace , cfName , tokenRange , conf ) ; 
 + subSplits = getSubSplits ( keyspace , cfName , tokenRange , conf , session ) ; 
 / / turn the sub - ranges into InputSplits 
 String [ ] endpoints = new String [ hosts . size ( ) ] ; 
 
 @ @ - 225 , 12 + 227 , 12 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < 
 } 
 } 
 
 - private Map < TokenRange , Long > getSubSplits ( String keyspace , String cfName , TokenRange range , Configuration conf ) throws IOException 
 + private Map < TokenRange , Long > getSubSplits ( String keyspace , String cfName , TokenRange range , Configuration conf , Session session ) throws IOException 
 { 
 int splitSize = ConfigHelper . getInputSplitSize ( conf ) ; 
 try 
 { 
 - return describeSplits ( keyspace , cfName , range , splitSize ) ; 
 + return describeSplits ( keyspace , cfName , range , splitSize , session ) ; 
 } 
 catch ( Exception e ) 
 { 
 @ @ - 240 , 17 + 242 , 17 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < 
 
 private Map < TokenRange , Set < Host > > getRangeMap ( Configuration conf , String keyspace ) 
 { 
 - try ( Session session = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) . connect ( ) ) 
 + try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) ) 
 { 
 Map < TokenRange , Set < Host > > map = new HashMap < > ( ) ; 
 - Metadata metadata = session . getCluster ( ) . getMetadata ( ) ; 
 + Metadata metadata = cluster . connect ( ) . getCluster ( ) . getMetadata ( ) ; 
 for ( TokenRange tokenRange : metadata . getTokenRanges ( ) ) 
 map . put ( tokenRange , metadata . getReplicas ( ' " ' + keyspace + ' " ' , tokenRange ) ) ; 
 return map ; 
 } 
 } 
 
 - private Map < TokenRange , Long > describeSplits ( String keyspace , String table , TokenRange tokenRange , int splitSize ) 
 + private Map < TokenRange , Long > describeSplits ( String keyspace , String table , TokenRange tokenRange , int splitSize , Session session ) 
 { 
 String query = String . format ( " SELECT mean _ partition _ size , partitions _ count " + 
 " FROM % s . % s " + 
 diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 index 6e8ffd9 . . 14e24fb 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 @ @ - 113 , 27 + 113 , 25 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 batchThreshold = conf . getLong ( ColumnFamilyOutputFormat . BATCH _ THRESHOLD , 32 ) ; 
 this . clients = new HashMap < > ( ) ; 
 
 - try 
 + try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ) 
 { 
 String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; 
 - try ( Session client = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) . connect ( keyspace ) ) 
 + Session client = cluster . connect ( keyspace ) ; 
 + ringCache = new NativeRingCache ( conf ) ; 
 + if ( client ! = null ) 
 { 
 - ringCache = new NativeRingCache ( conf ) ; 
 - if ( client ! = null ) 
 - { 
 - TableMetadata tableMetadata = client . getCluster ( ) . getMetadata ( ) . getKeyspace ( client . getLoggedKeyspace ( ) ) . getTable ( ConfigHelper . getOutputColumnFamily ( conf ) ) ; 
 - clusterColumns = tableMetadata . getClusteringColumns ( ) ; 
 - partitionKeyColumns = tableMetadata . getPartitionKey ( ) ; 
 - 
 - String cqlQuery = CqlConfigHelper . getOutputCql ( conf ) . trim ( ) ; 
 - if ( cqlQuery . toLowerCase ( ) . startsWith ( " insert " ) ) 
 - throw new UnsupportedOperationException ( " INSERT with CqlRecordWriter is not supported , please use UPDATE / DELETE statement " ) ; 
 - cql = appendKeyWhereClauses ( cqlQuery ) ; 
 - } 
 - else 
 - { 
 - throw new IllegalArgumentException ( " Invalid configuration specified " + conf ) ; 
 - } 
 + TableMetadata tableMetadata = client . getCluster ( ) . getMetadata ( ) . getKeyspace ( client . getLoggedKeyspace ( ) ) . getTable ( ConfigHelper . getOutputColumnFamily ( conf ) ) ; 
 + clusterColumns = tableMetadata . getClusteringColumns ( ) ; 
 + partitionKeyColumns = tableMetadata . getPartitionKey ( ) ; 
 + 
 + String cqlQuery = CqlConfigHelper . getOutputCql ( conf ) . trim ( ) ; 
 + if ( cqlQuery . toLowerCase ( ) . startsWith ( " insert " ) ) 
 + throw new UnsupportedOperationException ( " INSERT with CqlRecordWriter is not supported , please use UPDATE / DELETE statement " ) ; 
 + cql = appendKeyWhereClauses ( cqlQuery ) ; 
 + } 
 + else 
 + { 
 + throw new IllegalArgumentException ( " Invalid configuration specified " + conf ) ; 
 } 
 } 
 catch ( Exception e ) 
 @ @ - 235 , 7 + 233 , 7 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 { 
 / / The list of endpoints for this range 
 protected final List < InetAddress > endpoints ; 
 - protected Session client ; 
 + protected Cluster cluster = null ; 
 / / A bounded queue of incoming mutations for this range 
 protected final BlockingQueue < List < ByteBuffer > > queue = new ArrayBlockingQueue < List < ByteBuffer > > ( queueSize ) ; 
 
 @ @ - 281 , 6 + 279 , 7 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 * / 
 public void run ( ) 
 { 
 + Session session = null ; 
 outer : 
 while ( run | | ! queue . isEmpty ( ) ) 
 { 
 @ @ - 299 , 34 + 298 , 37 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 while ( true ) 
 { 
 / / send the mutation to the last - used endpoint . first time through , this will NPE harmlessly . 
 - try 
 + if ( session ! = null ) 
 { 
 - int i = 0 ; 
 - PreparedStatement statement = preparedStatement ( client ) ; 
 - while ( bindVariables ! = null ) 
 + try 
 { 
 - BoundStatement boundStatement = new BoundStatement ( statement ) ; 
 - for ( int columnPosition = 0 ; columnPosition < bindVariables . size ( ) ; columnPosition + + ) 
 + int i = 0 ; 
 + PreparedStatement statement = preparedStatement ( session ) ; 
 + while ( bindVariables ! = null ) 
 { 
 - boundStatement . setBytesUnsafe ( columnPosition , bindVariables . get ( columnPosition ) ) ; 
 + BoundStatement boundStatement = new BoundStatement ( statement ) ; 
 + for ( int columnPosition = 0 ; columnPosition < bindVariables . size ( ) ; columnPosition + + ) 
 + { 
 + boundStatement . setBytesUnsafe ( columnPosition , bindVariables . get ( columnPosition ) ) ; 
 + } 
 + session . execute ( boundStatement ) ; 
 + i + + ; 
 + 
 + if ( i > = batchThreshold ) 
 + break ; 
 + bindVariables = queue . poll ( ) ; 
 } 
 - client . execute ( boundStatement ) ; 
 - i + + ; 
 - 
 - if ( i > = batchThreshold ) 
 - break ; 
 - bindVariables = queue . poll ( ) ; 
 + break ; 
 } 
 - break ; 
 - } 
 - catch ( Exception e ) 
 - { 
 - closeInternal ( ) ; 
 - if ( ! iter . hasNext ( ) ) 
 + catch ( Exception e ) 
 { 
 - lastException = new IOException ( e ) ; 
 - break outer ; 
 - } 
 + closeInternal ( ) ; 
 + if ( ! iter . hasNext ( ) ) 
 + { 
 + lastException = new IOException ( e ) ; 
 + break outer ; 
 + } 
 + } 
 } 
 
 / / attempt to connect to a different endpoint 
 @ @ - 334 , 7 + 336 , 8 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 { 
 InetAddress address = iter . next ( ) ; 
 String host = address . getHostName ( ) ; 
 - client = CqlConfigHelper . getOutputCluster ( host , conf ) . connect ( ) ; 
 + cluster = CqlConfigHelper . getOutputCluster ( host , conf ) ; 
 + session = cluster . connect ( ) ; 
 } 
 catch ( Exception e ) 
 { 
 @ @ - 404 , 9 + 407 , 9 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 
 protected void closeInternal ( ) 
 { 
 - if ( client ! = null ) 
 + if ( cluster ! = null ) 
 { 
 - client . close ( ) ; 
 + cluster . close ( ) ; 
 } 
 } 
 
 @ @ - 486 , 15 + 489 , 14 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 private void refreshEndpointMap ( ) 
 { 
 String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; 
 - try ( Session session = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) . connect ( keyspace ) ) 
 + try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ) 
 { 
 + Session session = cluster . connect ( keyspace ) ; 
 rangeMap = new HashMap < > ( ) ; 
 metadata = session . getCluster ( ) . getMetadata ( ) ; 
 Set < TokenRange > ranges = metadata . getTokenRanges ( ) ; 
 for ( TokenRange range : ranges ) 
 - { 
 rangeMap . put ( range , metadata . getReplicas ( keyspace , range ) ) ; 
 - } 
 } 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java 
 index ba0a37d . . 74058b1 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java 
 + + + b / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java 
 @ @ - 28 , 6 + 28 , 7 @ @ import java . net . URLDecoder ; 
 import java . nio . ByteBuffer ; 
 import java . util . * ; 
 
 + import com . datastax . driver . core . Cluster ; 
 import com . datastax . driver . core . ColumnMetadata ; 
 import com . datastax . driver . core . Metadata ; 
 import com . datastax . driver . core . Row ; 
 @ @ - 723 , 8 + 724 , 9 @ @ public class CqlNativeStorage extends LoadFunc implements StoreFuncInterface , Lo 
 / / Only get the schema if we haven ' t already gotten it 
 if ( ! properties . containsKey ( signature ) ) 
 { 
 - try ( Session client = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) , conf ) . connect ( ) ) 
 + try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) , conf ) ) 
 { 
 + Session client = cluster . connect ( ) ; 
 client . execute ( " USE " + keyspace ) ; 
 
 / / compose the CfDef for the columfamily

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index cda41dc . . 76e710e 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 62 , 6 + 62 , 7 @ @ 
 1 . 2 . 6 
 * Reduce SSTableLoader memory usage ( CASSANDRA - 5555 ) 
 * Scale hinted _ handoff _ throttle _ in _ kb to cluster size ( CASSANDRA - 5272 ) 
 + * ( Hadoop ) Add CQL3 input / output formats ( CASSANDRA - 4421 , 5622 ) 
 * ( Hadoop ) Fix InputKeyRange in CFIF ( CASSANDRA - 5536 ) 
 * Fix dealing with ridiculously large max sstable sizes in LCS ( CASSANDRA - 5589 ) 
 * Ignore pre - truncate hints ( CASSANDRA - 4655 ) 
 diff - - git a / examples / hadoop _ cql3 _ word _ count / src / WordCount . java b / examples / hadoop _ cql3 _ word _ count / src / WordCount . java 
 index 611f9c2 . . c92f047 100644 
 - - - a / examples / hadoop _ cql3 _ word _ count / src / WordCount . java 
 + + + b / examples / hadoop _ cql3 _ word _ count / src / WordCount . java 
 @ @ - 166 , 9 + 166 , 7 @ @ public class WordCount extends Configured implements Tool 
 private List < ByteBuffer > getBindVariables ( Text word , int sum ) 
 { 
 List < ByteBuffer > variables = new ArrayList < ByteBuffer > ( ) ; 
 - variables . add ( keys . get ( " row _ id1 " ) ) ; 
 - variables . add ( keys . get ( " row _ id2 " ) ) ; 
 - variables . add ( ByteBufferUtil . bytes ( word . toString ( ) ) ) ; 
 + keys . put ( " word " , ByteBufferUtil . bytes ( word . toString ( ) ) ) ; 
 variables . add ( ByteBufferUtil . bytes ( String . valueOf ( sum ) ) ) ; 
 return variables ; 
 } 
 @ @ - 210 , 9 + 208 , 8 @ @ public class WordCount extends Configured implements Tool 
 
 ConfigHelper . setOutputColumnFamily ( job . getConfiguration ( ) , KEYSPACE , OUTPUT _ COLUMN _ FAMILY ) ; 
 job . getConfiguration ( ) . set ( PRIMARY _ KEY , " word , sum " ) ; 
 - String query = " INSERT INTO " + KEYSPACE + " . " + OUTPUT _ COLUMN _ FAMILY + 
 - " ( row _ id1 , row _ id2 , word , count _ num ) " + 
 - " values ( ? , ? , ? , ? ) " ; 
 + String query = " UPDATE " + KEYSPACE + " . " + OUTPUT _ COLUMN _ FAMILY + 
 + " SET count _ num = ? " ; 
 CqlConfigHelper . setOutputCql ( job . getConfiguration ( ) , query ) ; 
 ConfigHelper . setOutputInitialAddress ( job . getConfiguration ( ) , " localhost " ) ; 
 ConfigHelper . setOutputPartitioner ( job . getConfiguration ( ) , " Murmur3Partitioner " ) ; 
 diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java 
 index 6428db3 . . 456130d 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java 
 + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java 
 @ @ - 109 , 7 + 109 , 7 @ @ public abstract class AbstractColumnFamilyRecordWriter < K , Y > extends RecordWrite 
 / / The list of endpoints for this range 
 protected final List < InetAddress > endpoints ; 
 / / A bounded queue of incoming mutations for this range 
 - protected final BlockingQueue < Pair < ByteBuffer , K > > queue = new ArrayBlockingQueue < Pair < ByteBuffer , K > > ( queueSize ) ; 
 + protected final BlockingQueue < K > queue = new ArrayBlockingQueue < K > ( queueSize ) ; 
 
 protected volatile boolean run = true ; 
 / / we want the caller to know if something went wrong , so we record any unrecoverable exception while writing 
 @ @ - 132 , 7 + 132 , 7 @ @ public abstract class AbstractColumnFamilyRecordWriter < K , Y > extends RecordWrite 
 / * * 
 * enqueues the given value to Cassandra 
 * / 
 - public void put ( Pair < ByteBuffer , K > value ) throws IOException 
 + public void put ( K value ) throws IOException 
 { 
 while ( true ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java 
 index 50ec059 . . 6823342 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java 
 @ @ - 134 , 7 + 134 , 7 @ @ final class ColumnFamilyRecordWriter extends AbstractColumnFamilyRecordWriter < By 
 * A client that runs in a threadpool and connects to the list of endpoints for a particular 
 * range . Mutations for keys in that range are sent to this client via a queue . 
 * / 
 - public class RangeClient extends AbstractRangeClient < Mutation > 
 + public class RangeClient extends AbstractRangeClient < Pair < ByteBuffer , Mutation > > 
 { 
 public final String columnFamily = ConfigHelper . getOutputColumnFamily ( conf ) ; 
 
 diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 index dde6b1f . . 642d8c4 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 @ @ - 20 , 13 + 20 , 12 @ @ package org . apache . cassandra . hadoop . cql3 ; 
 import java . io . IOException ; 
 import java . net . InetAddress ; 
 import java . nio . ByteBuffer ; 
 - import java . util . HashMap ; 
 - import java . util . Iterator ; 
 - import java . util . List ; 
 - import java . util . Map ; 
 + import java . util . * ; 
 import java . util . concurrent . ConcurrentHashMap ; 
 
 - import org . apache . cassandra . thrift . * ; 
 + import org . slf4j . Logger ; 
 + import org . slf4j . LoggerFactory ; 
 + 
 import org . apache . cassandra . db . marshal . AbstractType ; 
 import org . apache . cassandra . db . marshal . CompositeType ; 
 import org . apache . cassandra . db . marshal . LongType ; 
 @ @ - 38 , 15 + 37 , 13 @ @ import org . apache . cassandra . exceptions . SyntaxException ; 
 import org . apache . cassandra . hadoop . AbstractColumnFamilyRecordWriter ; 
 import org . apache . cassandra . hadoop . ConfigHelper ; 
 import org . apache . cassandra . hadoop . Progressable ; 
 + import org . apache . cassandra . thrift . * ; 
 import org . apache . cassandra . utils . ByteBufferUtil ; 
 import org . apache . cassandra . utils . FBUtilities ; 
 - import org . apache . cassandra . utils . Pair ; 
 import org . apache . hadoop . conf . Configuration ; 
 import org . apache . hadoop . mapreduce . TaskAttemptContext ; 
 import org . apache . thrift . TException ; 
 import org . apache . thrift . transport . TTransport ; 
 - import org . slf4j . Logger ; 
 - import org . slf4j . LoggerFactory ; 
 
 / * * 
 * The < code > ColumnFamilyRecordWriter < / code > maps the output & lt ; key , value & gt ; 
 @ @ - 75 , 7 + 72 , 8 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , 
 private final String cql ; 
 
 private AbstractType < ? > keyValidator ; 
 - private String [ ] partitionkeys ; 
 + private String [ ] partitionKeyColumns ; 
 + private List < String > clusterColumns ; 
 
 / * * 
 * Upon construction , obtain the map that this writer will use to collect 
 @ @ - 96 , 30 + 94 , 30 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , 
 this . progressable = progressable ; 
 } 
 
 - CqlRecordWriter ( Configuration conf ) throws IOException 
 + CqlRecordWriter ( Configuration conf ) 
 { 
 super ( conf ) ; 
 this . clients = new HashMap < Range , RangeClient > ( ) ; 
 - cql = CqlConfigHelper . getOutputCql ( conf ) ; 
 
 try 
 { 
 - String host = getAnyHost ( ) ; 
 - int port = ConfigHelper . getOutputRpcPort ( conf ) ; 
 - Cassandra . Client client = CqlOutputFormat . createAuthenticatedClient ( host , port , conf ) ; 
 + Cassandra . Client client = ConfigHelper . getClientFromOutputAddressList ( conf ) ; 
 retrievePartitionKeyValidator ( client ) ; 
 - 
 + String cqlQuery = CqlConfigHelper . getOutputCql ( conf ) . trim ( ) ; 
 + if ( cqlQuery . toLowerCase ( ) . startsWith ( " insert " ) ) 
 + throw new UnsupportedOperationException ( " INSERT with CqlRecordWriter is not supported , please use UPDATE / DELETE statement " ) ; 
 + cql = appendKeyWhereClauses ( cqlQuery ) ; 
 + 
 if ( client ! = null ) 
 { 
 TTransport transport = client . getOutputProtocol ( ) . getTransport ( ) ; 
 if ( transport . isOpen ( ) ) 
 transport . close ( ) ; 
 - client = null ; 
 } 
 } 
 catch ( Exception e ) 
 { 
 - throw new IOException ( e ) ; 
 + throw new RuntimeException ( e ) ; 
 } 
 } 
 
 @ @ - 161 , 8 + 159 , 7 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , 
 @ Override 
 public void write ( Map < String , ByteBuffer > keyColumns , List < ByteBuffer > values ) throws IOException 
 { 
 - ByteBuffer rowKey = getRowKey ( keyColumns ) ; 
 - Range < Token > range = ringCache . getRange ( rowKey ) ; 
 + Range < Token > range = ringCache . getRange ( getPartitionKey ( keyColumns ) ) ; 
 
 / / get the client for the given range , or create a new one 
 RangeClient client = clients . get ( range ) ; 
 @ @ - 174 , 7 + 171 , 14 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , 
 clients . put ( range , client ) ; 
 } 
 
 - client . put ( Pair . create ( rowKey , values ) ) ; 
 + / / add primary key columns to the bind variables 
 + List < ByteBuffer > allValues = new ArrayList < ByteBuffer > ( values ) ; 
 + for ( String column : partitionKeyColumns ) 
 + allValues . add ( keyColumns . get ( column ) ) ; 
 + for ( String column : clusterColumns ) 
 + allValues . add ( keyColumns . get ( column ) ) ; 
 + 
 + client . put ( allValues ) ; 
 progressable . progress ( ) ; 
 } 
 
 @ @ - 201 , 10 + 205 , 10 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , 
 outer : 
 while ( run | | ! queue . isEmpty ( ) ) 
 { 
 - Pair < ByteBuffer , List < ByteBuffer > > item ; 
 + List < ByteBuffer > bindVariables ; 
 try 
 { 
 - item = queue . take ( ) ; 
 + bindVariables = queue . take ( ) ; 
 } 
 catch ( InterruptedException e ) 
 { 
 @ @ - 220 , 16 + 224 , 15 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , 
 { 
 int i = 0 ; 
 int itemId = preparedStatement ( client ) ; 
 - while ( item ! = null ) 
 + while ( bindVariables ! = null ) 
 { 
 - List < ByteBuffer > bindVariables = item . right ; 
 client . execute _ prepared _ cql3 _ query ( itemId , bindVariables , ConsistencyLevel . ONE ) ; 
 i + + ; 
 
 if ( i > = batchThreshold ) 
 break ; 
 
 - item = queue . poll ( ) ; 
 + bindVariables = queue . poll ( ) ; 
 } 
 
 break ; 
 @ @ - 294 , 23 + 297 , 22 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , 
 } 
 } 
 
 - private ByteBuffer getRowKey ( Map < String , ByteBuffer > keyColumns ) 
 + private ByteBuffer getPartitionKey ( Map < String , ByteBuffer > keyColumns ) 
 { 
 - / / current row key 
 - ByteBuffer rowKey ; 
 + ByteBuffer partitionKey ; 
 if ( keyValidator instanceof CompositeType ) 
 { 
 - ByteBuffer [ ] keys = new ByteBuffer [ partitionkeys . length ] ; 
 + ByteBuffer [ ] keys = new ByteBuffer [ partitionKeyColumns . length ] ; 
 for ( int i = 0 ; i < keys . length ; i + + ) 
 - keys [ i ] = keyColumns . get ( partitionkeys [ i ] ) ; 
 + keys [ i ] = keyColumns . get ( partitionKeyColumns [ i ] ) ; 
 
 - rowKey = ( ( CompositeType ) keyValidator ) . build ( keys ) ; 
 + partitionKey = ( ( CompositeType ) keyValidator ) . build ( keys ) ; 
 } 
 else 
 { 
 - rowKey = keyColumns . get ( partitionkeys [ 0 ] ) ; 
 + partitionKey = keyColumns . get ( partitionKeyColumns [ 0 ] ) ; 
 } 
 - return rowKey ; 
 + return partitionKey ; 
 } 
 
 / * * retrieve the key validator from system . schema _ columnfamilies table * / 
 @ @ - 319 , 7 + 321 , 8 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , 
 String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; 
 String cfName = ConfigHelper . getOutputColumnFamily ( conf ) ; 
 String query = " SELECT key _ validator , " + 
 - 	 	 " key _ aliases " + 
 + 	 	 " key _ aliases , " + 
 + 	 	 " column _ aliases " + 
 " FROM system . schema _ columnfamilies " + 
 " WHERE keyspace _ name = ' % s ' and columnfamily _ name = ' % s ' " ; 
 String formatted = String . format ( query , keyspace , cfName ) ; 
 @ @ - 334 , 16 + 337 , 22 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , 
 logger . debug ( " partition keys : " + keyString ) ; 
 
 List < String > keys = FBUtilities . fromJsonList ( keyString ) ; 
 - partitionkeys = new String [ keys . size ( ) ] ; 
 + partitionKeyColumns = new String [ keys . size ( ) ] ; 
 int i = 0 ; 
 for ( String key : keys ) 
 { 
 - partitionkeys [ i ] = key ; 
 + partitionKeyColumns [ i ] = key ; 
 i + + ; 
 } 
 + 
 + Column rawClusterColumns = result . rows . get ( 0 ) . columns . get ( 2 ) ; 
 + String clusterColumnString = ByteBufferUtil . string ( ByteBuffer . wrap ( rawClusterColumns . getValue ( ) ) ) ; 
 + 
 + logger . debug ( " cluster columns : " + clusterColumnString ) ; 
 + clusterColumns = FBUtilities . fromJsonList ( clusterColumnString ) ; 
 } 
 
 - private AbstractType < ? > parseType ( String type ) throws IOException 
 + private AbstractType < ? > parseType ( String type ) throws ConfigurationException 
 { 
 try 
 { 
 @ @ - 352 , 32 + 361 , 24 @ @ final class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , 
 return LongType . instance ; 
 return TypeParser . parse ( type ) ; 
 } 
 - catch ( ConfigurationException e ) 
 - { 
 - throw new IOException ( e ) ; 
 - } 
 catch ( SyntaxException e ) 
 { 
 - throw new IOException ( e ) ; 
 + throw new ConfigurationException ( e . getMessage ( ) , e ) ; 
 } 
 } 
 - 
 - private String getAnyHost ( ) throws IOException , InvalidRequestException , TException 
 + 
 + / * * 
 + * add where clauses for partition keys and cluster columns 
 + * / 
 + private String appendKeyWhereClauses ( String cqlQuery ) 
 { 
 - Cassandra . Client client = ConfigHelper . getClientFromOutputAddressList ( conf ) ; 
 - List < TokenRange > ring = client . describe _ ring ( ConfigHelper . getOutputKeyspace ( conf ) ) ; 
 - try 
 - { 
 - for ( TokenRange range : ring ) 
 - return range . endpoints . get ( 0 ) ; 
 - } 
 - finally 
 - { 
 - TTransport transport = client . getOutputProtocol ( ) . getTransport ( ) ; 
 - if ( transport . isOpen ( ) ) 
 - transport . close ( ) ; 
 - } 
 - throw new IOException ( " There are no endpoints " ) ; 
 - } 
 + String keyWhereClause = " " ; 
 + 
 + for ( String partitionKey : partitionKeyColumns ) 
 + keyWhereClause + = String . format ( " % s = ? " , keyWhereClause . isEmpty ( ) ? partitionKey : ( " AND " + partitionKey ) ) ; 
 + for ( String clusterColumn : clusterColumns ) 
 + keyWhereClause + = " AND " + clusterColumn + " = ? " ; 
 
 + return cqlQuery + " WHERE " + keyWhereClause ; 
 + } 
 }
