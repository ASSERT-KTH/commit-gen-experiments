BLEU SCORE: 0.028465126651392333

TEST MSG: Fill in Replication , Tuneable Consistency sections
GENERATED MSG: Add snitch and range movements section on Operations

TEST DIFF (one line): diff - - git a / doc / source / architecture . rst b / doc / source / architecture . rst <nl> index 37a0027 . . 3f8a8ca 100644 <nl> - - - a / doc / source / architecture . rst <nl> + + + b / doc / source / architecture . rst <nl> @ @ - 43 , 12 + 43 , 104 @ @ Token Ring / Ranges <nl> Replication <nl> ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> <nl> - . . todo : : todo <nl> + The replication strategy of a keyspace determines which nodes are replicas for a given token range . The two main <nl> + replication strategies are : ref : ` simple - strategy ` and : ref : ` network - topology - strategy ` . <nl> + <nl> + . . _ simple - strategy : <nl> + <nl> + SimpleStrategy <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + SimpleStrategy allows a single integer ` ` replication _ factor ` ` to be defined . This determines the number of nodes that <nl> + should contain a copy of each row . For example , if ` ` replication _ factor ` ` is 3 , then three different nodes should store <nl> + a copy of each row . <nl> + <nl> + SimpleStrategy treats all nodes identically , ignoring any configured datacenters or racks . To determine the replicas <nl> + for a token range , Cassandra iterates through the tokens in the ring , starting with the token range of interest . For <nl> + each token , it checks whether the owning node has been added to the set of replicas , and if it has not , it is added to <nl> + the set . This process continues until ` ` replication _ factor ` ` distinct nodes have been added to the set of replicas . <nl> + <nl> + . . _ network - topology - strategy : <nl> + <nl> + NetworkTopologyStrategy <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + NetworkTopologyStrategy allows a replication factor to be specified for each datacenter in the cluster . Even if your <nl> + cluster only uses a single datacenter , NetworkTopologyStrategy should be prefered over SimpleStrategy to make it easier <nl> + to add new physical or virtual datacenters to the cluster later . <nl> + <nl> + In addition to allowing the replication factor to be specified per - DC , NetworkTopologyStrategy also attempts to choose <nl> + replicas within a datacenter from different racks . If the number of racks is greater than or equal to the replication <nl> + factor for the DC , each replica will be chosen from a different rack . Otherwise , each rack will hold at least one <nl> + replica , but some racks may hold more than one . Note that this rack - aware behavior has some potentially ` surprising <nl> + implications < https : / / issues . apache . org / jira / browse / CASSANDRA - 3810 > ` _ . For example , if there are not an even number of <nl> + nodes in each rack , the data load on the smallest rack may be much higher . Similarly , if a single node is bootstrapped <nl> + into a new rack , it will be considered a replica for the entire ring . For this reason , many operators choose to <nl> + configure all nodes on a single " rack " . <nl> <nl> Tunable Consistency <nl> ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> <nl> - . . todo : : todo <nl> + Cassandra supports a per - operation tradeoff between consistency and availability through * Consistency Levels * . <nl> + Essentially , an operation ' s consistency level specifies how many of the replicas need to respond to the coordinator in <nl> + order to consider the operation a success . <nl> + <nl> + The following consistency levels are available : <nl> + <nl> + ` ` ONE ` ` <nl> + Only a single replica must respond . <nl> + <nl> + ` ` TWO ` ` <nl> + Two replicas must respond . <nl> + <nl> + ` ` THREE ` ` <nl> + Three replicas must respond . <nl> + <nl> + ` ` QUORUM ` ` <nl> + A majority ( n / 2 + 1 ) of the replicas must respond . <nl> + <nl> + ` ` ALL ` ` <nl> + All of the replicas must respond . <nl> + <nl> + ` ` LOCAL _ QUORUM ` ` <nl> + A majority of the replicas in the local datacenter ( whichever datacenter the coordinator is in ) must respond . <nl> + <nl> + ` ` EACH _ QUORUM ` ` <nl> + A majority of the replicas in each datacenter must respond . <nl> + <nl> + ` ` LOCAL _ ONE ` ` <nl> + Only a single replica must respond . In a multi - datacenter cluster , this also gaurantees that read requests are not <nl> + sent to replicas in a remote datacenter . <nl> + <nl> + ` ` ANY ` ` <nl> + A single replica may respond , or the coordinator may store a hint . If a hint is stored , the coordinator will later <nl> + attempt to replay the hint and deliver the mutation to the replicas . This consistency level is only accepted for <nl> + write operations . <nl> + <nl> + Write operations are always sent to all replicas , regardless of consistency level . The consistency level simply <nl> + controls how many responses the coordinator waits for before responding to the client . <nl> + <nl> + For read operations , the coordinator generally only issues read commands to enough replicas to satisfy the consistency <nl> + level . There are a couple of exceptions to this : <nl> + <nl> + - Speculative retry may issue a redundant read request to an extra replica if the other replicas have not responded <nl> + within a specified time window . <nl> + - Based on ` ` read _ repair _ chance ` ` and ` ` dclocal _ read _ repair _ chance ` ` ( part of a table ' s schema ) , read requests may be <nl> + randomly sent to all replicas in order to repair potentially inconsistent data . <nl> + <nl> + Picking Consistency Levels <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + It is common to pick read and write consistency levels that are high enough to overlap , resulting in " strong " <nl> + consistency . This is typically expressed as ` ` W + R > RF ` ` , where ` ` W ` ` is the write consistency level , ` ` R ` ` is the <nl> + read consistency level , and ` ` RF ` ` is the replication factor . For example , if ` ` RF = 3 ` ` , a ` ` QUORUM ` ` request will <nl> + require responses from at least two of the three replicas . If ` ` QUORUM ` ` is used for both writes and reads , at least <nl> + one of the replicas is guaranteed to participate in * both * the write and the read request , which in turn guarantees that <nl> + the latest write will be read . In a multi - datacenter environment , ` ` LOCAL _ QUORUM ` ` can be used to provide a weaker but <nl> + still useful guarantee : reads are guaranteed to see the latest write from within the same datacenter . <nl> + <nl> + If this type of strong consistency isn ' t required , lower consistency levels like ` ` ONE ` ` may be used to improve <nl> + throughput , latency , and availability . <nl> <nl> Storage Engine <nl> - - - - - - - - - - - - - -
NEAREST DIFF (one line): ELIMINATEDSENTENCE

TEST DIFF:
diff - - git a / doc / source / architecture . rst b / doc / source / architecture . rst 
 index 37a0027 . . 3f8a8ca 100644 
 - - - a / doc / source / architecture . rst 
 + + + b / doc / source / architecture . rst 
 @ @ - 43 , 12 + 43 , 104 @ @ Token Ring / Ranges 
 Replication 
 ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 
 - . . todo : : todo 
 + The replication strategy of a keyspace determines which nodes are replicas for a given token range . The two main 
 + replication strategies are : ref : ` simple - strategy ` and : ref : ` network - topology - strategy ` . 
 + 
 + . . _ simple - strategy : 
 + 
 + SimpleStrategy 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + SimpleStrategy allows a single integer ` ` replication _ factor ` ` to be defined . This determines the number of nodes that 
 + should contain a copy of each row . For example , if ` ` replication _ factor ` ` is 3 , then three different nodes should store 
 + a copy of each row . 
 + 
 + SimpleStrategy treats all nodes identically , ignoring any configured datacenters or racks . To determine the replicas 
 + for a token range , Cassandra iterates through the tokens in the ring , starting with the token range of interest . For 
 + each token , it checks whether the owning node has been added to the set of replicas , and if it has not , it is added to 
 + the set . This process continues until ` ` replication _ factor ` ` distinct nodes have been added to the set of replicas . 
 + 
 + . . _ network - topology - strategy : 
 + 
 + NetworkTopologyStrategy 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + NetworkTopologyStrategy allows a replication factor to be specified for each datacenter in the cluster . Even if your 
 + cluster only uses a single datacenter , NetworkTopologyStrategy should be prefered over SimpleStrategy to make it easier 
 + to add new physical or virtual datacenters to the cluster later . 
 + 
 + In addition to allowing the replication factor to be specified per - DC , NetworkTopologyStrategy also attempts to choose 
 + replicas within a datacenter from different racks . If the number of racks is greater than or equal to the replication 
 + factor for the DC , each replica will be chosen from a different rack . Otherwise , each rack will hold at least one 
 + replica , but some racks may hold more than one . Note that this rack - aware behavior has some potentially ` surprising 
 + implications < https : / / issues . apache . org / jira / browse / CASSANDRA - 3810 > ` _ . For example , if there are not an even number of 
 + nodes in each rack , the data load on the smallest rack may be much higher . Similarly , if a single node is bootstrapped 
 + into a new rack , it will be considered a replica for the entire ring . For this reason , many operators choose to 
 + configure all nodes on a single " rack " . 
 
 Tunable Consistency 
 ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 
 - . . todo : : todo 
 + Cassandra supports a per - operation tradeoff between consistency and availability through * Consistency Levels * . 
 + Essentially , an operation ' s consistency level specifies how many of the replicas need to respond to the coordinator in 
 + order to consider the operation a success . 
 + 
 + The following consistency levels are available : 
 + 
 + ` ` ONE ` ` 
 + Only a single replica must respond . 
 + 
 + ` ` TWO ` ` 
 + Two replicas must respond . 
 + 
 + ` ` THREE ` ` 
 + Three replicas must respond . 
 + 
 + ` ` QUORUM ` ` 
 + A majority ( n / 2 + 1 ) of the replicas must respond . 
 + 
 + ` ` ALL ` ` 
 + All of the replicas must respond . 
 + 
 + ` ` LOCAL _ QUORUM ` ` 
 + A majority of the replicas in the local datacenter ( whichever datacenter the coordinator is in ) must respond . 
 + 
 + ` ` EACH _ QUORUM ` ` 
 + A majority of the replicas in each datacenter must respond . 
 + 
 + ` ` LOCAL _ ONE ` ` 
 + Only a single replica must respond . In a multi - datacenter cluster , this also gaurantees that read requests are not 
 + sent to replicas in a remote datacenter . 
 + 
 + ` ` ANY ` ` 
 + A single replica may respond , or the coordinator may store a hint . If a hint is stored , the coordinator will later 
 + attempt to replay the hint and deliver the mutation to the replicas . This consistency level is only accepted for 
 + write operations . 
 + 
 + Write operations are always sent to all replicas , regardless of consistency level . The consistency level simply 
 + controls how many responses the coordinator waits for before responding to the client . 
 + 
 + For read operations , the coordinator generally only issues read commands to enough replicas to satisfy the consistency 
 + level . There are a couple of exceptions to this : 
 + 
 + - Speculative retry may issue a redundant read request to an extra replica if the other replicas have not responded 
 + within a specified time window . 
 + - Based on ` ` read _ repair _ chance ` ` and ` ` dclocal _ read _ repair _ chance ` ` ( part of a table ' s schema ) , read requests may be 
 + randomly sent to all replicas in order to repair potentially inconsistent data . 
 + 
 + Picking Consistency Levels 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + It is common to pick read and write consistency levels that are high enough to overlap , resulting in " strong " 
 + consistency . This is typically expressed as ` ` W + R > RF ` ` , where ` ` W ` ` is the write consistency level , ` ` R ` ` is the 
 + read consistency level , and ` ` RF ` ` is the replication factor . For example , if ` ` RF = 3 ` ` , a ` ` QUORUM ` ` request will 
 + require responses from at least two of the three replicas . If ` ` QUORUM ` ` is used for both writes and reads , at least 
 + one of the replicas is guaranteed to participate in * both * the write and the read request , which in turn guarantees that 
 + the latest write will be read . In a multi - datacenter environment , ` ` LOCAL _ QUORUM ` ` can be used to provide a weaker but 
 + still useful guarantee : reads are guaranteed to see the latest write from within the same datacenter . 
 + 
 + If this type of strong consistency isn ' t required , lower consistency levels like ` ` ONE ` ` may be used to improve 
 + throughput , latency , and availability . 
 
 Storage Engine 
 - - - - - - - - - - - - - -

NEAREST DIFF:
ELIMINATEDSENTENCE
