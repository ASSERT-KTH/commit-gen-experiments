BLEU SCORE: 0.07160476144948855

TEST MSG: Add hadoop progressable compatibility .
GENERATED MSG: move FBU . hex methods to BBU

TEST DIFF (one line): diff - - git a / build . xml b / build . xml <nl> index bb8673e . . 304b5fe 100644 <nl> - - - a / build . xml <nl> + + + b / build . xml <nl> @ @ - 374 , 7 + 374 , 6 @ @ <nl> 	 < exclusion groupId = " org . mortbay . jetty " artifactId = " servlet - api " / > <nl> < / dependency > <nl> < dependency groupId = " org . apache . hadoop " artifactId = " hadoop - minicluster " version = " 1 . 0 . 3 " / > <nl> - < dependency groupId = " com . twitter . elephantbird " artifactId = " elephant - bird - hadoop - compat " version = " 4 . 3 " / > <nl> < dependency groupId = " org . apache . pig " artifactId = " pig " version = " 0 . 11 . 1 " / > <nl> < dependency groupId = " net . java . dev . jna " artifactId = " jna " version = " 4 . 0 . 0 " / > <nl> <nl> @ @ - 418 , 7 + 417 , 6 @ @ <nl> < dependency groupId = " org . apache . rat " artifactId = " apache - rat " / > <nl> < dependency groupId = " org . apache . hadoop " artifactId = " hadoop - core " / > <nl> 	 < dependency groupId = " org . apache . hadoop " artifactId = " hadoop - minicluster " / > <nl> - < dependency groupId = " com . twitter . elephantbird " artifactId = " elephant - bird - hadoop - compat " / > <nl> < dependency groupId = " org . apache . pig " artifactId = " pig " / > <nl> 	 < dependency groupId = " com . google . code . findbugs " artifactId = " jsr305 " / > <nl> < / artifact : pom > <nl> @ @ - 485 , 7 + 483 , 6 @ @ <nl> < ! - - don ' t need hadoop classes to run , but if you use the hadoop stuff - - > <nl> < dependency groupId = " org . apache . hadoop " artifactId = " hadoop - core " optional = " true " / > <nl> < dependency groupId = " org . apache . hadoop " artifactId = " hadoop - minicluster " optional = " true " / > <nl> - < dependency groupId = " com . twitter . elephantbird " artifactId = " elephant - bird - hadoop - compat " optional = " true " / > <nl> < dependency groupId = " org . apache . pig " artifactId = " pig " optional = " true " / > <nl> <nl> < ! - - don ' t need jna to run , but nice to have - - > <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> index 760193f . . cb106e9 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> @ @ - 29 , 7 + 29 , 6 @ @ import java . util . concurrent . TimeUnit ; <nl> <nl> import com . google . common . collect . ImmutableList ; <nl> import com . google . common . collect . Lists ; <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyOutputFormat . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyOutputFormat . java <nl> index a3c4234 . . 3041829 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyOutputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyOutputFormat . java <nl> @ @ - 22 , 7 + 22 , 6 @ @ import java . io . IOException ; <nl> import java . util . HashMap ; <nl> import java . util . Map ; <nl> <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java <nl> index 1956262 . . 501ca65 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java <nl> @ @ - 32 , 6 + 32 , 7 @ @ import org . apache . hadoop . conf . Configuration ; <nl> import org . apache . hadoop . mapreduce . RecordWriter ; <nl> import org . apache . hadoop . mapreduce . TaskAttemptContext ; <nl> import org . apache . thrift . transport . TTransport ; <nl> + import org . apache . hadoop . util . Progressable ; <nl> <nl> <nl> / * * <nl> @ @ - 67 , 6 + 68 , 7 @ @ public abstract class AbstractColumnFamilyRecordWriter < K , Y > extends RecordWrite <nl> <nl> protected final ConsistencyLevel consistencyLevel ; <nl> protected Progressable progressable ; <nl> + protected TaskAttemptContext context ; <nl> <nl> protected AbstractColumnFamilyRecordWriter ( Configuration conf ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / BulkOutputFormat . java b / src / java / org / apache / cassandra / hadoop / BulkOutputFormat . java <nl> index 566d5ee . . c3d8e05 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / BulkOutputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / BulkOutputFormat . java <nl> @ @ - 22 , 7 + 22 , 6 @ @ import java . io . IOException ; <nl> import java . nio . ByteBuffer ; <nl> import java . util . List ; <nl> <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> import org . apache . cassandra . thrift . Mutation ; <nl> import org . apache . hadoop . conf . Configuration ; <nl> import org . apache . hadoop . mapreduce . * ; <nl> @ @ - 61 , 7 + 60 , 7 @ @ public class BulkOutputFormat extends OutputFormat < ByteBuffer , List < Mutation > > <nl> @ Deprecated <nl> public BulkRecordWriter getRecordWriter ( org . apache . hadoop . fs . FileSystem filesystem , org . apache . hadoop . mapred . JobConf job , String name , org . apache . hadoop . util . Progressable progress ) throws IOException <nl> { <nl> - return new BulkRecordWriter ( job , new Progressable ( progress ) ) ; <nl> + return new BulkRecordWriter ( job , progress ) ; <nl> } <nl> <nl> @ Override <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / BulkRecordWriter . java b / src / java / org / apache / cassandra / hadoop / BulkRecordWriter . java <nl> index a8e2e13 . . d6136a2 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / BulkRecordWriter . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / BulkRecordWriter . java <nl> @ @ - 28 , 7 + 28 , 6 @ @ import java . util . concurrent . Future ; <nl> import java . util . concurrent . TimeUnit ; <nl> import java . util . concurrent . TimeoutException ; <nl> <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> @ @ - 53 , 6 + 52 , 7 @ @ import org . apache . thrift . transport . TFramedTransport ; <nl> import org . apache . thrift . transport . TSocket ; <nl> import org . apache . thrift . transport . TTransport ; <nl> import org . apache . thrift . transport . TTransportException ; <nl> + import org . apache . hadoop . util . Progressable ; <nl> <nl> final class BulkRecordWriter extends RecordWriter < ByteBuffer , List < Mutation > > <nl> implements org . apache . hadoop . mapred . RecordWriter < ByteBuffer , List < Mutation > > <nl> @ @ - 67 , 6 + 67 , 7 @ @ implements org . apache . hadoop . mapred . RecordWriter < ByteBuffer , List < Mutation > > <nl> private SSTableLoader loader ; <nl> private File outputdir ; <nl> private Progressable progress ; <nl> + private TaskAttemptContext context ; <nl> private int maxFailures ; <nl> <nl> private enum CFType <nl> @ @ - 87 , 10 + 88 , 9 @ @ implements org . apache . hadoop . mapred . RecordWriter < ByteBuffer , List < Mutation > > <nl> BulkRecordWriter ( TaskAttemptContext context ) <nl> { <nl> this ( HadoopCompat . getConfiguration ( context ) ) ; <nl> - this . progress = new Progressable ( context ) ; <nl> + this . context = context ; <nl> } <nl> <nl> - <nl> BulkRecordWriter ( Configuration conf , Progressable progress ) <nl> { <nl> this ( conf ) ; <nl> @ @ - 205 , 7 + 205 , 10 @ @ implements org . apache . hadoop . mapred . RecordWriter < ByteBuffer , List < Mutation > > <nl> writer . addExpiringColumn ( mut . getColumn _ or _ supercolumn ( ) . column . name , mut . getColumn _ or _ supercolumn ( ) . column . value , mut . getColumn _ or _ supercolumn ( ) . column . timestamp , mut . getColumn _ or _ supercolumn ( ) . column . ttl , System . currentTimeMillis ( ) + ( ( long ) ( mut . getColumn _ or _ supercolumn ( ) . column . ttl ) * 1000 ) ) ; <nl> } <nl> } <nl> - progress . progress ( ) ; <nl> + if ( null ! = progress ) <nl> + progress . progress ( ) ; <nl> + if ( null ! = context ) <nl> + HadoopCompat . progress ( context ) ; <nl> } <nl> } <nl> @ Override <nl> @ @ - 236 , 7 + 239 , 10 @ @ implements org . apache . hadoop . mapred . RecordWriter < ByteBuffer , List < Mutation > > <nl> } <nl> catch ( ExecutionException | TimeoutException te ) <nl> { <nl> - progress . progress ( ) ; <nl> + if ( null ! = progress ) <nl> + progress . progress ( ) ; <nl> + if ( null ! = context ) <nl> + HadoopCompat . progress ( context ) ; <nl> } <nl> catch ( InterruptedException e ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java <nl> index 362cd70 . . a2c7a36 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java <nl> @ @ - 21 , 7 + 21 , 6 @ @ import java . io . IOException ; <nl> import java . nio . ByteBuffer ; <nl> import java . util . * ; <nl> <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> import org . apache . cassandra . db . Cell ; <nl> import org . apache . cassandra . db . composites . CellName ; <nl> import org . apache . hadoop . conf . Configuration ; <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java <nl> index 724ba7d . . 49aaf99 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java <nl> @ @ - 51 , 7 + 51 , 7 @ @ public class ColumnFamilyOutputFormat extends AbstractColumnFamilyOutputFormat < B <nl> @ Deprecated <nl> public ColumnFamilyRecordWriter getRecordWriter ( org . apache . hadoop . fs . FileSystem filesystem , org . apache . hadoop . mapred . JobConf job , String name , org . apache . hadoop . util . Progressable progress ) <nl> { <nl> - return new ColumnFamilyRecordWriter ( job , new Progressable ( progress ) ) ; <nl> + return new ColumnFamilyRecordWriter ( job , progress ) ; <nl> } <nl> <nl> / * * <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java <nl> index ef883fd . . f6d2b7e 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java <nl> @ @ - 24 , 7 + 24 , 6 @ @ import java . nio . ByteBuffer ; <nl> import java . util . * ; <nl> <nl> import com . google . common . collect . * ; <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> import org . apache . cassandra . db . Cell ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java <nl> index 0ae2a67 . . d6a873b 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java <nl> @ @ - 23 , 7 + 23 , 6 @ @ import java . net . InetAddress ; <nl> import java . nio . ByteBuffer ; <nl> import java . util . * ; <nl> <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> import org . apache . cassandra . dht . Range ; <nl> import org . apache . cassandra . dht . Token ; <nl> import org . apache . cassandra . thrift . * ; <nl> @ @ - 31 , 6 + 30 , 7 @ @ import org . apache . cassandra . utils . Pair ; <nl> import org . apache . hadoop . conf . Configuration ; <nl> import org . apache . hadoop . mapreduce . TaskAttemptContext ; <nl> import org . apache . thrift . TException ; <nl> + import org . apache . hadoop . util . Progressable ; <nl> <nl> <nl> / * * <nl> @ @ - 62 , 9 + 62 , 9 @ @ final class ColumnFamilyRecordWriter extends AbstractColumnFamilyRecordWriter < By <nl> ColumnFamilyRecordWriter ( TaskAttemptContext context ) <nl> { <nl> this ( HadoopCompat . getConfiguration ( context ) ) ; <nl> - this . progressable = new Progressable ( context ) ; <nl> - } <nl> + this . context = context ; <nl> <nl> + } <nl> ColumnFamilyRecordWriter ( Configuration conf , Progressable progressable ) <nl> { <nl> this ( conf ) ; <nl> @ @ - 128 , 7 + 128 , 10 @ @ final class ColumnFamilyRecordWriter extends AbstractColumnFamilyRecordWriter < By <nl> <nl> for ( Mutation amut : value ) <nl> client . put ( Pair . create ( keybuff , amut ) ) ; <nl> + if ( progressable ! = null ) <nl> progressable . progress ( ) ; <nl> + if ( context ! = null ) <nl> + HadoopCompat . progress ( context ) ; <nl> } <nl> <nl> / * * <nl> @ @ - 140 , 9 + 143 , 9 @ @ final class ColumnFamilyRecordWriter extends AbstractColumnFamilyRecordWriter < By <nl> public final String columnFamily = ConfigHelper . getOutputColumnFamily ( conf ) ; <nl> <nl> / * * <nl> - * Constructs an { @ link RangeClient } for the given endpoints . <nl> - * @ param endpoints the possible endpoints to execute the mutations on <nl> - * / <nl> + * Constructs an { @ link RangeClient } for the given endpoints . <nl> + * @ param endpoints the possible endpoints to execute the mutations on <nl> + * / <nl> public RangeClient ( List < InetAddress > endpoints ) <nl> { <nl> super ( endpoints ) ; <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / HadoopCompat . java b / src / java / org / apache / cassandra / hadoop / HadoopCompat . java <nl> new file mode 100644 <nl> index 0000000 . . f2f7033 <nl> - - - / dev / null <nl> + + + b / src / java / org / apache / cassandra / hadoop / HadoopCompat . java <nl> @ @ - 0 , 0 + 1 , 309 @ @ <nl> + / * * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , software <nl> + * distributed under the License is distributed on an " AS IS " BASIS , <nl> + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> + * See the License for the specific language governing permissions and <nl> + * limitations under the License . <nl> + * / <nl> + <nl> + package org . apache . cassandra . hadoop ; <nl> + <nl> + import java . lang . reflect . Constructor ; <nl> + import java . lang . reflect . Field ; <nl> + import java . lang . reflect . InvocationTargetException ; <nl> + import java . lang . reflect . Method ; <nl> + <nl> + import org . apache . hadoop . conf . Configuration ; <nl> + import org . apache . hadoop . mapreduce . Counter ; <nl> + import org . apache . hadoop . mapreduce . InputSplit ; <nl> + import org . apache . hadoop . mapreduce . JobContext ; <nl> + import org . apache . hadoop . mapreduce . JobID ; <nl> + import org . apache . hadoop . mapreduce . MapContext ; <nl> + import org . apache . hadoop . mapreduce . OutputCommitter ; <nl> + import org . apache . hadoop . mapreduce . RecordReader ; <nl> + import org . apache . hadoop . mapreduce . RecordWriter ; <nl> + import org . apache . hadoop . mapreduce . StatusReporter ; <nl> + import org . apache . hadoop . mapreduce . TaskAttemptContext ; <nl> + import org . apache . hadoop . mapreduce . TaskAttemptID ; <nl> + import org . apache . hadoop . mapreduce . TaskInputOutputContext ; <nl> + <nl> + / * <nl> + * This is based on ContextFactory . java from hadoop - 2 . 0 . x sources . <nl> + * / <nl> + <nl> + / * * <nl> + * Utility methods to allow applications to deal with inconsistencies between <nl> + * MapReduce Context Objects API between Hadoop 1 . x and 2 . x . <nl> + * / <nl> + public class HadoopCompat { <nl> + <nl> + private static final boolean useV21 ; <nl> + <nl> + private static final Constructor < ? > JOB _ CONTEXT _ CONSTRUCTOR ; <nl> + private static final Constructor < ? > TASK _ CONTEXT _ CONSTRUCTOR ; <nl> + private static final Constructor < ? > MAP _ CONTEXT _ CONSTRUCTOR ; <nl> + private static final Constructor < ? > GENERIC _ COUNTER _ CONSTRUCTOR ; <nl> + <nl> + private static final Field READER _ FIELD ; <nl> + private static final Field WRITER _ FIELD ; <nl> + <nl> + private static final Method GET _ CONFIGURATION _ METHOD ; <nl> + private static final Method SET _ STATUS _ METHOD ; <nl> + private static final Method GET _ COUNTER _ METHOD ; <nl> + private static final Method INCREMENT _ COUNTER _ METHOD ; <nl> + private static final Method GET _ TASK _ ATTEMPT _ ID ; <nl> + private static final Method PROGRESS _ METHOD ; <nl> + <nl> + static { <nl> + boolean v21 = true ; <nl> + final String PACKAGE = " org . apache . hadoop . mapreduce " ; <nl> + try { <nl> + Class . forName ( PACKAGE + " . task . JobContextImpl " ) ; <nl> + } catch ( ClassNotFoundException cnfe ) { <nl> + v21 = false ; <nl> + } <nl> + useV21 = v21 ; <nl> + Class < ? > jobContextCls ; <nl> + Class < ? > taskContextCls ; <nl> + Class < ? > taskIOContextCls ; <nl> + Class < ? > mapContextCls ; <nl> + Class < ? > genericCounterCls ; <nl> + try { <nl> + if ( v21 ) { <nl> + jobContextCls = <nl> + Class . forName ( PACKAGE + " . task . JobContextImpl " ) ; <nl> + taskContextCls = <nl> + Class . forName ( PACKAGE + " . task . TaskAttemptContextImpl " ) ; <nl> + taskIOContextCls = <nl> + Class . forName ( PACKAGE + " . task . TaskInputOutputContextImpl " ) ; <nl> + mapContextCls = Class . forName ( PACKAGE + " . task . MapContextImpl " ) ; <nl> + genericCounterCls = Class . forName ( PACKAGE + " . counters . GenericCounter " ) ; <nl> + } else { <nl> + jobContextCls = <nl> + Class . forName ( PACKAGE + " . JobContext " ) ; <nl> + taskContextCls = <nl> + Class . forName ( PACKAGE + " . TaskAttemptContext " ) ; <nl> + taskIOContextCls = <nl> + Class . forName ( PACKAGE + " . TaskInputOutputContext " ) ; <nl> + mapContextCls = Class . forName ( PACKAGE + " . MapContext " ) ; <nl> + genericCounterCls = <nl> + Class . forName ( " org . apache . hadoop . mapred . Counters $ Counter " ) ; <nl> + <nl> + } <nl> + } catch ( ClassNotFoundException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t find class " , e ) ; <nl> + } <nl> + try { <nl> + JOB _ CONTEXT _ CONSTRUCTOR = <nl> + jobContextCls . getConstructor ( Configuration . class , JobID . class ) ; <nl> + JOB _ CONTEXT _ CONSTRUCTOR . setAccessible ( true ) ; <nl> + TASK _ CONTEXT _ CONSTRUCTOR = <nl> + taskContextCls . getConstructor ( Configuration . class , <nl> + TaskAttemptID . class ) ; <nl> + TASK _ CONTEXT _ CONSTRUCTOR . setAccessible ( true ) ; <nl> + GENERIC _ COUNTER _ CONSTRUCTOR = <nl> + genericCounterCls . getDeclaredConstructor ( String . class , <nl> + String . class , <nl> + Long . TYPE ) ; <nl> + GENERIC _ COUNTER _ CONSTRUCTOR . setAccessible ( true ) ; <nl> + <nl> + if ( useV21 ) { <nl> + MAP _ CONTEXT _ CONSTRUCTOR = <nl> + mapContextCls . getDeclaredConstructor ( Configuration . class , <nl> + TaskAttemptID . class , <nl> + RecordReader . class , <nl> + RecordWriter . class , <nl> + OutputCommitter . class , <nl> + StatusReporter . class , <nl> + InputSplit . class ) ; <nl> + Method get _ counter ; <nl> + try { <nl> + get _ counter = Class . forName ( PACKAGE + " . TaskAttemptContext " ) . getMethod ( " getCounter " , String . class , <nl> + String . class ) ; <nl> + } catch ( Exception e ) { <nl> + get _ counter = Class . forName ( PACKAGE + " . TaskInputOutputContext " ) . getMethod ( " getCounter " , <nl> + String . class , String . class ) ; <nl> + } <nl> + GET _ COUNTER _ METHOD = get _ counter ; <nl> + } else { <nl> + MAP _ CONTEXT _ CONSTRUCTOR = <nl> + mapContextCls . getConstructor ( Configuration . class , <nl> + TaskAttemptID . class , <nl> + RecordReader . class , <nl> + RecordWriter . class , <nl> + OutputCommitter . class , <nl> + StatusReporter . class , <nl> + InputSplit . class ) ; <nl> + GET _ COUNTER _ METHOD = Class . forName ( PACKAGE + " . TaskInputOutputContext " ) <nl> + . getMethod ( " getCounter " , String . class , String . class ) ; <nl> + } <nl> + MAP _ CONTEXT _ CONSTRUCTOR . setAccessible ( true ) ; <nl> + READER _ FIELD = mapContextCls . getDeclaredField ( " reader " ) ; <nl> + READER _ FIELD . setAccessible ( true ) ; <nl> + WRITER _ FIELD = taskIOContextCls . getDeclaredField ( " output " ) ; <nl> + WRITER _ FIELD . setAccessible ( true ) ; <nl> + GET _ CONFIGURATION _ METHOD = Class . forName ( PACKAGE + " . JobContext " ) <nl> + . getMethod ( " getConfiguration " ) ; <nl> + SET _ STATUS _ METHOD = Class . forName ( PACKAGE + " . TaskAttemptContext " ) <nl> + . getMethod ( " setStatus " , String . class ) ; <nl> + GET _ TASK _ ATTEMPT _ ID = Class . forName ( PACKAGE + " . TaskAttemptContext " ) <nl> + . getMethod ( " getTaskAttemptID " ) ; <nl> + INCREMENT _ COUNTER _ METHOD = Class . forName ( PACKAGE + " . Counter " ) <nl> + . getMethod ( " increment " , Long . TYPE ) ; <nl> + PROGRESS _ METHOD = Class . forName ( PACKAGE + " . TaskAttemptContext " ) <nl> + . getMethod ( " progress " ) ; <nl> + <nl> + } catch ( SecurityException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t run constructor " , e ) ; <nl> + } catch ( NoSuchMethodException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t find constructor " , e ) ; <nl> + } catch ( NoSuchFieldException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t find field " , e ) ; <nl> + } catch ( ClassNotFoundException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t find class " , e ) ; <nl> + } <nl> + } <nl> + <nl> + / * * <nl> + * True if runtime Hadoop version is 2 . x , false otherwise . <nl> + * / <nl> + public static boolean isVersion2x ( ) { <nl> + return useV21 ; <nl> + } <nl> + <nl> + private static Object newInstance ( Constructor < ? > constructor , Object . . . args ) { <nl> + try { <nl> + return constructor . newInstance ( args ) ; <nl> + } catch ( InstantiationException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t instantiate " + constructor , e ) ; <nl> + } catch ( IllegalAccessException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t instantiate " + constructor , e ) ; <nl> + } catch ( InvocationTargetException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t instantiate " + constructor , e ) ; <nl> + } <nl> + } <nl> + <nl> + / * * <nl> + * Creates JobContext from a JobConf and jobId using the correct constructor <nl> + * for based on Hadoop version . < code > jobId < / code > could be null . <nl> + * / <nl> + public static JobContext newJobContext ( Configuration conf , JobID jobId ) { <nl> + return ( JobContext ) newInstance ( JOB _ CONTEXT _ CONSTRUCTOR , conf , jobId ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Creates TaskAttempContext from a JobConf and jobId using the correct <nl> + * constructor for based on Hadoop version . <nl> + * / <nl> + public static TaskAttemptContext newTaskAttemptContext ( <nl> + Configuration conf , TaskAttemptID taskAttemptId ) { <nl> + return ( TaskAttemptContext ) <nl> + newInstance ( TASK _ CONTEXT _ CONSTRUCTOR , conf , taskAttemptId ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Instantiates MapContext under Hadoop 1 and MapContextImpl under Hadoop 2 . <nl> + * / <nl> + public static MapContext newMapContext ( Configuration conf , <nl> + TaskAttemptID taskAttemptID , <nl> + RecordReader recordReader , <nl> + RecordWriter recordWriter , <nl> + OutputCommitter outputCommitter , <nl> + StatusReporter statusReporter , <nl> + InputSplit inputSplit ) { <nl> + return ( MapContext ) newInstance ( MAP _ CONTEXT _ CONSTRUCTOR , <nl> + conf , taskAttemptID , recordReader , recordWriter , outputCommitter , <nl> + statusReporter , inputSplit ) ; <nl> + } <nl> + <nl> + / * * <nl> + * @ return with Hadoop 2 : < code > new GenericCounter ( args ) < / code > , < br > <nl> + * with Hadoop 1 : < code > new Counter ( args ) < / code > <nl> + * / <nl> + public static Counter newGenericCounter ( String name , String displayName , long value ) { <nl> + try { <nl> + return ( Counter ) <nl> + GENERIC _ COUNTER _ CONSTRUCTOR . newInstance ( name , displayName , value ) ; <nl> + } catch ( InstantiationException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t instantiate Counter " , e ) ; <nl> + } catch ( IllegalAccessException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t instantiate Counter " , e ) ; <nl> + } catch ( InvocationTargetException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t instantiate Counter " , e ) ; <nl> + } <nl> + } <nl> + <nl> + / * * <nl> + * Invokes a method and rethrows any exception as runtime excetpions . <nl> + * / <nl> + private static Object invoke ( Method method , Object obj , Object . . . args ) { <nl> + try { <nl> + return method . invoke ( obj , args ) ; <nl> + } catch ( IllegalAccessException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t invoke method " + method . getName ( ) , e ) ; <nl> + } catch ( InvocationTargetException e ) { <nl> + throw new IllegalArgumentException ( " Can ' t invoke method " + method . getName ( ) , e ) ; <nl> + } <nl> + } <nl> + <nl> + / * * <nl> + * Invoke getConfiguration ( ) on JobContext . Works with both <nl> + * Hadoop 1 and 2 . <nl> + * / <nl> + public static Configuration getConfiguration ( JobContext context ) { <nl> + return ( Configuration ) invoke ( GET _ CONFIGURATION _ METHOD , context ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Invoke setStatus ( ) on TaskAttemptContext . Works with both <nl> + * Hadoop 1 and 2 . <nl> + * / <nl> + public static void setStatus ( TaskAttemptContext context , String status ) { <nl> + invoke ( SET _ STATUS _ METHOD , context , status ) ; <nl> + } <nl> + <nl> + / * * <nl> + * returns TaskAttemptContext . getTaskAttemptID ( ) . Works with both <nl> + * Hadoop 1 and 2 . <nl> + * / <nl> + public static TaskAttemptID getTaskAttemptID ( TaskAttemptContext taskContext ) { <nl> + return ( TaskAttemptID ) invoke ( GET _ TASK _ ATTEMPT _ ID , taskContext ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Invoke getCounter ( ) on TaskInputOutputContext . Works with both <nl> + * Hadoop 1 and 2 . <nl> + * / <nl> + public static Counter getCounter ( TaskInputOutputContext context , <nl> + String groupName , String counterName ) { <nl> + return ( Counter ) invoke ( GET _ COUNTER _ METHOD , context , groupName , counterName ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Invoke TaskAttemptContext . progress ( ) . Works with both <nl> + * Hadoop 1 and 2 . <nl> + * / <nl> + public static void progress ( TaskAttemptContext context ) { <nl> + invoke ( PROGRESS _ METHOD , context ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Increment the counter . Works with both Hadoop 1 and 2 <nl> + * / <nl> + public static void incrementCounter ( Counter counter , long increment ) { <nl> + / / incrementing a count might be called often . Might be affected by <nl> + / / cost of invoke ( ) . might be good candidate to handle in a shim . <nl> + / / ( TODO Raghu ) figure out how achieve such a build with maven <nl> + invoke ( INCREMENT _ COUNTER _ METHOD , counter , increment ) ; <nl> + } <nl> + } <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / Progressable . java b / src / java / org / apache / cassandra / hadoop / Progressable . java <nl> deleted file mode 100644 <nl> index ac253ef . . 0000000 <nl> - - - a / src / java / org / apache / cassandra / hadoop / Progressable . java <nl> + + + / dev / null <nl> @ @ - 1 , 50 + 0 , 0 @ @ <nl> - package org . apache . cassandra . hadoop ; <nl> - <nl> - / * <nl> - * <nl> - * Licensed to the Apache Software Foundation ( ASF ) under one <nl> - * or more contributor license agreements . See the NOTICE file <nl> - * distributed with this work for additional information <nl> - * regarding copyright ownership . The ASF licenses this file <nl> - * to you under the Apache License , Version 2 . 0 ( the <nl> - * " License " ) ; you may not use this file except in compliance <nl> - * with the License . You may obtain a copy of the License at <nl> - * <nl> - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> - * <nl> - * Unless required by applicable law or agreed to in writing , <nl> - * software distributed under the License is distributed on an <nl> - * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY <nl> - * KIND , either express or implied . See the License for the <nl> - * specific language governing permissions and limitations <nl> - * under the License . <nl> - * <nl> - * / <nl> - <nl> - import org . apache . hadoop . mapreduce . TaskAttemptContext ; <nl> - <nl> - <nl> - public class Progressable <nl> - { <nl> - private TaskAttemptContext context ; <nl> - private org . apache . hadoop . util . Progressable progressable ; <nl> - <nl> - public Progressable ( TaskAttemptContext context ) <nl> - { <nl> - this . context = context ; <nl> - } <nl> - <nl> - public Progressable ( org . apache . hadoop . util . Progressable progressable ) <nl> - { <nl> - this . progressable = progressable ; <nl> - } <nl> - <nl> - public void progress ( ) <nl> - { <nl> - if ( context ! = null ) <nl> - context . progress ( ) ; <nl> - else <nl> - progressable . progress ( ) ; <nl> - } <nl> - <nl> - } <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlOutputFormat . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlOutputFormat . java <nl> index 3cc0cd1 . . 7c89bef 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlOutputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlOutputFormat . java <nl> @ @ - 25 , 7 + 25 , 6 @ @ import java . util . Map ; <nl> <nl> import org . apache . cassandra . hadoop . AbstractColumnFamilyOutputFormat ; <nl> import org . apache . cassandra . hadoop . ConfigHelper ; <nl> - import org . apache . cassandra . hadoop . Progressable ; <nl> import org . apache . hadoop . mapreduce . * ; <nl> <nl> / * * <nl> @ @ - 59 , 7 + 58 , 7 @ @ public class CqlOutputFormat extends AbstractColumnFamilyOutputFormat < Map < String <nl> @ Deprecated <nl> public CqlRecordWriter getRecordWriter ( org . apache . hadoop . fs . FileSystem filesystem , org . apache . hadoop . mapred . JobConf job , String name , org . apache . hadoop . util . Progressable progress ) throws IOException <nl> { <nl> - return new CqlRecordWriter ( job , new Progressable ( progress ) ) ; <nl> + return new CqlRecordWriter ( job , progress ) ; <nl> } <nl> <nl> / * * <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java <nl> index 6f4478e . . 96f2f94 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java <nl> @ @ - 21 , 7 + 21 , 7 @ @ import java . io . IOException ; <nl> import java . nio . ByteBuffer ; <nl> import java . util . Map ; <nl> <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> + import org . apache . cassandra . hadoop . HadoopCompat ; <nl> import org . apache . cassandra . hadoop . AbstractColumnFamilyInputFormat ; <nl> import org . apache . cassandra . hadoop . ReporterWrapper ; <nl> import org . apache . hadoop . mapred . InputSplit ; <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java <nl> index f712584 . . 9d60485 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java <nl> @ @ - 26 , 7 + 26 , 7 @ @ import java . util . * ; <nl> <nl> import com . google . common . collect . AbstractIterator ; <nl> import com . google . common . collect . Iterables ; <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> + import org . apache . cassandra . hadoop . HadoopCompat ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> index 826fc0d . . a030462 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> @ @ - 23 , 7 + 23 , 8 @ @ import java . nio . ByteBuffer ; <nl> import java . util . * ; <nl> import java . util . concurrent . ConcurrentHashMap ; <nl> <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> + import org . apache . cassandra . hadoop . HadoopCompat ; <nl> + import org . apache . hadoop . util . Progressable ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> @ @ - 37 , 7 + 38 , 6 @ @ import org . apache . cassandra . exceptions . ConfigurationException ; <nl> import org . apache . cassandra . exceptions . SyntaxException ; <nl> import org . apache . cassandra . hadoop . AbstractColumnFamilyRecordWriter ; <nl> import org . apache . cassandra . hadoop . ConfigHelper ; <nl> - import org . apache . cassandra . hadoop . Progressable ; <nl> import org . apache . cassandra . thrift . * ; <nl> import org . apache . cassandra . utils . ByteBufferUtil ; <nl> import org . apache . cassandra . utils . FBUtilities ; <nl> @ @ - 86 , 7 + 86 , 7 @ @ class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , ByteB <nl> CqlRecordWriter ( TaskAttemptContext context ) throws IOException <nl> { <nl> this ( HadoopCompat . getConfiguration ( context ) ) ; <nl> - this . progressable = new Progressable ( context ) ; <nl> + this . context = context ; <nl> } <nl> <nl> CqlRecordWriter ( Configuration conf , Progressable progressable ) throws IOException <nl> @ @ - 181 , 7 + 181 , 11 @ @ class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , ByteB <nl> allValues . add ( keyColumns . get ( column ) ) ; <nl> <nl> client . put ( allValues ) ; <nl> - progressable . progress ( ) ; <nl> + <nl> + if ( progressable ! = null ) <nl> + progressable . progress ( ) ; <nl> + if ( context ! = null ) <nl> + HadoopCompat . progress ( context ) ; <nl> } <nl> <nl> / * * <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java <nl> index 56f66bb . . 14d30d5 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java <nl> @ @ - 22 , 7 + 22 , 7 @ @ import java . nio . ByteBuffer ; <nl> import java . nio . charset . CharacterCodingException ; <nl> import java . util . * ; <nl> <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> + import org . apache . cassandra . hadoop . HadoopCompat ; <nl> import org . apache . cassandra . db . Cell ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CqlStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CqlStorage . java <nl> index b349cf7 . . efa6d34 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / pig / CqlStorage . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / pig / CqlStorage . java <nl> @ @ - 22 , 7 + 22 , 6 @ @ import java . nio . ByteBuffer ; <nl> import java . nio . charset . CharacterCodingException ; <nl> import java . util . * ; <nl> <nl> - import com . twitter . elephantbird . util . HadoopCompat ; <nl> import org . apache . cassandra . config . CFMetaData ; <nl> import org . apache . cassandra . config . ColumnDefinition ; <nl> import org . apache . cassandra . db . composites . CellNames ;
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> index 2061553 . . 56753c2 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> @ @ - 295 , 9 + 295 , 9 @ @ public class SSTableReader extends SSTable implements Comparable < SSTableReader > <nl> break ; <nl> <nl> boolean shouldAddEntry = indexSummary . shouldAddEntry ( ) ; <nl> - ByteBuffer key = ( ByteBuffer ) ( ( shouldAddEntry | | cacheLoading | | recreatebloom ) <nl> + ByteBuffer key = ( shouldAddEntry | | cacheLoading | | recreatebloom ) <nl> ? ByteBufferUtil . readWithShortLength ( input ) <nl> - : ByteBufferUtil . skipShortLength ( input ) ) ; <nl> + : ByteBufferUtil . skipShortLength ( input ) ; <nl> long dataPosition = input . readLong ( ) ; <nl> if ( key ! = null ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / utils / ByteBufferUtil . java b / src / java / org / apache / cassandra / utils / ByteBufferUtil . java <nl> index 6bbb04e . . 169eb7b 100644 <nl> - - - a / src / java / org / apache / cassandra / utils / ByteBufferUtil . java <nl> + + + b / src / java / org / apache / cassandra / utils / ByteBufferUtil . java <nl> @ @ - 305 , 7 + 305 , 7 @ @ public class ByteBufferUtil <nl> * @ return null <nl> * @ throws IOException if an I / O error occurs . <nl> * / <nl> - public static void skipShortLength ( DataInput in ) throws IOException <nl> + public static ByteBuffer skipShortLength ( DataInput in ) throws IOException <nl> { <nl> int skip = readShortLength ( in ) ; <nl> while ( skip > 0 ) <nl> @ @ - 314 , 6 + 314 , 7 @ @ public class ByteBufferUtil <nl> if ( skipped = = 0 ) throw new EOFException ( ) ; <nl> skip - = skipped ; <nl> } <nl> + return null ; <nl> } <nl> <nl> private static ByteBuffer read ( DataInput in , int length ) throws IOException

TEST DIFF:
diff - - git a / build . xml b / build . xml 
 index bb8673e . . 304b5fe 100644 
 - - - a / build . xml 
 + + + b / build . xml 
 @ @ - 374 , 7 + 374 , 6 @ @ 
 	 < exclusion groupId = " org . mortbay . jetty " artifactId = " servlet - api " / > 
 < / dependency > 
 < dependency groupId = " org . apache . hadoop " artifactId = " hadoop - minicluster " version = " 1 . 0 . 3 " / > 
 - < dependency groupId = " com . twitter . elephantbird " artifactId = " elephant - bird - hadoop - compat " version = " 4 . 3 " / > 
 < dependency groupId = " org . apache . pig " artifactId = " pig " version = " 0 . 11 . 1 " / > 
 < dependency groupId = " net . java . dev . jna " artifactId = " jna " version = " 4 . 0 . 0 " / > 
 
 @ @ - 418 , 7 + 417 , 6 @ @ 
 < dependency groupId = " org . apache . rat " artifactId = " apache - rat " / > 
 < dependency groupId = " org . apache . hadoop " artifactId = " hadoop - core " / > 
 	 < dependency groupId = " org . apache . hadoop " artifactId = " hadoop - minicluster " / > 
 - < dependency groupId = " com . twitter . elephantbird " artifactId = " elephant - bird - hadoop - compat " / > 
 < dependency groupId = " org . apache . pig " artifactId = " pig " / > 
 	 < dependency groupId = " com . google . code . findbugs " artifactId = " jsr305 " / > 
 < / artifact : pom > 
 @ @ - 485 , 7 + 483 , 6 @ @ 
 < ! - - don ' t need hadoop classes to run , but if you use the hadoop stuff - - > 
 < dependency groupId = " org . apache . hadoop " artifactId = " hadoop - core " optional = " true " / > 
 < dependency groupId = " org . apache . hadoop " artifactId = " hadoop - minicluster " optional = " true " / > 
 - < dependency groupId = " com . twitter . elephantbird " artifactId = " elephant - bird - hadoop - compat " optional = " true " / > 
 < dependency groupId = " org . apache . pig " artifactId = " pig " optional = " true " / > 
 
 < ! - - don ' t need jna to run , but nice to have - - > 
 diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 index 760193f . . cb106e9 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 @ @ - 29 , 7 + 29 , 6 @ @ import java . util . concurrent . TimeUnit ; 
 
 import com . google . common . collect . ImmutableList ; 
 import com . google . common . collect . Lists ; 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyOutputFormat . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyOutputFormat . java 
 index a3c4234 . . 3041829 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyOutputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyOutputFormat . java 
 @ @ - 22 , 7 + 22 , 6 @ @ import java . io . IOException ; 
 import java . util . HashMap ; 
 import java . util . Map ; 
 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java 
 index 1956262 . . 501ca65 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java 
 + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyRecordWriter . java 
 @ @ - 32 , 6 + 32 , 7 @ @ import org . apache . hadoop . conf . Configuration ; 
 import org . apache . hadoop . mapreduce . RecordWriter ; 
 import org . apache . hadoop . mapreduce . TaskAttemptContext ; 
 import org . apache . thrift . transport . TTransport ; 
 + import org . apache . hadoop . util . Progressable ; 
 
 
 / * * 
 @ @ - 67 , 6 + 68 , 7 @ @ public abstract class AbstractColumnFamilyRecordWriter < K , Y > extends RecordWrite 
 
 protected final ConsistencyLevel consistencyLevel ; 
 protected Progressable progressable ; 
 + protected TaskAttemptContext context ; 
 
 protected AbstractColumnFamilyRecordWriter ( Configuration conf ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / hadoop / BulkOutputFormat . java b / src / java / org / apache / cassandra / hadoop / BulkOutputFormat . java 
 index 566d5ee . . c3d8e05 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / BulkOutputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / BulkOutputFormat . java 
 @ @ - 22 , 7 + 22 , 6 @ @ import java . io . IOException ; 
 import java . nio . ByteBuffer ; 
 import java . util . List ; 
 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 import org . apache . cassandra . thrift . Mutation ; 
 import org . apache . hadoop . conf . Configuration ; 
 import org . apache . hadoop . mapreduce . * ; 
 @ @ - 61 , 7 + 60 , 7 @ @ public class BulkOutputFormat extends OutputFormat < ByteBuffer , List < Mutation > > 
 @ Deprecated 
 public BulkRecordWriter getRecordWriter ( org . apache . hadoop . fs . FileSystem filesystem , org . apache . hadoop . mapred . JobConf job , String name , org . apache . hadoop . util . Progressable progress ) throws IOException 
 { 
 - return new BulkRecordWriter ( job , new Progressable ( progress ) ) ; 
 + return new BulkRecordWriter ( job , progress ) ; 
 } 
 
 @ Override 
 diff - - git a / src / java / org / apache / cassandra / hadoop / BulkRecordWriter . java b / src / java / org / apache / cassandra / hadoop / BulkRecordWriter . java 
 index a8e2e13 . . d6136a2 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / BulkRecordWriter . java 
 + + + b / src / java / org / apache / cassandra / hadoop / BulkRecordWriter . java 
 @ @ - 28 , 7 + 28 , 6 @ @ import java . util . concurrent . Future ; 
 import java . util . concurrent . TimeUnit ; 
 import java . util . concurrent . TimeoutException ; 
 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 @ @ - 53 , 6 + 52 , 7 @ @ import org . apache . thrift . transport . TFramedTransport ; 
 import org . apache . thrift . transport . TSocket ; 
 import org . apache . thrift . transport . TTransport ; 
 import org . apache . thrift . transport . TTransportException ; 
 + import org . apache . hadoop . util . Progressable ; 
 
 final class BulkRecordWriter extends RecordWriter < ByteBuffer , List < Mutation > > 
 implements org . apache . hadoop . mapred . RecordWriter < ByteBuffer , List < Mutation > > 
 @ @ - 67 , 6 + 67 , 7 @ @ implements org . apache . hadoop . mapred . RecordWriter < ByteBuffer , List < Mutation > > 
 private SSTableLoader loader ; 
 private File outputdir ; 
 private Progressable progress ; 
 + private TaskAttemptContext context ; 
 private int maxFailures ; 
 
 private enum CFType 
 @ @ - 87 , 10 + 88 , 9 @ @ implements org . apache . hadoop . mapred . RecordWriter < ByteBuffer , List < Mutation > > 
 BulkRecordWriter ( TaskAttemptContext context ) 
 { 
 this ( HadoopCompat . getConfiguration ( context ) ) ; 
 - this . progress = new Progressable ( context ) ; 
 + this . context = context ; 
 } 
 
 - 
 BulkRecordWriter ( Configuration conf , Progressable progress ) 
 { 
 this ( conf ) ; 
 @ @ - 205 , 7 + 205 , 10 @ @ implements org . apache . hadoop . mapred . RecordWriter < ByteBuffer , List < Mutation > > 
 writer . addExpiringColumn ( mut . getColumn _ or _ supercolumn ( ) . column . name , mut . getColumn _ or _ supercolumn ( ) . column . value , mut . getColumn _ or _ supercolumn ( ) . column . timestamp , mut . getColumn _ or _ supercolumn ( ) . column . ttl , System . currentTimeMillis ( ) + ( ( long ) ( mut . getColumn _ or _ supercolumn ( ) . column . ttl ) * 1000 ) ) ; 
 } 
 } 
 - progress . progress ( ) ; 
 + if ( null ! = progress ) 
 + progress . progress ( ) ; 
 + if ( null ! = context ) 
 + HadoopCompat . progress ( context ) ; 
 } 
 } 
 @ Override 
 @ @ - 236 , 7 + 239 , 10 @ @ implements org . apache . hadoop . mapred . RecordWriter < ByteBuffer , List < Mutation > > 
 } 
 catch ( ExecutionException | TimeoutException te ) 
 { 
 - progress . progress ( ) ; 
 + if ( null ! = progress ) 
 + progress . progress ( ) ; 
 + if ( null ! = context ) 
 + HadoopCompat . progress ( context ) ; 
 } 
 catch ( InterruptedException e ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java 
 index 362cd70 . . a2c7a36 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java 
 @ @ - 21 , 7 + 21 , 6 @ @ import java . io . IOException ; 
 import java . nio . ByteBuffer ; 
 import java . util . * ; 
 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 import org . apache . cassandra . db . Cell ; 
 import org . apache . cassandra . db . composites . CellName ; 
 import org . apache . hadoop . conf . Configuration ; 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java 
 index 724ba7d . . 49aaf99 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java 
 @ @ - 51 , 7 + 51 , 7 @ @ public class ColumnFamilyOutputFormat extends AbstractColumnFamilyOutputFormat < B 
 @ Deprecated 
 public ColumnFamilyRecordWriter getRecordWriter ( org . apache . hadoop . fs . FileSystem filesystem , org . apache . hadoop . mapred . JobConf job , String name , org . apache . hadoop . util . Progressable progress ) 
 { 
 - return new ColumnFamilyRecordWriter ( job , new Progressable ( progress ) ) ; 
 + return new ColumnFamilyRecordWriter ( job , progress ) ; 
 } 
 
 / * * 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java 
 index ef883fd . . f6d2b7e 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java 
 @ @ - 24 , 7 + 24 , 6 @ @ import java . nio . ByteBuffer ; 
 import java . util . * ; 
 
 import com . google . common . collect . * ; 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 import org . apache . cassandra . db . Cell ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java 
 index 0ae2a67 . . d6a873b 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java 
 @ @ - 23 , 7 + 23 , 6 @ @ import java . net . InetAddress ; 
 import java . nio . ByteBuffer ; 
 import java . util . * ; 
 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 import org . apache . cassandra . dht . Range ; 
 import org . apache . cassandra . dht . Token ; 
 import org . apache . cassandra . thrift . * ; 
 @ @ - 31 , 6 + 30 , 7 @ @ import org . apache . cassandra . utils . Pair ; 
 import org . apache . hadoop . conf . Configuration ; 
 import org . apache . hadoop . mapreduce . TaskAttemptContext ; 
 import org . apache . thrift . TException ; 
 + import org . apache . hadoop . util . Progressable ; 
 
 
 / * * 
 @ @ - 62 , 9 + 62 , 9 @ @ final class ColumnFamilyRecordWriter extends AbstractColumnFamilyRecordWriter < By 
 ColumnFamilyRecordWriter ( TaskAttemptContext context ) 
 { 
 this ( HadoopCompat . getConfiguration ( context ) ) ; 
 - this . progressable = new Progressable ( context ) ; 
 - } 
 + this . context = context ; 
 
 + } 
 ColumnFamilyRecordWriter ( Configuration conf , Progressable progressable ) 
 { 
 this ( conf ) ; 
 @ @ - 128 , 7 + 128 , 10 @ @ final class ColumnFamilyRecordWriter extends AbstractColumnFamilyRecordWriter < By 
 
 for ( Mutation amut : value ) 
 client . put ( Pair . create ( keybuff , amut ) ) ; 
 + if ( progressable ! = null ) 
 progressable . progress ( ) ; 
 + if ( context ! = null ) 
 + HadoopCompat . progress ( context ) ; 
 } 
 
 / * * 
 @ @ - 140 , 9 + 143 , 9 @ @ final class ColumnFamilyRecordWriter extends AbstractColumnFamilyRecordWriter < By 
 public final String columnFamily = ConfigHelper . getOutputColumnFamily ( conf ) ; 
 
 / * * 
 - * Constructs an { @ link RangeClient } for the given endpoints . 
 - * @ param endpoints the possible endpoints to execute the mutations on 
 - * / 
 + * Constructs an { @ link RangeClient } for the given endpoints . 
 + * @ param endpoints the possible endpoints to execute the mutations on 
 + * / 
 public RangeClient ( List < InetAddress > endpoints ) 
 { 
 super ( endpoints ) ; 
 diff - - git a / src / java / org / apache / cassandra / hadoop / HadoopCompat . java b / src / java / org / apache / cassandra / hadoop / HadoopCompat . java 
 new file mode 100644 
 index 0000000 . . f2f7033 
 - - - / dev / null 
 + + + b / src / java / org / apache / cassandra / hadoop / HadoopCompat . java 
 @ @ - 0 , 0 + 1 , 309 @ @ 
 + / * * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , software 
 + * distributed under the License is distributed on an " AS IS " BASIS , 
 + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 + * See the License for the specific language governing permissions and 
 + * limitations under the License . 
 + * / 
 + 
 + package org . apache . cassandra . hadoop ; 
 + 
 + import java . lang . reflect . Constructor ; 
 + import java . lang . reflect . Field ; 
 + import java . lang . reflect . InvocationTargetException ; 
 + import java . lang . reflect . Method ; 
 + 
 + import org . apache . hadoop . conf . Configuration ; 
 + import org . apache . hadoop . mapreduce . Counter ; 
 + import org . apache . hadoop . mapreduce . InputSplit ; 
 + import org . apache . hadoop . mapreduce . JobContext ; 
 + import org . apache . hadoop . mapreduce . JobID ; 
 + import org . apache . hadoop . mapreduce . MapContext ; 
 + import org . apache . hadoop . mapreduce . OutputCommitter ; 
 + import org . apache . hadoop . mapreduce . RecordReader ; 
 + import org . apache . hadoop . mapreduce . RecordWriter ; 
 + import org . apache . hadoop . mapreduce . StatusReporter ; 
 + import org . apache . hadoop . mapreduce . TaskAttemptContext ; 
 + import org . apache . hadoop . mapreduce . TaskAttemptID ; 
 + import org . apache . hadoop . mapreduce . TaskInputOutputContext ; 
 + 
 + / * 
 + * This is based on ContextFactory . java from hadoop - 2 . 0 . x sources . 
 + * / 
 + 
 + / * * 
 + * Utility methods to allow applications to deal with inconsistencies between 
 + * MapReduce Context Objects API between Hadoop 1 . x and 2 . x . 
 + * / 
 + public class HadoopCompat { 
 + 
 + private static final boolean useV21 ; 
 + 
 + private static final Constructor < ? > JOB _ CONTEXT _ CONSTRUCTOR ; 
 + private static final Constructor < ? > TASK _ CONTEXT _ CONSTRUCTOR ; 
 + private static final Constructor < ? > MAP _ CONTEXT _ CONSTRUCTOR ; 
 + private static final Constructor < ? > GENERIC _ COUNTER _ CONSTRUCTOR ; 
 + 
 + private static final Field READER _ FIELD ; 
 + private static final Field WRITER _ FIELD ; 
 + 
 + private static final Method GET _ CONFIGURATION _ METHOD ; 
 + private static final Method SET _ STATUS _ METHOD ; 
 + private static final Method GET _ COUNTER _ METHOD ; 
 + private static final Method INCREMENT _ COUNTER _ METHOD ; 
 + private static final Method GET _ TASK _ ATTEMPT _ ID ; 
 + private static final Method PROGRESS _ METHOD ; 
 + 
 + static { 
 + boolean v21 = true ; 
 + final String PACKAGE = " org . apache . hadoop . mapreduce " ; 
 + try { 
 + Class . forName ( PACKAGE + " . task . JobContextImpl " ) ; 
 + } catch ( ClassNotFoundException cnfe ) { 
 + v21 = false ; 
 + } 
 + useV21 = v21 ; 
 + Class < ? > jobContextCls ; 
 + Class < ? > taskContextCls ; 
 + Class < ? > taskIOContextCls ; 
 + Class < ? > mapContextCls ; 
 + Class < ? > genericCounterCls ; 
 + try { 
 + if ( v21 ) { 
 + jobContextCls = 
 + Class . forName ( PACKAGE + " . task . JobContextImpl " ) ; 
 + taskContextCls = 
 + Class . forName ( PACKAGE + " . task . TaskAttemptContextImpl " ) ; 
 + taskIOContextCls = 
 + Class . forName ( PACKAGE + " . task . TaskInputOutputContextImpl " ) ; 
 + mapContextCls = Class . forName ( PACKAGE + " . task . MapContextImpl " ) ; 
 + genericCounterCls = Class . forName ( PACKAGE + " . counters . GenericCounter " ) ; 
 + } else { 
 + jobContextCls = 
 + Class . forName ( PACKAGE + " . JobContext " ) ; 
 + taskContextCls = 
 + Class . forName ( PACKAGE + " . TaskAttemptContext " ) ; 
 + taskIOContextCls = 
 + Class . forName ( PACKAGE + " . TaskInputOutputContext " ) ; 
 + mapContextCls = Class . forName ( PACKAGE + " . MapContext " ) ; 
 + genericCounterCls = 
 + Class . forName ( " org . apache . hadoop . mapred . Counters $ Counter " ) ; 
 + 
 + } 
 + } catch ( ClassNotFoundException e ) { 
 + throw new IllegalArgumentException ( " Can ' t find class " , e ) ; 
 + } 
 + try { 
 + JOB _ CONTEXT _ CONSTRUCTOR = 
 + jobContextCls . getConstructor ( Configuration . class , JobID . class ) ; 
 + JOB _ CONTEXT _ CONSTRUCTOR . setAccessible ( true ) ; 
 + TASK _ CONTEXT _ CONSTRUCTOR = 
 + taskContextCls . getConstructor ( Configuration . class , 
 + TaskAttemptID . class ) ; 
 + TASK _ CONTEXT _ CONSTRUCTOR . setAccessible ( true ) ; 
 + GENERIC _ COUNTER _ CONSTRUCTOR = 
 + genericCounterCls . getDeclaredConstructor ( String . class , 
 + String . class , 
 + Long . TYPE ) ; 
 + GENERIC _ COUNTER _ CONSTRUCTOR . setAccessible ( true ) ; 
 + 
 + if ( useV21 ) { 
 + MAP _ CONTEXT _ CONSTRUCTOR = 
 + mapContextCls . getDeclaredConstructor ( Configuration . class , 
 + TaskAttemptID . class , 
 + RecordReader . class , 
 + RecordWriter . class , 
 + OutputCommitter . class , 
 + StatusReporter . class , 
 + InputSplit . class ) ; 
 + Method get _ counter ; 
 + try { 
 + get _ counter = Class . forName ( PACKAGE + " . TaskAttemptContext " ) . getMethod ( " getCounter " , String . class , 
 + String . class ) ; 
 + } catch ( Exception e ) { 
 + get _ counter = Class . forName ( PACKAGE + " . TaskInputOutputContext " ) . getMethod ( " getCounter " , 
 + String . class , String . class ) ; 
 + } 
 + GET _ COUNTER _ METHOD = get _ counter ; 
 + } else { 
 + MAP _ CONTEXT _ CONSTRUCTOR = 
 + mapContextCls . getConstructor ( Configuration . class , 
 + TaskAttemptID . class , 
 + RecordReader . class , 
 + RecordWriter . class , 
 + OutputCommitter . class , 
 + StatusReporter . class , 
 + InputSplit . class ) ; 
 + GET _ COUNTER _ METHOD = Class . forName ( PACKAGE + " . TaskInputOutputContext " ) 
 + . getMethod ( " getCounter " , String . class , String . class ) ; 
 + } 
 + MAP _ CONTEXT _ CONSTRUCTOR . setAccessible ( true ) ; 
 + READER _ FIELD = mapContextCls . getDeclaredField ( " reader " ) ; 
 + READER _ FIELD . setAccessible ( true ) ; 
 + WRITER _ FIELD = taskIOContextCls . getDeclaredField ( " output " ) ; 
 + WRITER _ FIELD . setAccessible ( true ) ; 
 + GET _ CONFIGURATION _ METHOD = Class . forName ( PACKAGE + " . JobContext " ) 
 + . getMethod ( " getConfiguration " ) ; 
 + SET _ STATUS _ METHOD = Class . forName ( PACKAGE + " . TaskAttemptContext " ) 
 + . getMethod ( " setStatus " , String . class ) ; 
 + GET _ TASK _ ATTEMPT _ ID = Class . forName ( PACKAGE + " . TaskAttemptContext " ) 
 + . getMethod ( " getTaskAttemptID " ) ; 
 + INCREMENT _ COUNTER _ METHOD = Class . forName ( PACKAGE + " . Counter " ) 
 + . getMethod ( " increment " , Long . TYPE ) ; 
 + PROGRESS _ METHOD = Class . forName ( PACKAGE + " . TaskAttemptContext " ) 
 + . getMethod ( " progress " ) ; 
 + 
 + } catch ( SecurityException e ) { 
 + throw new IllegalArgumentException ( " Can ' t run constructor " , e ) ; 
 + } catch ( NoSuchMethodException e ) { 
 + throw new IllegalArgumentException ( " Can ' t find constructor " , e ) ; 
 + } catch ( NoSuchFieldException e ) { 
 + throw new IllegalArgumentException ( " Can ' t find field " , e ) ; 
 + } catch ( ClassNotFoundException e ) { 
 + throw new IllegalArgumentException ( " Can ' t find class " , e ) ; 
 + } 
 + } 
 + 
 + / * * 
 + * True if runtime Hadoop version is 2 . x , false otherwise . 
 + * / 
 + public static boolean isVersion2x ( ) { 
 + return useV21 ; 
 + } 
 + 
 + private static Object newInstance ( Constructor < ? > constructor , Object . . . args ) { 
 + try { 
 + return constructor . newInstance ( args ) ; 
 + } catch ( InstantiationException e ) { 
 + throw new IllegalArgumentException ( " Can ' t instantiate " + constructor , e ) ; 
 + } catch ( IllegalAccessException e ) { 
 + throw new IllegalArgumentException ( " Can ' t instantiate " + constructor , e ) ; 
 + } catch ( InvocationTargetException e ) { 
 + throw new IllegalArgumentException ( " Can ' t instantiate " + constructor , e ) ; 
 + } 
 + } 
 + 
 + / * * 
 + * Creates JobContext from a JobConf and jobId using the correct constructor 
 + * for based on Hadoop version . < code > jobId < / code > could be null . 
 + * / 
 + public static JobContext newJobContext ( Configuration conf , JobID jobId ) { 
 + return ( JobContext ) newInstance ( JOB _ CONTEXT _ CONSTRUCTOR , conf , jobId ) ; 
 + } 
 + 
 + / * * 
 + * Creates TaskAttempContext from a JobConf and jobId using the correct 
 + * constructor for based on Hadoop version . 
 + * / 
 + public static TaskAttemptContext newTaskAttemptContext ( 
 + Configuration conf , TaskAttemptID taskAttemptId ) { 
 + return ( TaskAttemptContext ) 
 + newInstance ( TASK _ CONTEXT _ CONSTRUCTOR , conf , taskAttemptId ) ; 
 + } 
 + 
 + / * * 
 + * Instantiates MapContext under Hadoop 1 and MapContextImpl under Hadoop 2 . 
 + * / 
 + public static MapContext newMapContext ( Configuration conf , 
 + TaskAttemptID taskAttemptID , 
 + RecordReader recordReader , 
 + RecordWriter recordWriter , 
 + OutputCommitter outputCommitter , 
 + StatusReporter statusReporter , 
 + InputSplit inputSplit ) { 
 + return ( MapContext ) newInstance ( MAP _ CONTEXT _ CONSTRUCTOR , 
 + conf , taskAttemptID , recordReader , recordWriter , outputCommitter , 
 + statusReporter , inputSplit ) ; 
 + } 
 + 
 + / * * 
 + * @ return with Hadoop 2 : < code > new GenericCounter ( args ) < / code > , < br > 
 + * with Hadoop 1 : < code > new Counter ( args ) < / code > 
 + * / 
 + public static Counter newGenericCounter ( String name , String displayName , long value ) { 
 + try { 
 + return ( Counter ) 
 + GENERIC _ COUNTER _ CONSTRUCTOR . newInstance ( name , displayName , value ) ; 
 + } catch ( InstantiationException e ) { 
 + throw new IllegalArgumentException ( " Can ' t instantiate Counter " , e ) ; 
 + } catch ( IllegalAccessException e ) { 
 + throw new IllegalArgumentException ( " Can ' t instantiate Counter " , e ) ; 
 + } catch ( InvocationTargetException e ) { 
 + throw new IllegalArgumentException ( " Can ' t instantiate Counter " , e ) ; 
 + } 
 + } 
 + 
 + / * * 
 + * Invokes a method and rethrows any exception as runtime excetpions . 
 + * / 
 + private static Object invoke ( Method method , Object obj , Object . . . args ) { 
 + try { 
 + return method . invoke ( obj , args ) ; 
 + } catch ( IllegalAccessException e ) { 
 + throw new IllegalArgumentException ( " Can ' t invoke method " + method . getName ( ) , e ) ; 
 + } catch ( InvocationTargetException e ) { 
 + throw new IllegalArgumentException ( " Can ' t invoke method " + method . getName ( ) , e ) ; 
 + } 
 + } 
 + 
 + / * * 
 + * Invoke getConfiguration ( ) on JobContext . Works with both 
 + * Hadoop 1 and 2 . 
 + * / 
 + public static Configuration getConfiguration ( JobContext context ) { 
 + return ( Configuration ) invoke ( GET _ CONFIGURATION _ METHOD , context ) ; 
 + } 
 + 
 + / * * 
 + * Invoke setStatus ( ) on TaskAttemptContext . Works with both 
 + * Hadoop 1 and 2 . 
 + * / 
 + public static void setStatus ( TaskAttemptContext context , String status ) { 
 + invoke ( SET _ STATUS _ METHOD , context , status ) ; 
 + } 
 + 
 + / * * 
 + * returns TaskAttemptContext . getTaskAttemptID ( ) . Works with both 
 + * Hadoop 1 and 2 . 
 + * / 
 + public static TaskAttemptID getTaskAttemptID ( TaskAttemptContext taskContext ) { 
 + return ( TaskAttemptID ) invoke ( GET _ TASK _ ATTEMPT _ ID , taskContext ) ; 
 + } 
 + 
 + / * * 
 + * Invoke getCounter ( ) on TaskInputOutputContext . Works with both 
 + * Hadoop 1 and 2 . 
 + * / 
 + public static Counter getCounter ( TaskInputOutputContext context , 
 + String groupName , String counterName ) { 
 + return ( Counter ) invoke ( GET _ COUNTER _ METHOD , context , groupName , counterName ) ; 
 + } 
 + 
 + / * * 
 + * Invoke TaskAttemptContext . progress ( ) . Works with both 
 + * Hadoop 1 and 2 . 
 + * / 
 + public static void progress ( TaskAttemptContext context ) { 
 + invoke ( PROGRESS _ METHOD , context ) ; 
 + } 
 + 
 + / * * 
 + * Increment the counter . Works with both Hadoop 1 and 2 
 + * / 
 + public static void incrementCounter ( Counter counter , long increment ) { 
 + / / incrementing a count might be called often . Might be affected by 
 + / / cost of invoke ( ) . might be good candidate to handle in a shim . 
 + / / ( TODO Raghu ) figure out how achieve such a build with maven 
 + invoke ( INCREMENT _ COUNTER _ METHOD , counter , increment ) ; 
 + } 
 + } 
 diff - - git a / src / java / org / apache / cassandra / hadoop / Progressable . java b / src / java / org / apache / cassandra / hadoop / Progressable . java 
 deleted file mode 100644 
 index ac253ef . . 0000000 
 - - - a / src / java / org / apache / cassandra / hadoop / Progressable . java 
 + + + / dev / null 
 @ @ - 1 , 50 + 0 , 0 @ @ 
 - package org . apache . cassandra . hadoop ; 
 - 
 - / * 
 - * 
 - * Licensed to the Apache Software Foundation ( ASF ) under one 
 - * or more contributor license agreements . See the NOTICE file 
 - * distributed with this work for additional information 
 - * regarding copyright ownership . The ASF licenses this file 
 - * to you under the Apache License , Version 2 . 0 ( the 
 - * " License " ) ; you may not use this file except in compliance 
 - * with the License . You may obtain a copy of the License at 
 - * 
 - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 - * 
 - * Unless required by applicable law or agreed to in writing , 
 - * software distributed under the License is distributed on an 
 - * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY 
 - * KIND , either express or implied . See the License for the 
 - * specific language governing permissions and limitations 
 - * under the License . 
 - * 
 - * / 
 - 
 - import org . apache . hadoop . mapreduce . TaskAttemptContext ; 
 - 
 - 
 - public class Progressable 
 - { 
 - private TaskAttemptContext context ; 
 - private org . apache . hadoop . util . Progressable progressable ; 
 - 
 - public Progressable ( TaskAttemptContext context ) 
 - { 
 - this . context = context ; 
 - } 
 - 
 - public Progressable ( org . apache . hadoop . util . Progressable progressable ) 
 - { 
 - this . progressable = progressable ; 
 - } 
 - 
 - public void progress ( ) 
 - { 
 - if ( context ! = null ) 
 - context . progress ( ) ; 
 - else 
 - progressable . progress ( ) ; 
 - } 
 - 
 - } 
 diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlOutputFormat . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlOutputFormat . java 
 index 3cc0cd1 . . 7c89bef 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlOutputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlOutputFormat . java 
 @ @ - 25 , 7 + 25 , 6 @ @ import java . util . Map ; 
 
 import org . apache . cassandra . hadoop . AbstractColumnFamilyOutputFormat ; 
 import org . apache . cassandra . hadoop . ConfigHelper ; 
 - import org . apache . cassandra . hadoop . Progressable ; 
 import org . apache . hadoop . mapreduce . * ; 
 
 / * * 
 @ @ - 59 , 7 + 58 , 7 @ @ public class CqlOutputFormat extends AbstractColumnFamilyOutputFormat < Map < String 
 @ Deprecated 
 public CqlRecordWriter getRecordWriter ( org . apache . hadoop . fs . FileSystem filesystem , org . apache . hadoop . mapred . JobConf job , String name , org . apache . hadoop . util . Progressable progress ) throws IOException 
 { 
 - return new CqlRecordWriter ( job , new Progressable ( progress ) ) ; 
 + return new CqlRecordWriter ( job , progress ) ; 
 } 
 
 / * * 
 diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java 
 index 6f4478e . . 96f2f94 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java 
 @ @ - 21 , 7 + 21 , 7 @ @ import java . io . IOException ; 
 import java . nio . ByteBuffer ; 
 import java . util . Map ; 
 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 + import org . apache . cassandra . hadoop . HadoopCompat ; 
 import org . apache . cassandra . hadoop . AbstractColumnFamilyInputFormat ; 
 import org . apache . cassandra . hadoop . ReporterWrapper ; 
 import org . apache . hadoop . mapred . InputSplit ; 
 diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java 
 index f712584 . . 9d60485 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java 
 + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java 
 @ @ - 26 , 7 + 26 , 7 @ @ import java . util . * ; 
 
 import com . google . common . collect . AbstractIterator ; 
 import com . google . common . collect . Iterables ; 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 + import org . apache . cassandra . hadoop . HadoopCompat ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 index 826fc0d . . a030462 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 @ @ - 23 , 7 + 23 , 8 @ @ import java . nio . ByteBuffer ; 
 import java . util . * ; 
 import java . util . concurrent . ConcurrentHashMap ; 
 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 + import org . apache . cassandra . hadoop . HadoopCompat ; 
 + import org . apache . hadoop . util . Progressable ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 @ @ - 37 , 7 + 38 , 6 @ @ import org . apache . cassandra . exceptions . ConfigurationException ; 
 import org . apache . cassandra . exceptions . SyntaxException ; 
 import org . apache . cassandra . hadoop . AbstractColumnFamilyRecordWriter ; 
 import org . apache . cassandra . hadoop . ConfigHelper ; 
 - import org . apache . cassandra . hadoop . Progressable ; 
 import org . apache . cassandra . thrift . * ; 
 import org . apache . cassandra . utils . ByteBufferUtil ; 
 import org . apache . cassandra . utils . FBUtilities ; 
 @ @ - 86 , 7 + 86 , 7 @ @ class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , ByteB 
 CqlRecordWriter ( TaskAttemptContext context ) throws IOException 
 { 
 this ( HadoopCompat . getConfiguration ( context ) ) ; 
 - this . progressable = new Progressable ( context ) ; 
 + this . context = context ; 
 } 
 
 CqlRecordWriter ( Configuration conf , Progressable progressable ) throws IOException 
 @ @ - 181 , 7 + 181 , 11 @ @ class CqlRecordWriter extends AbstractColumnFamilyRecordWriter < Map < String , ByteB 
 allValues . add ( keyColumns . get ( column ) ) ; 
 
 client . put ( allValues ) ; 
 - progressable . progress ( ) ; 
 + 
 + if ( progressable ! = null ) 
 + progressable . progress ( ) ; 
 + if ( context ! = null ) 
 + HadoopCompat . progress ( context ) ; 
 } 
 
 / * * 
 diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java 
 index 56f66bb . . 14d30d5 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java 
 + + + b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java 
 @ @ - 22 , 7 + 22 , 7 @ @ import java . nio . ByteBuffer ; 
 import java . nio . charset . CharacterCodingException ; 
 import java . util . * ; 
 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 + import org . apache . cassandra . hadoop . HadoopCompat ; 
 import org . apache . cassandra . db . Cell ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CqlStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CqlStorage . java 
 index b349cf7 . . efa6d34 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / pig / CqlStorage . java 
 + + + b / src / java / org / apache / cassandra / hadoop / pig / CqlStorage . java 
 @ @ - 22 , 7 + 22 , 6 @ @ import java . nio . ByteBuffer ; 
 import java . nio . charset . CharacterCodingException ; 
 import java . util . * ; 
 
 - import com . twitter . elephantbird . util . HadoopCompat ; 
 import org . apache . cassandra . config . CFMetaData ; 
 import org . apache . cassandra . config . ColumnDefinition ; 
 import org . apache . cassandra . db . composites . CellNames ;

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 index 2061553 . . 56753c2 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 @ @ - 295 , 9 + 295 , 9 @ @ public class SSTableReader extends SSTable implements Comparable < SSTableReader > 
 break ; 
 
 boolean shouldAddEntry = indexSummary . shouldAddEntry ( ) ; 
 - ByteBuffer key = ( ByteBuffer ) ( ( shouldAddEntry | | cacheLoading | | recreatebloom ) 
 + ByteBuffer key = ( shouldAddEntry | | cacheLoading | | recreatebloom ) 
 ? ByteBufferUtil . readWithShortLength ( input ) 
 - : ByteBufferUtil . skipShortLength ( input ) ) ; 
 + : ByteBufferUtil . skipShortLength ( input ) ; 
 long dataPosition = input . readLong ( ) ; 
 if ( key ! = null ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / utils / ByteBufferUtil . java b / src / java / org / apache / cassandra / utils / ByteBufferUtil . java 
 index 6bbb04e . . 169eb7b 100644 
 - - - a / src / java / org / apache / cassandra / utils / ByteBufferUtil . java 
 + + + b / src / java / org / apache / cassandra / utils / ByteBufferUtil . java 
 @ @ - 305 , 7 + 305 , 7 @ @ public class ByteBufferUtil 
 * @ return null 
 * @ throws IOException if an I / O error occurs . 
 * / 
 - public static void skipShortLength ( DataInput in ) throws IOException 
 + public static ByteBuffer skipShortLength ( DataInput in ) throws IOException 
 { 
 int skip = readShortLength ( in ) ; 
 while ( skip > 0 ) 
 @ @ - 314 , 6 + 314 , 7 @ @ public class ByteBufferUtil 
 if ( skipped = = 0 ) throw new EOFException ( ) ; 
 skip - = skipped ; 
 } 
 + return null ; 
 } 
 
 private static ByteBuffer read ( DataInput in , int length ) throws IOException
