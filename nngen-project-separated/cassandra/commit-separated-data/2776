BLEU SCORE: 0.012079826799606154

TEST MSG: Let scrub optionally skip broken counter partitions
GENERATED MSG: off - heap cache to use sun . misc . Unsafe instead of JNA

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 13b4c5b . . a1a58a3 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 3 + 1 , 7 @ @ <nl> + 2 . 0 . 6 <nl> + * Let scrub optionally skip broken counter partitions ( CASSANDRA - 5930 ) <nl> + <nl> + <nl> 2 . 0 . 5 <nl> * Reduce garbage generated by bloom filter lookups ( CASSANDRA - 6609 ) <nl> * Add ks . cf names to tombstone logging ( CASSANDRA - 6597 ) <nl> diff - - git a / NEWS . txt b / NEWS . txt <nl> index 92446c8 . . b21fbaa 100644 <nl> - - - a / NEWS . txt <nl> + + + b / NEWS . txt <nl> @ @ - 14 , 11 + 14 , 21 @ @ restore snapshots created with the previous major version using the <nl> using the provided ' sstableupgrade ' tool . <nl> <nl> <nl> + 2 . 0 . 6 <nl> + = = = = = <nl> + <nl> + New features <nl> + - - - - - - - - - - - - <nl> + - Scrub can now optionally skip corrupt counter partitions . Please note <nl> + that this will lead to the loss of all the counter updates in the skipped <nl> + partition . See the - - skip - corrupted option . <nl> + <nl> + <nl> 2 . 0 . 5 <nl> = = = = = <nl> <nl> New features <nl> - - - - - - - - - <nl> + - - - - - - - - - - - - <nl> - Batchlog replay can be , and is throttled by default now . <nl> See batchlog _ replay _ throttle _ in _ kb setting in cassandra . yaml . <nl> <nl> diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> index 8750026 . . 38d87db 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> @ @ - 1115 , 12 + 1115 , 12 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> CompactionManager . instance . performCleanup ( ColumnFamilyStore . this , renewer ) ; <nl> } <nl> <nl> - public void scrub ( boolean disableSnapshot ) throws ExecutionException , InterruptedException <nl> + public void scrub ( boolean disableSnapshot , boolean skipCorrupted ) throws ExecutionException , InterruptedException <nl> { <nl> / / skip snapshot creation during scrub , SEE JIRA 5891 <nl> if ( ! disableSnapshot ) <nl> snapshotWithoutFlush ( " pre - scrub - " + System . currentTimeMillis ( ) ) ; <nl> - CompactionManager . instance . performScrub ( ColumnFamilyStore . this ) ; <nl> + CompactionManager . instance . performScrub ( ColumnFamilyStore . this , skipCorrupted ) ; <nl> } <nl> <nl> public void sstablesRewrite ( boolean excludeCurrentVersion ) throws ExecutionException , InterruptedException <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> index 168ee02 . . 48900c8 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> @ @ - 227 , 13 + 227 , 13 @ @ public class CompactionManager implements CompactionManagerMBean <nl> executor . submit ( runnable ) . get ( ) ; <nl> } <nl> <nl> - public void performScrub ( ColumnFamilyStore cfStore ) throws InterruptedException , ExecutionException <nl> + public void performScrub ( ColumnFamilyStore cfStore , final boolean skipCorrupted ) throws InterruptedException , ExecutionException <nl> { <nl> performAllSSTableOperation ( cfStore , new AllSSTablesOperation ( ) <nl> { <nl> public void perform ( ColumnFamilyStore store , Iterable < SSTableReader > sstables ) throws IOException <nl> { <nl> - doScrub ( store , sstables ) ; <nl> + doScrub ( store , sstables , skipCorrupted ) ; <nl> } <nl> } ) ; <nl> } <nl> @ @ - 425 , 16 + 425 , 16 @ @ public class CompactionManager implements CompactionManagerMBean <nl> * <nl> * @ throws IOException <nl> * / <nl> - private void doScrub ( ColumnFamilyStore cfs , Iterable < SSTableReader > sstables ) throws IOException <nl> + private void doScrub ( ColumnFamilyStore cfs , Iterable < SSTableReader > sstables , boolean skipCorrupted ) throws IOException <nl> { <nl> assert ! cfs . isIndex ( ) ; <nl> for ( final SSTableReader sstable : sstables ) <nl> - scrubOne ( cfs , sstable ) ; <nl> + scrubOne ( cfs , sstable , skipCorrupted ) ; <nl> } <nl> <nl> - private void scrubOne ( ColumnFamilyStore cfs , SSTableReader sstable ) throws IOException <nl> + private void scrubOne ( ColumnFamilyStore cfs , SSTableReader sstable , boolean skipCorrupted ) throws IOException <nl> { <nl> - Scrubber scrubber = new Scrubber ( cfs , sstable ) ; <nl> + Scrubber scrubber = new Scrubber ( cfs , sstable , skipCorrupted ) ; <nl> <nl> CompactionInfo . Holder scrubInfo = scrubber . getScrubInfo ( ) ; <nl> metrics . beginCompaction ( scrubInfo ) ; <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / Scrubber . java b / src / java / org / apache / cassandra / db / compaction / Scrubber . java <nl> index 708e929 . . 820761c 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / Scrubber . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / Scrubber . java <nl> @ @ - 35 , 6 + 35 , 7 @ @ public class Scrubber implements Closeable <nl> public final ColumnFamilyStore cfs ; <nl> public final SSTableReader sstable ; <nl> public final File destination ; <nl> + public final boolean skipCorrupted ; <nl> <nl> private final CompactionController controller ; <nl> private final boolean isCommutative ; <nl> @ @ - 63 , 16 + 64 , 17 @ @ public class Scrubber implements Closeable <nl> } ; <nl> private final SortedSet < Row > outOfOrderRows = new TreeSet < > ( rowComparator ) ; <nl> <nl> - public Scrubber ( ColumnFamilyStore cfs , SSTableReader sstable ) throws IOException <nl> + public Scrubber ( ColumnFamilyStore cfs , SSTableReader sstable , boolean skipCorrupted ) throws IOException <nl> { <nl> - this ( cfs , sstable , new OutputHandler . LogOutput ( ) , false ) ; <nl> + this ( cfs , sstable , skipCorrupted , new OutputHandler . LogOutput ( ) , false ) ; <nl> } <nl> <nl> - public Scrubber ( ColumnFamilyStore cfs , SSTableReader sstable , OutputHandler outputHandler , boolean isOffline ) throws IOException <nl> + public Scrubber ( ColumnFamilyStore cfs , SSTableReader sstable , boolean skipCorrupted , OutputHandler outputHandler , boolean isOffline ) throws IOException <nl> { <nl> this . cfs = cfs ; <nl> this . sstable = sstable ; <nl> this . outputHandler = outputHandler ; <nl> + this . skipCorrupted = skipCorrupted ; <nl> <nl> / / Calculate the expected compacted filesize <nl> this . destination = cfs . directories . getDirectoryForNewSSTables ( ) ; <nl> @ @ - 166 , 7 + 168 , 9 @ @ public class Scrubber implements Closeable <nl> if ( ! sstable . descriptor . version . hasRowSizeAndColumnCount ) <nl> { <nl> dataSize = dataSizeFromIndex ; <nl> - outputHandler . debug ( String . format ( " row % s is % s bytes " , ByteBufferUtil . bytesToHex ( key . key ) , dataSize ) ) ; <nl> + / / avoid an NPE if key is null <nl> + String keyName = key = = null ? " ( unreadable key ) " : ByteBufferUtil . bytesToHex ( key . key ) ; <nl> + outputHandler . debug ( String . format ( " row % s is % s bytes " , keyName , dataSize ) ) ; <nl> } <nl> else <nl> { <nl> @ @ - 203 , 7 + 207 , 7 @ @ public class Scrubber implements Closeable <nl> catch ( Throwable th ) <nl> { <nl> throwIfFatal ( th ) ; <nl> - outputHandler . warn ( " Non - fatal error reading row ( stacktrace follows ) " , th ) ; <nl> + outputHandler . warn ( " Error reading row ( stacktrace follows ) : " , th ) ; <nl> writer . resetAndTruncate ( ) ; <nl> <nl> if ( currentIndexKey ! = null <nl> @ @ - 231 , 9 + 235 , 7 @ @ public class Scrubber implements Closeable <nl> catch ( Throwable th2 ) <nl> { <nl> throwIfFatal ( th2 ) ; <nl> - / / Skipping rows is dangerous for counters ( see CASSANDRA - 2759 ) <nl> - if ( isCommutative ) <nl> - throw new IOError ( th2 ) ; <nl> + throwIfCommutative ( key , th2 ) ; <nl> <nl> outputHandler . warn ( " Retry failed too . Skipping to next row ( retry ' s stacktrace follows ) " , th2 ) ; <nl> writer . resetAndTruncate ( ) ; <nl> @ @ - 243 , 11 + 245 , 9 @ @ public class Scrubber implements Closeable <nl> } <nl> else <nl> { <nl> - / / Skipping rows is dangerous for counters ( see CASSANDRA - 2759 ) <nl> - if ( isCommutative ) <nl> - throw new IOError ( th ) ; <nl> + throwIfCommutative ( key , th ) ; <nl> <nl> - outputHandler . warn ( " Row at " + dataStart + " is unreadable ; skipping to next " ) ; <nl> + outputHandler . warn ( " Row starting at position " + dataStart + " is unreadable ; skipping to next " ) ; <nl> if ( currentIndexKey ! = null ) <nl> dataFile . seek ( nextRowPositionFromIndex ) ; <nl> badRows + + ; <nl> @ @ - 324 , 6 + 324 , 19 @ @ public class Scrubber implements Closeable <nl> throw ( Error ) th ; <nl> } <nl> <nl> + private void throwIfCommutative ( DecoratedKey key , Throwable th ) <nl> + { <nl> + if ( isCommutative & & ! skipCorrupted ) <nl> + { <nl> + outputHandler . warn ( String . format ( " An error occurred while scrubbing the row with key ' % s ' . Skipping corrupt " + <nl> + " rows in counter tables will result in undercounts for the affected " + <nl> + " counters ( see CASSANDRA - 2759 for more details ) , so by default the scrub will " + <nl> + " stop at this point . If you would like to skip the row anyway and continue " + <nl> + " scrubbing , re - run the scrub with the - - skip - corrupted option . " , key ) ) ; <nl> + throw new IOError ( th ) ; <nl> + } <nl> + } <nl> + <nl> public void close ( ) <nl> { <nl> FileUtils . closeQuietly ( dataFile ) ; <nl> diff - - git a / src / java / org / apache / cassandra / service / StorageService . java b / src / java / org / apache / cassandra / service / StorageService . java <nl> index 700966f . . f46ae66 100644 <nl> - - - a / src / java / org / apache / cassandra / service / StorageService . java <nl> + + + b / src / java / org / apache / cassandra / service / StorageService . java <nl> @ @ - 2155 , 10 + 2155 , 10 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE <nl> } <nl> } <nl> <nl> - public void scrub ( boolean disableSnapshot , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException <nl> + public void scrub ( boolean disableSnapshot , boolean skipCorrupted , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException <nl> { <nl> for ( ColumnFamilyStore cfStore : getValidColumnFamilies ( false , false , keyspaceName , columnFamilies ) ) <nl> - cfStore . scrub ( disableSnapshot ) ; <nl> + cfStore . scrub ( disableSnapshot , skipCorrupted ) ; <nl> } <nl> <nl> public void upgradeSSTables ( String keyspaceName , boolean excludeCurrentVersion , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException <nl> diff - - git a / src / java / org / apache / cassandra / service / StorageServiceMBean . java b / src / java / org / apache / cassandra / service / StorageServiceMBean . java <nl> index df85901 . . d31e8b9 100644 <nl> - - - a / src / java / org / apache / cassandra / service / StorageServiceMBean . java <nl> + + + b / src / java / org / apache / cassandra / service / StorageServiceMBean . java <nl> @ @ - 231 , 7 + 231 , 7 @ @ public interface StorageServiceMBean extends NotificationEmitter <nl> * <nl> * Scrubbed CFs will be snapshotted first , if disableSnapshot is false <nl> * / <nl> - public void scrub ( boolean disableSnapshot , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException ; <nl> + public void scrub ( boolean disableSnapshot , boolean skipCorrupted , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException ; <nl> <nl> / * * <nl> * Rewrite all sstables to the latest version . <nl> diff - - git a / src / java / org / apache / cassandra / tools / NodeCmd . java b / src / java / org / apache / cassandra / tools / NodeCmd . java <nl> index 0cc7320 . . ab05d16 100644 <nl> - - - a / src / java / org / apache / cassandra / tools / NodeCmd . java <nl> + + + b / src / java / org / apache / cassandra / tools / NodeCmd . java <nl> @ @ - 74 , 6 + 74 , 8 @ @ public class NodeCmd <nl> private static final Pair < String , String > NO _ SNAPSHOT = Pair . create ( " ns " , " no - snapshot " ) ; <nl> private static final Pair < String , String > CFSTATS _ IGNORE _ OPT = Pair . create ( " i " , " ignore " ) ; <nl> private static final Pair < String , String > RESOLVE _ IP = Pair . create ( " r " , " resolve - ip " ) ; <nl> + private static final Pair < String , String > SCRUB _ SKIP _ CORRUPTED _ OPT = Pair . create ( " s " , " skip - corrupted " ) ; <nl> + <nl> <nl> private static final String DEFAULT _ HOST = " 127 . 0 . 0 . 1 " ; <nl> private static final int DEFAULT _ PORT = 7199 ; <nl> @ @ - 101 , 6 + 103 , 7 @ @ public class NodeCmd <nl> options . addOption ( NO _ SNAPSHOT , false , " disables snapshot creation for scrub " ) ; <nl> options . addOption ( CFSTATS _ IGNORE _ OPT , false , " ignore the supplied list of keyspace . columnfamiles in statistics " ) ; <nl> options . addOption ( RESOLVE _ IP , false , " show node domain names instead of IPs " ) ; <nl> + options . addOption ( SCRUB _ SKIP _ CORRUPTED _ OPT , false , " when scrubbing counter tables , skip corrupted rows " ) ; <nl> } <nl> <nl> public NodeCmd ( NodeProbe probe ) <nl> @ @ - 1562 , 7 + 1565 , 8 @ @ public class NodeCmd <nl> break ; <nl> case SCRUB : <nl> boolean disableSnapshot = cmd . hasOption ( NO _ SNAPSHOT . left ) ; <nl> - try { probe . scrub ( disableSnapshot , keyspace , columnFamilies ) ; } <nl> + boolean skipCorrupted = cmd . hasOption ( SCRUB _ SKIP _ CORRUPTED _ OPT . left ) ; <nl> + try { probe . scrub ( disableSnapshot , skipCorrupted , keyspace , columnFamilies ) ; } <nl> catch ( ExecutionException ee ) { err ( ee , " Error occurred while scrubbing keyspace " + keyspace ) ; } <nl> break ; <nl> case UPGRADESSTABLES : <nl> diff - - git a / src / java / org / apache / cassandra / tools / NodeProbe . java b / src / java / org / apache / cassandra / tools / NodeProbe . java <nl> index 1bb9d4e . . 0fbb12a 100644 <nl> - - - a / src / java / org / apache / cassandra / tools / NodeProbe . java <nl> + + + b / src / java / org / apache / cassandra / tools / NodeProbe . java <nl> @ @ - 190 , 9 + 190 , 9 @ @ public class NodeProbe <nl> ssProxy . forceKeyspaceCleanup ( keyspaceName , columnFamilies ) ; <nl> } <nl> <nl> - public void scrub ( boolean disableSnapshot , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException <nl> + public void scrub ( boolean disableSnapshot , boolean skipCorrupted , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException <nl> { <nl> - ssProxy . scrub ( disableSnapshot , keyspaceName , columnFamilies ) ; <nl> + ssProxy . scrub ( disableSnapshot , skipCorrupted , keyspaceName , columnFamilies ) ; <nl> } <nl> <nl> public void upgradeSSTables ( String keyspaceName , boolean excludeCurrentVersion , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException <nl> diff - - git a / src / java / org / apache / cassandra / tools / StandaloneScrubber . java b / src / java / org / apache / cassandra / tools / StandaloneScrubber . java <nl> index 00e0a5a . . 6556c3a 100644 <nl> - - - a / src / java / org / apache / cassandra / tools / StandaloneScrubber . java <nl> + + + b / src / java / org / apache / cassandra / tools / StandaloneScrubber . java <nl> @ @ - 49 , 6 + 49 , 7 @ @ public class StandaloneScrubber <nl> private static final String DEBUG _ OPTION = " debug " ; <nl> private static final String HELP _ OPTION = " help " ; <nl> private static final String MANIFEST _ CHECK _ OPTION = " manifest - check " ; <nl> + private static final String SKIP _ CORRUPTED _ OPTION = " skip - corrupted " ; <nl> <nl> public static void main ( String args [ ] ) <nl> { <nl> @ @ - 119 , 7 + 120 , 7 @ @ public class StandaloneScrubber <nl> { <nl> try <nl> { <nl> - Scrubber scrubber = new Scrubber ( cfs , sstable , handler , true ) ; <nl> + Scrubber scrubber = new Scrubber ( cfs , sstable , options . skipCorrupted , handler , true ) ; <nl> try <nl> { <nl> scrubber . scrub ( ) ; <nl> @ @ - 184 , 6 + 185 , 7 @ @ public class StandaloneScrubber <nl> public boolean debug ; <nl> public boolean verbose ; <nl> public boolean manifestCheckOnly ; <nl> + public boolean skipCorrupted ; <nl> <nl> private Options ( String keyspaceName , String cfName ) <nl> { <nl> @ @ - 222 , 6 + 224 , 7 @ @ public class StandaloneScrubber <nl> opts . debug = cmd . hasOption ( DEBUG _ OPTION ) ; <nl> opts . verbose = cmd . hasOption ( VERBOSE _ OPTION ) ; <nl> opts . manifestCheckOnly = cmd . hasOption ( MANIFEST _ CHECK _ OPTION ) ; <nl> + opts . skipCorrupted = cmd . hasOption ( SKIP _ CORRUPTED _ OPTION ) ; <nl> <nl> return opts ; <nl> } <nl> @ @ - 246 , 6 + 249 , 7 @ @ public class StandaloneScrubber <nl> options . addOption ( " v " , VERBOSE _ OPTION , " verbose output " ) ; <nl> options . addOption ( " h " , HELP _ OPTION , " display this help message " ) ; <nl> options . addOption ( " m " , MANIFEST _ CHECK _ OPTION , " only check and repair the leveled manifest , without actually scrubbing the sstables " ) ; <nl> + options . addOption ( " s " , SKIP _ CORRUPTED _ OPTION , " skip corrupt rows in counter tables " ) ; <nl> return options ; <nl> } <nl> <nl> diff - - git a / src / resources / org / apache / cassandra / tools / NodeToolHelp . yaml b / src / resources / org / apache / cassandra / tools / NodeToolHelp . yaml <nl> index 42fda0d . . b28e300 100644 <nl> - - - a / src / resources / org / apache / cassandra / tools / NodeToolHelp . yaml <nl> + + + b / src / resources / org / apache / cassandra / tools / NodeToolHelp . yaml <nl> @ @ - 163 , 9 + 163 , 11 @ @ commands : <nl> - name : compact [ keyspace ] [ cfnames ] <nl> help : | <nl> Force a ( major ) compaction on one or more column families <nl> - - name : scrub [ keyspace ] [ cfnames ] <nl> + - name : scrub [ keyspace ] [ cfnames ] [ - s | - - skip - corrupted ] <nl> help : | <nl> - Scrub ( rebuild sstables for ) one or more column families <nl> + Scrub ( rebuild sstables for ) one or more column families . <nl> + Use - s / - - skip - corrupted to skip corrupted rows even when scrubbing <nl> + tables that use counters . <nl> - name : upgradesstables [ - a | - - include - all - sstables ] [ keyspace ] [ cfnames ] <nl> help : | <nl> Rewrite sstables ( for the requested column families ) that are not on the current version ( thus upgrading them to said current version ) . <nl> diff - - git a / test / unit / org / apache / cassandra / db / ScrubTest . java b / test / unit / org / apache / cassandra / db / ScrubTest . java <nl> index a83d3c6 . . 08dd435 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / ScrubTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / ScrubTest . java <nl> @ @ - 20 , 13 + 20 , 15 @ @ package org . apache . cassandra . db ; <nl> * <nl> * / <nl> <nl> - import java . io . File ; <nl> - import java . io . IOException ; <nl> + import java . io . * ; <nl> + import java . util . Collections ; <nl> import java . util . HashSet ; <nl> import java . util . List ; <nl> import java . util . Set ; <nl> import java . util . concurrent . ExecutionException ; <nl> <nl> + import org . apache . cassandra . db . compaction . OperationType ; <nl> + import org . apache . commons . lang3 . StringUtils ; <nl> import org . junit . Test ; <nl> import org . junit . runner . RunWith ; <nl> <nl> @ @ - 52 , 6 + 54 , 7 @ @ public class ScrubTest extends SchemaLoader <nl> public String KEYSPACE = " Keyspace1 " ; <nl> public String CF = " Standard1 " ; <nl> public String CF3 = " Standard2 " ; <nl> + public String COUNTER _ CF = " Counter1 " ; <nl> <nl> @ Test <nl> public void testScrubOneRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException <nl> @ @ - 68 , 7 + 71 , 7 @ @ public class ScrubTest extends SchemaLoader <nl> rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; <nl> assertEquals ( 1 , rows . size ( ) ) ; <nl> <nl> - CompactionManager . instance . performScrub ( cfs ) ; <nl> + CompactionManager . instance . performScrub ( cfs , false ) ; <nl> <nl> / / check data is still there <nl> rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; <nl> @ @ - 76 , 6 + 79 , 53 @ @ public class ScrubTest extends SchemaLoader <nl> } <nl> <nl> @ Test <nl> + public void testScrubCorruptedCounterRow ( ) throws IOException , InterruptedException , ExecutionException <nl> + { <nl> + CompactionManager . instance . disableAutoCompaction ( ) ; <nl> + Keyspace keyspace = Keyspace . open ( KEYSPACE ) ; <nl> + ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( COUNTER _ CF ) ; <nl> + cfs . clearUnsafe ( ) ; <nl> + <nl> + fillCounterCF ( cfs , 2 ) ; <nl> + <nl> + List < Row > rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; <nl> + assertEquals ( 2 , rows . size ( ) ) ; <nl> + <nl> + SSTableReader sstable = cfs . getSSTables ( ) . iterator ( ) . next ( ) ; <nl> + <nl> + / / overwrite one row with garbage <nl> + long row0Start = sstable . getPosition ( RowPosition . forKey ( ByteBufferUtil . bytes ( " 0 " ) , sstable . partitioner ) , SSTableReader . Operator . EQ ) . position ; <nl> + long row1Start = sstable . getPosition ( RowPosition . forKey ( ByteBufferUtil . bytes ( " 1 " ) , sstable . partitioner ) , SSTableReader . Operator . EQ ) . position ; <nl> + long startPosition = row0Start < row1Start ? row0Start : row1Start ; <nl> + long endPosition = row0Start < row1Start ? row1Start : row0Start ; <nl> + <nl> + RandomAccessFile file = new RandomAccessFile ( sstable . getFilename ( ) , " rw " ) ; <nl> + file . seek ( startPosition ) ; <nl> + file . writeBytes ( StringUtils . repeat ( ' z ' , ( int ) ( endPosition - startPosition ) ) ) ; <nl> + file . close ( ) ; <nl> + <nl> + / / with skipCorrupted = = false , the scrub is expected to fail <nl> + Scrubber scrubber = new Scrubber ( cfs , sstable , false ) ; <nl> + try <nl> + { <nl> + scrubber . scrub ( ) ; <nl> + fail ( " Expected a CorruptSSTableException to be thrown " ) ; <nl> + } <nl> + catch ( IOError err ) { } <nl> + <nl> + / / with skipCorrupted = = true , the corrupt row will be skipped <nl> + scrubber = new Scrubber ( cfs , sstable , true ) ; <nl> + scrubber . scrub ( ) ; <nl> + scrubber . close ( ) ; <nl> + cfs . replaceCompactedSSTables ( Collections . singletonList ( sstable ) , Collections . singletonList ( scrubber . getNewSSTable ( ) ) , OperationType . SCRUB ) ; <nl> + assertEquals ( 1 , cfs . getSSTables ( ) . size ( ) ) ; <nl> + <nl> + / / verify that we can read all of the rows , and there is now one less row <nl> + rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; <nl> + assertEquals ( 1 , rows . size ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> public void testScrubDeletedRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException <nl> { <nl> CompactionManager . instance . disableAutoCompaction ( ) ; <nl> @ @ - 89 , 7 + 139 , 7 @ @ public class ScrubTest extends SchemaLoader <nl> rm . applyUnsafe ( ) ; <nl> cfs . forceBlockingFlush ( ) ; <nl> <nl> - CompactionManager . instance . performScrub ( cfs ) ; <nl> + CompactionManager . instance . performScrub ( cfs , false ) ; <nl> assert cfs . getSSTables ( ) . isEmpty ( ) ; <nl> } <nl> <nl> @ @ - 108 , 7 + 158 , 7 @ @ public class ScrubTest extends SchemaLoader <nl> rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; <nl> assertEquals ( 10 , rows . size ( ) ) ; <nl> <nl> - CompactionManager . instance . performScrub ( cfs ) ; <nl> + CompactionManager . instance . performScrub ( cfs , false ) ; <nl> <nl> / / check data is still there <nl> rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; <nl> @ @ - 145 , 7 + 195 , 6 @ @ public class ScrubTest extends SchemaLoader <nl> writer . closeAndOpenReader ( ) ; <nl> * / <nl> <nl> - <nl> String root = System . getProperty ( " corrupt - sstable - root " ) ; <nl> assert root ! = null ; <nl> File rootDir = new File ( root ) ; <nl> @ @ - 171 , 7 + 220 , 7 @ @ public class ScrubTest extends SchemaLoader <nl> components . add ( Component . TOC ) ; <nl> SSTableReader sstable = SSTableReader . openNoValidation ( desc , components , metadata ) ; <nl> <nl> - Scrubber scrubber = new Scrubber ( cfs , sstable ) ; <nl> + Scrubber scrubber = new Scrubber ( cfs , sstable , false ) ; <nl> scrubber . scrub ( ) ; <nl> <nl> cfs . loadNewSSTables ( ) ; <nl> @ @ - 207 , 4 + 256 , 20 @ @ public class ScrubTest extends SchemaLoader <nl> <nl> cfs . forceBlockingFlush ( ) ; <nl> } <nl> - } <nl> + <nl> + protected void fillCounterCF ( ColumnFamilyStore cfs , int rowsPerSSTable ) throws ExecutionException , InterruptedException , IOException <nl> + { <nl> + for ( int i = 0 ; i < rowsPerSSTable ; i + + ) <nl> + { <nl> + String key = String . valueOf ( i ) ; <nl> + ColumnFamily cf = TreeMapBackedSortedColumns . factory . create ( KEYSPACE , COUNTER _ CF ) ; <nl> + RowMutation rm = new RowMutation ( KEYSPACE , ByteBufferUtil . bytes ( key ) , cf ) ; <nl> + rm . addCounter ( COUNTER _ CF , ByteBufferUtil . bytes ( " Column1 " ) , 100 ) ; <nl> + CounterMutation cm = new CounterMutation ( rm , ConsistencyLevel . ONE ) ; <nl> + cm . apply ( ) ; <nl> + } <nl> + <nl> + cfs . forceBlockingFlush ( ) ; <nl> + } <nl> + <nl> + } <nl> \ No newline at end of file
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index ddfd099 . . a81d7b8 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 1 . 0 . 0 - final <nl> + * close scrubbed sstable fd before deleting it ( CASSANDRA - 3318 ) <nl> * fix bug preventing obsolete commitlog segments from being removed <nl> ( CASSANDRA - 3269 ) <nl> * tolerate whitespace in seed CDL ( CASSANDRA - 3263 ) <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> index 43bbdce . . 616fd35 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> @ @ - 483 , 10 + 483 , 13 @ @ public class CompactionManager implements CompactionManagerMBean <nl> / / row header ( key or data size ) is corrupt . ( This means our position in the index file will be one row <nl> / / " ahead " of the data file . ) <nl> final RandomAccessReader dataFile = sstable . openDataReader ( true ) ; <nl> - <nl> - String indexFilename = sstable . descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ; <nl> - RandomAccessReader indexFile = RandomAccessReader . open ( new File ( indexFilename ) , true ) ; <nl> + RandomAccessReader indexFile = RandomAccessReader . open ( new File ( sstable . descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) , true ) ; <nl> ScrubInfo scrubInfo = new ScrubInfo ( dataFile , sstable ) ; <nl> + executor . beginCompaction ( scrubInfo ) ; <nl> + <nl> + SSTableWriter writer = null ; <nl> + SSTableReader newSstable = null ; <nl> + int goodRows = 0 , badRows = 0 , emptyRows = 0 ; <nl> <nl> try <nl> { <nl> @ @ - 497 , 170 + 500 , 155 @ @ public class CompactionManager implements CompactionManagerMBean <nl> assert firstRowPositionFromIndex = = 0 : firstRowPositionFromIndex ; <nl> } <nl> <nl> - SSTableReader newSstable = null ; <nl> - <nl> - / / errors when creating the writer may leave empty temp files . <nl> - SSTableWriter writer = maybeCreateWriter ( cfs , <nl> - compactionFileLocation , <nl> - expectedBloomFilterSize , <nl> - null , <nl> - Collections . singletonList ( sstable ) ) ; <nl> - <nl> - int goodRows = 0 , badRows = 0 , emptyRows = 0 ; <nl> + / / TODO errors when creating the writer may leave empty temp files . <nl> + writer = maybeCreateWriter ( cfs , compactionFileLocation , expectedBloomFilterSize , null , Collections . singletonList ( sstable ) ) ; <nl> <nl> - executor . beginCompaction ( scrubInfo ) ; <nl> - <nl> - try <nl> + while ( ! dataFile . isEOF ( ) ) <nl> { <nl> - while ( ! dataFile . isEOF ( ) ) <nl> + long rowStart = dataFile . getFilePointer ( ) ; <nl> + if ( logger . isDebugEnabled ( ) ) <nl> + logger . debug ( " Reading row at " + rowStart ) ; <nl> + <nl> + DecoratedKey key = null ; <nl> + long dataSize = - 1 ; <nl> + try <nl> { <nl> - long rowStart = dataFile . getFilePointer ( ) ; <nl> + key = SSTableReader . decodeKey ( sstable . partitioner , sstable . descriptor , ByteBufferUtil . readWithShortLength ( dataFile ) ) ; <nl> + dataSize = sstable . descriptor . hasIntRowSize ? dataFile . readInt ( ) : dataFile . readLong ( ) ; <nl> if ( logger . isDebugEnabled ( ) ) <nl> - logger . debug ( " Reading row at " + rowStart ) ; <nl> + logger . debug ( String . format ( " row % s is % s bytes " , ByteBufferUtil . bytesToHex ( key . key ) , dataSize ) ) ; <nl> + } <nl> + catch ( Throwable th ) <nl> + { <nl> + throwIfFatal ( th ) ; <nl> + / / check for null key below <nl> + } <nl> <nl> - DecoratedKey key = null ; <nl> - long dataSize = - 1 ; <nl> - try <nl> - { <nl> - key = SSTableReader . decodeKey ( sstable . partitioner , sstable . descriptor , ByteBufferUtil . readWithShortLength ( dataFile ) ) ; <nl> - dataSize = sstable . descriptor . hasIntRowSize ? dataFile . readInt ( ) : dataFile . readLong ( ) ; <nl> - if ( logger . isDebugEnabled ( ) ) <nl> - logger . debug ( String . format ( " row % s is % s bytes " , ByteBufferUtil . bytesToHex ( key . key ) , dataSize ) ) ; <nl> - } <nl> - catch ( Throwable th ) <nl> - { <nl> - throwIfFatal ( th ) ; <nl> - / / check for null key below <nl> - } <nl> + ByteBuffer currentIndexKey = nextIndexKey ; <nl> + long nextRowPositionFromIndex ; <nl> + try <nl> + { <nl> + nextIndexKey = indexFile . isEOF ( ) ? null : ByteBufferUtil . readWithShortLength ( indexFile ) ; <nl> + nextRowPositionFromIndex = indexFile . isEOF ( ) ? dataFile . length ( ) : indexFile . readLong ( ) ; <nl> + } <nl> + catch ( Throwable th ) <nl> + { <nl> + logger . warn ( " Error reading index file " , th ) ; <nl> + nextIndexKey = null ; <nl> + nextRowPositionFromIndex = dataFile . length ( ) ; <nl> + } <nl> <nl> - ByteBuffer currentIndexKey = nextIndexKey ; <nl> - long nextRowPositionFromIndex ; <nl> - try <nl> + long dataStart = dataFile . getFilePointer ( ) ; <nl> + long dataStartFromIndex = currentIndexKey = = null <nl> + ? - 1 <nl> + : rowStart + 2 + currentIndexKey . remaining ( ) + ( sstable . descriptor . hasIntRowSize ? 4 : 8 ) ; <nl> + long dataSizeFromIndex = nextRowPositionFromIndex - dataStartFromIndex ; <nl> + assert currentIndexKey ! = null | | indexFile . isEOF ( ) ; <nl> + if ( logger . isDebugEnabled ( ) & & currentIndexKey ! = null ) <nl> + logger . debug ( String . format ( " Index doublecheck : row % s is % s bytes " , ByteBufferUtil . bytesToHex ( currentIndexKey ) , dataSizeFromIndex ) ) ; <nl> + <nl> + writer . mark ( ) ; <nl> + try <nl> + { <nl> + if ( key = = null ) <nl> + throw new IOError ( new IOException ( " Unable to read row key from data file " ) ) ; <nl> + if ( dataSize > dataFile . length ( ) ) <nl> + throw new IOError ( new IOException ( " Impossible row size " + dataSize ) ) ; <nl> + SSTableIdentityIterator row = new SSTableIdentityIterator ( sstable , dataFile , key , dataStart , dataSize , true ) ; <nl> + AbstractCompactedRow compactedRow = controller . getCompactedRow ( row ) ; <nl> + if ( compactedRow . isEmpty ( ) ) <nl> { <nl> - nextIndexKey = indexFile . isEOF ( ) ? null : ByteBufferUtil . readWithShortLength ( indexFile ) ; <nl> - nextRowPositionFromIndex = indexFile . isEOF ( ) ? dataFile . length ( ) : indexFile . readLong ( ) ; <nl> + emptyRows + + ; <nl> } <nl> - catch ( Throwable th ) <nl> + else <nl> { <nl> - logger . warn ( " Error reading index file " , th ) ; <nl> - nextIndexKey = null ; <nl> - nextRowPositionFromIndex = dataFile . length ( ) ; <nl> + writer . append ( compactedRow ) ; <nl> + goodRows + + ; <nl> } <nl> + if ( ! key . key . equals ( currentIndexKey ) | | dataStart ! = dataStartFromIndex ) <nl> + logger . warn ( " Index file contained a different key or row size ; using key from data file " ) ; <nl> + } <nl> + catch ( Throwable th ) <nl> + { <nl> + throwIfFatal ( th ) ; <nl> + logger . warn ( " Non - fatal error reading row ( stacktrace follows ) " , th ) ; <nl> + writer . resetAndTruncate ( ) ; <nl> <nl> - long dataStart = dataFile . getFilePointer ( ) ; <nl> - long dataStartFromIndex = currentIndexKey = = null <nl> - ? - 1 <nl> - : rowStart + 2 + currentIndexKey . remaining ( ) + ( sstable . descriptor . hasIntRowSize ? 4 : 8 ) ; <nl> - long dataSizeFromIndex = nextRowPositionFromIndex - dataStartFromIndex ; <nl> - assert currentIndexKey ! = null | | indexFile . isEOF ( ) ; <nl> - if ( logger . isDebugEnabled ( ) & & currentIndexKey ! = null ) <nl> - logger . debug ( String . format ( " Index doublecheck : row % s is % s bytes " , ByteBufferUtil . bytesToHex ( currentIndexKey ) , dataSizeFromIndex ) ) ; <nl> - <nl> - writer . mark ( ) ; <nl> - try <nl> - { <nl> - if ( key = = null ) <nl> - throw new IOError ( new IOException ( " Unable to read row key from data file " ) ) ; <nl> - if ( dataSize > dataFile . length ( ) ) <nl> - throw new IOError ( new IOException ( " Impossible row size " + dataSize ) ) ; <nl> - SSTableIdentityIterator row = new SSTableIdentityIterator ( sstable , dataFile , key , dataStart , dataSize , true ) ; <nl> - AbstractCompactedRow compactedRow = controller . getCompactedRow ( row ) ; <nl> - if ( compactedRow . isEmpty ( ) ) <nl> - { <nl> - emptyRows + + ; <nl> - } <nl> - else <nl> - { <nl> - writer . append ( compactedRow ) ; <nl> - goodRows + + ; <nl> - } <nl> - if ( ! key . key . equals ( currentIndexKey ) | | dataStart ! = dataStartFromIndex ) <nl> - logger . warn ( " Index file contained a different key or row size ; using key from data file " ) ; <nl> - } <nl> - catch ( Throwable th ) <nl> + if ( currentIndexKey ! = null <nl> + & & ( key = = null | | ! key . key . equals ( currentIndexKey ) | | dataStart ! = dataStartFromIndex | | dataSize ! = dataSizeFromIndex ) ) <nl> { <nl> - throwIfFatal ( th ) ; <nl> - logger . warn ( " Non - fatal error reading row ( stacktrace follows ) " , th ) ; <nl> - writer . resetAndTruncate ( ) ; <nl> - <nl> - if ( currentIndexKey ! = null <nl> - & & ( key = = null | | ! key . key . equals ( currentIndexKey ) | | dataStart ! = dataStartFromIndex | | dataSize ! = dataSizeFromIndex ) ) <nl> + logger . info ( String . format ( " Retrying from row index ; data is % s bytes starting at % s " , <nl> + dataSizeFromIndex , dataStartFromIndex ) ) ; <nl> + key = SSTableReader . decodeKey ( sstable . partitioner , sstable . descriptor , currentIndexKey ) ; <nl> + try <nl> { <nl> - logger . info ( String . format ( " Retrying from row index ; data is % s bytes starting at % s " , <nl> - dataSizeFromIndex , dataStartFromIndex ) ) ; <nl> - key = SSTableReader . decodeKey ( sstable . partitioner , sstable . descriptor , currentIndexKey ) ; <nl> - try <nl> + SSTableIdentityIterator row = new SSTableIdentityIterator ( sstable , dataFile , key , dataStartFromIndex , dataSizeFromIndex , true ) ; <nl> + AbstractCompactedRow compactedRow = controller . getCompactedRow ( row ) ; <nl> + if ( compactedRow . isEmpty ( ) ) <nl> { <nl> - SSTableIdentityIterator row = new SSTableIdentityIterator ( sstable , dataFile , key , dataStartFromIndex , dataSizeFromIndex , true ) ; <nl> - AbstractCompactedRow compactedRow = controller . getCompactedRow ( row ) ; <nl> - if ( compactedRow . isEmpty ( ) ) <nl> - { <nl> - emptyRows + + ; <nl> - } <nl> - else <nl> - { <nl> - writer . append ( compactedRow ) ; <nl> - goodRows + + ; <nl> - } <nl> + emptyRows + + ; <nl> } <nl> - catch ( Throwable th2 ) <nl> + else <nl> { <nl> - throwIfFatal ( th2 ) ; <nl> - / / Skipping rows is dangerous for counters ( see CASSANDRA - 2759 ) <nl> - if ( isCommutative ) <nl> - throw new IOError ( th2 ) ; <nl> - <nl> - logger . warn ( " Retry failed too . Skipping to next row ( retry ' s stacktrace follows ) " , th2 ) ; <nl> - writer . resetAndTruncate ( ) ; <nl> - dataFile . seek ( nextRowPositionFromIndex ) ; <nl> - badRows + + ; <nl> + writer . append ( compactedRow ) ; <nl> + goodRows + + ; <nl> } <nl> } <nl> - else <nl> + catch ( Throwable th2 ) <nl> { <nl> + throwIfFatal ( th2 ) ; <nl> / / Skipping rows is dangerous for counters ( see CASSANDRA - 2759 ) <nl> if ( isCommutative ) <nl> - throw new IOError ( th ) ; <nl> + throw new IOError ( th2 ) ; <nl> <nl> - logger . warn ( " Row at " + dataStart + " is unreadable ; skipping to next " ) ; <nl> - if ( currentIndexKey ! = null ) <nl> - dataFile . seek ( nextRowPositionFromIndex ) ; <nl> + logger . warn ( " Retry failed too . Skipping to next row ( retry ' s stacktrace follows ) " , th2 ) ; <nl> + writer . resetAndTruncate ( ) ; <nl> + dataFile . seek ( nextRowPositionFromIndex ) ; <nl> badRows + + ; <nl> } <nl> } <nl> + else <nl> + { <nl> + / / Skipping rows is dangerous for counters ( see CASSANDRA - 2759 ) <nl> + if ( isCommutative ) <nl> + throw new IOError ( th ) ; <nl> + <nl> + logger . warn ( " Row at " + dataStart + " is unreadable ; skipping to next " ) ; <nl> + if ( currentIndexKey ! = null ) <nl> + dataFile . seek ( nextRowPositionFromIndex ) ; <nl> + badRows + + ; <nl> + } <nl> } <nl> - <nl> - if ( writer . getFilePointer ( ) > 0 ) <nl> - newSstable = writer . closeAndOpenReader ( sstable . maxDataAge ) ; <nl> - } <nl> - finally <nl> - { <nl> - writer . cleanupIfNecessary ( ) ; <nl> } <nl> <nl> - if ( newSstable ! = null ) <nl> - { <nl> - cfs . replaceCompactedSSTables ( Arrays . asList ( sstable ) , Arrays . asList ( newSstable ) ) ; <nl> - logger . info ( " Scrub of " + sstable + " complete : " + goodRows + " rows in new sstable and " + emptyRows + " empty ( tombstoned ) rows dropped " ) ; <nl> - if ( badRows > 0 ) <nl> - logger . warn ( " Unable to recover " + badRows + " rows that were skipped . You can attempt manual recovery from the pre - scrub snapshot . You can also run nodetool repair to transfer the data from a healthy replica , if any " ) ; <nl> - } <nl> - else <nl> - { <nl> - cfs . markCompacted ( Arrays . asList ( sstable ) ) ; <nl> - if ( badRows > 0 ) <nl> - logger . warn ( " No valid rows found while scrubbing " + sstable + " ; it is marked for deletion now . If you want to attempt manual recovery , you can find a copy in the pre - scrub snapshot " ) ; <nl> - else <nl> - logger . info ( " Scrub of " + sstable + " complete ; looks like all " + emptyRows + " rows were tombstoned " ) ; <nl> - } <nl> + if ( writer . getFilePointer ( ) > 0 ) <nl> + newSstable = writer . closeAndOpenReader ( sstable . maxDataAge ) ; <nl> } <nl> finally <nl> { <nl> + if ( writer ! = null ) <nl> + writer . cleanupIfNecessary ( ) ; <nl> FileUtils . closeQuietly ( dataFile ) ; <nl> FileUtils . closeQuietly ( indexFile ) ; <nl> <nl> executor . finishCompaction ( scrubInfo ) ; <nl> } <nl> + <nl> + if ( newSstable = = null ) <nl> + { <nl> + cfs . markCompacted ( Arrays . asList ( sstable ) ) ; <nl> + if ( badRows > 0 ) <nl> + logger . warn ( " No valid rows found while scrubbing " + sstable + " ; it is marked for deletion now . If you want to attempt manual recovery , you can find a copy in the pre - scrub snapshot " ) ; <nl> + else <nl> + logger . info ( " Scrub of " + sstable + " complete ; looks like all " + emptyRows + " rows were tombstoned " ) ; <nl> + } <nl> + else <nl> + { <nl> + cfs . replaceCompactedSSTables ( Arrays . asList ( sstable ) , Arrays . asList ( newSstable ) ) ; <nl> + logger . info ( " Scrub of " + sstable + " complete : " + goodRows + " rows in new sstable and " + emptyRows + " empty ( tombstoned ) rows dropped " ) ; <nl> + if ( badRows > 0 ) <nl> + logger . warn ( " Unable to recover " + badRows + " rows that were skipped . You can attempt manual recovery from the pre - scrub snapshot . You can also run nodetool repair to transfer the data from a healthy replica , if any " ) ; <nl> + } <nl> } <nl> <nl> private void throwIfFatal ( Throwable th )

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 13b4c5b . . a1a58a3 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 3 + 1 , 7 @ @ 
 + 2 . 0 . 6 
 + * Let scrub optionally skip broken counter partitions ( CASSANDRA - 5930 ) 
 + 
 + 
 2 . 0 . 5 
 * Reduce garbage generated by bloom filter lookups ( CASSANDRA - 6609 ) 
 * Add ks . cf names to tombstone logging ( CASSANDRA - 6597 ) 
 diff - - git a / NEWS . txt b / NEWS . txt 
 index 92446c8 . . b21fbaa 100644 
 - - - a / NEWS . txt 
 + + + b / NEWS . txt 
 @ @ - 14 , 11 + 14 , 21 @ @ restore snapshots created with the previous major version using the 
 using the provided ' sstableupgrade ' tool . 
 
 
 + 2 . 0 . 6 
 + = = = = = 
 + 
 + New features 
 + - - - - - - - - - - - - 
 + - Scrub can now optionally skip corrupt counter partitions . Please note 
 + that this will lead to the loss of all the counter updates in the skipped 
 + partition . See the - - skip - corrupted option . 
 + 
 + 
 2 . 0 . 5 
 = = = = = 
 
 New features 
 - - - - - - - - - 
 + - - - - - - - - - - - - 
 - Batchlog replay can be , and is throttled by default now . 
 See batchlog _ replay _ throttle _ in _ kb setting in cassandra . yaml . 
 
 diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 index 8750026 . . 38d87db 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 @ @ - 1115 , 12 + 1115 , 12 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 CompactionManager . instance . performCleanup ( ColumnFamilyStore . this , renewer ) ; 
 } 
 
 - public void scrub ( boolean disableSnapshot ) throws ExecutionException , InterruptedException 
 + public void scrub ( boolean disableSnapshot , boolean skipCorrupted ) throws ExecutionException , InterruptedException 
 { 
 / / skip snapshot creation during scrub , SEE JIRA 5891 
 if ( ! disableSnapshot ) 
 snapshotWithoutFlush ( " pre - scrub - " + System . currentTimeMillis ( ) ) ; 
 - CompactionManager . instance . performScrub ( ColumnFamilyStore . this ) ; 
 + CompactionManager . instance . performScrub ( ColumnFamilyStore . this , skipCorrupted ) ; 
 } 
 
 public void sstablesRewrite ( boolean excludeCurrentVersion ) throws ExecutionException , InterruptedException 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 index 168ee02 . . 48900c8 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 @ @ - 227 , 13 + 227 , 13 @ @ public class CompactionManager implements CompactionManagerMBean 
 executor . submit ( runnable ) . get ( ) ; 
 } 
 
 - public void performScrub ( ColumnFamilyStore cfStore ) throws InterruptedException , ExecutionException 
 + public void performScrub ( ColumnFamilyStore cfStore , final boolean skipCorrupted ) throws InterruptedException , ExecutionException 
 { 
 performAllSSTableOperation ( cfStore , new AllSSTablesOperation ( ) 
 { 
 public void perform ( ColumnFamilyStore store , Iterable < SSTableReader > sstables ) throws IOException 
 { 
 - doScrub ( store , sstables ) ; 
 + doScrub ( store , sstables , skipCorrupted ) ; 
 } 
 } ) ; 
 } 
 @ @ - 425 , 16 + 425 , 16 @ @ public class CompactionManager implements CompactionManagerMBean 
 * 
 * @ throws IOException 
 * / 
 - private void doScrub ( ColumnFamilyStore cfs , Iterable < SSTableReader > sstables ) throws IOException 
 + private void doScrub ( ColumnFamilyStore cfs , Iterable < SSTableReader > sstables , boolean skipCorrupted ) throws IOException 
 { 
 assert ! cfs . isIndex ( ) ; 
 for ( final SSTableReader sstable : sstables ) 
 - scrubOne ( cfs , sstable ) ; 
 + scrubOne ( cfs , sstable , skipCorrupted ) ; 
 } 
 
 - private void scrubOne ( ColumnFamilyStore cfs , SSTableReader sstable ) throws IOException 
 + private void scrubOne ( ColumnFamilyStore cfs , SSTableReader sstable , boolean skipCorrupted ) throws IOException 
 { 
 - Scrubber scrubber = new Scrubber ( cfs , sstable ) ; 
 + Scrubber scrubber = new Scrubber ( cfs , sstable , skipCorrupted ) ; 
 
 CompactionInfo . Holder scrubInfo = scrubber . getScrubInfo ( ) ; 
 metrics . beginCompaction ( scrubInfo ) ; 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / Scrubber . java b / src / java / org / apache / cassandra / db / compaction / Scrubber . java 
 index 708e929 . . 820761c 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / Scrubber . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / Scrubber . java 
 @ @ - 35 , 6 + 35 , 7 @ @ public class Scrubber implements Closeable 
 public final ColumnFamilyStore cfs ; 
 public final SSTableReader sstable ; 
 public final File destination ; 
 + public final boolean skipCorrupted ; 
 
 private final CompactionController controller ; 
 private final boolean isCommutative ; 
 @ @ - 63 , 16 + 64 , 17 @ @ public class Scrubber implements Closeable 
 } ; 
 private final SortedSet < Row > outOfOrderRows = new TreeSet < > ( rowComparator ) ; 
 
 - public Scrubber ( ColumnFamilyStore cfs , SSTableReader sstable ) throws IOException 
 + public Scrubber ( ColumnFamilyStore cfs , SSTableReader sstable , boolean skipCorrupted ) throws IOException 
 { 
 - this ( cfs , sstable , new OutputHandler . LogOutput ( ) , false ) ; 
 + this ( cfs , sstable , skipCorrupted , new OutputHandler . LogOutput ( ) , false ) ; 
 } 
 
 - public Scrubber ( ColumnFamilyStore cfs , SSTableReader sstable , OutputHandler outputHandler , boolean isOffline ) throws IOException 
 + public Scrubber ( ColumnFamilyStore cfs , SSTableReader sstable , boolean skipCorrupted , OutputHandler outputHandler , boolean isOffline ) throws IOException 
 { 
 this . cfs = cfs ; 
 this . sstable = sstable ; 
 this . outputHandler = outputHandler ; 
 + this . skipCorrupted = skipCorrupted ; 
 
 / / Calculate the expected compacted filesize 
 this . destination = cfs . directories . getDirectoryForNewSSTables ( ) ; 
 @ @ - 166 , 7 + 168 , 9 @ @ public class Scrubber implements Closeable 
 if ( ! sstable . descriptor . version . hasRowSizeAndColumnCount ) 
 { 
 dataSize = dataSizeFromIndex ; 
 - outputHandler . debug ( String . format ( " row % s is % s bytes " , ByteBufferUtil . bytesToHex ( key . key ) , dataSize ) ) ; 
 + / / avoid an NPE if key is null 
 + String keyName = key = = null ? " ( unreadable key ) " : ByteBufferUtil . bytesToHex ( key . key ) ; 
 + outputHandler . debug ( String . format ( " row % s is % s bytes " , keyName , dataSize ) ) ; 
 } 
 else 
 { 
 @ @ - 203 , 7 + 207 , 7 @ @ public class Scrubber implements Closeable 
 catch ( Throwable th ) 
 { 
 throwIfFatal ( th ) ; 
 - outputHandler . warn ( " Non - fatal error reading row ( stacktrace follows ) " , th ) ; 
 + outputHandler . warn ( " Error reading row ( stacktrace follows ) : " , th ) ; 
 writer . resetAndTruncate ( ) ; 
 
 if ( currentIndexKey ! = null 
 @ @ - 231 , 9 + 235 , 7 @ @ public class Scrubber implements Closeable 
 catch ( Throwable th2 ) 
 { 
 throwIfFatal ( th2 ) ; 
 - / / Skipping rows is dangerous for counters ( see CASSANDRA - 2759 ) 
 - if ( isCommutative ) 
 - throw new IOError ( th2 ) ; 
 + throwIfCommutative ( key , th2 ) ; 
 
 outputHandler . warn ( " Retry failed too . Skipping to next row ( retry ' s stacktrace follows ) " , th2 ) ; 
 writer . resetAndTruncate ( ) ; 
 @ @ - 243 , 11 + 245 , 9 @ @ public class Scrubber implements Closeable 
 } 
 else 
 { 
 - / / Skipping rows is dangerous for counters ( see CASSANDRA - 2759 ) 
 - if ( isCommutative ) 
 - throw new IOError ( th ) ; 
 + throwIfCommutative ( key , th ) ; 
 
 - outputHandler . warn ( " Row at " + dataStart + " is unreadable ; skipping to next " ) ; 
 + outputHandler . warn ( " Row starting at position " + dataStart + " is unreadable ; skipping to next " ) ; 
 if ( currentIndexKey ! = null ) 
 dataFile . seek ( nextRowPositionFromIndex ) ; 
 badRows + + ; 
 @ @ - 324 , 6 + 324 , 19 @ @ public class Scrubber implements Closeable 
 throw ( Error ) th ; 
 } 
 
 + private void throwIfCommutative ( DecoratedKey key , Throwable th ) 
 + { 
 + if ( isCommutative & & ! skipCorrupted ) 
 + { 
 + outputHandler . warn ( String . format ( " An error occurred while scrubbing the row with key ' % s ' . Skipping corrupt " + 
 + " rows in counter tables will result in undercounts for the affected " + 
 + " counters ( see CASSANDRA - 2759 for more details ) , so by default the scrub will " + 
 + " stop at this point . If you would like to skip the row anyway and continue " + 
 + " scrubbing , re - run the scrub with the - - skip - corrupted option . " , key ) ) ; 
 + throw new IOError ( th ) ; 
 + } 
 + } 
 + 
 public void close ( ) 
 { 
 FileUtils . closeQuietly ( dataFile ) ; 
 diff - - git a / src / java / org / apache / cassandra / service / StorageService . java b / src / java / org / apache / cassandra / service / StorageService . java 
 index 700966f . . f46ae66 100644 
 - - - a / src / java / org / apache / cassandra / service / StorageService . java 
 + + + b / src / java / org / apache / cassandra / service / StorageService . java 
 @ @ - 2155 , 10 + 2155 , 10 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE 
 } 
 } 
 
 - public void scrub ( boolean disableSnapshot , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException 
 + public void scrub ( boolean disableSnapshot , boolean skipCorrupted , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException 
 { 
 for ( ColumnFamilyStore cfStore : getValidColumnFamilies ( false , false , keyspaceName , columnFamilies ) ) 
 - cfStore . scrub ( disableSnapshot ) ; 
 + cfStore . scrub ( disableSnapshot , skipCorrupted ) ; 
 } 
 
 public void upgradeSSTables ( String keyspaceName , boolean excludeCurrentVersion , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException 
 diff - - git a / src / java / org / apache / cassandra / service / StorageServiceMBean . java b / src / java / org / apache / cassandra / service / StorageServiceMBean . java 
 index df85901 . . d31e8b9 100644 
 - - - a / src / java / org / apache / cassandra / service / StorageServiceMBean . java 
 + + + b / src / java / org / apache / cassandra / service / StorageServiceMBean . java 
 @ @ - 231 , 7 + 231 , 7 @ @ public interface StorageServiceMBean extends NotificationEmitter 
 * 
 * Scrubbed CFs will be snapshotted first , if disableSnapshot is false 
 * / 
 - public void scrub ( boolean disableSnapshot , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException ; 
 + public void scrub ( boolean disableSnapshot , boolean skipCorrupted , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException ; 
 
 / * * 
 * Rewrite all sstables to the latest version . 
 diff - - git a / src / java / org / apache / cassandra / tools / NodeCmd . java b / src / java / org / apache / cassandra / tools / NodeCmd . java 
 index 0cc7320 . . ab05d16 100644 
 - - - a / src / java / org / apache / cassandra / tools / NodeCmd . java 
 + + + b / src / java / org / apache / cassandra / tools / NodeCmd . java 
 @ @ - 74 , 6 + 74 , 8 @ @ public class NodeCmd 
 private static final Pair < String , String > NO _ SNAPSHOT = Pair . create ( " ns " , " no - snapshot " ) ; 
 private static final Pair < String , String > CFSTATS _ IGNORE _ OPT = Pair . create ( " i " , " ignore " ) ; 
 private static final Pair < String , String > RESOLVE _ IP = Pair . create ( " r " , " resolve - ip " ) ; 
 + private static final Pair < String , String > SCRUB _ SKIP _ CORRUPTED _ OPT = Pair . create ( " s " , " skip - corrupted " ) ; 
 + 
 
 private static final String DEFAULT _ HOST = " 127 . 0 . 0 . 1 " ; 
 private static final int DEFAULT _ PORT = 7199 ; 
 @ @ - 101 , 6 + 103 , 7 @ @ public class NodeCmd 
 options . addOption ( NO _ SNAPSHOT , false , " disables snapshot creation for scrub " ) ; 
 options . addOption ( CFSTATS _ IGNORE _ OPT , false , " ignore the supplied list of keyspace . columnfamiles in statistics " ) ; 
 options . addOption ( RESOLVE _ IP , false , " show node domain names instead of IPs " ) ; 
 + options . addOption ( SCRUB _ SKIP _ CORRUPTED _ OPT , false , " when scrubbing counter tables , skip corrupted rows " ) ; 
 } 
 
 public NodeCmd ( NodeProbe probe ) 
 @ @ - 1562 , 7 + 1565 , 8 @ @ public class NodeCmd 
 break ; 
 case SCRUB : 
 boolean disableSnapshot = cmd . hasOption ( NO _ SNAPSHOT . left ) ; 
 - try { probe . scrub ( disableSnapshot , keyspace , columnFamilies ) ; } 
 + boolean skipCorrupted = cmd . hasOption ( SCRUB _ SKIP _ CORRUPTED _ OPT . left ) ; 
 + try { probe . scrub ( disableSnapshot , skipCorrupted , keyspace , columnFamilies ) ; } 
 catch ( ExecutionException ee ) { err ( ee , " Error occurred while scrubbing keyspace " + keyspace ) ; } 
 break ; 
 case UPGRADESSTABLES : 
 diff - - git a / src / java / org / apache / cassandra / tools / NodeProbe . java b / src / java / org / apache / cassandra / tools / NodeProbe . java 
 index 1bb9d4e . . 0fbb12a 100644 
 - - - a / src / java / org / apache / cassandra / tools / NodeProbe . java 
 + + + b / src / java / org / apache / cassandra / tools / NodeProbe . java 
 @ @ - 190 , 9 + 190 , 9 @ @ public class NodeProbe 
 ssProxy . forceKeyspaceCleanup ( keyspaceName , columnFamilies ) ; 
 } 
 
 - public void scrub ( boolean disableSnapshot , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException 
 + public void scrub ( boolean disableSnapshot , boolean skipCorrupted , String keyspaceName , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException 
 { 
 - ssProxy . scrub ( disableSnapshot , keyspaceName , columnFamilies ) ; 
 + ssProxy . scrub ( disableSnapshot , skipCorrupted , keyspaceName , columnFamilies ) ; 
 } 
 
 public void upgradeSSTables ( String keyspaceName , boolean excludeCurrentVersion , String . . . columnFamilies ) throws IOException , ExecutionException , InterruptedException 
 diff - - git a / src / java / org / apache / cassandra / tools / StandaloneScrubber . java b / src / java / org / apache / cassandra / tools / StandaloneScrubber . java 
 index 00e0a5a . . 6556c3a 100644 
 - - - a / src / java / org / apache / cassandra / tools / StandaloneScrubber . java 
 + + + b / src / java / org / apache / cassandra / tools / StandaloneScrubber . java 
 @ @ - 49 , 6 + 49 , 7 @ @ public class StandaloneScrubber 
 private static final String DEBUG _ OPTION = " debug " ; 
 private static final String HELP _ OPTION = " help " ; 
 private static final String MANIFEST _ CHECK _ OPTION = " manifest - check " ; 
 + private static final String SKIP _ CORRUPTED _ OPTION = " skip - corrupted " ; 
 
 public static void main ( String args [ ] ) 
 { 
 @ @ - 119 , 7 + 120 , 7 @ @ public class StandaloneScrubber 
 { 
 try 
 { 
 - Scrubber scrubber = new Scrubber ( cfs , sstable , handler , true ) ; 
 + Scrubber scrubber = new Scrubber ( cfs , sstable , options . skipCorrupted , handler , true ) ; 
 try 
 { 
 scrubber . scrub ( ) ; 
 @ @ - 184 , 6 + 185 , 7 @ @ public class StandaloneScrubber 
 public boolean debug ; 
 public boolean verbose ; 
 public boolean manifestCheckOnly ; 
 + public boolean skipCorrupted ; 
 
 private Options ( String keyspaceName , String cfName ) 
 { 
 @ @ - 222 , 6 + 224 , 7 @ @ public class StandaloneScrubber 
 opts . debug = cmd . hasOption ( DEBUG _ OPTION ) ; 
 opts . verbose = cmd . hasOption ( VERBOSE _ OPTION ) ; 
 opts . manifestCheckOnly = cmd . hasOption ( MANIFEST _ CHECK _ OPTION ) ; 
 + opts . skipCorrupted = cmd . hasOption ( SKIP _ CORRUPTED _ OPTION ) ; 
 
 return opts ; 
 } 
 @ @ - 246 , 6 + 249 , 7 @ @ public class StandaloneScrubber 
 options . addOption ( " v " , VERBOSE _ OPTION , " verbose output " ) ; 
 options . addOption ( " h " , HELP _ OPTION , " display this help message " ) ; 
 options . addOption ( " m " , MANIFEST _ CHECK _ OPTION , " only check and repair the leveled manifest , without actually scrubbing the sstables " ) ; 
 + options . addOption ( " s " , SKIP _ CORRUPTED _ OPTION , " skip corrupt rows in counter tables " ) ; 
 return options ; 
 } 
 
 diff - - git a / src / resources / org / apache / cassandra / tools / NodeToolHelp . yaml b / src / resources / org / apache / cassandra / tools / NodeToolHelp . yaml 
 index 42fda0d . . b28e300 100644 
 - - - a / src / resources / org / apache / cassandra / tools / NodeToolHelp . yaml 
 + + + b / src / resources / org / apache / cassandra / tools / NodeToolHelp . yaml 
 @ @ - 163 , 9 + 163 , 11 @ @ commands : 
 - name : compact [ keyspace ] [ cfnames ] 
 help : | 
 Force a ( major ) compaction on one or more column families 
 - - name : scrub [ keyspace ] [ cfnames ] 
 + - name : scrub [ keyspace ] [ cfnames ] [ - s | - - skip - corrupted ] 
 help : | 
 - Scrub ( rebuild sstables for ) one or more column families 
 + Scrub ( rebuild sstables for ) one or more column families . 
 + Use - s / - - skip - corrupted to skip corrupted rows even when scrubbing 
 + tables that use counters . 
 - name : upgradesstables [ - a | - - include - all - sstables ] [ keyspace ] [ cfnames ] 
 help : | 
 Rewrite sstables ( for the requested column families ) that are not on the current version ( thus upgrading them to said current version ) . 
 diff - - git a / test / unit / org / apache / cassandra / db / ScrubTest . java b / test / unit / org / apache / cassandra / db / ScrubTest . java 
 index a83d3c6 . . 08dd435 100644 
 - - - a / test / unit / org / apache / cassandra / db / ScrubTest . java 
 + + + b / test / unit / org / apache / cassandra / db / ScrubTest . java 
 @ @ - 20 , 13 + 20 , 15 @ @ package org . apache . cassandra . db ; 
 * 
 * / 
 
 - import java . io . File ; 
 - import java . io . IOException ; 
 + import java . io . * ; 
 + import java . util . Collections ; 
 import java . util . HashSet ; 
 import java . util . List ; 
 import java . util . Set ; 
 import java . util . concurrent . ExecutionException ; 
 
 + import org . apache . cassandra . db . compaction . OperationType ; 
 + import org . apache . commons . lang3 . StringUtils ; 
 import org . junit . Test ; 
 import org . junit . runner . RunWith ; 
 
 @ @ - 52 , 6 + 54 , 7 @ @ public class ScrubTest extends SchemaLoader 
 public String KEYSPACE = " Keyspace1 " ; 
 public String CF = " Standard1 " ; 
 public String CF3 = " Standard2 " ; 
 + public String COUNTER _ CF = " Counter1 " ; 
 
 @ Test 
 public void testScrubOneRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException 
 @ @ - 68 , 7 + 71 , 7 @ @ public class ScrubTest extends SchemaLoader 
 rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; 
 assertEquals ( 1 , rows . size ( ) ) ; 
 
 - CompactionManager . instance . performScrub ( cfs ) ; 
 + CompactionManager . instance . performScrub ( cfs , false ) ; 
 
 / / check data is still there 
 rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; 
 @ @ - 76 , 6 + 79 , 53 @ @ public class ScrubTest extends SchemaLoader 
 } 
 
 @ Test 
 + public void testScrubCorruptedCounterRow ( ) throws IOException , InterruptedException , ExecutionException 
 + { 
 + CompactionManager . instance . disableAutoCompaction ( ) ; 
 + Keyspace keyspace = Keyspace . open ( KEYSPACE ) ; 
 + ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( COUNTER _ CF ) ; 
 + cfs . clearUnsafe ( ) ; 
 + 
 + fillCounterCF ( cfs , 2 ) ; 
 + 
 + List < Row > rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; 
 + assertEquals ( 2 , rows . size ( ) ) ; 
 + 
 + SSTableReader sstable = cfs . getSSTables ( ) . iterator ( ) . next ( ) ; 
 + 
 + / / overwrite one row with garbage 
 + long row0Start = sstable . getPosition ( RowPosition . forKey ( ByteBufferUtil . bytes ( " 0 " ) , sstable . partitioner ) , SSTableReader . Operator . EQ ) . position ; 
 + long row1Start = sstable . getPosition ( RowPosition . forKey ( ByteBufferUtil . bytes ( " 1 " ) , sstable . partitioner ) , SSTableReader . Operator . EQ ) . position ; 
 + long startPosition = row0Start < row1Start ? row0Start : row1Start ; 
 + long endPosition = row0Start < row1Start ? row1Start : row0Start ; 
 + 
 + RandomAccessFile file = new RandomAccessFile ( sstable . getFilename ( ) , " rw " ) ; 
 + file . seek ( startPosition ) ; 
 + file . writeBytes ( StringUtils . repeat ( ' z ' , ( int ) ( endPosition - startPosition ) ) ) ; 
 + file . close ( ) ; 
 + 
 + / / with skipCorrupted = = false , the scrub is expected to fail 
 + Scrubber scrubber = new Scrubber ( cfs , sstable , false ) ; 
 + try 
 + { 
 + scrubber . scrub ( ) ; 
 + fail ( " Expected a CorruptSSTableException to be thrown " ) ; 
 + } 
 + catch ( IOError err ) { } 
 + 
 + / / with skipCorrupted = = true , the corrupt row will be skipped 
 + scrubber = new Scrubber ( cfs , sstable , true ) ; 
 + scrubber . scrub ( ) ; 
 + scrubber . close ( ) ; 
 + cfs . replaceCompactedSSTables ( Collections . singletonList ( sstable ) , Collections . singletonList ( scrubber . getNewSSTable ( ) ) , OperationType . SCRUB ) ; 
 + assertEquals ( 1 , cfs . getSSTables ( ) . size ( ) ) ; 
 + 
 + / / verify that we can read all of the rows , and there is now one less row 
 + rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; 
 + assertEquals ( 1 , rows . size ( ) ) ; 
 + } 
 + 
 + @ Test 
 public void testScrubDeletedRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException 
 { 
 CompactionManager . instance . disableAutoCompaction ( ) ; 
 @ @ - 89 , 7 + 139 , 7 @ @ public class ScrubTest extends SchemaLoader 
 rm . applyUnsafe ( ) ; 
 cfs . forceBlockingFlush ( ) ; 
 
 - CompactionManager . instance . performScrub ( cfs ) ; 
 + CompactionManager . instance . performScrub ( cfs , false ) ; 
 assert cfs . getSSTables ( ) . isEmpty ( ) ; 
 } 
 
 @ @ - 108 , 7 + 158 , 7 @ @ public class ScrubTest extends SchemaLoader 
 rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; 
 assertEquals ( 10 , rows . size ( ) ) ; 
 
 - CompactionManager . instance . performScrub ( cfs ) ; 
 + CompactionManager . instance . performScrub ( cfs , false ) ; 
 
 / / check data is still there 
 rows = cfs . getRangeSlice ( Util . range ( " " , " " ) , null , new IdentityQueryFilter ( ) , 1000 ) ; 
 @ @ - 145 , 7 + 195 , 6 @ @ public class ScrubTest extends SchemaLoader 
 writer . closeAndOpenReader ( ) ; 
 * / 
 
 - 
 String root = System . getProperty ( " corrupt - sstable - root " ) ; 
 assert root ! = null ; 
 File rootDir = new File ( root ) ; 
 @ @ - 171 , 7 + 220 , 7 @ @ public class ScrubTest extends SchemaLoader 
 components . add ( Component . TOC ) ; 
 SSTableReader sstable = SSTableReader . openNoValidation ( desc , components , metadata ) ; 
 
 - Scrubber scrubber = new Scrubber ( cfs , sstable ) ; 
 + Scrubber scrubber = new Scrubber ( cfs , sstable , false ) ; 
 scrubber . scrub ( ) ; 
 
 cfs . loadNewSSTables ( ) ; 
 @ @ - 207 , 4 + 256 , 20 @ @ public class ScrubTest extends SchemaLoader 
 
 cfs . forceBlockingFlush ( ) ; 
 } 
 - } 
 + 
 + protected void fillCounterCF ( ColumnFamilyStore cfs , int rowsPerSSTable ) throws ExecutionException , InterruptedException , IOException 
 + { 
 + for ( int i = 0 ; i < rowsPerSSTable ; i + + ) 
 + { 
 + String key = String . valueOf ( i ) ; 
 + ColumnFamily cf = TreeMapBackedSortedColumns . factory . create ( KEYSPACE , COUNTER _ CF ) ; 
 + RowMutation rm = new RowMutation ( KEYSPACE , ByteBufferUtil . bytes ( key ) , cf ) ; 
 + rm . addCounter ( COUNTER _ CF , ByteBufferUtil . bytes ( " Column1 " ) , 100 ) ; 
 + CounterMutation cm = new CounterMutation ( rm , ConsistencyLevel . ONE ) ; 
 + cm . apply ( ) ; 
 + } 
 + 
 + cfs . forceBlockingFlush ( ) ; 
 + } 
 + 
 + } 
 \ No newline at end of file

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index ddfd099 . . a81d7b8 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 1 . 0 . 0 - final 
 + * close scrubbed sstable fd before deleting it ( CASSANDRA - 3318 ) 
 * fix bug preventing obsolete commitlog segments from being removed 
 ( CASSANDRA - 3269 ) 
 * tolerate whitespace in seed CDL ( CASSANDRA - 3263 ) 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 index 43bbdce . . 616fd35 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 @ @ - 483 , 10 + 483 , 13 @ @ public class CompactionManager implements CompactionManagerMBean 
 / / row header ( key or data size ) is corrupt . ( This means our position in the index file will be one row 
 / / " ahead " of the data file . ) 
 final RandomAccessReader dataFile = sstable . openDataReader ( true ) ; 
 - 
 - String indexFilename = sstable . descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ; 
 - RandomAccessReader indexFile = RandomAccessReader . open ( new File ( indexFilename ) , true ) ; 
 + RandomAccessReader indexFile = RandomAccessReader . open ( new File ( sstable . descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) , true ) ; 
 ScrubInfo scrubInfo = new ScrubInfo ( dataFile , sstable ) ; 
 + executor . beginCompaction ( scrubInfo ) ; 
 + 
 + SSTableWriter writer = null ; 
 + SSTableReader newSstable = null ; 
 + int goodRows = 0 , badRows = 0 , emptyRows = 0 ; 
 
 try 
 { 
 @ @ - 497 , 170 + 500 , 155 @ @ public class CompactionManager implements CompactionManagerMBean 
 assert firstRowPositionFromIndex = = 0 : firstRowPositionFromIndex ; 
 } 
 
 - SSTableReader newSstable = null ; 
 - 
 - / / errors when creating the writer may leave empty temp files . 
 - SSTableWriter writer = maybeCreateWriter ( cfs , 
 - compactionFileLocation , 
 - expectedBloomFilterSize , 
 - null , 
 - Collections . singletonList ( sstable ) ) ; 
 - 
 - int goodRows = 0 , badRows = 0 , emptyRows = 0 ; 
 + / / TODO errors when creating the writer may leave empty temp files . 
 + writer = maybeCreateWriter ( cfs , compactionFileLocation , expectedBloomFilterSize , null , Collections . singletonList ( sstable ) ) ; 
 
 - executor . beginCompaction ( scrubInfo ) ; 
 - 
 - try 
 + while ( ! dataFile . isEOF ( ) ) 
 { 
 - while ( ! dataFile . isEOF ( ) ) 
 + long rowStart = dataFile . getFilePointer ( ) ; 
 + if ( logger . isDebugEnabled ( ) ) 
 + logger . debug ( " Reading row at " + rowStart ) ; 
 + 
 + DecoratedKey key = null ; 
 + long dataSize = - 1 ; 
 + try 
 { 
 - long rowStart = dataFile . getFilePointer ( ) ; 
 + key = SSTableReader . decodeKey ( sstable . partitioner , sstable . descriptor , ByteBufferUtil . readWithShortLength ( dataFile ) ) ; 
 + dataSize = sstable . descriptor . hasIntRowSize ? dataFile . readInt ( ) : dataFile . readLong ( ) ; 
 if ( logger . isDebugEnabled ( ) ) 
 - logger . debug ( " Reading row at " + rowStart ) ; 
 + logger . debug ( String . format ( " row % s is % s bytes " , ByteBufferUtil . bytesToHex ( key . key ) , dataSize ) ) ; 
 + } 
 + catch ( Throwable th ) 
 + { 
 + throwIfFatal ( th ) ; 
 + / / check for null key below 
 + } 
 
 - DecoratedKey key = null ; 
 - long dataSize = - 1 ; 
 - try 
 - { 
 - key = SSTableReader . decodeKey ( sstable . partitioner , sstable . descriptor , ByteBufferUtil . readWithShortLength ( dataFile ) ) ; 
 - dataSize = sstable . descriptor . hasIntRowSize ? dataFile . readInt ( ) : dataFile . readLong ( ) ; 
 - if ( logger . isDebugEnabled ( ) ) 
 - logger . debug ( String . format ( " row % s is % s bytes " , ByteBufferUtil . bytesToHex ( key . key ) , dataSize ) ) ; 
 - } 
 - catch ( Throwable th ) 
 - { 
 - throwIfFatal ( th ) ; 
 - / / check for null key below 
 - } 
 + ByteBuffer currentIndexKey = nextIndexKey ; 
 + long nextRowPositionFromIndex ; 
 + try 
 + { 
 + nextIndexKey = indexFile . isEOF ( ) ? null : ByteBufferUtil . readWithShortLength ( indexFile ) ; 
 + nextRowPositionFromIndex = indexFile . isEOF ( ) ? dataFile . length ( ) : indexFile . readLong ( ) ; 
 + } 
 + catch ( Throwable th ) 
 + { 
 + logger . warn ( " Error reading index file " , th ) ; 
 + nextIndexKey = null ; 
 + nextRowPositionFromIndex = dataFile . length ( ) ; 
 + } 
 
 - ByteBuffer currentIndexKey = nextIndexKey ; 
 - long nextRowPositionFromIndex ; 
 - try 
 + long dataStart = dataFile . getFilePointer ( ) ; 
 + long dataStartFromIndex = currentIndexKey = = null 
 + ? - 1 
 + : rowStart + 2 + currentIndexKey . remaining ( ) + ( sstable . descriptor . hasIntRowSize ? 4 : 8 ) ; 
 + long dataSizeFromIndex = nextRowPositionFromIndex - dataStartFromIndex ; 
 + assert currentIndexKey ! = null | | indexFile . isEOF ( ) ; 
 + if ( logger . isDebugEnabled ( ) & & currentIndexKey ! = null ) 
 + logger . debug ( String . format ( " Index doublecheck : row % s is % s bytes " , ByteBufferUtil . bytesToHex ( currentIndexKey ) , dataSizeFromIndex ) ) ; 
 + 
 + writer . mark ( ) ; 
 + try 
 + { 
 + if ( key = = null ) 
 + throw new IOError ( new IOException ( " Unable to read row key from data file " ) ) ; 
 + if ( dataSize > dataFile . length ( ) ) 
 + throw new IOError ( new IOException ( " Impossible row size " + dataSize ) ) ; 
 + SSTableIdentityIterator row = new SSTableIdentityIterator ( sstable , dataFile , key , dataStart , dataSize , true ) ; 
 + AbstractCompactedRow compactedRow = controller . getCompactedRow ( row ) ; 
 + if ( compactedRow . isEmpty ( ) ) 
 { 
 - nextIndexKey = indexFile . isEOF ( ) ? null : ByteBufferUtil . readWithShortLength ( indexFile ) ; 
 - nextRowPositionFromIndex = indexFile . isEOF ( ) ? dataFile . length ( ) : indexFile . readLong ( ) ; 
 + emptyRows + + ; 
 } 
 - catch ( Throwable th ) 
 + else 
 { 
 - logger . warn ( " Error reading index file " , th ) ; 
 - nextIndexKey = null ; 
 - nextRowPositionFromIndex = dataFile . length ( ) ; 
 + writer . append ( compactedRow ) ; 
 + goodRows + + ; 
 } 
 + if ( ! key . key . equals ( currentIndexKey ) | | dataStart ! = dataStartFromIndex ) 
 + logger . warn ( " Index file contained a different key or row size ; using key from data file " ) ; 
 + } 
 + catch ( Throwable th ) 
 + { 
 + throwIfFatal ( th ) ; 
 + logger . warn ( " Non - fatal error reading row ( stacktrace follows ) " , th ) ; 
 + writer . resetAndTruncate ( ) ; 
 
 - long dataStart = dataFile . getFilePointer ( ) ; 
 - long dataStartFromIndex = currentIndexKey = = null 
 - ? - 1 
 - : rowStart + 2 + currentIndexKey . remaining ( ) + ( sstable . descriptor . hasIntRowSize ? 4 : 8 ) ; 
 - long dataSizeFromIndex = nextRowPositionFromIndex - dataStartFromIndex ; 
 - assert currentIndexKey ! = null | | indexFile . isEOF ( ) ; 
 - if ( logger . isDebugEnabled ( ) & & currentIndexKey ! = null ) 
 - logger . debug ( String . format ( " Index doublecheck : row % s is % s bytes " , ByteBufferUtil . bytesToHex ( currentIndexKey ) , dataSizeFromIndex ) ) ; 
 - 
 - writer . mark ( ) ; 
 - try 
 - { 
 - if ( key = = null ) 
 - throw new IOError ( new IOException ( " Unable to read row key from data file " ) ) ; 
 - if ( dataSize > dataFile . length ( ) ) 
 - throw new IOError ( new IOException ( " Impossible row size " + dataSize ) ) ; 
 - SSTableIdentityIterator row = new SSTableIdentityIterator ( sstable , dataFile , key , dataStart , dataSize , true ) ; 
 - AbstractCompactedRow compactedRow = controller . getCompactedRow ( row ) ; 
 - if ( compactedRow . isEmpty ( ) ) 
 - { 
 - emptyRows + + ; 
 - } 
 - else 
 - { 
 - writer . append ( compactedRow ) ; 
 - goodRows + + ; 
 - } 
 - if ( ! key . key . equals ( currentIndexKey ) | | dataStart ! = dataStartFromIndex ) 
 - logger . warn ( " Index file contained a different key or row size ; using key from data file " ) ; 
 - } 
 - catch ( Throwable th ) 
 + if ( currentIndexKey ! = null 
 + & & ( key = = null | | ! key . key . equals ( currentIndexKey ) | | dataStart ! = dataStartFromIndex | | dataSize ! = dataSizeFromIndex ) ) 
 { 
 - throwIfFatal ( th ) ; 
 - logger . warn ( " Non - fatal error reading row ( stacktrace follows ) " , th ) ; 
 - writer . resetAndTruncate ( ) ; 
 - 
 - if ( currentIndexKey ! = null 
 - & & ( key = = null | | ! key . key . equals ( currentIndexKey ) | | dataStart ! = dataStartFromIndex | | dataSize ! = dataSizeFromIndex ) ) 
 + logger . info ( String . format ( " Retrying from row index ; data is % s bytes starting at % s " , 
 + dataSizeFromIndex , dataStartFromIndex ) ) ; 
 + key = SSTableReader . decodeKey ( sstable . partitioner , sstable . descriptor , currentIndexKey ) ; 
 + try 
 { 
 - logger . info ( String . format ( " Retrying from row index ; data is % s bytes starting at % s " , 
 - dataSizeFromIndex , dataStartFromIndex ) ) ; 
 - key = SSTableReader . decodeKey ( sstable . partitioner , sstable . descriptor , currentIndexKey ) ; 
 - try 
 + SSTableIdentityIterator row = new SSTableIdentityIterator ( sstable , dataFile , key , dataStartFromIndex , dataSizeFromIndex , true ) ; 
 + AbstractCompactedRow compactedRow = controller . getCompactedRow ( row ) ; 
 + if ( compactedRow . isEmpty ( ) ) 
 { 
 - SSTableIdentityIterator row = new SSTableIdentityIterator ( sstable , dataFile , key , dataStartFromIndex , dataSizeFromIndex , true ) ; 
 - AbstractCompactedRow compactedRow = controller . getCompactedRow ( row ) ; 
 - if ( compactedRow . isEmpty ( ) ) 
 - { 
 - emptyRows + + ; 
 - } 
 - else 
 - { 
 - writer . append ( compactedRow ) ; 
 - goodRows + + ; 
 - } 
 + emptyRows + + ; 
 } 
 - catch ( Throwable th2 ) 
 + else 
 { 
 - throwIfFatal ( th2 ) ; 
 - / / Skipping rows is dangerous for counters ( see CASSANDRA - 2759 ) 
 - if ( isCommutative ) 
 - throw new IOError ( th2 ) ; 
 - 
 - logger . warn ( " Retry failed too . Skipping to next row ( retry ' s stacktrace follows ) " , th2 ) ; 
 - writer . resetAndTruncate ( ) ; 
 - dataFile . seek ( nextRowPositionFromIndex ) ; 
 - badRows + + ; 
 + writer . append ( compactedRow ) ; 
 + goodRows + + ; 
 } 
 } 
 - else 
 + catch ( Throwable th2 ) 
 { 
 + throwIfFatal ( th2 ) ; 
 / / Skipping rows is dangerous for counters ( see CASSANDRA - 2759 ) 
 if ( isCommutative ) 
 - throw new IOError ( th ) ; 
 + throw new IOError ( th2 ) ; 
 
 - logger . warn ( " Row at " + dataStart + " is unreadable ; skipping to next " ) ; 
 - if ( currentIndexKey ! = null ) 
 - dataFile . seek ( nextRowPositionFromIndex ) ; 
 + logger . warn ( " Retry failed too . Skipping to next row ( retry ' s stacktrace follows ) " , th2 ) ; 
 + writer . resetAndTruncate ( ) ; 
 + dataFile . seek ( nextRowPositionFromIndex ) ; 
 badRows + + ; 
 } 
 } 
 + else 
 + { 
 + / / Skipping rows is dangerous for counters ( see CASSANDRA - 2759 ) 
 + if ( isCommutative ) 
 + throw new IOError ( th ) ; 
 + 
 + logger . warn ( " Row at " + dataStart + " is unreadable ; skipping to next " ) ; 
 + if ( currentIndexKey ! = null ) 
 + dataFile . seek ( nextRowPositionFromIndex ) ; 
 + badRows + + ; 
 + } 
 } 
 - 
 - if ( writer . getFilePointer ( ) > 0 ) 
 - newSstable = writer . closeAndOpenReader ( sstable . maxDataAge ) ; 
 - } 
 - finally 
 - { 
 - writer . cleanupIfNecessary ( ) ; 
 } 
 
 - if ( newSstable ! = null ) 
 - { 
 - cfs . replaceCompactedSSTables ( Arrays . asList ( sstable ) , Arrays . asList ( newSstable ) ) ; 
 - logger . info ( " Scrub of " + sstable + " complete : " + goodRows + " rows in new sstable and " + emptyRows + " empty ( tombstoned ) rows dropped " ) ; 
 - if ( badRows > 0 ) 
 - logger . warn ( " Unable to recover " + badRows + " rows that were skipped . You can attempt manual recovery from the pre - scrub snapshot . You can also run nodetool repair to transfer the data from a healthy replica , if any " ) ; 
 - } 
 - else 
 - { 
 - cfs . markCompacted ( Arrays . asList ( sstable ) ) ; 
 - if ( badRows > 0 ) 
 - logger . warn ( " No valid rows found while scrubbing " + sstable + " ; it is marked for deletion now . If you want to attempt manual recovery , you can find a copy in the pre - scrub snapshot " ) ; 
 - else 
 - logger . info ( " Scrub of " + sstable + " complete ; looks like all " + emptyRows + " rows were tombstoned " ) ; 
 - } 
 + if ( writer . getFilePointer ( ) > 0 ) 
 + newSstable = writer . closeAndOpenReader ( sstable . maxDataAge ) ; 
 } 
 finally 
 { 
 + if ( writer ! = null ) 
 + writer . cleanupIfNecessary ( ) ; 
 FileUtils . closeQuietly ( dataFile ) ; 
 FileUtils . closeQuietly ( indexFile ) ; 
 
 executor . finishCompaction ( scrubInfo ) ; 
 } 
 + 
 + if ( newSstable = = null ) 
 + { 
 + cfs . markCompacted ( Arrays . asList ( sstable ) ) ; 
 + if ( badRows > 0 ) 
 + logger . warn ( " No valid rows found while scrubbing " + sstable + " ; it is marked for deletion now . If you want to attempt manual recovery , you can find a copy in the pre - scrub snapshot " ) ; 
 + else 
 + logger . info ( " Scrub of " + sstable + " complete ; looks like all " + emptyRows + " rows were tombstoned " ) ; 
 + } 
 + else 
 + { 
 + cfs . replaceCompactedSSTables ( Arrays . asList ( sstable ) , Arrays . asList ( newSstable ) ) ; 
 + logger . info ( " Scrub of " + sstable + " complete : " + goodRows + " rows in new sstable and " + emptyRows + " empty ( tombstoned ) rows dropped " ) ; 
 + if ( badRows > 0 ) 
 + logger . warn ( " Unable to recover " + badRows + " rows that were skipped . You can attempt manual recovery from the pre - scrub snapshot . You can also run nodetool repair to transfer the data from a healthy replica , if any " ) ; 
 + } 
 } 
 
 private void throwIfFatal ( Throwable th )
