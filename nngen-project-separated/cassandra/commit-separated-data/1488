BLEU SCORE: 0.06220117374063391

TEST MSG: Fix NullPointerException creating digest
GENERATED MSG: merge from 0 . 6

TEST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / ReadResponse . java b / src / java / org / apache / cassandra / db / ReadResponse . java <nl> index 39022a4 . . 6a8aa8c 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ReadResponse . java <nl> + + + b / src / java / org / apache / cassandra / db / ReadResponse . java <nl> @ @ - 19 , 6 + 19 , 7 @ @ package org . apache . cassandra . db ; <nl> <nl> import java . io . * ; <nl> import java . nio . ByteBuffer ; <nl> + import java . util . concurrent . atomic . AtomicReferenceFieldUpdater ; <nl> <nl> import org . apache . cassandra . io . IVersionedSerializer ; <nl> import org . apache . cassandra . io . util . DataOutputPlus ; <nl> @ @ - 32 , 22 + 33 , 27 @ @ import org . apache . cassandra . utils . ByteBufferUtil ; <nl> public class ReadResponse <nl> { <nl> public static final IVersionedSerializer < ReadResponse > serializer = new ReadResponseSerializer ( ) ; <nl> + private static final AtomicReferenceFieldUpdater < ReadResponse , ByteBuffer > digestUpdater = AtomicReferenceFieldUpdater . newUpdater ( ReadResponse . class , ByteBuffer . class , " digest " ) ; <nl> <nl> private final Row row ; <nl> - private final ByteBuffer digest ; <nl> + private volatile ByteBuffer digest ; <nl> <nl> public ReadResponse ( ByteBuffer digest ) <nl> { <nl> + this ( null , digest ) ; <nl> assert digest ! = null ; <nl> - this . digest = digest ; <nl> - this . row = null ; <nl> } <nl> <nl> public ReadResponse ( Row row ) <nl> { <nl> + this ( row , null ) ; <nl> assert row ! = null ; <nl> + } <nl> + <nl> + public ReadResponse ( Row row , ByteBuffer digest ) <nl> + { <nl> this . row = row ; <nl> - this . digest = null ; <nl> + this . digest = digest ; <nl> } <nl> <nl> public Row row ( ) <nl> @ @ - 60 , 9 + 66 , 19 @ @ public class ReadResponse <nl> return digest ; <nl> } <nl> <nl> + public void setDigest ( ByteBuffer digest ) <nl> + { <nl> + ByteBuffer curr = this . digest ; <nl> + if ( ! digestUpdater . compareAndSet ( this , curr , digest ) ) <nl> + { <nl> + assert digest . equals ( this . digest ) : <nl> + String . format ( " Digest mismatch : % s vs % s " , digest . array ( ) , this . digest . array ( ) ) ; <nl> + } <nl> + } <nl> + <nl> public boolean isDigestQuery ( ) <nl> { <nl> - return digest ! = null ; <nl> + return digest ! = null & & row = = null ; <nl> } <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / service / RangeSliceResponseResolver . java b / src / java / org / apache / cassandra / service / RangeSliceResponseResolver . java <nl> index 640681b . . 4242481 100644 <nl> - - - a / src / java / org / apache / cassandra / service / RangeSliceResponseResolver . java <nl> + + + b / src / java / org / apache / cassandra / service / RangeSliceResponseResolver . java <nl> @ @ - 50 , 8 + 50 , 8 @ @ public class RangeSliceResponseResolver implements IResponseResolver < RangeSliceR <nl> private final String keyspaceName ; <nl> private final long timestamp ; <nl> private List < InetAddress > sources ; <nl> - protected final Collection < MessageIn < RangeSliceReply > > responses = new ConcurrentLinkedQueue < MessageIn < RangeSliceReply > > ( ) ; <nl> - public final List < AsyncOneResponse > repairResults = new ArrayList < AsyncOneResponse > ( ) ; <nl> + protected final Queue < MessageIn < RangeSliceReply > > responses = new ConcurrentLinkedQueue < > ( ) ; <nl> + public final List < AsyncOneResponse > repairResults = new ArrayList < > ( ) ; <nl> <nl> public RangeSliceResponseResolver ( String keyspaceName , long timestamp ) <nl> { <nl> @ @ - 66 , 15 + 66 , 15 @ @ public class RangeSliceResponseResolver implements IResponseResolver < RangeSliceR <nl> <nl> public List < Row > getData ( ) <nl> { <nl> - MessageIn < RangeSliceReply > response = responses . iterator ( ) . next ( ) ; <nl> - return response . payload . rows ; <nl> + assert ! responses . isEmpty ( ) ; <nl> + return responses . peek ( ) . payload . rows ; <nl> } <nl> <nl> / / Note : this would deserialize the response a 2nd time if getData was called first . <nl> / / ( this is not currently an issue since we don ' t do read repair for range queries . ) <nl> public Iterable < Row > resolve ( ) <nl> { <nl> - ArrayList < RowIterator > iters = new ArrayList < RowIterator > ( responses . size ( ) ) ; <nl> + ArrayList < RowIterator > iters = new ArrayList < > ( responses . size ( ) ) ; <nl> int n = 0 ; <nl> for ( MessageIn < RangeSliceReply > response : responses ) <nl> { <nl> @ @ - 86 , 7 + 86 , 7 @ @ public class RangeSliceResponseResolver implements IResponseResolver < RangeSliceR <nl> / / TODO do we need to call close ? <nl> CloseableIterator < Row > iter = MergeIterator . get ( iters , pairComparator , new Reducer ( ) ) ; <nl> <nl> - List < Row > resolvedRows = new ArrayList < Row > ( n ) ; <nl> + List < Row > resolvedRows = new ArrayList < > ( n ) ; <nl> while ( iter . hasNext ( ) ) <nl> resolvedRows . add ( iter . next ( ) ) ; <nl> <nl> @ @ - 129 , 7 + 129 , 7 @ @ public class RangeSliceResponseResolver implements IResponseResolver < RangeSliceR <nl> <nl> private class Reducer extends MergeIterator . Reducer < Pair < Row , InetAddress > , Row > <nl> { <nl> - List < ColumnFamily > versions = new ArrayList < ColumnFamily > ( sources . size ( ) ) ; <nl> + List < ColumnFamily > versions = new ArrayList < > ( sources . size ( ) ) ; <nl> List < InetAddress > versionSources = new ArrayList < InetAddress > ( sources . size ( ) ) ; <nl> DecoratedKey key ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / service / ReadCallback . java b / src / java / org / apache / cassandra / service / ReadCallback . java <nl> index cf9be55 . . e0646a9 100644 <nl> - - - a / src / java / org / apache / cassandra / service / ReadCallback . java <nl> + + + b / src / java / org / apache / cassandra / service / ReadCallback . java <nl> @ @ - 121 , 6 + 121 , 7 @ @ public class ReadCallback < TMessage , TResolved > implements IAsyncCallback < TMessag <nl> if ( n > = blockfor & & resolver . isDataPresent ( ) ) <nl> { <nl> condition . signalAll ( ) ; <nl> + <nl> / / kick off a background digest comparison if this is a result that ( may have ) arrived after <nl> / / the original resolve that get ( ) kicks off as soon as the condition is signaled <nl> if ( blockfor < endpoints . size ( ) & & n = = endpoints . size ( ) ) <nl> diff - - git a / src / java / org / apache / cassandra / service / RowDigestResolver . java b / src / java / org / apache / cassandra / service / RowDigestResolver . java <nl> index 21b16bf . . 7f2e17d 100644 <nl> - - - a / src / java / org / apache / cassandra / service / RowDigestResolver . java <nl> + + + b / src / java / org / apache / cassandra / service / RowDigestResolver . java <nl> @ @ - 41 , 7 + 41 , 12 @ @ public class RowDigestResolver extends AbstractRowResolver <nl> { <nl> ReadResponse result = message . payload ; <nl> if ( ! result . isDigestQuery ( ) ) <nl> + { <nl> + if ( result . digest ( ) = = null ) <nl> + result . setDigest ( ColumnFamily . digest ( result . row ( ) . cf ) ) ; <nl> + <nl> return result . row ( ) ; <nl> + } <nl> } <nl> return null ; <nl> } <nl> @ @ - 81 , 7 + 86 , 10 @ @ public class RowDigestResolver extends AbstractRowResolver <nl> { <nl> / / note that this allows for multiple data replies , post - CASSANDRA - 5932 <nl> data = response . row ( ) . cf ; <nl> - newDigest = ColumnFamily . digest ( data ) ; <nl> + if ( response . digest ( ) = = null ) <nl> + message . payload . setDigest ( ColumnFamily . digest ( data ) ) ; <nl> + <nl> + newDigest = response . digest ( ) ; <nl> } <nl> <nl> if ( digest = = null )
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 8d9e2ea . . c97b17f 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 7 + 1 , 10 @ @ <nl> dev <nl> * sstable versioning ( CASSANDRA - 389 ) <nl> <nl> - 0 . 6 . 0 - dev <nl> + 0 . 6 . 0 - RC1 <nl> + * fix compaction bucketing bug ( CASSANDRA - 814 ) <nl> + <nl> + 0 . 6 . 0 - beta1 / beta2 <nl> * add batch _ mutate thrift command , deprecating batch _ insert ( CASSANDRA - 336 ) <nl> * remove get _ key _ range Thrift API , deprecated in 0 . 5 ( CASSANDRA - 710 ) <nl> * add optional login ( ) Thrift call for authentication ( CASSANDRA - 547 ) <nl> @ @ - 42 , 7 + 45 , 9 @ @ dev <nl> * allow larger numbers of keys ( > 140M ) in a sstable bloom filter <nl> ( CASSANDRA - 790 ) <nl> * include jvm argument improvements from CASSANDRA - 504 in debian package <nl> - * change streaming chunk size to 32MB ( was 64MB ) ( CASSANDRA - 795 ) <nl> + * change streaming chunk size to 32MB to accomodate Windows XP limitations <nl> + ( was 64MB ) ( CASSANDRA - 795 ) <nl> + * fix get _ range _ slice returning results in the wrong order ( CASSANDRA - 781 ) <nl> <nl> <nl> 0 . 5 . 0 final <nl> diff - - git a / build . xml b / build . xml <nl> index ea79876 . . 54ef12d 100644 <nl> - - - a / build . xml <nl> + + + b / build . xml <nl> @ @ - 314 , 6 + 314 , 7 @ @ <nl> < include name = " * * " / > <nl> < exclude name = " build / * * " / > <nl> < exclude name = " src / gen - java / * * " / > <nl> + < exclude name = " interface / avro / * * " / > <nl> < / tarfileset > <nl> < / tar > <nl> < / target > <nl> diff - - git a / src / java / org / apache / cassandra / db / CompactionManager . java b / src / java / org / apache / cassandra / db / CompactionManager . java <nl> index fbd5ebb . . 1b5b6b1 100644 <nl> - - - a / src / java / org / apache / cassandra / db / CompactionManager . java <nl> + + + b / src / java / org / apache / cassandra / db / CompactionManager . java <nl> @ @ - 89 , 7 + 89 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> return 0 ; <nl> } <nl> logger . debug ( " Checking to see if compaction of " + cfs . columnFamily _ + " would be useful " ) ; <nl> - Set < List < SSTableReader > > buckets = getCompactionBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; <nl> + Set < List < SSTableReader > > buckets = getBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; <nl> updateEstimateFor ( cfs , buckets ) ; <nl> <nl> for ( List < SSTableReader > sstables : buckets ) <nl> @ @ - 441 , 7 + 441 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> / * <nl> * Group files of similar size into buckets . <nl> * / <nl> - static Set < List < SSTableReader > > getCompactionBuckets ( Iterable < SSTableReader > files , long min ) <nl> + static Set < List < SSTableReader > > getBuckets ( Iterable < SSTableReader > files , long min ) <nl> { <nl> Map < List < SSTableReader > , Long > buckets = new HashMap < List < SSTableReader > , Long > ( ) ; <nl> for ( SSTableReader sstable : files ) <nl> @ @ - 461 , 7 + 461 , 8 @ @ public class CompactionManager implements CompactionManagerMBean <nl> { <nl> / / remove and re - add because adding changes the hash <nl> buckets . remove ( bucket ) ; <nl> - averageSize = ( averageSize + size ) / 2 ; <nl> + long totalSize = bucket . size ( ) * averageSize ; <nl> + averageSize = ( totalSize + size ) / ( bucket . size ( ) + 1 ) ; <nl> bucket . add ( sstable ) ; <nl> buckets . put ( bucket , averageSize ) ; <nl> bFound = true ; <nl> @ @ - 538 , 7 + 539 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> public void run ( ) <nl> { <nl> logger . debug ( " Estimating compactions for " + cfs . columnFamily _ ) ; <nl> - final Set < List < SSTableReader > > buckets = getCompactionBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; <nl> + final Set < List < SSTableReader > > buckets = getBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; <nl> updateEstimateFor ( cfs , buckets ) ; <nl> } <nl> } ;

TEST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / ReadResponse . java b / src / java / org / apache / cassandra / db / ReadResponse . java 
 index 39022a4 . . 6a8aa8c 100644 
 - - - a / src / java / org / apache / cassandra / db / ReadResponse . java 
 + + + b / src / java / org / apache / cassandra / db / ReadResponse . java 
 @ @ - 19 , 6 + 19 , 7 @ @ package org . apache . cassandra . db ; 
 
 import java . io . * ; 
 import java . nio . ByteBuffer ; 
 + import java . util . concurrent . atomic . AtomicReferenceFieldUpdater ; 
 
 import org . apache . cassandra . io . IVersionedSerializer ; 
 import org . apache . cassandra . io . util . DataOutputPlus ; 
 @ @ - 32 , 22 + 33 , 27 @ @ import org . apache . cassandra . utils . ByteBufferUtil ; 
 public class ReadResponse 
 { 
 public static final IVersionedSerializer < ReadResponse > serializer = new ReadResponseSerializer ( ) ; 
 + private static final AtomicReferenceFieldUpdater < ReadResponse , ByteBuffer > digestUpdater = AtomicReferenceFieldUpdater . newUpdater ( ReadResponse . class , ByteBuffer . class , " digest " ) ; 
 
 private final Row row ; 
 - private final ByteBuffer digest ; 
 + private volatile ByteBuffer digest ; 
 
 public ReadResponse ( ByteBuffer digest ) 
 { 
 + this ( null , digest ) ; 
 assert digest ! = null ; 
 - this . digest = digest ; 
 - this . row = null ; 
 } 
 
 public ReadResponse ( Row row ) 
 { 
 + this ( row , null ) ; 
 assert row ! = null ; 
 + } 
 + 
 + public ReadResponse ( Row row , ByteBuffer digest ) 
 + { 
 this . row = row ; 
 - this . digest = null ; 
 + this . digest = digest ; 
 } 
 
 public Row row ( ) 
 @ @ - 60 , 9 + 66 , 19 @ @ public class ReadResponse 
 return digest ; 
 } 
 
 + public void setDigest ( ByteBuffer digest ) 
 + { 
 + ByteBuffer curr = this . digest ; 
 + if ( ! digestUpdater . compareAndSet ( this , curr , digest ) ) 
 + { 
 + assert digest . equals ( this . digest ) : 
 + String . format ( " Digest mismatch : % s vs % s " , digest . array ( ) , this . digest . array ( ) ) ; 
 + } 
 + } 
 + 
 public boolean isDigestQuery ( ) 
 { 
 - return digest ! = null ; 
 + return digest ! = null & & row = = null ; 
 } 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / service / RangeSliceResponseResolver . java b / src / java / org / apache / cassandra / service / RangeSliceResponseResolver . java 
 index 640681b . . 4242481 100644 
 - - - a / src / java / org / apache / cassandra / service / RangeSliceResponseResolver . java 
 + + + b / src / java / org / apache / cassandra / service / RangeSliceResponseResolver . java 
 @ @ - 50 , 8 + 50 , 8 @ @ public class RangeSliceResponseResolver implements IResponseResolver < RangeSliceR 
 private final String keyspaceName ; 
 private final long timestamp ; 
 private List < InetAddress > sources ; 
 - protected final Collection < MessageIn < RangeSliceReply > > responses = new ConcurrentLinkedQueue < MessageIn < RangeSliceReply > > ( ) ; 
 - public final List < AsyncOneResponse > repairResults = new ArrayList < AsyncOneResponse > ( ) ; 
 + protected final Queue < MessageIn < RangeSliceReply > > responses = new ConcurrentLinkedQueue < > ( ) ; 
 + public final List < AsyncOneResponse > repairResults = new ArrayList < > ( ) ; 
 
 public RangeSliceResponseResolver ( String keyspaceName , long timestamp ) 
 { 
 @ @ - 66 , 15 + 66 , 15 @ @ public class RangeSliceResponseResolver implements IResponseResolver < RangeSliceR 
 
 public List < Row > getData ( ) 
 { 
 - MessageIn < RangeSliceReply > response = responses . iterator ( ) . next ( ) ; 
 - return response . payload . rows ; 
 + assert ! responses . isEmpty ( ) ; 
 + return responses . peek ( ) . payload . rows ; 
 } 
 
 / / Note : this would deserialize the response a 2nd time if getData was called first . 
 / / ( this is not currently an issue since we don ' t do read repair for range queries . ) 
 public Iterable < Row > resolve ( ) 
 { 
 - ArrayList < RowIterator > iters = new ArrayList < RowIterator > ( responses . size ( ) ) ; 
 + ArrayList < RowIterator > iters = new ArrayList < > ( responses . size ( ) ) ; 
 int n = 0 ; 
 for ( MessageIn < RangeSliceReply > response : responses ) 
 { 
 @ @ - 86 , 7 + 86 , 7 @ @ public class RangeSliceResponseResolver implements IResponseResolver < RangeSliceR 
 / / TODO do we need to call close ? 
 CloseableIterator < Row > iter = MergeIterator . get ( iters , pairComparator , new Reducer ( ) ) ; 
 
 - List < Row > resolvedRows = new ArrayList < Row > ( n ) ; 
 + List < Row > resolvedRows = new ArrayList < > ( n ) ; 
 while ( iter . hasNext ( ) ) 
 resolvedRows . add ( iter . next ( ) ) ; 
 
 @ @ - 129 , 7 + 129 , 7 @ @ public class RangeSliceResponseResolver implements IResponseResolver < RangeSliceR 
 
 private class Reducer extends MergeIterator . Reducer < Pair < Row , InetAddress > , Row > 
 { 
 - List < ColumnFamily > versions = new ArrayList < ColumnFamily > ( sources . size ( ) ) ; 
 + List < ColumnFamily > versions = new ArrayList < > ( sources . size ( ) ) ; 
 List < InetAddress > versionSources = new ArrayList < InetAddress > ( sources . size ( ) ) ; 
 DecoratedKey key ; 
 
 diff - - git a / src / java / org / apache / cassandra / service / ReadCallback . java b / src / java / org / apache / cassandra / service / ReadCallback . java 
 index cf9be55 . . e0646a9 100644 
 - - - a / src / java / org / apache / cassandra / service / ReadCallback . java 
 + + + b / src / java / org / apache / cassandra / service / ReadCallback . java 
 @ @ - 121 , 6 + 121 , 7 @ @ public class ReadCallback < TMessage , TResolved > implements IAsyncCallback < TMessag 
 if ( n > = blockfor & & resolver . isDataPresent ( ) ) 
 { 
 condition . signalAll ( ) ; 
 + 
 / / kick off a background digest comparison if this is a result that ( may have ) arrived after 
 / / the original resolve that get ( ) kicks off as soon as the condition is signaled 
 if ( blockfor < endpoints . size ( ) & & n = = endpoints . size ( ) ) 
 diff - - git a / src / java / org / apache / cassandra / service / RowDigestResolver . java b / src / java / org / apache / cassandra / service / RowDigestResolver . java 
 index 21b16bf . . 7f2e17d 100644 
 - - - a / src / java / org / apache / cassandra / service / RowDigestResolver . java 
 + + + b / src / java / org / apache / cassandra / service / RowDigestResolver . java 
 @ @ - 41 , 7 + 41 , 12 @ @ public class RowDigestResolver extends AbstractRowResolver 
 { 
 ReadResponse result = message . payload ; 
 if ( ! result . isDigestQuery ( ) ) 
 + { 
 + if ( result . digest ( ) = = null ) 
 + result . setDigest ( ColumnFamily . digest ( result . row ( ) . cf ) ) ; 
 + 
 return result . row ( ) ; 
 + } 
 } 
 return null ; 
 } 
 @ @ - 81 , 7 + 86 , 10 @ @ public class RowDigestResolver extends AbstractRowResolver 
 { 
 / / note that this allows for multiple data replies , post - CASSANDRA - 5932 
 data = response . row ( ) . cf ; 
 - newDigest = ColumnFamily . digest ( data ) ; 
 + if ( response . digest ( ) = = null ) 
 + message . payload . setDigest ( ColumnFamily . digest ( data ) ) ; 
 + 
 + newDigest = response . digest ( ) ; 
 } 
 
 if ( digest = = null )

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 8d9e2ea . . c97b17f 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 7 + 1 , 10 @ @ 
 dev 
 * sstable versioning ( CASSANDRA - 389 ) 
 
 - 0 . 6 . 0 - dev 
 + 0 . 6 . 0 - RC1 
 + * fix compaction bucketing bug ( CASSANDRA - 814 ) 
 + 
 + 0 . 6 . 0 - beta1 / beta2 
 * add batch _ mutate thrift command , deprecating batch _ insert ( CASSANDRA - 336 ) 
 * remove get _ key _ range Thrift API , deprecated in 0 . 5 ( CASSANDRA - 710 ) 
 * add optional login ( ) Thrift call for authentication ( CASSANDRA - 547 ) 
 @ @ - 42 , 7 + 45 , 9 @ @ dev 
 * allow larger numbers of keys ( > 140M ) in a sstable bloom filter 
 ( CASSANDRA - 790 ) 
 * include jvm argument improvements from CASSANDRA - 504 in debian package 
 - * change streaming chunk size to 32MB ( was 64MB ) ( CASSANDRA - 795 ) 
 + * change streaming chunk size to 32MB to accomodate Windows XP limitations 
 + ( was 64MB ) ( CASSANDRA - 795 ) 
 + * fix get _ range _ slice returning results in the wrong order ( CASSANDRA - 781 ) 
 
 
 0 . 5 . 0 final 
 diff - - git a / build . xml b / build . xml 
 index ea79876 . . 54ef12d 100644 
 - - - a / build . xml 
 + + + b / build . xml 
 @ @ - 314 , 6 + 314 , 7 @ @ 
 < include name = " * * " / > 
 < exclude name = " build / * * " / > 
 < exclude name = " src / gen - java / * * " / > 
 + < exclude name = " interface / avro / * * " / > 
 < / tarfileset > 
 < / tar > 
 < / target > 
 diff - - git a / src / java / org / apache / cassandra / db / CompactionManager . java b / src / java / org / apache / cassandra / db / CompactionManager . java 
 index fbd5ebb . . 1b5b6b1 100644 
 - - - a / src / java / org / apache / cassandra / db / CompactionManager . java 
 + + + b / src / java / org / apache / cassandra / db / CompactionManager . java 
 @ @ - 89 , 7 + 89 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 return 0 ; 
 } 
 logger . debug ( " Checking to see if compaction of " + cfs . columnFamily _ + " would be useful " ) ; 
 - Set < List < SSTableReader > > buckets = getCompactionBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; 
 + Set < List < SSTableReader > > buckets = getBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; 
 updateEstimateFor ( cfs , buckets ) ; 
 
 for ( List < SSTableReader > sstables : buckets ) 
 @ @ - 441 , 7 + 441 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 / * 
 * Group files of similar size into buckets . 
 * / 
 - static Set < List < SSTableReader > > getCompactionBuckets ( Iterable < SSTableReader > files , long min ) 
 + static Set < List < SSTableReader > > getBuckets ( Iterable < SSTableReader > files , long min ) 
 { 
 Map < List < SSTableReader > , Long > buckets = new HashMap < List < SSTableReader > , Long > ( ) ; 
 for ( SSTableReader sstable : files ) 
 @ @ - 461 , 7 + 461 , 8 @ @ public class CompactionManager implements CompactionManagerMBean 
 { 
 / / remove and re - add because adding changes the hash 
 buckets . remove ( bucket ) ; 
 - averageSize = ( averageSize + size ) / 2 ; 
 + long totalSize = bucket . size ( ) * averageSize ; 
 + averageSize = ( totalSize + size ) / ( bucket . size ( ) + 1 ) ; 
 bucket . add ( sstable ) ; 
 buckets . put ( bucket , averageSize ) ; 
 bFound = true ; 
 @ @ - 538 , 7 + 539 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 public void run ( ) 
 { 
 logger . debug ( " Estimating compactions for " + cfs . columnFamily _ ) ; 
 - final Set < List < SSTableReader > > buckets = getCompactionBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; 
 + final Set < List < SSTableReader > > buckets = getBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; 
 updateEstimateFor ( cfs , buckets ) ; 
 } 
 } ;
