BLEU SCORE: 0.0039670168611924085

TEST MSG: remove unused legacy migration code
GENERATED MSG: use Avro objects in ColumnFamilyOutputFormat . patch by Stu Hood ; reviewed by jbellis for CASSANDRA - 1315

TEST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / schema / IndexMetadata . java b / src / java / org / apache / cassandra / schema / IndexMetadata . java <nl> index 04e06ab . . b8c4854 100644 <nl> - - - a / src / java / org / apache / cassandra / schema / IndexMetadata . java <nl> + + + b / src / java / org / apache / cassandra / schema / IndexMetadata . java <nl> @ @ - 47 , 7 + 47 , 7 @ @ import org . apache . cassandra . utils . UUIDSerializer ; <nl> public final class IndexMetadata <nl> { <nl> private static final Logger logger = LoggerFactory . getLogger ( IndexMetadata . class ) ; <nl> - <nl> + <nl> private static final Pattern PATTERN _ NON _ WORD _ CHAR = Pattern . compile ( " \ \ W " ) ; <nl> private static final Pattern PATTERN _ WORD _ CHARS = Pattern . compile ( " \ \ w + " ) ; <nl> <nl> @ @ - 76 , 42 + 76 , 6 @ @ public final class IndexMetadata <nl> this . kind = kind ; <nl> } <nl> <nl> - public static IndexMetadata fromLegacyMetadata ( CFMetaData cfm , <nl> - ColumnDefinition column , <nl> - String name , <nl> - Kind kind , <nl> - Map < String , String > options ) <nl> - { <nl> - Map < String , String > newOptions = new HashMap < > ( ) ; <nl> - if ( options ! = null ) <nl> - newOptions . putAll ( options ) ; <nl> - <nl> - IndexTarget target ; <nl> - if ( newOptions . containsKey ( IndexTarget . INDEX _ KEYS _ OPTION _ NAME ) ) <nl> - { <nl> - newOptions . remove ( IndexTarget . INDEX _ KEYS _ OPTION _ NAME ) ; <nl> - target = new IndexTarget ( column . name , IndexTarget . Type . KEYS ) ; <nl> - } <nl> - else if ( newOptions . containsKey ( IndexTarget . INDEX _ ENTRIES _ OPTION _ NAME ) ) <nl> - { <nl> - newOptions . remove ( IndexTarget . INDEX _ KEYS _ OPTION _ NAME ) ; <nl> - target = new IndexTarget ( column . name , IndexTarget . Type . KEYS _ AND _ VALUES ) ; <nl> - } <nl> - else <nl> - { <nl> - if ( column . type . isCollection ( ) & & ! column . type . isMultiCell ( ) ) <nl> - { <nl> - target = new IndexTarget ( column . name , IndexTarget . Type . FULL ) ; <nl> - } <nl> - else <nl> - { <nl> - target = new IndexTarget ( column . name , IndexTarget . Type . VALUES ) ; <nl> - } <nl> - } <nl> - newOptions . put ( IndexTarget . TARGET _ OPTION _ NAME , target . asCqlString ( cfm ) ) ; <nl> - return new IndexMetadata ( name , newOptions , kind ) ; <nl> - } <nl> - <nl> public static IndexMetadata fromSchemaMetadata ( String name , Kind kind , Map < String , String > options ) <nl> { <nl> return new IndexMetadata ( name , options , kind ) ;
NEAREST DIFF (one line): diff - - git a / src / cassandra . avpr b / src / cassandra . avpr <nl> new file mode 100644 <nl> index 0000000 . . 41ff763 <nl> - - - / dev / null <nl> + + + b / src / cassandra . avpr <nl> @ @ - 0 , 0 + 1 , 202 @ @ <nl> + { <nl> + " protocol " : " InterNode " , <nl> + " namespace " : " org . apache . cassandra " , <nl> + " types " : [ { <nl> + " type " : " fixed " , <nl> + " name " : " UUID " , <nl> + " namespace " : " org . apache . cassandra . utils . avro " , <nl> + " size " : 16 <nl> + } , { <nl> + " type " : " enum " , <nl> + " name " : " IndexType " , <nl> + " namespace " : " org . apache . cassandra . config . avro " , <nl> + " symbols " : [ " KEYS " ] <nl> + } , { <nl> + " type " : " record " , <nl> + " name " : " ColumnDef " , <nl> + " namespace " : " org . apache . cassandra . config . avro " , <nl> + " fields " : [ { <nl> + " name " : " name " , <nl> + " type " : " bytes " <nl> + } , { <nl> + " name " : " validation _ class " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " index _ type " , <nl> + " type " : [ " org . apache . cassandra . config . avro . IndexType " , " null " ] <nl> + } , { <nl> + " name " : " index _ name " , <nl> + " type " : [ " string " , " null " ] <nl> + } ] <nl> + } , { <nl> + " type " : " record " , <nl> + " name " : " CfDef " , <nl> + " namespace " : " org . apache . cassandra . config . avro " , <nl> + " fields " : [ { <nl> + " name " : " keyspace " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " name " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " column _ type " , <nl> + " type " : [ " string " , " null " ] <nl> + } , { <nl> + " name " : " clock _ type " , <nl> + " type " : [ " string " , " null " ] <nl> + } , { <nl> + " name " : " comparator _ type " , <nl> + " type " : [ " string " , " null " ] <nl> + } , { <nl> + " name " : " subcomparator _ type " , <nl> + " type " : [ " string " , " null " ] <nl> + } , { <nl> + " name " : " reconciler " , <nl> + " type " : [ " string " , " null " ] <nl> + } , { <nl> + " name " : " comment " , <nl> + " type " : [ " string " , " null " ] <nl> + } , { <nl> + " name " : " row _ cache _ size " , <nl> + " type " : [ " double " , " null " ] <nl> + } , { <nl> + " name " : " preload _ row _ cache " , <nl> + " type " : [ " boolean " , " null " ] <nl> + } , { <nl> + " name " : " key _ cache _ size " , <nl> + " type " : [ " double " , " null " ] <nl> + } , { <nl> + " name " : " read _ repair _ chance " , <nl> + " type " : [ " double " , " null " ] <nl> + } , { <nl> + " name " : " gc _ grace _ seconds " , <nl> + " type " : [ " int " , " null " ] <nl> + } , { <nl> + " name " : " column _ metadata " , <nl> + " type " : [ { <nl> + " type " : " array " , <nl> + " items " : " org . apache . cassandra . config . avro . ColumnDef " <nl> + } , " null " ] <nl> + } , { <nl> + " name " : " id " , <nl> + " type " : [ " int " , " null " ] <nl> + } ] <nl> + } , { <nl> + " type " : " record " , <nl> + " name " : " KsDef " , <nl> + " namespace " : " org . apache . cassandra . config . avro " , <nl> + " fields " : [ { <nl> + " name " : " name " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " strategy _ class " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " strategy _ options " , <nl> + " type " : [ { <nl> + " type " : " map " , <nl> + " values " : " string " <nl> + } , " null " ] <nl> + } , { <nl> + " name " : " replication _ factor " , <nl> + " type " : " int " <nl> + } , { <nl> + " name " : " cf _ defs " , <nl> + " type " : { <nl> + " type " : " array " , <nl> + " items " : " org . apache . cassandra . config . avro . CfDef " <nl> + } <nl> + } ] <nl> + } , { <nl> + " type " : " record " , <nl> + " name " : " AddColumnFamily " , <nl> + " namespace " : " org . apache . cassandra . db . migration . avro " , <nl> + " fields " : [ { <nl> + " name " : " cf " , <nl> + " type " : " org . apache . cassandra . config . avro . CfDef " <nl> + } ] <nl> + } , { <nl> + " type " : " record " , <nl> + " name " : " AddKeyspace " , <nl> + " namespace " : " org . apache . cassandra . db . migration . avro " , <nl> + " fields " : [ { <nl> + " name " : " ks " , <nl> + " type " : " org . apache . cassandra . config . avro . KsDef " <nl> + } ] <nl> + } , { <nl> + " type " : " record " , <nl> + " name " : " DropColumnFamily " , <nl> + " namespace " : " org . apache . cassandra . db . migration . avro " , <nl> + " fields " : [ { <nl> + " name " : " ksname " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " cfname " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " block _ on _ deletion " , <nl> + " type " : " boolean " <nl> + } ] <nl> + } , { <nl> + " type " : " record " , <nl> + " name " : " DropKeyspace " , <nl> + " namespace " : " org . apache . cassandra . db . migration . avro " , <nl> + " fields " : [ { <nl> + " name " : " ksname " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " block _ on _ deletion " , <nl> + " type " : " boolean " <nl> + } ] <nl> + } , { <nl> + " type " : " record " , <nl> + " name " : " RenameColumnFamily " , <nl> + " namespace " : " org . apache . cassandra . db . migration . avro " , <nl> + " fields " : [ { <nl> + " name " : " ksname " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " cfid " , <nl> + " type " : " int " <nl> + } , { <nl> + " name " : " old _ cfname " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " new _ cfname " , <nl> + " type " : " string " <nl> + } ] <nl> + } , { <nl> + " type " : " record " , <nl> + " name " : " RenameKeyspace " , <nl> + " namespace " : " org . apache . cassandra . db . migration . avro " , <nl> + " fields " : [ { <nl> + " name " : " old _ ksname " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " new _ ksname " , <nl> + " type " : " string " <nl> + } ] <nl> + } , { <nl> + " type " : " record " , <nl> + " name " : " Migration " , <nl> + " namespace " : " org . apache . cassandra . db . migration . avro " , <nl> + " fields " : [ { <nl> + " name " : " old _ version " , <nl> + " type " : " org . apache . cassandra . utils . avro . UUID " <nl> + } , { <nl> + " name " : " new _ version " , <nl> + " type " : " org . apache . cassandra . utils . avro . UUID " <nl> + } , { <nl> + " name " : " row _ mutation " , <nl> + " type " : " bytes " <nl> + } , { <nl> + " name " : " classname " , <nl> + " type " : " string " <nl> + } , { <nl> + " name " : " migration " , <nl> + " type " : [ " org . apache . cassandra . db . migration . avro . AddColumnFamily " , " org . apache . cassandra . db . migration . avro . DropColumnFamily " , " org . apache . cassandra . db . migration . avro . RenameColumnFamily " , " org . apache . cassandra . db . migration . avro . AddKeyspace " , " org . apache . cassandra . db . migration . avro . DropKeyspace " , " org . apache . cassandra . db . migration . avro . RenameKeyspace " ] <nl> + } ] <nl> + } ] , <nl> + " messages " : { <nl> + } <nl> + } <nl> \ No newline at end of file <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java <nl> index 9731571 . . 98fbf70 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java <nl> @ @ - 25 , 10 + 25 , 10 @ @ import java . io . IOException ; <nl> import java . util . HashMap ; <nl> import java . util . List ; <nl> import java . util . Map ; <nl> + import java . nio . ByteBuffer ; <nl> <nl> import org . apache . cassandra . auth . SimpleAuthenticator ; <nl> - import org . apache . cassandra . db . ColumnFamily ; <nl> - import org . apache . cassandra . db . IColumn ; <nl> + import org . apache . cassandra . avro . Mutation ; <nl> import org . apache . cassandra . thrift . AuthenticationException ; <nl> import org . apache . cassandra . thrift . AuthenticationRequest ; <nl> import org . apache . cassandra . thrift . AuthorizationException ; <nl> @ @ - 64 , 11 + 64 , 11 @ @ import org . slf4j . LoggerFactory ; <nl> * < p > <nl> * For the sake of performance , this class employs a lazy write - back caching <nl> * mechanism , where its record writer batches mutations created based on the <nl> - * reduce ' s inputs ( in a task - specific map ) . When the writer is closed , then it <nl> - * makes the changes official by sending a batch mutate request to Cassandra . <nl> + * reduce ' s inputs ( in a task - specific map ) , and periodically makes the changes <nl> + * official by sending a batch mutate request to Cassandra . <nl> * < / p > <nl> * / <nl> - public class ColumnFamilyOutputFormat extends OutputFormat < byte [ ] , List < IColumn > > <nl> + public class ColumnFamilyOutputFormat extends OutputFormat < ByteBuffer , List < Mutation > > <nl> { <nl> private static final Logger logger = LoggerFactory . getLogger ( ColumnFamilyOutputFormat . class ) ; <nl> <nl> @ @ - 93 , 15 + 93 , 7 @ @ public class ColumnFamilyOutputFormat extends OutputFormat < byte [ ] , List < IColumn > > <nl> } <nl> <nl> / * * <nl> - * Get the output committer for this output format . This is responsible for <nl> - * ensuring the output is committed correctly . <nl> - * <nl> - * < p > <nl> - * This output format employs a lazy write - back caching mechanism , where the <nl> - * { @ link RecordWriter } is responsible for collecting mutations in the <nl> - * { @ link # MUTATIONS _ CACHE } , and the { @ link OutputCommitter } makes the <nl> - * changes official by making the change request to Cassandra . <nl> - * < / p > <nl> + * The OutputCommitter for this format does not write any data to the DFS . <nl> * <nl> * @ param context <nl> * the task context <nl> @ @ - 118 , 19 + 110 , 13 @ @ public class ColumnFamilyOutputFormat extends OutputFormat < byte [ ] , List < IColumn > > <nl> / * * <nl> * Get the { @ link RecordWriter } for the given task . <nl> * <nl> - * < p > <nl> - * As stated above , this { @ link RecordWriter } merely batches the mutations <nl> - * that it defines in the { @ link # MUTATIONS _ CACHE } . In other words , it <nl> - * doesn ' t literally cause any changes on the Cassandra server . <nl> - * < / p > <nl> - * <nl> * @ param context <nl> * the information about the current task . <nl> * @ return a { @ link RecordWriter } to write the output for the job . <nl> * @ throws IOException <nl> * / <nl> @ Override <nl> - public RecordWriter < byte [ ] , List < IColumn > > getRecordWriter ( final TaskAttemptContext context ) throws IOException , InterruptedException <nl> + public RecordWriter < ByteBuffer , List < Mutation > > getRecordWriter ( final TaskAttemptContext context ) throws IOException , InterruptedException <nl> { <nl> return new ColumnFamilyRecordWriter ( context ) ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java <nl> index 91b87ae . . 9b19f50 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java <nl> @ @ - 22 , 10 + 22 , 12 @ @ package org . apache . cassandra . hadoop ; <nl> * / <nl> import java . io . IOException ; <nl> import java . net . InetAddress ; <nl> + import java . nio . ByteBuffer ; <nl> import java . util . ArrayList ; <nl> import java . util . HashMap ; <nl> import java . util . List ; <nl> import java . util . Map ; <nl> + import java . util . TreeMap ; <nl> import java . util . concurrent . Callable ; <nl> import java . util . concurrent . ExecutionException ; <nl> import java . util . concurrent . ExecutorService ; <nl> @ @ - 33 , 7 + 35 , 7 @ @ import java . util . concurrent . Executors ; <nl> import java . util . concurrent . Future ; <nl> <nl> import org . apache . cassandra . client . RingCache ; <nl> - import org . apache . cassandra . db . IColumn ; <nl> + import static org . apache . cassandra . io . SerDeUtils . copy ; <nl> import org . apache . cassandra . thrift . Cassandra ; <nl> import org . apache . cassandra . thrift . Clock ; <nl> import org . apache . cassandra . thrift . Column ; <nl> @ @ - 43 , 6 + 45 , 8 @ @ import org . apache . cassandra . thrift . Deletion ; <nl> import org . apache . cassandra . thrift . Mutation ; <nl> import org . apache . cassandra . thrift . SlicePredicate ; <nl> import org . apache . cassandra . thrift . SliceRange ; <nl> + import org . apache . cassandra . thrift . SuperColumn ; <nl> + import org . apache . cassandra . utils . FBUtilities ; <nl> import org . apache . hadoop . mapreduce . OutputFormat ; <nl> import org . apache . hadoop . mapreduce . RecordWriter ; <nl> import org . apache . hadoop . mapreduce . TaskAttemptContext ; <nl> @ @ - 50 , 31 + 54 , 28 @ @ import org . apache . thrift . transport . TSocket ; <nl> <nl> / * * <nl> * The < code > ColumnFamilyRecordWriter < / code > maps the output & lt ; key , value & gt ; <nl> - * pairs to a Cassandra column family . In particular , it creates mutations for <nl> - * each column in the value , which it associates with the key , and in turn the <nl> - * responsible endpoint . <nl> + * pairs to a Cassandra column family . In particular , it applies all mutations <nl> + * in the value , which it associates with the key , and in turn the responsible <nl> + * endpoint . <nl> * <nl> * < p > <nl> * Note that , given that round trips to the server are fairly expensive , it <nl> - * merely batches the mutations in - memory ( specifically in <nl> - * { @ link ColumnFamilyOutputFormat # MUTATIONS _ CACHE } ) , and leaves it to the <nl> - * { @ link ColumnFamilyOutputCommitter } to send the batched mutations to the <nl> - * server in one shot . <nl> + * merely batches the mutations in - memory and periodically sends the batched <nl> + * mutations to the server in one shot . <nl> * < / p > <nl> * <nl> * < p > <nl> * Furthermore , this writer groups the mutations by the endpoint responsible for <nl> - * the rows being affected . This allows the { @ link ColumnFamilyOutputCommitter } <nl> - * to execute the mutations in parallel , on a endpoint - by - endpoint basis . <nl> + * the rows being affected . This allows the mutations to be executed in parallel , <nl> + * directly to a responsible endpoint . <nl> * < / p > <nl> * <nl> * @ author Karthick Sankarachary <nl> - * @ see ColumnFamilyOutputCommitter <nl> * @ see ColumnFamilyOutputFormat <nl> * @ see OutputFormat <nl> * <nl> * / <nl> - final class ColumnFamilyRecordWriter extends RecordWriter < byte [ ] , List < IColumn > > <nl> + final class ColumnFamilyRecordWriter extends RecordWriter < ByteBuffer , List < org . apache . cassandra . avro . Mutation > > <nl> { <nl> / / The task attempt context this writer is associated with . <nl> private final TaskAttemptContext context ; <nl> @ @ - 87 , 12 + 88 , 12 @ @ final class ColumnFamilyRecordWriter extends RecordWriter < byte [ ] , List < IColumn > > <nl> / / the endpoints they should be targeted at . The targeted endpoint <nl> / / essentially <nl> / / acts as the primary replica for the rows being affected by the mutations . <nl> - private RingCache ringCache ; <nl> + private final RingCache ringCache ; <nl> <nl> / / The number of mutations currently held in the mutations cache . <nl> private long batchSize = 0L ; <nl> / / The maximum number of mutations to hold in the mutations cache . <nl> - private long batchThreshold = Long . MAX _ VALUE ; <nl> + private final long batchThreshold ; <nl> <nl> / * * <nl> * Upon construction , obtain the map that this writer will use to collect <nl> @ @ - 144 , 75 + 145 , 92 @ @ final class ColumnFamilyRecordWriter extends RecordWriter < byte [ ] , List < IColumn > > <nl> * @ throws IOException <nl> * / <nl> @ Override <nl> - public synchronized void write ( byte [ ] key , List < IColumn > value ) throws IOException , InterruptedException <nl> + public synchronized void write ( ByteBuffer keybuff , List < org . apache . cassandra . avro . Mutation > value ) throws IOException , InterruptedException <nl> { <nl> maybeFlush ( ) ; <nl> + byte [ ] key = copy ( keybuff ) ; <nl> InetAddress endpoint = getEndpoint ( key ) ; <nl> Map < byte [ ] , Map < String , List < Mutation > > > mutationsByKey = mutationsByEndpoint . get ( endpoint ) ; <nl> if ( mutationsByKey = = null ) <nl> { <nl> - mutationsByKey = new HashMap < byte [ ] , Map < String , List < Mutation > > > ( ) ; <nl> + mutationsByKey = new TreeMap < byte [ ] , Map < String , List < Mutation > > > ( FBUtilities . byteArrayComparator ) ; <nl> mutationsByEndpoint . put ( endpoint , mutationsByKey ) ; <nl> } <nl> <nl> Map < String , List < Mutation > > cfMutation = new HashMap < String , List < Mutation > > ( ) ; <nl> mutationsByKey . put ( key , cfMutation ) ; <nl> <nl> - Clock clock = new Clock ( System . currentTimeMillis ( ) ) ; <nl> List < Mutation > mutationList = new ArrayList < Mutation > ( ) ; <nl> cfMutation . put ( ConfigHelper . getOutputColumnFamily ( context . getConfiguration ( ) ) , mutationList ) ; <nl> <nl> - if ( value = = null ) <nl> + for ( org . apache . cassandra . avro . Mutation amut : value ) <nl> + mutationList . add ( avroToThrift ( amut ) ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Deep copies the given Avro mutation into a new Thrift mutation . <nl> + * / <nl> + private Mutation avroToThrift ( org . apache . cassandra . avro . Mutation amut ) <nl> + { <nl> + Mutation mutation = new Mutation ( ) ; <nl> + org . apache . cassandra . avro . ColumnOrSuperColumn acosc = amut . column _ or _ supercolumn ; <nl> + if ( acosc ! = null ) <nl> { <nl> - Mutation mutation = new Mutation ( ) ; <nl> - Deletion deletion = new Deletion ( clock ) ; <nl> - mutation . setDeletion ( deletion ) ; <nl> - mutationList . add ( mutation ) ; <nl> + / / creation <nl> + ColumnOrSuperColumn cosc = new ColumnOrSuperColumn ( ) ; <nl> + mutation . setColumn _ or _ supercolumn ( cosc ) ; <nl> + if ( acosc . column ! = null ) <nl> + / / standard column <nl> + cosc . setColumn ( avroToThrift ( acosc . column ) ) ; <nl> + else <nl> + { <nl> + / / super column <nl> + byte [ ] scolname = copy ( acosc . super _ column . name ) ; <nl> + List < Column > scolcols = new ArrayList < Column > ( ( int ) acosc . super _ column . columns . size ( ) ) ; <nl> + for ( org . apache . cassandra . avro . Column acol : acosc . super _ column . columns ) <nl> + scolcols . add ( avroToThrift ( acol ) ) ; <nl> + cosc . setSuper _ column ( new SuperColumn ( scolname , scolcols ) ) ; <nl> + } <nl> } <nl> else <nl> { <nl> - List < byte [ ] > columnsToDelete = new ArrayList < byte [ ] > ( ) ; <nl> - for ( IColumn column : value ) <nl> + / / deletion <nl> + Deletion deletion = new Deletion ( avroToThrift ( amut . deletion . clock ) ) ; <nl> + mutation . setDeletion ( deletion ) ; <nl> + org . apache . cassandra . avro . SlicePredicate apred = amut . deletion . predicate ; <nl> + if ( amut . deletion . super _ column ! = null ) <nl> + / / super column <nl> + deletion . setSuper _ column ( copy ( amut . deletion . super _ column ) ) ; <nl> + else if ( apred . column _ names ! = null ) <nl> { <nl> - Mutation mutation = new Mutation ( ) ; <nl> - if ( column . value ( ) = = null ) <nl> - { <nl> - if ( columnsToDelete . size ( ) ! = 1 | | columnsToDelete . get ( 0 ) ! = null ) <nl> - { <nl> - if ( column . name ( ) = = null ) <nl> - columnsToDelete . clear ( ) ; <nl> - columnsToDelete . add ( column . name ( ) ) ; <nl> - } <nl> - } <nl> - else <nl> - { <nl> - <nl> - ColumnOrSuperColumn cosc = new ColumnOrSuperColumn ( ) ; <nl> - cosc . setColumn ( new Column ( column . name ( ) , column . value ( ) , clock ) ) ; <nl> - mutation . setColumn _ or _ supercolumn ( cosc ) ; <nl> - } <nl> - mutationList . add ( mutation ) ; <nl> + / / column names <nl> + List < byte [ ] > colnames = new ArrayList < byte [ ] > ( ( int ) apred . column _ names . size ( ) ) ; <nl> + for ( ByteBuffer acolname : apred . column _ names ) <nl> + colnames . add ( copy ( acolname ) ) ; <nl> + deletion . setPredicate ( new SlicePredicate ( ) . setColumn _ names ( colnames ) ) ; <nl> } <nl> - <nl> - if ( columnsToDelete . size ( ) > 0 ) <nl> + else <nl> { <nl> - Mutation mutation = new Mutation ( ) ; <nl> - Deletion deletion = new Deletion ( clock ) ; <nl> - <nl> - if ( columnsToDelete . size ( ) ! = 1 | | columnsToDelete . get ( 0 ) ! = null ) <nl> - { <nl> - deletion . setPredicate ( new SlicePredicate ( ) . setColumn _ names ( columnsToDelete ) ) ; <nl> - } <nl> - else <nl> - { <nl> - SliceRange range = new SliceRange ( new byte [ ] { } , new byte [ ] { } , false , Integer . MAX _ VALUE ) ; <nl> - deletion . setPredicate ( new SlicePredicate ( ) . setSlice _ range ( range ) ) ; <nl> - } <nl> - <nl> - mutation . setDeletion ( deletion ) ; <nl> - mutationList . add ( mutation ) ; <nl> + / / range <nl> + deletion . setPredicate ( new SlicePredicate ( ) . setSlice _ range ( avroToThrift ( apred . slice _ range ) ) ) ; <nl> } <nl> } <nl> + return mutation ; <nl> + } <nl> + <nl> + private SliceRange avroToThrift ( org . apache . cassandra . avro . SliceRange asr ) <nl> + { <nl> + return new SliceRange ( copy ( asr . start ) , copy ( asr . finish ) , asr . reversed , asr . count ) ; <nl> + } <nl> + <nl> + private Column avroToThrift ( org . apache . cassandra . avro . Column acol ) <nl> + { <nl> + return new Column ( copy ( acol . name ) , copy ( acol . value ) , avroToThrift ( acol . clock ) ) ; <nl> + } <nl> + <nl> + private Clock avroToThrift ( org . apache . cassandra . avro . Clock aclo ) <nl> + { <nl> + return new Clock ( aclo . timestamp ) ; <nl> } <nl> <nl> / * * <nl> diff - - git a / src / java / org / apache / cassandra / io / SerDeUtils . java b / src / java / org / apache / cassandra / io / SerDeUtils . java <nl> index fe72522 . . 940fe8f 100644 <nl> - - - a / src / java / org / apache / cassandra / io / SerDeUtils . java <nl> + + + b / src / java / org / apache / cassandra / io / SerDeUtils . java <nl> @ @ - 46 , 6 + 46 , 14 @ @ public final class SerDeUtils <nl> / / unbuffered decoders <nl> private final static DecoderFactory DIRECT _ DECODERS = new DecoderFactory ( ) . configureDirectDecoder ( true ) ; <nl> <nl> + public static byte [ ] copy ( ByteBuffer buff ) <nl> + { <nl> + byte [ ] bytes = new byte [ buff . remaining ( ) ] ; <nl> + buff . get ( bytes ) ; <nl> + buff . rewind ( ) ; <nl> + return bytes ; <nl> + } <nl> + <nl> 	 / * * <nl> * Deserializes a single object based on the given Schema . <nl> * @ param writer writer ' s schema <nl> diff - - git a / test / unit / org / apache / cassandra / hadoop / SampleColumnFamilyOutputTool . java b / test / unit / org / apache / cassandra / hadoop / SampleColumnFamilyOutputTool . java <nl> index 37d9307 . . 4e12ad9 100644 <nl> - - - a / test / unit / org / apache / cassandra / hadoop / SampleColumnFamilyOutputTool . java <nl> + + + b / test / unit / org / apache / cassandra / hadoop / SampleColumnFamilyOutputTool . java <nl> @ @ - 68 , 6 + 68 , 7 @ @ public class SampleColumnFamilyOutputTool extends Configured implements Tool <nl> job . setMapOutputValueClass ( ColumnWritable . class ) ; <nl> job . setInputFormatClass ( SequenceFileInputFormat . class ) ; <nl> <nl> + / / TODO : no idea why this test is passing <nl> job . setReducerClass ( ColumnFamilyOutputReducer . class ) ; <nl> job . setOutputKeyClass ( byte [ ] . class ) ; <nl> job . setOutputValueClass ( SortedMap . class ) ; <nl> @ @ - 76 , 4 + 77 , 4 @ @ public class SampleColumnFamilyOutputTool extends Configured implements Tool <nl> job . waitForCompletion ( true ) ; <nl> return 0 ; <nl> } <nl> - } <nl> \ No newline at end of file <nl> + }

TEST DIFF:
diff - - git a / src / java / org / apache / cassandra / schema / IndexMetadata . java b / src / java / org / apache / cassandra / schema / IndexMetadata . java 
 index 04e06ab . . b8c4854 100644 
 - - - a / src / java / org / apache / cassandra / schema / IndexMetadata . java 
 + + + b / src / java / org / apache / cassandra / schema / IndexMetadata . java 
 @ @ - 47 , 7 + 47 , 7 @ @ import org . apache . cassandra . utils . UUIDSerializer ; 
 public final class IndexMetadata 
 { 
 private static final Logger logger = LoggerFactory . getLogger ( IndexMetadata . class ) ; 
 - 
 + 
 private static final Pattern PATTERN _ NON _ WORD _ CHAR = Pattern . compile ( " \ \ W " ) ; 
 private static final Pattern PATTERN _ WORD _ CHARS = Pattern . compile ( " \ \ w + " ) ; 
 
 @ @ - 76 , 42 + 76 , 6 @ @ public final class IndexMetadata 
 this . kind = kind ; 
 } 
 
 - public static IndexMetadata fromLegacyMetadata ( CFMetaData cfm , 
 - ColumnDefinition column , 
 - String name , 
 - Kind kind , 
 - Map < String , String > options ) 
 - { 
 - Map < String , String > newOptions = new HashMap < > ( ) ; 
 - if ( options ! = null ) 
 - newOptions . putAll ( options ) ; 
 - 
 - IndexTarget target ; 
 - if ( newOptions . containsKey ( IndexTarget . INDEX _ KEYS _ OPTION _ NAME ) ) 
 - { 
 - newOptions . remove ( IndexTarget . INDEX _ KEYS _ OPTION _ NAME ) ; 
 - target = new IndexTarget ( column . name , IndexTarget . Type . KEYS ) ; 
 - } 
 - else if ( newOptions . containsKey ( IndexTarget . INDEX _ ENTRIES _ OPTION _ NAME ) ) 
 - { 
 - newOptions . remove ( IndexTarget . INDEX _ KEYS _ OPTION _ NAME ) ; 
 - target = new IndexTarget ( column . name , IndexTarget . Type . KEYS _ AND _ VALUES ) ; 
 - } 
 - else 
 - { 
 - if ( column . type . isCollection ( ) & & ! column . type . isMultiCell ( ) ) 
 - { 
 - target = new IndexTarget ( column . name , IndexTarget . Type . FULL ) ; 
 - } 
 - else 
 - { 
 - target = new IndexTarget ( column . name , IndexTarget . Type . VALUES ) ; 
 - } 
 - } 
 - newOptions . put ( IndexTarget . TARGET _ OPTION _ NAME , target . asCqlString ( cfm ) ) ; 
 - return new IndexMetadata ( name , newOptions , kind ) ; 
 - } 
 - 
 public static IndexMetadata fromSchemaMetadata ( String name , Kind kind , Map < String , String > options ) 
 { 
 return new IndexMetadata ( name , options , kind ) ;

NEAREST DIFF:
diff - - git a / src / cassandra . avpr b / src / cassandra . avpr 
 new file mode 100644 
 index 0000000 . . 41ff763 
 - - - / dev / null 
 + + + b / src / cassandra . avpr 
 @ @ - 0 , 0 + 1 , 202 @ @ 
 + { 
 + " protocol " : " InterNode " , 
 + " namespace " : " org . apache . cassandra " , 
 + " types " : [ { 
 + " type " : " fixed " , 
 + " name " : " UUID " , 
 + " namespace " : " org . apache . cassandra . utils . avro " , 
 + " size " : 16 
 + } , { 
 + " type " : " enum " , 
 + " name " : " IndexType " , 
 + " namespace " : " org . apache . cassandra . config . avro " , 
 + " symbols " : [ " KEYS " ] 
 + } , { 
 + " type " : " record " , 
 + " name " : " ColumnDef " , 
 + " namespace " : " org . apache . cassandra . config . avro " , 
 + " fields " : [ { 
 + " name " : " name " , 
 + " type " : " bytes " 
 + } , { 
 + " name " : " validation _ class " , 
 + " type " : " string " 
 + } , { 
 + " name " : " index _ type " , 
 + " type " : [ " org . apache . cassandra . config . avro . IndexType " , " null " ] 
 + } , { 
 + " name " : " index _ name " , 
 + " type " : [ " string " , " null " ] 
 + } ] 
 + } , { 
 + " type " : " record " , 
 + " name " : " CfDef " , 
 + " namespace " : " org . apache . cassandra . config . avro " , 
 + " fields " : [ { 
 + " name " : " keyspace " , 
 + " type " : " string " 
 + } , { 
 + " name " : " name " , 
 + " type " : " string " 
 + } , { 
 + " name " : " column _ type " , 
 + " type " : [ " string " , " null " ] 
 + } , { 
 + " name " : " clock _ type " , 
 + " type " : [ " string " , " null " ] 
 + } , { 
 + " name " : " comparator _ type " , 
 + " type " : [ " string " , " null " ] 
 + } , { 
 + " name " : " subcomparator _ type " , 
 + " type " : [ " string " , " null " ] 
 + } , { 
 + " name " : " reconciler " , 
 + " type " : [ " string " , " null " ] 
 + } , { 
 + " name " : " comment " , 
 + " type " : [ " string " , " null " ] 
 + } , { 
 + " name " : " row _ cache _ size " , 
 + " type " : [ " double " , " null " ] 
 + } , { 
 + " name " : " preload _ row _ cache " , 
 + " type " : [ " boolean " , " null " ] 
 + } , { 
 + " name " : " key _ cache _ size " , 
 + " type " : [ " double " , " null " ] 
 + } , { 
 + " name " : " read _ repair _ chance " , 
 + " type " : [ " double " , " null " ] 
 + } , { 
 + " name " : " gc _ grace _ seconds " , 
 + " type " : [ " int " , " null " ] 
 + } , { 
 + " name " : " column _ metadata " , 
 + " type " : [ { 
 + " type " : " array " , 
 + " items " : " org . apache . cassandra . config . avro . ColumnDef " 
 + } , " null " ] 
 + } , { 
 + " name " : " id " , 
 + " type " : [ " int " , " null " ] 
 + } ] 
 + } , { 
 + " type " : " record " , 
 + " name " : " KsDef " , 
 + " namespace " : " org . apache . cassandra . config . avro " , 
 + " fields " : [ { 
 + " name " : " name " , 
 + " type " : " string " 
 + } , { 
 + " name " : " strategy _ class " , 
 + " type " : " string " 
 + } , { 
 + " name " : " strategy _ options " , 
 + " type " : [ { 
 + " type " : " map " , 
 + " values " : " string " 
 + } , " null " ] 
 + } , { 
 + " name " : " replication _ factor " , 
 + " type " : " int " 
 + } , { 
 + " name " : " cf _ defs " , 
 + " type " : { 
 + " type " : " array " , 
 + " items " : " org . apache . cassandra . config . avro . CfDef " 
 + } 
 + } ] 
 + } , { 
 + " type " : " record " , 
 + " name " : " AddColumnFamily " , 
 + " namespace " : " org . apache . cassandra . db . migration . avro " , 
 + " fields " : [ { 
 + " name " : " cf " , 
 + " type " : " org . apache . cassandra . config . avro . CfDef " 
 + } ] 
 + } , { 
 + " type " : " record " , 
 + " name " : " AddKeyspace " , 
 + " namespace " : " org . apache . cassandra . db . migration . avro " , 
 + " fields " : [ { 
 + " name " : " ks " , 
 + " type " : " org . apache . cassandra . config . avro . KsDef " 
 + } ] 
 + } , { 
 + " type " : " record " , 
 + " name " : " DropColumnFamily " , 
 + " namespace " : " org . apache . cassandra . db . migration . avro " , 
 + " fields " : [ { 
 + " name " : " ksname " , 
 + " type " : " string " 
 + } , { 
 + " name " : " cfname " , 
 + " type " : " string " 
 + } , { 
 + " name " : " block _ on _ deletion " , 
 + " type " : " boolean " 
 + } ] 
 + } , { 
 + " type " : " record " , 
 + " name " : " DropKeyspace " , 
 + " namespace " : " org . apache . cassandra . db . migration . avro " , 
 + " fields " : [ { 
 + " name " : " ksname " , 
 + " type " : " string " 
 + } , { 
 + " name " : " block _ on _ deletion " , 
 + " type " : " boolean " 
 + } ] 
 + } , { 
 + " type " : " record " , 
 + " name " : " RenameColumnFamily " , 
 + " namespace " : " org . apache . cassandra . db . migration . avro " , 
 + " fields " : [ { 
 + " name " : " ksname " , 
 + " type " : " string " 
 + } , { 
 + " name " : " cfid " , 
 + " type " : " int " 
 + } , { 
 + " name " : " old _ cfname " , 
 + " type " : " string " 
 + } , { 
 + " name " : " new _ cfname " , 
 + " type " : " string " 
 + } ] 
 + } , { 
 + " type " : " record " , 
 + " name " : " RenameKeyspace " , 
 + " namespace " : " org . apache . cassandra . db . migration . avro " , 
 + " fields " : [ { 
 + " name " : " old _ ksname " , 
 + " type " : " string " 
 + } , { 
 + " name " : " new _ ksname " , 
 + " type " : " string " 
 + } ] 
 + } , { 
 + " type " : " record " , 
 + " name " : " Migration " , 
 + " namespace " : " org . apache . cassandra . db . migration . avro " , 
 + " fields " : [ { 
 + " name " : " old _ version " , 
 + " type " : " org . apache . cassandra . utils . avro . UUID " 
 + } , { 
 + " name " : " new _ version " , 
 + " type " : " org . apache . cassandra . utils . avro . UUID " 
 + } , { 
 + " name " : " row _ mutation " , 
 + " type " : " bytes " 
 + } , { 
 + " name " : " classname " , 
 + " type " : " string " 
 + } , { 
 + " name " : " migration " , 
 + " type " : [ " org . apache . cassandra . db . migration . avro . AddColumnFamily " , " org . apache . cassandra . db . migration . avro . DropColumnFamily " , " org . apache . cassandra . db . migration . avro . RenameColumnFamily " , " org . apache . cassandra . db . migration . avro . AddKeyspace " , " org . apache . cassandra . db . migration . avro . DropKeyspace " , " org . apache . cassandra . db . migration . avro . RenameKeyspace " ] 
 + } ] 
 + } ] , 
 + " messages " : { 
 + } 
 + } 
 \ No newline at end of file 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java 
 index 9731571 . . 98fbf70 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyOutputFormat . java 
 @ @ - 25 , 10 + 25 , 10 @ @ import java . io . IOException ; 
 import java . util . HashMap ; 
 import java . util . List ; 
 import java . util . Map ; 
 + import java . nio . ByteBuffer ; 
 
 import org . apache . cassandra . auth . SimpleAuthenticator ; 
 - import org . apache . cassandra . db . ColumnFamily ; 
 - import org . apache . cassandra . db . IColumn ; 
 + import org . apache . cassandra . avro . Mutation ; 
 import org . apache . cassandra . thrift . AuthenticationException ; 
 import org . apache . cassandra . thrift . AuthenticationRequest ; 
 import org . apache . cassandra . thrift . AuthorizationException ; 
 @ @ - 64 , 11 + 64 , 11 @ @ import org . slf4j . LoggerFactory ; 
 * < p > 
 * For the sake of performance , this class employs a lazy write - back caching 
 * mechanism , where its record writer batches mutations created based on the 
 - * reduce ' s inputs ( in a task - specific map ) . When the writer is closed , then it 
 - * makes the changes official by sending a batch mutate request to Cassandra . 
 + * reduce ' s inputs ( in a task - specific map ) , and periodically makes the changes 
 + * official by sending a batch mutate request to Cassandra . 
 * < / p > 
 * / 
 - public class ColumnFamilyOutputFormat extends OutputFormat < byte [ ] , List < IColumn > > 
 + public class ColumnFamilyOutputFormat extends OutputFormat < ByteBuffer , List < Mutation > > 
 { 
 private static final Logger logger = LoggerFactory . getLogger ( ColumnFamilyOutputFormat . class ) ; 
 
 @ @ - 93 , 15 + 93 , 7 @ @ public class ColumnFamilyOutputFormat extends OutputFormat < byte [ ] , List < IColumn > > 
 } 
 
 / * * 
 - * Get the output committer for this output format . This is responsible for 
 - * ensuring the output is committed correctly . 
 - * 
 - * < p > 
 - * This output format employs a lazy write - back caching mechanism , where the 
 - * { @ link RecordWriter } is responsible for collecting mutations in the 
 - * { @ link # MUTATIONS _ CACHE } , and the { @ link OutputCommitter } makes the 
 - * changes official by making the change request to Cassandra . 
 - * < / p > 
 + * The OutputCommitter for this format does not write any data to the DFS . 
 * 
 * @ param context 
 * the task context 
 @ @ - 118 , 19 + 110 , 13 @ @ public class ColumnFamilyOutputFormat extends OutputFormat < byte [ ] , List < IColumn > > 
 / * * 
 * Get the { @ link RecordWriter } for the given task . 
 * 
 - * < p > 
 - * As stated above , this { @ link RecordWriter } merely batches the mutations 
 - * that it defines in the { @ link # MUTATIONS _ CACHE } . In other words , it 
 - * doesn ' t literally cause any changes on the Cassandra server . 
 - * < / p > 
 - * 
 * @ param context 
 * the information about the current task . 
 * @ return a { @ link RecordWriter } to write the output for the job . 
 * @ throws IOException 
 * / 
 @ Override 
 - public RecordWriter < byte [ ] , List < IColumn > > getRecordWriter ( final TaskAttemptContext context ) throws IOException , InterruptedException 
 + public RecordWriter < ByteBuffer , List < Mutation > > getRecordWriter ( final TaskAttemptContext context ) throws IOException , InterruptedException 
 { 
 return new ColumnFamilyRecordWriter ( context ) ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java 
 index 91b87ae . . 9b19f50 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordWriter . java 
 @ @ - 22 , 10 + 22 , 12 @ @ package org . apache . cassandra . hadoop ; 
 * / 
 import java . io . IOException ; 
 import java . net . InetAddress ; 
 + import java . nio . ByteBuffer ; 
 import java . util . ArrayList ; 
 import java . util . HashMap ; 
 import java . util . List ; 
 import java . util . Map ; 
 + import java . util . TreeMap ; 
 import java . util . concurrent . Callable ; 
 import java . util . concurrent . ExecutionException ; 
 import java . util . concurrent . ExecutorService ; 
 @ @ - 33 , 7 + 35 , 7 @ @ import java . util . concurrent . Executors ; 
 import java . util . concurrent . Future ; 
 
 import org . apache . cassandra . client . RingCache ; 
 - import org . apache . cassandra . db . IColumn ; 
 + import static org . apache . cassandra . io . SerDeUtils . copy ; 
 import org . apache . cassandra . thrift . Cassandra ; 
 import org . apache . cassandra . thrift . Clock ; 
 import org . apache . cassandra . thrift . Column ; 
 @ @ - 43 , 6 + 45 , 8 @ @ import org . apache . cassandra . thrift . Deletion ; 
 import org . apache . cassandra . thrift . Mutation ; 
 import org . apache . cassandra . thrift . SlicePredicate ; 
 import org . apache . cassandra . thrift . SliceRange ; 
 + import org . apache . cassandra . thrift . SuperColumn ; 
 + import org . apache . cassandra . utils . FBUtilities ; 
 import org . apache . hadoop . mapreduce . OutputFormat ; 
 import org . apache . hadoop . mapreduce . RecordWriter ; 
 import org . apache . hadoop . mapreduce . TaskAttemptContext ; 
 @ @ - 50 , 31 + 54 , 28 @ @ import org . apache . thrift . transport . TSocket ; 
 
 / * * 
 * The < code > ColumnFamilyRecordWriter < / code > maps the output & lt ; key , value & gt ; 
 - * pairs to a Cassandra column family . In particular , it creates mutations for 
 - * each column in the value , which it associates with the key , and in turn the 
 - * responsible endpoint . 
 + * pairs to a Cassandra column family . In particular , it applies all mutations 
 + * in the value , which it associates with the key , and in turn the responsible 
 + * endpoint . 
 * 
 * < p > 
 * Note that , given that round trips to the server are fairly expensive , it 
 - * merely batches the mutations in - memory ( specifically in 
 - * { @ link ColumnFamilyOutputFormat # MUTATIONS _ CACHE } ) , and leaves it to the 
 - * { @ link ColumnFamilyOutputCommitter } to send the batched mutations to the 
 - * server in one shot . 
 + * merely batches the mutations in - memory and periodically sends the batched 
 + * mutations to the server in one shot . 
 * < / p > 
 * 
 * < p > 
 * Furthermore , this writer groups the mutations by the endpoint responsible for 
 - * the rows being affected . This allows the { @ link ColumnFamilyOutputCommitter } 
 - * to execute the mutations in parallel , on a endpoint - by - endpoint basis . 
 + * the rows being affected . This allows the mutations to be executed in parallel , 
 + * directly to a responsible endpoint . 
 * < / p > 
 * 
 * @ author Karthick Sankarachary 
 - * @ see ColumnFamilyOutputCommitter 
 * @ see ColumnFamilyOutputFormat 
 * @ see OutputFormat 
 * 
 * / 
 - final class ColumnFamilyRecordWriter extends RecordWriter < byte [ ] , List < IColumn > > 
 + final class ColumnFamilyRecordWriter extends RecordWriter < ByteBuffer , List < org . apache . cassandra . avro . Mutation > > 
 { 
 / / The task attempt context this writer is associated with . 
 private final TaskAttemptContext context ; 
 @ @ - 87 , 12 + 88 , 12 @ @ final class ColumnFamilyRecordWriter extends RecordWriter < byte [ ] , List < IColumn > > 
 / / the endpoints they should be targeted at . The targeted endpoint 
 / / essentially 
 / / acts as the primary replica for the rows being affected by the mutations . 
 - private RingCache ringCache ; 
 + private final RingCache ringCache ; 
 
 / / The number of mutations currently held in the mutations cache . 
 private long batchSize = 0L ; 
 / / The maximum number of mutations to hold in the mutations cache . 
 - private long batchThreshold = Long . MAX _ VALUE ; 
 + private final long batchThreshold ; 
 
 / * * 
 * Upon construction , obtain the map that this writer will use to collect 
 @ @ - 144 , 75 + 145 , 92 @ @ final class ColumnFamilyRecordWriter extends RecordWriter < byte [ ] , List < IColumn > > 
 * @ throws IOException 
 * / 
 @ Override 
 - public synchronized void write ( byte [ ] key , List < IColumn > value ) throws IOException , InterruptedException 
 + public synchronized void write ( ByteBuffer keybuff , List < org . apache . cassandra . avro . Mutation > value ) throws IOException , InterruptedException 
 { 
 maybeFlush ( ) ; 
 + byte [ ] key = copy ( keybuff ) ; 
 InetAddress endpoint = getEndpoint ( key ) ; 
 Map < byte [ ] , Map < String , List < Mutation > > > mutationsByKey = mutationsByEndpoint . get ( endpoint ) ; 
 if ( mutationsByKey = = null ) 
 { 
 - mutationsByKey = new HashMap < byte [ ] , Map < String , List < Mutation > > > ( ) ; 
 + mutationsByKey = new TreeMap < byte [ ] , Map < String , List < Mutation > > > ( FBUtilities . byteArrayComparator ) ; 
 mutationsByEndpoint . put ( endpoint , mutationsByKey ) ; 
 } 
 
 Map < String , List < Mutation > > cfMutation = new HashMap < String , List < Mutation > > ( ) ; 
 mutationsByKey . put ( key , cfMutation ) ; 
 
 - Clock clock = new Clock ( System . currentTimeMillis ( ) ) ; 
 List < Mutation > mutationList = new ArrayList < Mutation > ( ) ; 
 cfMutation . put ( ConfigHelper . getOutputColumnFamily ( context . getConfiguration ( ) ) , mutationList ) ; 
 
 - if ( value = = null ) 
 + for ( org . apache . cassandra . avro . Mutation amut : value ) 
 + mutationList . add ( avroToThrift ( amut ) ) ; 
 + } 
 + 
 + / * * 
 + * Deep copies the given Avro mutation into a new Thrift mutation . 
 + * / 
 + private Mutation avroToThrift ( org . apache . cassandra . avro . Mutation amut ) 
 + { 
 + Mutation mutation = new Mutation ( ) ; 
 + org . apache . cassandra . avro . ColumnOrSuperColumn acosc = amut . column _ or _ supercolumn ; 
 + if ( acosc ! = null ) 
 { 
 - Mutation mutation = new Mutation ( ) ; 
 - Deletion deletion = new Deletion ( clock ) ; 
 - mutation . setDeletion ( deletion ) ; 
 - mutationList . add ( mutation ) ; 
 + / / creation 
 + ColumnOrSuperColumn cosc = new ColumnOrSuperColumn ( ) ; 
 + mutation . setColumn _ or _ supercolumn ( cosc ) ; 
 + if ( acosc . column ! = null ) 
 + / / standard column 
 + cosc . setColumn ( avroToThrift ( acosc . column ) ) ; 
 + else 
 + { 
 + / / super column 
 + byte [ ] scolname = copy ( acosc . super _ column . name ) ; 
 + List < Column > scolcols = new ArrayList < Column > ( ( int ) acosc . super _ column . columns . size ( ) ) ; 
 + for ( org . apache . cassandra . avro . Column acol : acosc . super _ column . columns ) 
 + scolcols . add ( avroToThrift ( acol ) ) ; 
 + cosc . setSuper _ column ( new SuperColumn ( scolname , scolcols ) ) ; 
 + } 
 } 
 else 
 { 
 - List < byte [ ] > columnsToDelete = new ArrayList < byte [ ] > ( ) ; 
 - for ( IColumn column : value ) 
 + / / deletion 
 + Deletion deletion = new Deletion ( avroToThrift ( amut . deletion . clock ) ) ; 
 + mutation . setDeletion ( deletion ) ; 
 + org . apache . cassandra . avro . SlicePredicate apred = amut . deletion . predicate ; 
 + if ( amut . deletion . super _ column ! = null ) 
 + / / super column 
 + deletion . setSuper _ column ( copy ( amut . deletion . super _ column ) ) ; 
 + else if ( apred . column _ names ! = null ) 
 { 
 - Mutation mutation = new Mutation ( ) ; 
 - if ( column . value ( ) = = null ) 
 - { 
 - if ( columnsToDelete . size ( ) ! = 1 | | columnsToDelete . get ( 0 ) ! = null ) 
 - { 
 - if ( column . name ( ) = = null ) 
 - columnsToDelete . clear ( ) ; 
 - columnsToDelete . add ( column . name ( ) ) ; 
 - } 
 - } 
 - else 
 - { 
 - 
 - ColumnOrSuperColumn cosc = new ColumnOrSuperColumn ( ) ; 
 - cosc . setColumn ( new Column ( column . name ( ) , column . value ( ) , clock ) ) ; 
 - mutation . setColumn _ or _ supercolumn ( cosc ) ; 
 - } 
 - mutationList . add ( mutation ) ; 
 + / / column names 
 + List < byte [ ] > colnames = new ArrayList < byte [ ] > ( ( int ) apred . column _ names . size ( ) ) ; 
 + for ( ByteBuffer acolname : apred . column _ names ) 
 + colnames . add ( copy ( acolname ) ) ; 
 + deletion . setPredicate ( new SlicePredicate ( ) . setColumn _ names ( colnames ) ) ; 
 } 
 - 
 - if ( columnsToDelete . size ( ) > 0 ) 
 + else 
 { 
 - Mutation mutation = new Mutation ( ) ; 
 - Deletion deletion = new Deletion ( clock ) ; 
 - 
 - if ( columnsToDelete . size ( ) ! = 1 | | columnsToDelete . get ( 0 ) ! = null ) 
 - { 
 - deletion . setPredicate ( new SlicePredicate ( ) . setColumn _ names ( columnsToDelete ) ) ; 
 - } 
 - else 
 - { 
 - SliceRange range = new SliceRange ( new byte [ ] { } , new byte [ ] { } , false , Integer . MAX _ VALUE ) ; 
 - deletion . setPredicate ( new SlicePredicate ( ) . setSlice _ range ( range ) ) ; 
 - } 
 - 
 - mutation . setDeletion ( deletion ) ; 
 - mutationList . add ( mutation ) ; 
 + / / range 
 + deletion . setPredicate ( new SlicePredicate ( ) . setSlice _ range ( avroToThrift ( apred . slice _ range ) ) ) ; 
 } 
 } 
 + return mutation ; 
 + } 
 + 
 + private SliceRange avroToThrift ( org . apache . cassandra . avro . SliceRange asr ) 
 + { 
 + return new SliceRange ( copy ( asr . start ) , copy ( asr . finish ) , asr . reversed , asr . count ) ; 
 + } 
 + 
 + private Column avroToThrift ( org . apache . cassandra . avro . Column acol ) 
 + { 
 + return new Column ( copy ( acol . name ) , copy ( acol . value ) , avroToThrift ( acol . clock ) ) ; 
 + } 
 + 
 + private Clock avroToThrift ( org . apache . cassandra . avro . Clock aclo ) 
 + { 
 + return new Clock ( aclo . timestamp ) ; 
 } 
 
 / * * 
 diff - - git a / src / java / org / apache / cassandra / io / SerDeUtils . java b / src / java / org / apache / cassandra / io / SerDeUtils . java 
 index fe72522 . . 940fe8f 100644 
 - - - a / src / java / org / apache / cassandra / io / SerDeUtils . java 
 + + + b / src / java / org / apache / cassandra / io / SerDeUtils . java 
 @ @ - 46 , 6 + 46 , 14 @ @ public final class SerDeUtils 
 / / unbuffered decoders 
 private final static DecoderFactory DIRECT _ DECODERS = new DecoderFactory ( ) . configureDirectDecoder ( true ) ; 
 
 + public static byte [ ] copy ( ByteBuffer buff ) 
 + { 
 + byte [ ] bytes = new byte [ buff . remaining ( ) ] ; 
 + buff . get ( bytes ) ; 
 + buff . rewind ( ) ; 
 + return bytes ; 
 + } 
 + 
 	 / * * 
 * Deserializes a single object based on the given Schema . 
 * @ param writer writer ' s schema 
 diff - - git a / test / unit / org / apache / cassandra / hadoop / SampleColumnFamilyOutputTool . java b / test / unit / org / apache / cassandra / hadoop / SampleColumnFamilyOutputTool . java 
 index 37d9307 . . 4e12ad9 100644 
 - - - a / test / unit / org / apache / cassandra / hadoop / SampleColumnFamilyOutputTool . java 
 + + + b / test / unit / org / apache / cassandra / hadoop / SampleColumnFamilyOutputTool . java 
 @ @ - 68 , 6 + 68 , 7 @ @ public class SampleColumnFamilyOutputTool extends Configured implements Tool 
 job . setMapOutputValueClass ( ColumnWritable . class ) ; 
 job . setInputFormatClass ( SequenceFileInputFormat . class ) ; 
 
 + / / TODO : no idea why this test is passing 
 job . setReducerClass ( ColumnFamilyOutputReducer . class ) ; 
 job . setOutputKeyClass ( byte [ ] . class ) ; 
 job . setOutputValueClass ( SortedMap . class ) ; 
 @ @ - 76 , 4 + 77 , 4 @ @ public class SampleColumnFamilyOutputTool extends Configured implements Tool 
 job . waitForCompletion ( true ) ; 
 return 0 ; 
 } 
 - } 
 \ No newline at end of file 
 + }
