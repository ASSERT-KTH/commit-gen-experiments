BLEU SCORE: 0.020980574531482755

TEST MSG: Fix startup error when upgrading nodes to 3 . 0
GENERATED MSG: remove per - row bloom filter of column names

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index fb33de0 . . 726eb04 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 3 . 0 . 0 - beta2 <nl> + * Fix startup error when upgrading nodes ( CASSANDRA - 10136 ) <nl> * Base table PRIMARY KEY can be assumed to be NOT NULL in MV creation ( CASSANDRA - 10147 ) <nl> * Improve batchlog write patch ( CASSANDRA - 9673 ) <nl> * Re - apply MaterializedView updates on commitlog replay ( CASSANDRA - 10164 ) <nl> diff - - git a / src / java / org / apache / cassandra / db / Serializers . java b / src / java / org / apache / cassandra / db / Serializers . java <nl> index 8056222 . . 2561bbe 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Serializers . java <nl> + + + b / src / java / org / apache / cassandra / db / Serializers . java <nl> @ @ - 18 , 12 + 18 , 16 @ @ <nl> package org . apache . cassandra . db ; <nl> <nl> import java . io . * ; <nl> + import java . nio . ByteBuffer ; <nl> + import java . util . List ; <nl> <nl> import org . apache . cassandra . config . CFMetaData ; <nl> + import org . apache . cassandra . db . marshal . CompositeType ; <nl> import org . apache . cassandra . io . ISerializer ; <nl> import org . apache . cassandra . io . util . DataInputPlus ; <nl> import org . apache . cassandra . io . util . DataOutputPlus ; <nl> import org . apache . cassandra . io . sstable . format . Version ; <nl> + import org . apache . cassandra . utils . ByteBufferUtil ; <nl> <nl> import static org . apache . cassandra . io . sstable . IndexHelper . IndexInfo ; <nl> <nl> @ @ - 44 , 12 + 48 , 65 @ @ public class Serializers <nl> return new IndexInfo . Serializer ( metadata , version ) ; <nl> } <nl> <nl> - / / Note that for the old layout , this will actually discard the cellname parts that are not strictly <nl> - / / part of the clustering prefix . Don ' t use this if that ' s not what you want . <nl> - public ISerializer < ClusteringPrefix > clusteringPrefixSerializer ( final Version version , final SerializationHeader header ) <nl> + / / TODO : Once we drop support for old ( pre - 3 . 0 ) sstables , we can drop this method and inline the calls to <nl> + / / ClusteringPrefix . serializer in IndexHelper directly . At which point this whole class probably becomes <nl> + / / unecessary ( since IndexInfo . Serializer won ' t depend on the metadata either ) . <nl> + public ISerializer < ClusteringPrefix > indexEntryClusteringPrefixSerializer ( final Version version , final SerializationHeader header ) <nl> { <nl> if ( ! version . storeRows ( ) ) <nl> - throw new UnsupportedOperationException ( ) ; <nl> + { <nl> + return new ISerializer < ClusteringPrefix > ( ) <nl> + { <nl> + public void serialize ( ClusteringPrefix clustering , DataOutputPlus out ) throws IOException <nl> + { <nl> + / / We should only use this for reading old sstable , never write new ones . <nl> + throw new UnsupportedOperationException ( ) ; <nl> + } <nl> + <nl> + public ClusteringPrefix deserialize ( DataInputPlus in ) throws IOException <nl> + { <nl> + / / We ' re reading the old cellname / composite <nl> + ByteBuffer bb = ByteBufferUtil . readWithShortLength ( in ) ; <nl> + assert bb . hasRemaining ( ) ; / / empty cellnames were invalid <nl> + <nl> + int clusteringSize = metadata . clusteringColumns ( ) . size ( ) ; <nl> + / / If the table has no clustering column , then the cellname will just be the " column " name , which we ignore here . <nl> + if ( clusteringSize = = 0 ) <nl> + return Clustering . EMPTY ; <nl> + <nl> + if ( ! metadata . isCompound ( ) ) <nl> + return new Clustering ( bb ) ; <nl> + <nl> + List < ByteBuffer > components = CompositeType . splitName ( bb ) ; <nl> + byte eoc = CompositeType . lastEOC ( bb ) ; <nl> + <nl> + if ( eoc = = 0 | | components . size ( ) > = clusteringSize ) <nl> + { <nl> + / / That ' s a clustering . <nl> + if ( components . size ( ) > clusteringSize ) <nl> + components = components . subList ( 0 , clusteringSize ) ; <nl> + <nl> + return new Clustering ( components . toArray ( new ByteBuffer [ clusteringSize ] ) ) ; <nl> + } <nl> + else <nl> + { <nl> + / / It ' s a range tombstone bound . It is a start since that ' s the only part we ' ve ever included <nl> + / / in the index entries . <nl> + Slice . Bound . Kind boundKind = eoc > 0 <nl> + ? Slice . Bound . Kind . EXCL _ START _ BOUND <nl> + : Slice . Bound . Kind . INCL _ START _ BOUND ; <nl> + <nl> + return Slice . Bound . create ( boundKind , components . toArray ( new ByteBuffer [ components . size ( ) ] ) ) ; <nl> + } <nl> + } <nl> + <nl> + public long serializedSize ( ClusteringPrefix clustering ) <nl> + { <nl> + / / We should only use this for reading old sstable , never write new ones . <nl> + throw new UnsupportedOperationException ( ) ; <nl> + } <nl> + } ; <nl> + } <nl> <nl> return new ISerializer < ClusteringPrefix > ( ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / db / SinglePartitionNamesCommand . java b / src / java / org / apache / cassandra / db / SinglePartitionNamesCommand . java <nl> index cee3fc4 . . 1b41005 100644 <nl> - - - a / src / java / org / apache / cassandra / db / SinglePartitionNamesCommand . java <nl> + + + b / src / java / org / apache / cassandra / db / SinglePartitionNamesCommand . java <nl> @ @ - 192 , 7 + 192 , 7 @ @ public class SinglePartitionNamesCommand extends SinglePartitionReadCommand < Clus <nl> if ( result = = null ) <nl> return ImmutableBTreePartition . create ( iter , maxRows ) ; <nl> <nl> - try ( UnfilteredRowIterator merged = UnfilteredRowIterators . merge ( Arrays . asList ( iter , result . unfilteredIterator ( columnFilter ( ) , Slices . ALL , false ) ) , nowInSec ( ) ) ) <nl> + try ( UnfilteredRowIterator merged = UnfilteredRowIterators . merge ( Arrays . asList ( iter , result . unfilteredIterator ( columnFilter ( ) , Slices . ALL , clusteringIndexFilter ( ) . isReversed ( ) ) ) , nowInSec ( ) ) ) <nl> { <nl> return ImmutableBTreePartition . create ( merged , maxRows ) ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java b / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java <nl> index cf4bff7 . . 87a57c6 100644 <nl> - - - a / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java <nl> + + + b / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java <nl> @ @ - 22 , 9 + 22 , 6 @ @ import java . util . Collections ; <nl> import java . util . Iterator ; <nl> import java . util . List ; <nl> <nl> - import org . slf4j . Logger ; <nl> - import org . slf4j . LoggerFactory ; <nl> - <nl> import org . apache . cassandra . config . CFMetaData ; <nl> import org . apache . cassandra . db . * ; <nl> import org . apache . cassandra . db . filter . ColumnFilter ; <nl> @ @ - 453 , 6 + 450 , 32 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator <nl> return indexes . size ( ) ; <nl> } <nl> <nl> + / / Update the block idx based on the current reader position if we ' re past the current block . <nl> + public void updateBlock ( ) throws IOException <nl> + { <nl> + assert currentIndexIdx > = 0 ; <nl> + while ( currentIndexIdx + 1 < indexes . size ( ) & & isPastCurrentBlock ( ) ) <nl> + { <nl> + reader . openMarker = currentIndex ( ) . endOpenMarker ; <nl> + + + currentIndexIdx ; <nl> + <nl> + / / We have to set the mark , and we have to set it at the beginning of the block . So if we ' re not at the beginning of the block , this forces us to a weird seek dance . <nl> + / / This can only happen when reading old file however . <nl> + long startOfBlock = indexEntry . position + indexes . get ( currentIndexIdx ) . offset ; <nl> + long currentFilePointer = reader . file . getFilePointer ( ) ; <nl> + if ( startOfBlock = = currentFilePointer ) <nl> + { <nl> + mark = reader . file . mark ( ) ; <nl> + } <nl> + else <nl> + { <nl> + reader . seekToPosition ( startOfBlock ) ; <nl> + mark = reader . file . mark ( ) ; <nl> + reader . seekToPosition ( currentFilePointer ) ; <nl> + } <nl> + } <nl> + } <nl> + <nl> / / Check if we ' ve crossed an index boundary ( based on the mark on the beginning of the index block ) . <nl> public boolean isPastCurrentBlock ( ) <nl> { <nl> @ @ - 466 , 7 + 489 , 12 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator <nl> <nl> public IndexHelper . IndexInfo currentIndex ( ) <nl> { <nl> - return indexes . get ( currentIndexIdx ) ; <nl> + return index ( currentIndexIdx ) ; <nl> + } <nl> + <nl> + public IndexHelper . IndexInfo index ( int i ) <nl> + { <nl> + return indexes . get ( i ) ; <nl> } <nl> <nl> / / Finds the index of the first block containing the provided bound , starting at the provided index . <nl> diff - - git a / src / java / org / apache / cassandra / db / columniterator / SSTableIterator . java b / src / java / org / apache / cassandra / db / columniterator / SSTableIterator . java <nl> index e985c18 . . 3536d65 100644 <nl> - - - a / src / java / org / apache / cassandra / db / columniterator / SSTableIterator . java <nl> + + + b / src / java / org / apache / cassandra / db / columniterator / SSTableIterator . java <nl> @ @ - 265 , 9 + 265 , 7 @ @ public class SSTableIterator extends AbstractSSTableIterator <nl> protected Unfiltered computeNext ( ) throws IOException <nl> { <nl> / / Our previous read might have made us cross an index block boundary . If so , update our informations . <nl> - int currentBlockIdx = indexState . currentBlockIdx ( ) ; <nl> - if ( indexState . isPastCurrentBlock ( ) & & currentBlockIdx + 1 < indexState . blocksCount ( ) ) <nl> - indexState . setToBlock ( currentBlockIdx + 1 ) ; <nl> + indexState . updateBlock ( ) ; <nl> <nl> / / Return the next unfiltered unless we ' ve reached the end , or we ' re beyond our slice <nl> / / end ( note that unless we ' re on the last block for the slice , there is no point <nl> diff - - git a / src / java / org / apache / cassandra / db / columniterator / SSTableReversedIterator . java b / src / java / org / apache / cassandra / db / columniterator / SSTableReversedIterator . java <nl> index a5a1938 . . 06855e3 100644 <nl> - - - a / src / java / org / apache / cassandra / db / columniterator / SSTableReversedIterator . java <nl> + + + b / src / java / org / apache / cassandra / db / columniterator / SSTableReversedIterator . java <nl> @ @ - 116 , 7 + 116 , 7 @ @ public class SSTableReversedIterator extends AbstractSSTableIterator <nl> / / Note that we can reuse that buffer between slices ( we could alternatively re - read from disk <nl> / / every time , but that feels more wasteful ) so we want to include everything from the beginning . <nl> / / We can stop at the slice end however since any following slice will be before that . <nl> - loadFromDisk ( null , slice . end ( ) ) ; <nl> + loadFromDisk ( null , slice . end ( ) , true ) ; <nl> } <nl> setIterator ( slice ) ; <nl> } <nl> @ @ - 150 , 15 + 150 , 18 @ @ public class SSTableReversedIterator extends AbstractSSTableIterator <nl> <nl> / / Reads the unfiltered from disk and load them into the reader buffer . It stops reading when either the partition <nl> / / is fully read , or when stopReadingDisk ( ) returns true . <nl> - protected void loadFromDisk ( Slice . Bound start , Slice . Bound end ) throws IOException <nl> + protected void loadFromDisk ( Slice . Bound start , Slice . Bound end , boolean includeFirst ) throws IOException <nl> { <nl> buffer . reset ( ) ; <nl> <nl> + boolean isFirst = true ; <nl> + <nl> / / If the start might be in this block , skip everything that comes before it . <nl> if ( start ! = null ) <nl> { <nl> while ( deserializer . hasNext ( ) & & deserializer . compareNextTo ( start ) < = 0 & & ! stopReadingDisk ( ) ) <nl> { <nl> + isFirst = false ; <nl> if ( deserializer . nextIsRow ( ) ) <nl> deserializer . skipNext ( ) ; <nl> else <nl> @ @ - 179 , 7 + 182 , 10 @ @ public class SSTableReversedIterator extends AbstractSSTableIterator <nl> & & ! stopReadingDisk ( ) ) <nl> { <nl> Unfiltered unfiltered = deserializer . readNext ( ) ; <nl> - buffer . add ( unfiltered ) ; <nl> + if ( ! isFirst | | includeFirst ) <nl> + buffer . add ( unfiltered ) ; <nl> + <nl> + isFirst = false ; <nl> <nl> if ( unfiltered . isRangeTombstoneMarker ( ) ) <nl> updateOpenMarker ( ( RangeTombstoneMarker ) unfiltered ) ; <nl> @ @ - 224 , 7 + 230 , 7 @ @ public class SSTableReversedIterator extends AbstractSSTableIterator <nl> this . slice = slice ; <nl> isInit = true ; <nl> <nl> - / / if our previous slicing already got us pas the beginning of the sstable , we ' re done <nl> + / / if our previous slicing already got us past the beginning of the sstable , we ' re done <nl> if ( indexState . isDone ( ) ) <nl> { <nl> iterator = Collections . emptyIterator ( ) ; <nl> @ @ - 293 , 8 + 299 , 25 @ @ public class SSTableReversedIterator extends AbstractSSTableIterator <nl> if ( buffer = = null ) <nl> buffer = createBuffer ( indexState . blocksCount ( ) ) ; <nl> <nl> - boolean canIncludeSliceStart = indexState . currentBlockIdx ( ) = = lastBlockIdx ; <nl> - loadFromDisk ( canIncludeSliceStart ? slice . start ( ) : null , canIncludeSliceEnd ? slice . end ( ) : null ) ; <nl> + int currentBlock = indexState . currentBlockIdx ( ) ; <nl> + <nl> + boolean canIncludeSliceStart = currentBlock = = lastBlockIdx ; <nl> + <nl> + / / When dealing with old format sstable , we have the problem that a row can span 2 index block , i . e . it can <nl> + / / start at the end of a block and end at the beginning of the next one . That ' s not a problem per se for <nl> + / / UnfilteredDeserializer . OldFormatSerializer , since it always read rows entirely , even if they span index <nl> + / / blocks , but as we reading index block in reverse we must be careful to not read the end of the row at <nl> + / / beginning of a block before we ' re reading the beginning of that row . So what we do is that if we detect <nl> + / / that the row starting this block is also the row ending the previous one , we skip that first result and <nl> + / / let it be read when we ' ll read the previous block . <nl> + boolean includeFirst = true ; <nl> + if ( ! sstable . descriptor . version . storeRows ( ) & & currentBlock > 0 ) <nl> + { <nl> + ClusteringPrefix lastOfPrevious = indexState . index ( currentBlock - 1 ) . lastName ; <nl> + ClusteringPrefix firstOfCurrent = indexState . index ( currentBlock ) . firstName ; <nl> + includeFirst = metadata ( ) . comparator . compare ( lastOfPrevious , firstOfCurrent ) ! = 0 ; <nl> + } <nl> + loadFromDisk ( canIncludeSliceStart ? slice . start ( ) : null , canIncludeSliceEnd ? slice . end ( ) : null , includeFirst ) ; <nl> } <nl> <nl> @ Override <nl> diff - - git a / src / java / org / apache / cassandra / db / marshal / CompositeType . java b / src / java / org / apache / cassandra / db / marshal / CompositeType . java <nl> index 633a994 . . 9a00c70 100644 <nl> - - - a / src / java / org / apache / cassandra / db / marshal / CompositeType . java <nl> + + + b / src / java / org / apache / cassandra / db / marshal / CompositeType . java <nl> @ @ - 200 , 6 + 200 , 11 @ @ public class CompositeType extends AbstractCompositeType <nl> return l ; <nl> } <nl> <nl> + public static byte lastEOC ( ByteBuffer name ) <nl> + { <nl> + return name . get ( name . limit ( ) - 1 ) ; <nl> + } <nl> + <nl> / / Extract component idx from bb . Return null if there is not enough component . <nl> public static ByteBuffer extractComponent ( ByteBuffer bb , int idx ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java <nl> index b57724a . . 4dabe69 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java <nl> @ @ - 152 , 7 + 152 , 7 @ @ public class IndexHelper <nl> <nl> public void serialize ( IndexInfo info , DataOutputPlus out , SerializationHeader header ) throws IOException <nl> { <nl> - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . clusteringPrefixSerializer ( version , header ) ; <nl> + ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; <nl> clusteringSerializer . serialize ( info . firstName , out ) ; <nl> clusteringSerializer . serialize ( info . lastName , out ) ; <nl> out . writeLong ( info . offset ) ; <nl> @ @ - 168 , 7 + 168 , 7 @ @ public class IndexHelper <nl> <nl> public IndexInfo deserialize ( DataInputPlus in , SerializationHeader header ) throws IOException <nl> { <nl> - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . clusteringPrefixSerializer ( version , header ) ; <nl> + ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; <nl> <nl> ClusteringPrefix firstName = clusteringSerializer . deserialize ( in ) ; <nl> ClusteringPrefix lastName = clusteringSerializer . deserialize ( in ) ; <nl> @ @ - 183 , 7 + 183 , 7 @ @ public class IndexHelper <nl> <nl> public long serializedSize ( IndexInfo info , SerializationHeader header ) <nl> { <nl> - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . clusteringPrefixSerializer ( version , header ) ; <nl> + ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; <nl> long size = clusteringSerializer . serializedSize ( info . firstName ) <nl> + clusteringSerializer . serializedSize ( info . lastName ) <nl> + TypeSizes . sizeof ( info . offset )
NEAREST DIFF (one line): diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db <nl> deleted file mode 100644 <nl> index 0f95b07 . . 0000000 <nl> Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db and / dev / null differ <nl> diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db <nl> deleted file mode 100644 <nl> index 88aac99 . . 0000000 <nl> Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db and / dev / null differ <nl> diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db <nl> deleted file mode 100644 <nl> index a7787c5 . . 0000000 <nl> Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db and / dev / null differ <nl> diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db <nl> deleted file mode 100644 <nl> index 6312c71 . . 0000000 <nl> Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db and / dev / null differ <nl> diff - - git a / test / unit / org / apache / cassandra / db / ScrubTest . java b / test / unit / org / apache / cassandra / db / ScrubTest . java <nl> new file mode 100644 <nl> index 0000000 . . 26f0e78 <nl> - - - / dev / null <nl> + + + b / test / unit / org / apache / cassandra / db / ScrubTest . java <nl> @ @ - 0 , 0 + 1 , 211 @ @ <nl> + package org . apache . cassandra . db ; <nl> + / * <nl> + * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , <nl> + * software distributed under the License is distributed on an <nl> + * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY <nl> + * KIND , either express or implied . See the License for the <nl> + * specific language governing permissions and limitations <nl> + * under the License . <nl> + * <nl> + * / <nl> + <nl> + <nl> + import java . io . File ; <nl> + import java . io . IOException ; <nl> + import java . util . List ; <nl> + import java . util . concurrent . ExecutionException ; <nl> + <nl> + import org . junit . Test ; <nl> + <nl> + import org . apache . cassandra . SchemaLoader ; <nl> + import org . apache . cassandra . Util ; <nl> + import org . apache . cassandra . exceptions . ConfigurationException ; <nl> + import org . apache . cassandra . db . columniterator . IdentityQueryFilter ; <nl> + import org . apache . cassandra . db . compaction . CompactionManager ; <nl> + import org . apache . cassandra . io . util . FileUtils ; <nl> + import org . apache . cassandra . utils . ByteBufferUtil ; <nl> + import org . apache . cassandra . utils . CLibrary ; <nl> + <nl> + import static org . apache . cassandra . Util . column ; <nl> + import static org . junit . Assert . assertEquals ; <nl> + import static org . junit . Assert . fail ; <nl> + <nl> + public class ScrubTest extends SchemaLoader <nl> + { <nl> + public String TABLE = " Keyspace1 " ; <nl> + public String CF = " Standard1 " ; <nl> + public String CF2 = " Super5 " ; <nl> + public String CF3 = " Standard2 " ; <nl> + <nl> + public String copySSTables ( String cf ) throws IOException <nl> + { <nl> + String root = System . getProperty ( " corrupt - sstable - root " ) ; <nl> + assert root ! = null ; <nl> + File rootDir = new File ( root ) ; <nl> + assert rootDir . isDirectory ( ) ; <nl> + <nl> + File destDir = Directories . create ( TABLE , cf ) . getDirectoryForNewSSTables ( 1 ) ; <nl> + <nl> + String corruptSSTableName = null ; <nl> + <nl> + FileUtils . createDirectory ( destDir ) ; <nl> + for ( File srcFile : rootDir . listFiles ( ) ) <nl> + { <nl> + if ( srcFile . getName ( ) . equals ( " . svn " ) ) <nl> + continue ; <nl> + if ( ! srcFile . getName ( ) . contains ( cf ) ) <nl> + continue ; <nl> + File destFile = new File ( destDir , srcFile . getName ( ) ) ; <nl> + CLibrary . createHardLink ( srcFile , destFile ) ; <nl> + <nl> + assert destFile . exists ( ) : destFile . getAbsoluteFile ( ) ; <nl> + <nl> + if ( destFile . getName ( ) . endsWith ( " Data . db " ) ) <nl> + corruptSSTableName = destFile . getCanonicalPath ( ) ; <nl> + } <nl> + <nl> + assert corruptSSTableName ! = null ; <nl> + return corruptSSTableName ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testScrubOneRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException <nl> + { <nl> + CompactionManager . instance . disableAutoCompaction ( ) ; <nl> + Table table = Table . open ( TABLE ) ; <nl> + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF ) ; <nl> + <nl> + List < Row > rows ; <nl> + <nl> + / / insert data and verify we get it back w / range query <nl> + fillCF ( cfs , 1 ) ; <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assertEquals ( 1 , rows . size ( ) ) ; <nl> + <nl> + CompactionManager . instance . performScrub ( cfs ) ; <nl> + <nl> + / / check data is still there <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assertEquals ( 1 , rows . size ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testScrubDeletedRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException <nl> + { <nl> + CompactionManager . instance . disableAutoCompaction ( ) ; <nl> + Table table = Table . open ( TABLE ) ; <nl> + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF3 ) ; <nl> + <nl> + RowMutation rm ; <nl> + rm = new RowMutation ( TABLE , ByteBufferUtil . bytes ( 1 ) ) ; <nl> + ColumnFamily cf = ColumnFamily . create ( TABLE , CF3 ) ; <nl> + cf . delete ( new DeletionInfo ( 0 , 1 ) ) ; / / expired tombstone <nl> + rm . add ( cf ) ; <nl> + rm . applyUnsafe ( ) ; <nl> + cfs . forceBlockingFlush ( ) ; <nl> + <nl> + CompactionManager . instance . performScrub ( cfs ) ; <nl> + assert cfs . getSSTables ( ) . isEmpty ( ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testScrubMultiRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException <nl> + { <nl> + CompactionManager . instance . disableAutoCompaction ( ) ; <nl> + Table table = Table . open ( TABLE ) ; <nl> + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF ) ; <nl> + <nl> + List < Row > rows ; <nl> + <nl> + / / insert data and verify we get it back w / range query <nl> + fillCF ( cfs , 10 ) ; <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assertEquals ( 10 , rows . size ( ) ) ; <nl> + <nl> + CompactionManager . instance . performScrub ( cfs ) ; <nl> + <nl> + / / check data is still there <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assertEquals ( 10 , rows . size ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testScubOutOfOrder ( ) throws Exception <nl> + { <nl> + CompactionManager . instance . disableAutoCompaction ( ) ; <nl> + Table table = Table . open ( TABLE ) ; <nl> + String columnFamily = " Standard3 " ; <nl> + ColumnFamilyStore cfs = table . getColumnFamilyStore ( columnFamily ) ; <nl> + <nl> + / * <nl> + * Code used to generate an outOfOrder sstable . The test for out - of - order key in SSTableWriter must also be commented out . <nl> + * The test also assumes an ordered partitioner . <nl> + * <nl> + * ColumnFamily cf = ColumnFamily . create ( TABLE , columnFamily ) ; <nl> + * cf . addColumn ( new Column ( ByteBufferUtil . bytes ( " someName " ) , ByteBufferUtil . bytes ( " someValue " ) , 0L ) ) ; <nl> + <nl> + * SSTableWriter writer = cfs . createCompactionWriter ( ( long ) DatabaseDescriptor . getIndexInterval ( ) , new File ( " . " ) , Collections . < SSTableReader > emptyList ( ) ) ; <nl> + * writer . append ( Util . dk ( " a " ) , cf ) ; <nl> + * writer . append ( Util . dk ( " b " ) , cf ) ; <nl> + * writer . append ( Util . dk ( " z " ) , cf ) ; <nl> + * writer . append ( Util . dk ( " c " ) , cf ) ; <nl> + * writer . append ( Util . dk ( " y " ) , cf ) ; <nl> + * writer . append ( Util . dk ( " d " ) , cf ) ; <nl> + * writer . closeAndOpenReader ( ) ; <nl> + * / <nl> + <nl> + copySSTables ( columnFamily ) ; <nl> + cfs . loadNewSSTables ( ) ; <nl> + assert cfs . getSSTables ( ) . size ( ) > 0 ; <nl> + <nl> + List < Row > rows ; <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assert ! isRowOrdered ( rows ) : " ' corrupt ' test file actually was not " ; <nl> + <nl> + CompactionManager . instance . performScrub ( cfs ) ; <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assert isRowOrdered ( rows ) : " Scrub failed : " + rows ; <nl> + assert rows . size ( ) = = 6 : " Got " + rows . size ( ) ; <nl> + } <nl> + <nl> + private static boolean isRowOrdered ( List < Row > rows ) <nl> + { <nl> + DecoratedKey prev = null ; <nl> + for ( Row row : rows ) <nl> + { <nl> + if ( prev ! = null & & prev . compareTo ( row . key ) > 0 ) <nl> + return false ; <nl> + prev = row . key ; <nl> + } <nl> + return true ; <nl> + } <nl> + <nl> + protected void fillCF ( ColumnFamilyStore cfs , int rowsPerSSTable ) throws ExecutionException , InterruptedException , IOException <nl> + { <nl> + for ( int i = 0 ; i < rowsPerSSTable ; i + + ) <nl> + { <nl> + String key = String . valueOf ( i ) ; <nl> + / / create a row and update the birthdate value , test that the index query fetches the new version <nl> + RowMutation rm ; <nl> + rm = new RowMutation ( TABLE , ByteBufferUtil . bytes ( key ) ) ; <nl> + ColumnFamily cf = ColumnFamily . create ( TABLE , CF ) ; <nl> + cf . addColumn ( column ( " c1 " , " 1 " , 1L ) ) ; <nl> + cf . addColumn ( column ( " c2 " , " 2 " , 1L ) ) ; <nl> + rm . add ( cf ) ; <nl> + rm . applyUnsafe ( ) ; <nl> + } <nl> + <nl> + cfs . forceBlockingFlush ( ) ; <nl> + } <nl> + }

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index fb33de0 . . 726eb04 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 3 . 0 . 0 - beta2 
 + * Fix startup error when upgrading nodes ( CASSANDRA - 10136 ) 
 * Base table PRIMARY KEY can be assumed to be NOT NULL in MV creation ( CASSANDRA - 10147 ) 
 * Improve batchlog write patch ( CASSANDRA - 9673 ) 
 * Re - apply MaterializedView updates on commitlog replay ( CASSANDRA - 10164 ) 
 diff - - git a / src / java / org / apache / cassandra / db / Serializers . java b / src / java / org / apache / cassandra / db / Serializers . java 
 index 8056222 . . 2561bbe 100644 
 - - - a / src / java / org / apache / cassandra / db / Serializers . java 
 + + + b / src / java / org / apache / cassandra / db / Serializers . java 
 @ @ - 18 , 12 + 18 , 16 @ @ 
 package org . apache . cassandra . db ; 
 
 import java . io . * ; 
 + import java . nio . ByteBuffer ; 
 + import java . util . List ; 
 
 import org . apache . cassandra . config . CFMetaData ; 
 + import org . apache . cassandra . db . marshal . CompositeType ; 
 import org . apache . cassandra . io . ISerializer ; 
 import org . apache . cassandra . io . util . DataInputPlus ; 
 import org . apache . cassandra . io . util . DataOutputPlus ; 
 import org . apache . cassandra . io . sstable . format . Version ; 
 + import org . apache . cassandra . utils . ByteBufferUtil ; 
 
 import static org . apache . cassandra . io . sstable . IndexHelper . IndexInfo ; 
 
 @ @ - 44 , 12 + 48 , 65 @ @ public class Serializers 
 return new IndexInfo . Serializer ( metadata , version ) ; 
 } 
 
 - / / Note that for the old layout , this will actually discard the cellname parts that are not strictly 
 - / / part of the clustering prefix . Don ' t use this if that ' s not what you want . 
 - public ISerializer < ClusteringPrefix > clusteringPrefixSerializer ( final Version version , final SerializationHeader header ) 
 + / / TODO : Once we drop support for old ( pre - 3 . 0 ) sstables , we can drop this method and inline the calls to 
 + / / ClusteringPrefix . serializer in IndexHelper directly . At which point this whole class probably becomes 
 + / / unecessary ( since IndexInfo . Serializer won ' t depend on the metadata either ) . 
 + public ISerializer < ClusteringPrefix > indexEntryClusteringPrefixSerializer ( final Version version , final SerializationHeader header ) 
 { 
 if ( ! version . storeRows ( ) ) 
 - throw new UnsupportedOperationException ( ) ; 
 + { 
 + return new ISerializer < ClusteringPrefix > ( ) 
 + { 
 + public void serialize ( ClusteringPrefix clustering , DataOutputPlus out ) throws IOException 
 + { 
 + / / We should only use this for reading old sstable , never write new ones . 
 + throw new UnsupportedOperationException ( ) ; 
 + } 
 + 
 + public ClusteringPrefix deserialize ( DataInputPlus in ) throws IOException 
 + { 
 + / / We ' re reading the old cellname / composite 
 + ByteBuffer bb = ByteBufferUtil . readWithShortLength ( in ) ; 
 + assert bb . hasRemaining ( ) ; / / empty cellnames were invalid 
 + 
 + int clusteringSize = metadata . clusteringColumns ( ) . size ( ) ; 
 + / / If the table has no clustering column , then the cellname will just be the " column " name , which we ignore here . 
 + if ( clusteringSize = = 0 ) 
 + return Clustering . EMPTY ; 
 + 
 + if ( ! metadata . isCompound ( ) ) 
 + return new Clustering ( bb ) ; 
 + 
 + List < ByteBuffer > components = CompositeType . splitName ( bb ) ; 
 + byte eoc = CompositeType . lastEOC ( bb ) ; 
 + 
 + if ( eoc = = 0 | | components . size ( ) > = clusteringSize ) 
 + { 
 + / / That ' s a clustering . 
 + if ( components . size ( ) > clusteringSize ) 
 + components = components . subList ( 0 , clusteringSize ) ; 
 + 
 + return new Clustering ( components . toArray ( new ByteBuffer [ clusteringSize ] ) ) ; 
 + } 
 + else 
 + { 
 + / / It ' s a range tombstone bound . It is a start since that ' s the only part we ' ve ever included 
 + / / in the index entries . 
 + Slice . Bound . Kind boundKind = eoc > 0 
 + ? Slice . Bound . Kind . EXCL _ START _ BOUND 
 + : Slice . Bound . Kind . INCL _ START _ BOUND ; 
 + 
 + return Slice . Bound . create ( boundKind , components . toArray ( new ByteBuffer [ components . size ( ) ] ) ) ; 
 + } 
 + } 
 + 
 + public long serializedSize ( ClusteringPrefix clustering ) 
 + { 
 + / / We should only use this for reading old sstable , never write new ones . 
 + throw new UnsupportedOperationException ( ) ; 
 + } 
 + } ; 
 + } 
 
 return new ISerializer < ClusteringPrefix > ( ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / db / SinglePartitionNamesCommand . java b / src / java / org / apache / cassandra / db / SinglePartitionNamesCommand . java 
 index cee3fc4 . . 1b41005 100644 
 - - - a / src / java / org / apache / cassandra / db / SinglePartitionNamesCommand . java 
 + + + b / src / java / org / apache / cassandra / db / SinglePartitionNamesCommand . java 
 @ @ - 192 , 7 + 192 , 7 @ @ public class SinglePartitionNamesCommand extends SinglePartitionReadCommand < Clus 
 if ( result = = null ) 
 return ImmutableBTreePartition . create ( iter , maxRows ) ; 
 
 - try ( UnfilteredRowIterator merged = UnfilteredRowIterators . merge ( Arrays . asList ( iter , result . unfilteredIterator ( columnFilter ( ) , Slices . ALL , false ) ) , nowInSec ( ) ) ) 
 + try ( UnfilteredRowIterator merged = UnfilteredRowIterators . merge ( Arrays . asList ( iter , result . unfilteredIterator ( columnFilter ( ) , Slices . ALL , clusteringIndexFilter ( ) . isReversed ( ) ) ) , nowInSec ( ) ) ) 
 { 
 return ImmutableBTreePartition . create ( merged , maxRows ) ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java b / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java 
 index cf4bff7 . . 87a57c6 100644 
 - - - a / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java 
 + + + b / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java 
 @ @ - 22 , 9 + 22 , 6 @ @ import java . util . Collections ; 
 import java . util . Iterator ; 
 import java . util . List ; 
 
 - import org . slf4j . Logger ; 
 - import org . slf4j . LoggerFactory ; 
 - 
 import org . apache . cassandra . config . CFMetaData ; 
 import org . apache . cassandra . db . * ; 
 import org . apache . cassandra . db . filter . ColumnFilter ; 
 @ @ - 453 , 6 + 450 , 32 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator 
 return indexes . size ( ) ; 
 } 
 
 + / / Update the block idx based on the current reader position if we ' re past the current block . 
 + public void updateBlock ( ) throws IOException 
 + { 
 + assert currentIndexIdx > = 0 ; 
 + while ( currentIndexIdx + 1 < indexes . size ( ) & & isPastCurrentBlock ( ) ) 
 + { 
 + reader . openMarker = currentIndex ( ) . endOpenMarker ; 
 + + + currentIndexIdx ; 
 + 
 + / / We have to set the mark , and we have to set it at the beginning of the block . So if we ' re not at the beginning of the block , this forces us to a weird seek dance . 
 + / / This can only happen when reading old file however . 
 + long startOfBlock = indexEntry . position + indexes . get ( currentIndexIdx ) . offset ; 
 + long currentFilePointer = reader . file . getFilePointer ( ) ; 
 + if ( startOfBlock = = currentFilePointer ) 
 + { 
 + mark = reader . file . mark ( ) ; 
 + } 
 + else 
 + { 
 + reader . seekToPosition ( startOfBlock ) ; 
 + mark = reader . file . mark ( ) ; 
 + reader . seekToPosition ( currentFilePointer ) ; 
 + } 
 + } 
 + } 
 + 
 / / Check if we ' ve crossed an index boundary ( based on the mark on the beginning of the index block ) . 
 public boolean isPastCurrentBlock ( ) 
 { 
 @ @ - 466 , 7 + 489 , 12 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator 
 
 public IndexHelper . IndexInfo currentIndex ( ) 
 { 
 - return indexes . get ( currentIndexIdx ) ; 
 + return index ( currentIndexIdx ) ; 
 + } 
 + 
 + public IndexHelper . IndexInfo index ( int i ) 
 + { 
 + return indexes . get ( i ) ; 
 } 
 
 / / Finds the index of the first block containing the provided bound , starting at the provided index . 
 diff - - git a / src / java / org / apache / cassandra / db / columniterator / SSTableIterator . java b / src / java / org / apache / cassandra / db / columniterator / SSTableIterator . java 
 index e985c18 . . 3536d65 100644 
 - - - a / src / java / org / apache / cassandra / db / columniterator / SSTableIterator . java 
 + + + b / src / java / org / apache / cassandra / db / columniterator / SSTableIterator . java 
 @ @ - 265 , 9 + 265 , 7 @ @ public class SSTableIterator extends AbstractSSTableIterator 
 protected Unfiltered computeNext ( ) throws IOException 
 { 
 / / Our previous read might have made us cross an index block boundary . If so , update our informations . 
 - int currentBlockIdx = indexState . currentBlockIdx ( ) ; 
 - if ( indexState . isPastCurrentBlock ( ) & & currentBlockIdx + 1 < indexState . blocksCount ( ) ) 
 - indexState . setToBlock ( currentBlockIdx + 1 ) ; 
 + indexState . updateBlock ( ) ; 
 
 / / Return the next unfiltered unless we ' ve reached the end , or we ' re beyond our slice 
 / / end ( note that unless we ' re on the last block for the slice , there is no point 
 diff - - git a / src / java / org / apache / cassandra / db / columniterator / SSTableReversedIterator . java b / src / java / org / apache / cassandra / db / columniterator / SSTableReversedIterator . java 
 index a5a1938 . . 06855e3 100644 
 - - - a / src / java / org / apache / cassandra / db / columniterator / SSTableReversedIterator . java 
 + + + b / src / java / org / apache / cassandra / db / columniterator / SSTableReversedIterator . java 
 @ @ - 116 , 7 + 116 , 7 @ @ public class SSTableReversedIterator extends AbstractSSTableIterator 
 / / Note that we can reuse that buffer between slices ( we could alternatively re - read from disk 
 / / every time , but that feels more wasteful ) so we want to include everything from the beginning . 
 / / We can stop at the slice end however since any following slice will be before that . 
 - loadFromDisk ( null , slice . end ( ) ) ; 
 + loadFromDisk ( null , slice . end ( ) , true ) ; 
 } 
 setIterator ( slice ) ; 
 } 
 @ @ - 150 , 15 + 150 , 18 @ @ public class SSTableReversedIterator extends AbstractSSTableIterator 
 
 / / Reads the unfiltered from disk and load them into the reader buffer . It stops reading when either the partition 
 / / is fully read , or when stopReadingDisk ( ) returns true . 
 - protected void loadFromDisk ( Slice . Bound start , Slice . Bound end ) throws IOException 
 + protected void loadFromDisk ( Slice . Bound start , Slice . Bound end , boolean includeFirst ) throws IOException 
 { 
 buffer . reset ( ) ; 
 
 + boolean isFirst = true ; 
 + 
 / / If the start might be in this block , skip everything that comes before it . 
 if ( start ! = null ) 
 { 
 while ( deserializer . hasNext ( ) & & deserializer . compareNextTo ( start ) < = 0 & & ! stopReadingDisk ( ) ) 
 { 
 + isFirst = false ; 
 if ( deserializer . nextIsRow ( ) ) 
 deserializer . skipNext ( ) ; 
 else 
 @ @ - 179 , 7 + 182 , 10 @ @ public class SSTableReversedIterator extends AbstractSSTableIterator 
 & & ! stopReadingDisk ( ) ) 
 { 
 Unfiltered unfiltered = deserializer . readNext ( ) ; 
 - buffer . add ( unfiltered ) ; 
 + if ( ! isFirst | | includeFirst ) 
 + buffer . add ( unfiltered ) ; 
 + 
 + isFirst = false ; 
 
 if ( unfiltered . isRangeTombstoneMarker ( ) ) 
 updateOpenMarker ( ( RangeTombstoneMarker ) unfiltered ) ; 
 @ @ - 224 , 7 + 230 , 7 @ @ public class SSTableReversedIterator extends AbstractSSTableIterator 
 this . slice = slice ; 
 isInit = true ; 
 
 - / / if our previous slicing already got us pas the beginning of the sstable , we ' re done 
 + / / if our previous slicing already got us past the beginning of the sstable , we ' re done 
 if ( indexState . isDone ( ) ) 
 { 
 iterator = Collections . emptyIterator ( ) ; 
 @ @ - 293 , 8 + 299 , 25 @ @ public class SSTableReversedIterator extends AbstractSSTableIterator 
 if ( buffer = = null ) 
 buffer = createBuffer ( indexState . blocksCount ( ) ) ; 
 
 - boolean canIncludeSliceStart = indexState . currentBlockIdx ( ) = = lastBlockIdx ; 
 - loadFromDisk ( canIncludeSliceStart ? slice . start ( ) : null , canIncludeSliceEnd ? slice . end ( ) : null ) ; 
 + int currentBlock = indexState . currentBlockIdx ( ) ; 
 + 
 + boolean canIncludeSliceStart = currentBlock = = lastBlockIdx ; 
 + 
 + / / When dealing with old format sstable , we have the problem that a row can span 2 index block , i . e . it can 
 + / / start at the end of a block and end at the beginning of the next one . That ' s not a problem per se for 
 + / / UnfilteredDeserializer . OldFormatSerializer , since it always read rows entirely , even if they span index 
 + / / blocks , but as we reading index block in reverse we must be careful to not read the end of the row at 
 + / / beginning of a block before we ' re reading the beginning of that row . So what we do is that if we detect 
 + / / that the row starting this block is also the row ending the previous one , we skip that first result and 
 + / / let it be read when we ' ll read the previous block . 
 + boolean includeFirst = true ; 
 + if ( ! sstable . descriptor . version . storeRows ( ) & & currentBlock > 0 ) 
 + { 
 + ClusteringPrefix lastOfPrevious = indexState . index ( currentBlock - 1 ) . lastName ; 
 + ClusteringPrefix firstOfCurrent = indexState . index ( currentBlock ) . firstName ; 
 + includeFirst = metadata ( ) . comparator . compare ( lastOfPrevious , firstOfCurrent ) ! = 0 ; 
 + } 
 + loadFromDisk ( canIncludeSliceStart ? slice . start ( ) : null , canIncludeSliceEnd ? slice . end ( ) : null , includeFirst ) ; 
 } 
 
 @ Override 
 diff - - git a / src / java / org / apache / cassandra / db / marshal / CompositeType . java b / src / java / org / apache / cassandra / db / marshal / CompositeType . java 
 index 633a994 . . 9a00c70 100644 
 - - - a / src / java / org / apache / cassandra / db / marshal / CompositeType . java 
 + + + b / src / java / org / apache / cassandra / db / marshal / CompositeType . java 
 @ @ - 200 , 6 + 200 , 11 @ @ public class CompositeType extends AbstractCompositeType 
 return l ; 
 } 
 
 + public static byte lastEOC ( ByteBuffer name ) 
 + { 
 + return name . get ( name . limit ( ) - 1 ) ; 
 + } 
 + 
 / / Extract component idx from bb . Return null if there is not enough component . 
 public static ByteBuffer extractComponent ( ByteBuffer bb , int idx ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java 
 index b57724a . . 4dabe69 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java 
 @ @ - 152 , 7 + 152 , 7 @ @ public class IndexHelper 
 
 public void serialize ( IndexInfo info , DataOutputPlus out , SerializationHeader header ) throws IOException 
 { 
 - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . clusteringPrefixSerializer ( version , header ) ; 
 + ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; 
 clusteringSerializer . serialize ( info . firstName , out ) ; 
 clusteringSerializer . serialize ( info . lastName , out ) ; 
 out . writeLong ( info . offset ) ; 
 @ @ - 168 , 7 + 168 , 7 @ @ public class IndexHelper 
 
 public IndexInfo deserialize ( DataInputPlus in , SerializationHeader header ) throws IOException 
 { 
 - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . clusteringPrefixSerializer ( version , header ) ; 
 + ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; 
 
 ClusteringPrefix firstName = clusteringSerializer . deserialize ( in ) ; 
 ClusteringPrefix lastName = clusteringSerializer . deserialize ( in ) ; 
 @ @ - 183 , 7 + 183 , 7 @ @ public class IndexHelper 
 
 public long serializedSize ( IndexInfo info , SerializationHeader header ) 
 { 
 - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . clusteringPrefixSerializer ( version , header ) ; 
 + ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; 
 long size = clusteringSerializer . serializedSize ( info . firstName ) 
 + clusteringSerializer . serializedSize ( info . lastName ) 
 + TypeSizes . sizeof ( info . offset )

NEAREST DIFF:
diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db 
 deleted file mode 100644 
 index 0f95b07 . . 0000000 
 Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db and / dev / null differ 
 diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db 
 deleted file mode 100644 
 index 88aac99 . . 0000000 
 Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db and / dev / null differ 
 diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db 
 deleted file mode 100644 
 index a7787c5 . . 0000000 
 Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db and / dev / null differ 
 diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db 
 deleted file mode 100644 
 index 6312c71 . . 0000000 
 Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db and / dev / null differ 
 diff - - git a / test / unit / org / apache / cassandra / db / ScrubTest . java b / test / unit / org / apache / cassandra / db / ScrubTest . java 
 new file mode 100644 
 index 0000000 . . 26f0e78 
 - - - / dev / null 
 + + + b / test / unit / org / apache / cassandra / db / ScrubTest . java 
 @ @ - 0 , 0 + 1 , 211 @ @ 
 + package org . apache . cassandra . db ; 
 + / * 
 + * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , 
 + * software distributed under the License is distributed on an 
 + * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY 
 + * KIND , either express or implied . See the License for the 
 + * specific language governing permissions and limitations 
 + * under the License . 
 + * 
 + * / 
 + 
 + 
 + import java . io . File ; 
 + import java . io . IOException ; 
 + import java . util . List ; 
 + import java . util . concurrent . ExecutionException ; 
 + 
 + import org . junit . Test ; 
 + 
 + import org . apache . cassandra . SchemaLoader ; 
 + import org . apache . cassandra . Util ; 
 + import org . apache . cassandra . exceptions . ConfigurationException ; 
 + import org . apache . cassandra . db . columniterator . IdentityQueryFilter ; 
 + import org . apache . cassandra . db . compaction . CompactionManager ; 
 + import org . apache . cassandra . io . util . FileUtils ; 
 + import org . apache . cassandra . utils . ByteBufferUtil ; 
 + import org . apache . cassandra . utils . CLibrary ; 
 + 
 + import static org . apache . cassandra . Util . column ; 
 + import static org . junit . Assert . assertEquals ; 
 + import static org . junit . Assert . fail ; 
 + 
 + public class ScrubTest extends SchemaLoader 
 + { 
 + public String TABLE = " Keyspace1 " ; 
 + public String CF = " Standard1 " ; 
 + public String CF2 = " Super5 " ; 
 + public String CF3 = " Standard2 " ; 
 + 
 + public String copySSTables ( String cf ) throws IOException 
 + { 
 + String root = System . getProperty ( " corrupt - sstable - root " ) ; 
 + assert root ! = null ; 
 + File rootDir = new File ( root ) ; 
 + assert rootDir . isDirectory ( ) ; 
 + 
 + File destDir = Directories . create ( TABLE , cf ) . getDirectoryForNewSSTables ( 1 ) ; 
 + 
 + String corruptSSTableName = null ; 
 + 
 + FileUtils . createDirectory ( destDir ) ; 
 + for ( File srcFile : rootDir . listFiles ( ) ) 
 + { 
 + if ( srcFile . getName ( ) . equals ( " . svn " ) ) 
 + continue ; 
 + if ( ! srcFile . getName ( ) . contains ( cf ) ) 
 + continue ; 
 + File destFile = new File ( destDir , srcFile . getName ( ) ) ; 
 + CLibrary . createHardLink ( srcFile , destFile ) ; 
 + 
 + assert destFile . exists ( ) : destFile . getAbsoluteFile ( ) ; 
 + 
 + if ( destFile . getName ( ) . endsWith ( " Data . db " ) ) 
 + corruptSSTableName = destFile . getCanonicalPath ( ) ; 
 + } 
 + 
 + assert corruptSSTableName ! = null ; 
 + return corruptSSTableName ; 
 + } 
 + 
 + @ Test 
 + public void testScrubOneRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException 
 + { 
 + CompactionManager . instance . disableAutoCompaction ( ) ; 
 + Table table = Table . open ( TABLE ) ; 
 + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF ) ; 
 + 
 + List < Row > rows ; 
 + 
 + / / insert data and verify we get it back w / range query 
 + fillCF ( cfs , 1 ) ; 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assertEquals ( 1 , rows . size ( ) ) ; 
 + 
 + CompactionManager . instance . performScrub ( cfs ) ; 
 + 
 + / / check data is still there 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assertEquals ( 1 , rows . size ( ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testScrubDeletedRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException 
 + { 
 + CompactionManager . instance . disableAutoCompaction ( ) ; 
 + Table table = Table . open ( TABLE ) ; 
 + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF3 ) ; 
 + 
 + RowMutation rm ; 
 + rm = new RowMutation ( TABLE , ByteBufferUtil . bytes ( 1 ) ) ; 
 + ColumnFamily cf = ColumnFamily . create ( TABLE , CF3 ) ; 
 + cf . delete ( new DeletionInfo ( 0 , 1 ) ) ; / / expired tombstone 
 + rm . add ( cf ) ; 
 + rm . applyUnsafe ( ) ; 
 + cfs . forceBlockingFlush ( ) ; 
 + 
 + CompactionManager . instance . performScrub ( cfs ) ; 
 + assert cfs . getSSTables ( ) . isEmpty ( ) ; 
 + } 
 + 
 + @ Test 
 + public void testScrubMultiRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException 
 + { 
 + CompactionManager . instance . disableAutoCompaction ( ) ; 
 + Table table = Table . open ( TABLE ) ; 
 + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF ) ; 
 + 
 + List < Row > rows ; 
 + 
 + / / insert data and verify we get it back w / range query 
 + fillCF ( cfs , 10 ) ; 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assertEquals ( 10 , rows . size ( ) ) ; 
 + 
 + CompactionManager . instance . performScrub ( cfs ) ; 
 + 
 + / / check data is still there 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assertEquals ( 10 , rows . size ( ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testScubOutOfOrder ( ) throws Exception 
 + { 
 + CompactionManager . instance . disableAutoCompaction ( ) ; 
 + Table table = Table . open ( TABLE ) ; 
 + String columnFamily = " Standard3 " ; 
 + ColumnFamilyStore cfs = table . getColumnFamilyStore ( columnFamily ) ; 
 + 
 + / * 
 + * Code used to generate an outOfOrder sstable . The test for out - of - order key in SSTableWriter must also be commented out . 
 + * The test also assumes an ordered partitioner . 
 + * 
 + * ColumnFamily cf = ColumnFamily . create ( TABLE , columnFamily ) ; 
 + * cf . addColumn ( new Column ( ByteBufferUtil . bytes ( " someName " ) , ByteBufferUtil . bytes ( " someValue " ) , 0L ) ) ; 
 + 
 + * SSTableWriter writer = cfs . createCompactionWriter ( ( long ) DatabaseDescriptor . getIndexInterval ( ) , new File ( " . " ) , Collections . < SSTableReader > emptyList ( ) ) ; 
 + * writer . append ( Util . dk ( " a " ) , cf ) ; 
 + * writer . append ( Util . dk ( " b " ) , cf ) ; 
 + * writer . append ( Util . dk ( " z " ) , cf ) ; 
 + * writer . append ( Util . dk ( " c " ) , cf ) ; 
 + * writer . append ( Util . dk ( " y " ) , cf ) ; 
 + * writer . append ( Util . dk ( " d " ) , cf ) ; 
 + * writer . closeAndOpenReader ( ) ; 
 + * / 
 + 
 + copySSTables ( columnFamily ) ; 
 + cfs . loadNewSSTables ( ) ; 
 + assert cfs . getSSTables ( ) . size ( ) > 0 ; 
 + 
 + List < Row > rows ; 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assert ! isRowOrdered ( rows ) : " ' corrupt ' test file actually was not " ; 
 + 
 + CompactionManager . instance . performScrub ( cfs ) ; 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assert isRowOrdered ( rows ) : " Scrub failed : " + rows ; 
 + assert rows . size ( ) = = 6 : " Got " + rows . size ( ) ; 
 + } 
 + 
 + private static boolean isRowOrdered ( List < Row > rows ) 
 + { 
 + DecoratedKey prev = null ; 
 + for ( Row row : rows ) 
 + { 
 + if ( prev ! = null & & prev . compareTo ( row . key ) > 0 ) 
 + return false ; 
 + prev = row . key ; 
 + } 
 + return true ; 
 + } 
 + 
 + protected void fillCF ( ColumnFamilyStore cfs , int rowsPerSSTable ) throws ExecutionException , InterruptedException , IOException 
 + { 
 + for ( int i = 0 ; i < rowsPerSSTable ; i + + ) 
 + { 
 + String key = String . valueOf ( i ) ; 
 + / / create a row and update the birthdate value , test that the index query fetches the new version 
 + RowMutation rm ; 
 + rm = new RowMutation ( TABLE , ByteBufferUtil . bytes ( key ) ) ; 
 + ColumnFamily cf = ColumnFamily . create ( TABLE , CF ) ; 
 + cf . addColumn ( column ( " c1 " , " 1 " , 1L ) ) ; 
 + cf . addColumn ( column ( " c2 " , " 2 " , 1L ) ) ; 
 + rm . add ( cf ) ; 
 + rm . applyUnsafe ( ) ; 
 + } 
 + 
 + cfs . forceBlockingFlush ( ) ; 
 + } 
 + }
