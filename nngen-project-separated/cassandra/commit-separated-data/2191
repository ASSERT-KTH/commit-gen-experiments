BLEU SCORE: 0.027611988917697356

TEST MSG: ( cqlsh ) COPY TO / FROM improvements
GENERATED MSG: python CQL driver result decoding

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index d29a2db . . 6facab9 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 2 . 1 . 1 <nl> + * ( cqlsh ) COPY TO / FROM improvements ( CASSANDRA - 7405 ) <nl> * Support list index operations with conditions ( CASSANDRA - 7499 ) <nl> * Add max live / tombstoned cells to nodetool cfstats output ( CASSANDRA - 7731 ) <nl> * Validate IPv6 wildcard addresses properly ( CASSANDRA - 7680 ) <nl> diff - - git a / bin / cqlsh b / bin / cqlsh <nl> index b65ae00 . . 852f4f5 100755 <nl> - - - a / bin / cqlsh <nl> + + + b / bin / cqlsh <nl> @ @ - 118 , 7 + 118 , 7 @ @ cqlshlibdir = os . path . join ( CASSANDRA _ PATH , ' pylib ' ) <nl> if os . path . isdir ( cqlshlibdir ) : <nl> sys . path . insert ( 0 , cqlshlibdir ) <nl> <nl> - from cqlshlib import cqlhandling , cql3handling , pylexotron , sslhandling <nl> + from cqlshlib import cqlhandling , cql3handling , pylexotron , sslhandling , async _ insert <nl> from cqlshlib . displaying import ( RED , BLUE , CYAN , ANSI _ RESET , COLUMN _ NAME _ COLORS , <nl> FormattedValue , colorme ) <nl> from cqlshlib . formatting import format _ by _ type , formatter _ for , format _ value _ utype <nl> @ @ - 1348 , 27 + 1348 , 33 @ @ class Shell ( cmd . Cmd ) : <nl> if header : <nl> linesource . next ( ) <nl> table _ meta = self . get _ table _ meta ( ks , cf ) <nl> - rownum = - 1 <nl> reader = csv . reader ( linesource , * * dialect _ options ) <nl> - for rownum , row in enumerate ( reader ) : <nl> - if len ( row ) ! = len ( columns ) : <nl> - self . printerr ( " Record # % d ( line % d ) has the wrong number of fields " <nl> - " ( % d instead of % d ) . " <nl> - % ( rownum , reader . line _ num , len ( row ) , len ( columns ) ) ) <nl> - return rownum <nl> - if not self . do _ import _ row ( columns , nullval , table _ meta , row ) : <nl> - self . printerr ( " Aborting import at record # % d ( line % d ) . " <nl> - " Previously - inserted values still present . " <nl> - % ( rownum , reader . line _ num ) ) <nl> - return rownum <nl> + from functools import partial <nl> + rownum , error = \ <nl> + async _ insert . insert _ concurrent ( self . session , enumerate ( reader , start = 1 ) , <nl> + partial ( <nl> + self . create _ insert _ statement , <nl> + columns , nullval , <nl> + table _ meta ) ) <nl> + if error : <nl> + self . printerr ( str ( error [ 0 ] ) ) <nl> + self . printerr ( " Aborting import at record # % d . " <nl> + " Previously - inserted values still present . " <nl> + % error [ 1 ] ) <nl> finally : <nl> if do _ close : <nl> linesource . close ( ) <nl> elif self . tty : <nl> print <nl> - return rownum + 1 <nl> + return rownum - 1 <nl> + <nl> + def create _ insert _ statement ( self , columns , nullval , table _ meta , row ) : <nl> + <nl> + if len ( row ) ! = len ( columns ) : <nl> + raise ValueError ( <nl> + " Record has the wrong number of fields ( % d instead of % d ) . " <nl> + % ( len ( row ) , len ( columns ) ) ) <nl> <nl> - def do _ import _ row ( self , columns , nullval , table _ meta , row ) : <nl> rowmap = { } <nl> primary _ key _ columns = [ col . name for col in table _ meta . primary _ key ] <nl> for name , value in zip ( columns , row ) : <nl> @ @ - 1390 , 9 + 1396 , 6 @ @ class Shell ( cmd . Cmd ) : <nl> return False <nl> else : <nl> rowmap [ name ] = ' null ' <nl> - return self . do _ import _ insert ( table _ meta , rowmap ) <nl> - <nl> - def do _ import _ insert ( self , table _ meta , rowmap ) : <nl> # would be nice to be able to use a prepared query here , but in order <nl> # to use that interface , we ' d need to have all the input as native <nl> # values already , reading them from text just like the various <nl> @ @ - 1406 , 7 + 1409 , 8 @ @ class Shell ( cmd . Cmd ) : <nl> ) <nl> if self . debug : <nl> print ' Import using CQL : % s ' % query <nl> - return self . perform _ simple _ statement ( SimpleStatement ( query ) ) <nl> + return SimpleStatement ( query ) <nl> + <nl> <nl> def perform _ csv _ export ( self , ks , cf , columns , fname , opts ) : <nl> dialect _ options = self . csv _ dialect _ defaults . copy ( ) <nl> @ @ - 1460 , 13 + 1464 , 7 @ @ class Shell ( cmd . Cmd ) : <nl> if columns is None : <nl> columns = self . get _ column _ names ( ks , cf ) <nl> columnlist = ' , ' . join ( protect _ names ( columns ) ) <nl> - # this limit is pretty awful . would be better to use row - key - paging , so <nl> - # that the dump could be pretty easily aborted if necessary , but that <nl> - # can be kind of tricky with cql3 . Punt for now , until the real cursor <nl> - # API is added in CASSANDRA - 4415 . <nl> - # https : / / datastax - oss . atlassian . net / browse / PYTHON - 16 <nl> - query = ' SELECT % s FROM % s . % s LIMIT 99999999 ' \ <nl> - % ( columnlist , protect _ name ( ks ) , protect _ name ( cf ) ) <nl> + query = ' SELECT % s FROM % s . % s ' % ( columnlist , protect _ name ( ks ) , protect _ name ( cf ) ) <nl> return self . session . execute ( query ) <nl> <nl> def do _ show ( self , parsed ) : <nl> diff - - git a / pylib / cqlshlib / async _ insert . py b / pylib / cqlshlib / async _ insert . py <nl> new file mode 100644 <nl> index 0000000 . . 145b0d6 <nl> - - - / dev / null <nl> + + + b / pylib / cqlshlib / async _ insert . py <nl> @ @ - 0 , 0 + 1 , 110 @ @ <nl> + # Licensed to the Apache Software Foundation ( ASF ) under one <nl> + # or more contributor license agreements . See the NOTICE file <nl> + # distributed with this work for additional information <nl> + # regarding copyright ownership . The ASF licenses this file <nl> + # to you under the Apache License , Version 2 . 0 ( the <nl> + # " License " ) ; you may not use this file except in compliance <nl> + # with the License . You may obtain a copy of the License at <nl> + # <nl> + # http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + # <nl> + # Unless required by applicable law or agreed to in writing , software <nl> + # distributed under the License is distributed on an " AS IS " BASIS , <nl> + # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> + # See the License for the specific language governing permissions and <nl> + # limitations under the License . <nl> + <nl> + from itertools import count <nl> + from threading import Event , Condition <nl> + import sys <nl> + <nl> + <nl> + class _ CountDownLatch ( object ) : <nl> + def _ _ init _ _ ( self , counter = 1 ) : <nl> + self . _ count = counter <nl> + self . _ lock = Condition ( ) <nl> + <nl> + def count _ down ( self ) : <nl> + with self . _ lock : <nl> + self . _ count - = 1 <nl> + if self . _ count < = 0 : <nl> + self . _ lock . notifyAll ( ) <nl> + <nl> + def await ( self ) : <nl> + with self . _ lock : <nl> + while self . _ count > 0 : <nl> + self . _ lock . wait ( ) <nl> + <nl> + <nl> + class _ ChainedWriter ( object ) : <nl> + <nl> + CONCURRENCY = 100 <nl> + <nl> + def _ _ init _ _ ( self , session , enumerated _ reader , statement _ func ) : <nl> + self . _ sentinel = object ( ) <nl> + self . _ session = session <nl> + self . _ cancellation _ event = Event ( ) <nl> + self . _ first _ error = None <nl> + self . _ num _ finished = count ( start = 1 ) <nl> + self . _ task _ counter = _ CountDownLatch ( self . CONCURRENCY ) <nl> + self . _ enumerated _ reader = enumerated _ reader <nl> + self . _ statement _ func = statement _ func <nl> + <nl> + def insert ( self ) : <nl> + if not self . _ enumerated _ reader : <nl> + return 0 , None <nl> + <nl> + for i in xrange ( self . CONCURRENCY ) : <nl> + self . _ execute _ next ( self . _ sentinel , 0 ) <nl> + <nl> + self . _ task _ counter . await ( ) <nl> + return next ( self . _ num _ finished ) , self . _ first _ error <nl> + <nl> + def _ abort ( self , error , failed _ record ) : <nl> + if not self . _ first _ error : <nl> + self . _ first _ error = error , failed _ record <nl> + self . _ task _ counter . count _ down ( ) <nl> + self . _ cancellation _ event . set ( ) <nl> + <nl> + def _ handle _ error ( self , error , failed _ record ) : <nl> + self . _ abort ( error , failed _ record ) <nl> + <nl> + def _ execute _ next ( self , result , last _ completed _ record ) : <nl> + if self . _ cancellation _ event . is _ set ( ) : <nl> + self . _ task _ counter . count _ down ( ) <nl> + return <nl> + <nl> + if result is not self . _ sentinel : <nl> + finished = next ( self . _ num _ finished ) <nl> + if not finished % 1000 : <nl> + sys . stdout . write ( ' Imported % s rows \ r ' % finished ) <nl> + sys . stdout . flush ( ) <nl> + <nl> + try : <nl> + ( current _ record , row ) = next ( self . _ enumerated _ reader ) <nl> + except StopIteration : <nl> + self . _ task _ counter . count _ down ( ) <nl> + return <nl> + except Exception as exc : <nl> + self . _ abort ( exc , last _ completed _ record ) <nl> + return <nl> + <nl> + if self . _ cancellation _ event . is _ set ( ) : <nl> + self . _ task _ counter . count _ down ( ) <nl> + return <nl> + <nl> + try : <nl> + statement = self . _ statement _ func ( row ) <nl> + future = self . _ session . execute _ async ( statement ) <nl> + future . add _ callbacks ( callback = self . _ execute _ next , <nl> + callback _ args = ( current _ record , ) , <nl> + errback = self . _ handle _ error , <nl> + errback _ args = ( current _ record , ) ) <nl> + except Exception as exc : <nl> + self . _ abort ( exc , current _ record ) <nl> + return <nl> + <nl> + <nl> + def insert _ concurrent ( session , enumerated _ reader , statement _ func ) : <nl> + return _ ChainedWriter ( session , enumerated _ reader , statement _ func ) . insert ( ) <nl> +
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 7cfd8ac . . 649c843 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 12 , 6 + 12 , 8 @ @ <nl> * validate index names for \ w + ( CASSANDRA - 2196 ) <nl> * Fix Cassandra cli to respect timeout if schema does not settle ( CASSANDRA - 2187 ) <nl> * update memtable _ throughput to be a long ( CASSANDRA - 2158 ) <nl> + * fix for cleanup writing old - format data into new - version sstable <nl> + ( CASSANRDRA - 2211 ) <nl> <nl> <nl> 0 . 7 . 2 <nl> diff - - git a / src / java / org / apache / cassandra / db / CompactionManager . java b / src / java / org / apache / cassandra / db / CompactionManager . java <nl> index b730b9c . . 8ded7e9 100644 <nl> - - - a / src / java / org / apache / cassandra / db / CompactionManager . java <nl> + + + b / src / java / org / apache / cassandra / db / CompactionManager . java <nl> @ @ - 41 , 9 + 41 , 7 @ @ import org . slf4j . LoggerFactory ; <nl> import org . apache . cassandra . concurrent . DebuggableThreadPoolExecutor ; <nl> import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . dht . Range ; <nl> - import org . apache . cassandra . io . AbstractCompactedRow ; <nl> - import org . apache . cassandra . io . CompactionIterator ; <nl> - import org . apache . cassandra . io . ICompactionInfo ; <nl> + import org . apache . cassandra . io . * ; <nl> import org . apache . cassandra . io . sstable . * ; <nl> import org . apache . cassandra . io . util . FileUtils ; <nl> import org . apache . cassandra . service . AntiEntropyService ; <nl> @ @ - 119 , 7 + 117 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> Collections . sort ( sstables ) ; <nl> int gcBefore = cfs . isIndex ( ) <nl> ? Integer . MAX _ VALUE <nl> - : ( int ) ( System . currentTimeMillis ( ) / 1000 ) - cfs . metadata . getGcGraceSeconds ( ) ; <nl> + : getDefaultGcBefore ( cfs ) ; <nl> return doCompaction ( cfs , <nl> sstables . subList ( 0 , Math . min ( sstables . size ( ) , maxThreshold ) ) , <nl> gcBefore ) ; <nl> @ @ - 183 , 7 + 181 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> <nl> public void performMajor ( final ColumnFamilyStore cfStore ) throws InterruptedException , ExecutionException <nl> { <nl> - submitMajor ( cfStore , 0 , ( int ) ( System . currentTimeMillis ( ) / 1000 ) - cfStore . metadata . getGcGraceSeconds ( ) ) . get ( ) ; <nl> + submitMajor ( cfStore , 0 , getDefaultGcBefore ( cfStore ) ) . get ( ) ; <nl> } <nl> <nl> public Future < Object > submitMajor ( final ColumnFamilyStore cfStore , final long skip , final int gcBefore ) <nl> @ @ - 256 , 7 + 254 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> } <nl> <nl> ColumnFamilyStore cfs = Table . open ( ksname ) . getColumnFamilyStore ( cfname ) ; <nl> - submitUserDefined ( cfs , descriptors , ( int ) ( System . currentTimeMillis ( ) / 1000 ) - cfs . metadata . getGcGraceSeconds ( ) ) ; <nl> + submitUserDefined ( cfs , descriptors , getDefaultGcBefore ( cfs ) ) ; <nl> } <nl> <nl> private Future < Object > submitUserDefined ( final ColumnFamilyStore cfs , final Collection < Descriptor > dataFiles , final int gcBefore ) <nl> @ @ - 515 , 7 + 513 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> if ( Range . isTokenInRanges ( row . getKey ( ) . token , ranges ) ) <nl> { <nl> writer = maybeCreateWriter ( cfs , compactionFileLocation , expectedBloomFilterSize , writer ) ; <nl> - writer . append ( new EchoedRow ( row ) ) ; <nl> + writer . append ( getCompactedRow ( row , cfs , sstable . descriptor ) ) ; <nl> totalkeysWritten + + ; <nl> } <nl> else <nl> @ @ - 568 , 6 + 566 , 21 @ @ public class CompactionManager implements CompactionManagerMBean <nl> } <nl> } <nl> <nl> + / * * <nl> + * @ return an AbstractCompactedRow implementation to write the row in question . <nl> + * If the data is from a current - version sstable , write it unchanged . Otherwise , <nl> + * re - serialize it in the latest version . <nl> + * / <nl> + private AbstractCompactedRow getCompactedRow ( SSTableIdentityIterator row , ColumnFamilyStore cfs , Descriptor descriptor ) <nl> + { <nl> + if ( descriptor . isLatestVersion ) <nl> + return new EchoedRow ( row ) ; <nl> + <nl> + return row . dataSize > DatabaseDescriptor . getInMemoryCompactionLimit ( ) <nl> + ? new LazilyCompactedRow ( cfs , Arrays . asList ( row ) , false , getDefaultGcBefore ( cfs ) ) <nl> + : new PrecompactedRow ( cfs , Arrays . asList ( row ) , false , getDefaultGcBefore ( cfs ) ) ; <nl> + } <nl> + <nl> private SSTableWriter maybeCreateWriter ( ColumnFamilyStore cfs , String compactionFileLocation , int expectedBloomFilterSize , SSTableWriter writer ) <nl> throws IOException <nl> { <nl> @ @ - 752 , 11 + 765 , 16 @ @ public class CompactionManager implements CompactionManagerMBean <nl> return executor . submit ( runnable ) ; <nl> } <nl> <nl> + private static int getDefaultGcBefore ( ColumnFamilyStore cfs ) <nl> + { <nl> + return ( int ) ( System . currentTimeMillis ( ) / 1000 ) - cfs . metadata . getGcGraceSeconds ( ) ; <nl> + } <nl> + <nl> private static class ValidationCompactionIterator extends CompactionIterator <nl> { <nl> public ValidationCompactionIterator ( ColumnFamilyStore cfs ) throws IOException <nl> { <nl> - super ( cfs , cfs . getSSTables ( ) , ( int ) ( System . currentTimeMillis ( ) / 1000 ) - cfs . metadata . getGcGraceSeconds ( ) , true ) ; <nl> + super ( cfs , cfs . getSSTables ( ) , getDefaultGcBefore ( cfs ) , true ) ; <nl> } <nl> <nl> @ Override

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index d29a2db . . 6facab9 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 2 . 1 . 1 
 + * ( cqlsh ) COPY TO / FROM improvements ( CASSANDRA - 7405 ) 
 * Support list index operations with conditions ( CASSANDRA - 7499 ) 
 * Add max live / tombstoned cells to nodetool cfstats output ( CASSANDRA - 7731 ) 
 * Validate IPv6 wildcard addresses properly ( CASSANDRA - 7680 ) 
 diff - - git a / bin / cqlsh b / bin / cqlsh 
 index b65ae00 . . 852f4f5 100755 
 - - - a / bin / cqlsh 
 + + + b / bin / cqlsh 
 @ @ - 118 , 7 + 118 , 7 @ @ cqlshlibdir = os . path . join ( CASSANDRA _ PATH , ' pylib ' ) 
 if os . path . isdir ( cqlshlibdir ) : 
 sys . path . insert ( 0 , cqlshlibdir ) 
 
 - from cqlshlib import cqlhandling , cql3handling , pylexotron , sslhandling 
 + from cqlshlib import cqlhandling , cql3handling , pylexotron , sslhandling , async _ insert 
 from cqlshlib . displaying import ( RED , BLUE , CYAN , ANSI _ RESET , COLUMN _ NAME _ COLORS , 
 FormattedValue , colorme ) 
 from cqlshlib . formatting import format _ by _ type , formatter _ for , format _ value _ utype 
 @ @ - 1348 , 27 + 1348 , 33 @ @ class Shell ( cmd . Cmd ) : 
 if header : 
 linesource . next ( ) 
 table _ meta = self . get _ table _ meta ( ks , cf ) 
 - rownum = - 1 
 reader = csv . reader ( linesource , * * dialect _ options ) 
 - for rownum , row in enumerate ( reader ) : 
 - if len ( row ) ! = len ( columns ) : 
 - self . printerr ( " Record # % d ( line % d ) has the wrong number of fields " 
 - " ( % d instead of % d ) . " 
 - % ( rownum , reader . line _ num , len ( row ) , len ( columns ) ) ) 
 - return rownum 
 - if not self . do _ import _ row ( columns , nullval , table _ meta , row ) : 
 - self . printerr ( " Aborting import at record # % d ( line % d ) . " 
 - " Previously - inserted values still present . " 
 - % ( rownum , reader . line _ num ) ) 
 - return rownum 
 + from functools import partial 
 + rownum , error = \ 
 + async _ insert . insert _ concurrent ( self . session , enumerate ( reader , start = 1 ) , 
 + partial ( 
 + self . create _ insert _ statement , 
 + columns , nullval , 
 + table _ meta ) ) 
 + if error : 
 + self . printerr ( str ( error [ 0 ] ) ) 
 + self . printerr ( " Aborting import at record # % d . " 
 + " Previously - inserted values still present . " 
 + % error [ 1 ] ) 
 finally : 
 if do _ close : 
 linesource . close ( ) 
 elif self . tty : 
 print 
 - return rownum + 1 
 + return rownum - 1 
 + 
 + def create _ insert _ statement ( self , columns , nullval , table _ meta , row ) : 
 + 
 + if len ( row ) ! = len ( columns ) : 
 + raise ValueError ( 
 + " Record has the wrong number of fields ( % d instead of % d ) . " 
 + % ( len ( row ) , len ( columns ) ) ) 
 
 - def do _ import _ row ( self , columns , nullval , table _ meta , row ) : 
 rowmap = { } 
 primary _ key _ columns = [ col . name for col in table _ meta . primary _ key ] 
 for name , value in zip ( columns , row ) : 
 @ @ - 1390 , 9 + 1396 , 6 @ @ class Shell ( cmd . Cmd ) : 
 return False 
 else : 
 rowmap [ name ] = ' null ' 
 - return self . do _ import _ insert ( table _ meta , rowmap ) 
 - 
 - def do _ import _ insert ( self , table _ meta , rowmap ) : 
 # would be nice to be able to use a prepared query here , but in order 
 # to use that interface , we ' d need to have all the input as native 
 # values already , reading them from text just like the various 
 @ @ - 1406 , 7 + 1409 , 8 @ @ class Shell ( cmd . Cmd ) : 
 ) 
 if self . debug : 
 print ' Import using CQL : % s ' % query 
 - return self . perform _ simple _ statement ( SimpleStatement ( query ) ) 
 + return SimpleStatement ( query ) 
 + 
 
 def perform _ csv _ export ( self , ks , cf , columns , fname , opts ) : 
 dialect _ options = self . csv _ dialect _ defaults . copy ( ) 
 @ @ - 1460 , 13 + 1464 , 7 @ @ class Shell ( cmd . Cmd ) : 
 if columns is None : 
 columns = self . get _ column _ names ( ks , cf ) 
 columnlist = ' , ' . join ( protect _ names ( columns ) ) 
 - # this limit is pretty awful . would be better to use row - key - paging , so 
 - # that the dump could be pretty easily aborted if necessary , but that 
 - # can be kind of tricky with cql3 . Punt for now , until the real cursor 
 - # API is added in CASSANDRA - 4415 . 
 - # https : / / datastax - oss . atlassian . net / browse / PYTHON - 16 
 - query = ' SELECT % s FROM % s . % s LIMIT 99999999 ' \ 
 - % ( columnlist , protect _ name ( ks ) , protect _ name ( cf ) ) 
 + query = ' SELECT % s FROM % s . % s ' % ( columnlist , protect _ name ( ks ) , protect _ name ( cf ) ) 
 return self . session . execute ( query ) 
 
 def do _ show ( self , parsed ) : 
 diff - - git a / pylib / cqlshlib / async _ insert . py b / pylib / cqlshlib / async _ insert . py 
 new file mode 100644 
 index 0000000 . . 145b0d6 
 - - - / dev / null 
 + + + b / pylib / cqlshlib / async _ insert . py 
 @ @ - 0 , 0 + 1 , 110 @ @ 
 + # Licensed to the Apache Software Foundation ( ASF ) under one 
 + # or more contributor license agreements . See the NOTICE file 
 + # distributed with this work for additional information 
 + # regarding copyright ownership . The ASF licenses this file 
 + # to you under the Apache License , Version 2 . 0 ( the 
 + # " License " ) ; you may not use this file except in compliance 
 + # with the License . You may obtain a copy of the License at 
 + # 
 + # http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + # 
 + # Unless required by applicable law or agreed to in writing , software 
 + # distributed under the License is distributed on an " AS IS " BASIS , 
 + # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 + # See the License for the specific language governing permissions and 
 + # limitations under the License . 
 + 
 + from itertools import count 
 + from threading import Event , Condition 
 + import sys 
 + 
 + 
 + class _ CountDownLatch ( object ) : 
 + def _ _ init _ _ ( self , counter = 1 ) : 
 + self . _ count = counter 
 + self . _ lock = Condition ( ) 
 + 
 + def count _ down ( self ) : 
 + with self . _ lock : 
 + self . _ count - = 1 
 + if self . _ count < = 0 : 
 + self . _ lock . notifyAll ( ) 
 + 
 + def await ( self ) : 
 + with self . _ lock : 
 + while self . _ count > 0 : 
 + self . _ lock . wait ( ) 
 + 
 + 
 + class _ ChainedWriter ( object ) : 
 + 
 + CONCURRENCY = 100 
 + 
 + def _ _ init _ _ ( self , session , enumerated _ reader , statement _ func ) : 
 + self . _ sentinel = object ( ) 
 + self . _ session = session 
 + self . _ cancellation _ event = Event ( ) 
 + self . _ first _ error = None 
 + self . _ num _ finished = count ( start = 1 ) 
 + self . _ task _ counter = _ CountDownLatch ( self . CONCURRENCY ) 
 + self . _ enumerated _ reader = enumerated _ reader 
 + self . _ statement _ func = statement _ func 
 + 
 + def insert ( self ) : 
 + if not self . _ enumerated _ reader : 
 + return 0 , None 
 + 
 + for i in xrange ( self . CONCURRENCY ) : 
 + self . _ execute _ next ( self . _ sentinel , 0 ) 
 + 
 + self . _ task _ counter . await ( ) 
 + return next ( self . _ num _ finished ) , self . _ first _ error 
 + 
 + def _ abort ( self , error , failed _ record ) : 
 + if not self . _ first _ error : 
 + self . _ first _ error = error , failed _ record 
 + self . _ task _ counter . count _ down ( ) 
 + self . _ cancellation _ event . set ( ) 
 + 
 + def _ handle _ error ( self , error , failed _ record ) : 
 + self . _ abort ( error , failed _ record ) 
 + 
 + def _ execute _ next ( self , result , last _ completed _ record ) : 
 + if self . _ cancellation _ event . is _ set ( ) : 
 + self . _ task _ counter . count _ down ( ) 
 + return 
 + 
 + if result is not self . _ sentinel : 
 + finished = next ( self . _ num _ finished ) 
 + if not finished % 1000 : 
 + sys . stdout . write ( ' Imported % s rows \ r ' % finished ) 
 + sys . stdout . flush ( ) 
 + 
 + try : 
 + ( current _ record , row ) = next ( self . _ enumerated _ reader ) 
 + except StopIteration : 
 + self . _ task _ counter . count _ down ( ) 
 + return 
 + except Exception as exc : 
 + self . _ abort ( exc , last _ completed _ record ) 
 + return 
 + 
 + if self . _ cancellation _ event . is _ set ( ) : 
 + self . _ task _ counter . count _ down ( ) 
 + return 
 + 
 + try : 
 + statement = self . _ statement _ func ( row ) 
 + future = self . _ session . execute _ async ( statement ) 
 + future . add _ callbacks ( callback = self . _ execute _ next , 
 + callback _ args = ( current _ record , ) , 
 + errback = self . _ handle _ error , 
 + errback _ args = ( current _ record , ) ) 
 + except Exception as exc : 
 + self . _ abort ( exc , current _ record ) 
 + return 
 + 
 + 
 + def insert _ concurrent ( session , enumerated _ reader , statement _ func ) : 
 + return _ ChainedWriter ( session , enumerated _ reader , statement _ func ) . insert ( ) 
 +

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 7cfd8ac . . 649c843 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 12 , 6 + 12 , 8 @ @ 
 * validate index names for \ w + ( CASSANDRA - 2196 ) 
 * Fix Cassandra cli to respect timeout if schema does not settle ( CASSANDRA - 2187 ) 
 * update memtable _ throughput to be a long ( CASSANDRA - 2158 ) 
 + * fix for cleanup writing old - format data into new - version sstable 
 + ( CASSANRDRA - 2211 ) 
 
 
 0 . 7 . 2 
 diff - - git a / src / java / org / apache / cassandra / db / CompactionManager . java b / src / java / org / apache / cassandra / db / CompactionManager . java 
 index b730b9c . . 8ded7e9 100644 
 - - - a / src / java / org / apache / cassandra / db / CompactionManager . java 
 + + + b / src / java / org / apache / cassandra / db / CompactionManager . java 
 @ @ - 41 , 9 + 41 , 7 @ @ import org . slf4j . LoggerFactory ; 
 import org . apache . cassandra . concurrent . DebuggableThreadPoolExecutor ; 
 import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . dht . Range ; 
 - import org . apache . cassandra . io . AbstractCompactedRow ; 
 - import org . apache . cassandra . io . CompactionIterator ; 
 - import org . apache . cassandra . io . ICompactionInfo ; 
 + import org . apache . cassandra . io . * ; 
 import org . apache . cassandra . io . sstable . * ; 
 import org . apache . cassandra . io . util . FileUtils ; 
 import org . apache . cassandra . service . AntiEntropyService ; 
 @ @ - 119 , 7 + 117 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 Collections . sort ( sstables ) ; 
 int gcBefore = cfs . isIndex ( ) 
 ? Integer . MAX _ VALUE 
 - : ( int ) ( System . currentTimeMillis ( ) / 1000 ) - cfs . metadata . getGcGraceSeconds ( ) ; 
 + : getDefaultGcBefore ( cfs ) ; 
 return doCompaction ( cfs , 
 sstables . subList ( 0 , Math . min ( sstables . size ( ) , maxThreshold ) ) , 
 gcBefore ) ; 
 @ @ - 183 , 7 + 181 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 
 public void performMajor ( final ColumnFamilyStore cfStore ) throws InterruptedException , ExecutionException 
 { 
 - submitMajor ( cfStore , 0 , ( int ) ( System . currentTimeMillis ( ) / 1000 ) - cfStore . metadata . getGcGraceSeconds ( ) ) . get ( ) ; 
 + submitMajor ( cfStore , 0 , getDefaultGcBefore ( cfStore ) ) . get ( ) ; 
 } 
 
 public Future < Object > submitMajor ( final ColumnFamilyStore cfStore , final long skip , final int gcBefore ) 
 @ @ - 256 , 7 + 254 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 } 
 
 ColumnFamilyStore cfs = Table . open ( ksname ) . getColumnFamilyStore ( cfname ) ; 
 - submitUserDefined ( cfs , descriptors , ( int ) ( System . currentTimeMillis ( ) / 1000 ) - cfs . metadata . getGcGraceSeconds ( ) ) ; 
 + submitUserDefined ( cfs , descriptors , getDefaultGcBefore ( cfs ) ) ; 
 } 
 
 private Future < Object > submitUserDefined ( final ColumnFamilyStore cfs , final Collection < Descriptor > dataFiles , final int gcBefore ) 
 @ @ - 515 , 7 + 513 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 if ( Range . isTokenInRanges ( row . getKey ( ) . token , ranges ) ) 
 { 
 writer = maybeCreateWriter ( cfs , compactionFileLocation , expectedBloomFilterSize , writer ) ; 
 - writer . append ( new EchoedRow ( row ) ) ; 
 + writer . append ( getCompactedRow ( row , cfs , sstable . descriptor ) ) ; 
 totalkeysWritten + + ; 
 } 
 else 
 @ @ - 568 , 6 + 566 , 21 @ @ public class CompactionManager implements CompactionManagerMBean 
 } 
 } 
 
 + / * * 
 + * @ return an AbstractCompactedRow implementation to write the row in question . 
 + * If the data is from a current - version sstable , write it unchanged . Otherwise , 
 + * re - serialize it in the latest version . 
 + * / 
 + private AbstractCompactedRow getCompactedRow ( SSTableIdentityIterator row , ColumnFamilyStore cfs , Descriptor descriptor ) 
 + { 
 + if ( descriptor . isLatestVersion ) 
 + return new EchoedRow ( row ) ; 
 + 
 + return row . dataSize > DatabaseDescriptor . getInMemoryCompactionLimit ( ) 
 + ? new LazilyCompactedRow ( cfs , Arrays . asList ( row ) , false , getDefaultGcBefore ( cfs ) ) 
 + : new PrecompactedRow ( cfs , Arrays . asList ( row ) , false , getDefaultGcBefore ( cfs ) ) ; 
 + } 
 + 
 private SSTableWriter maybeCreateWriter ( ColumnFamilyStore cfs , String compactionFileLocation , int expectedBloomFilterSize , SSTableWriter writer ) 
 throws IOException 
 { 
 @ @ - 752 , 11 + 765 , 16 @ @ public class CompactionManager implements CompactionManagerMBean 
 return executor . submit ( runnable ) ; 
 } 
 
 + private static int getDefaultGcBefore ( ColumnFamilyStore cfs ) 
 + { 
 + return ( int ) ( System . currentTimeMillis ( ) / 1000 ) - cfs . metadata . getGcGraceSeconds ( ) ; 
 + } 
 + 
 private static class ValidationCompactionIterator extends CompactionIterator 
 { 
 public ValidationCompactionIterator ( ColumnFamilyStore cfs ) throws IOException 
 { 
 - super ( cfs , cfs . getSSTables ( ) , ( int ) ( System . currentTimeMillis ( ) / 1000 ) - cfs . metadata . getGcGraceSeconds ( ) , true ) ; 
 + super ( cfs , cfs . getSSTables ( ) , getDefaultGcBefore ( cfs ) , true ) ; 
 } 
 
 @ Override
