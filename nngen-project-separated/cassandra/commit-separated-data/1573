BLEU SCORE: 0.016932492841722675

TEST MSG: Make sure RangeTombstone . Tracker only keeps the ranges it needs to
GENERATED MSG: add wordcount hadoop example

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index c555a91 . . 16ce060 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 2 . 0 . 16 : <nl> + * Don ' t accumulate more range than necessary in RangeTombstone . Tracker ( CASSANDRA - 9486 ) <nl> * Add broadcast and rpc addresses to system . local ( CASSANDRA - 9436 ) <nl> * Always mark sstable suspect when corrupted ( CASSANDRA - 9478 ) <nl> * Add database users and permissions to CQL3 documentation ( CASSANDRA - 7558 ) <nl> diff - - git a / src / java / org / apache / cassandra / db / DeletionTime . java b / src / java / org / apache / cassandra / db / DeletionTime . java <nl> index dd2ccaf . . b39d681 100644 <nl> - - - a / src / java / org / apache / cassandra / db / DeletionTime . java <nl> + + + b / src / java / org / apache / cassandra / db / DeletionTime . java <nl> @ @ - 114 , 6 + 114 , 11 @ @ public class DeletionTime implements Comparable < DeletionTime > <nl> return column . timestamp ( ) < = markedForDeleteAt ; <nl> } <nl> <nl> + public boolean supersedes ( DeletionTime dt ) <nl> + { <nl> + return this . markedForDeleteAt > dt . markedForDeleteAt ; <nl> + } <nl> + <nl> public long memorySize ( ) <nl> { <nl> long fields = TypeSizes . NATIVE . sizeof ( markedForDeleteAt ) + TypeSizes . NATIVE . sizeof ( localDeletionTime ) ; <nl> diff - - git a / src / java / org / apache / cassandra / db / RangeTombstone . java b / src / java / org / apache / cassandra / db / RangeTombstone . java <nl> index 16fc27a . . fe9da20 100644 <nl> - - - a / src / java / org / apache / cassandra / db / RangeTombstone . java <nl> + + + b / src / java / org / apache / cassandra / db / RangeTombstone . java <nl> @ @ - 114 , 52 + 114 , 73 @ @ public class RangeTombstone extends Interval < ByteBuffer , DeletionTime > implement <nl> return comparator . compare ( min , rt . min ) < = 0 & & comparator . compare ( max , rt . max ) > = 0 ; <nl> } <nl> <nl> + / * * <nl> + * Tracks opened RangeTombstones when iterating over a partition . <nl> + * < p > <nl> + * This tracker must be provided all the atoms of a given partition in <nl> + * order ( to the { @ code update } method ) . Given this , it keeps enough <nl> + * information to be able to decide if one of an atom is deleted ( shadowed ) <nl> + * by a previously open RT . One the tracker can prove a given range <nl> + * tombstone cannot be useful anymore ( that is , as soon as we ' ve seen an <nl> + * atom that is after the end of that RT ) , it discards this RT . In other <nl> + * words , the maximum memory used by this object should be proportional to <nl> + * the maximum number of RT that can be simultaneously open ( and this <nl> + * should fairly low in practice ) . <nl> + * / <nl> public static class Tracker <nl> { <nl> private final Comparator < ByteBuffer > comparator ; <nl> - private final Deque < RangeTombstone > ranges = new ArrayDeque < RangeTombstone > ( ) ; <nl> - private final SortedSet < RangeTombstone > maxOrderingSet = new TreeSet < RangeTombstone > ( new Comparator < RangeTombstone > ( ) <nl> - { <nl> - public int compare ( RangeTombstone t1 , RangeTombstone t2 ) <nl> - { <nl> - return comparator . compare ( t1 . max , t2 . max ) ; <nl> - } <nl> - } ) ; <nl> - public final Set < RangeTombstone > expired = new HashSet < RangeTombstone > ( ) ; <nl> + <nl> + / / A list the currently open RTs . We keep the list sorted in order of growing end bounds as for a <nl> + / / new atom , this allows to efficiently find the RTs that are now useless ( if any ) . Also note that because <nl> + / / atom are passed to the tracker in order , any RT that is tracked can be assumed as opened , i . e . we <nl> + / / never have to test the RTs start since it ' s always assumed to be less than what we have . <nl> + / / Also note that this will store expired RTs ( # 7810 ) . Those will be of type ExpiredRangeTombstone and <nl> + / / will be ignored by writeOpenedMarker . <nl> + private final List < RangeTombstone > openedTombstones = new LinkedList < RangeTombstone > ( ) ; <nl> + <nl> + / / Total number of atoms written by writeOpenedMarker ( ) . <nl> private int atomCount ; <nl> <nl> + / * * <nl> + * Creates a new tracker given the table comparator . <nl> + * <nl> + * @ param comparator the comparator for the table this will track atoms <nl> + * for . The tracker assumes that atoms will be later provided to the <nl> + * tracker in { @ code comparator } order . <nl> + * / <nl> public Tracker ( Comparator < ByteBuffer > comparator ) <nl> { <nl> this . comparator = comparator ; <nl> } <nl> <nl> / * * <nl> - * Compute RangeTombstone that are needed at the beginning of an index <nl> + * Computes the RangeTombstone that are needed at the beginning of an index <nl> * block starting with { @ code firstColumn } . <nl> - * Returns the total serialized size of said tombstones and write them <nl> - * to { @ code out } it if isn ' t null . <nl> + * <nl> + * @ return the total serialized size of said tombstones and write them to <nl> + * { @ code out } it if isn ' t null . <nl> * / <nl> public long writeOpenedMarker ( OnDiskAtom firstColumn , DataOutput out , OnDiskAtom . Serializer atomSerializer ) throws IOException <nl> { <nl> long size = 0 ; <nl> - if ( ranges . isEmpty ( ) ) <nl> + if ( openedTombstones . isEmpty ( ) ) <nl> return size ; <nl> <nl> / * <nl> - * Compute the marker that needs to be written at the beginning of <nl> - * this block . We need to write one if it the more recent <nl> + * Compute the markers that needs to be written at the beginning of <nl> + * this block . We need to write one if it is the more recent <nl> * ( opened ) tombstone for at least some part of its range . <nl> * / <nl> List < RangeTombstone > toWrite = new LinkedList < RangeTombstone > ( ) ; <nl> outer : <nl> - for ( RangeTombstone tombstone : ranges ) <nl> + for ( RangeTombstone tombstone : openedTombstones ) <nl> { <nl> - / / If ever the first column is outside the range , skip it ( in <nl> - / / case update ( ) hasn ' t been called yet ) <nl> + / / If the first column is outside the range , skip it ( in case update ( ) hasn ' t been called yet ) <nl> if ( comparator . compare ( firstColumn . name ( ) , tombstone . max ) > 0 ) <nl> continue ; <nl> <nl> - if ( expired . contains ( tombstone ) ) <nl> + if ( tombstone instanceof ExpiredRangeTombstone ) <nl> continue ; <nl> <nl> RangeTombstone updated = new RangeTombstone ( firstColumn . name ( ) , tombstone . max , tombstone . data ) ; <nl> @ @ - 186 , 6 + 207 , 9 @ @ public class RangeTombstone extends Interval < ByteBuffer , DeletionTime > implement <nl> return size ; <nl> } <nl> <nl> + / * * <nl> + * The total number of atoms written by calls to the method { @ link # writeOpenedMarker } . <nl> + * / <nl> public int writtenAtom ( ) <nl> { <nl> return atomCount ; <nl> @ @ - 193 , 69 + 217 , 129 @ @ public class RangeTombstone extends Interval < ByteBuffer , DeletionTime > implement <nl> <nl> / * * <nl> * Update this tracker given an { @ code atom } . <nl> - * If column is a Column , check if any tracked range is useless and <nl> - * can be removed . If it is a RangeTombstone , add it to this tracker . <nl> + * < p > <nl> + * This method first test if some range tombstone can be discarded due <nl> + * to the knowledge of that new atom . Then , if it ' s a range tombstone , <nl> + * it adds it to the tracker . <nl> + * < p > <nl> + * Note that this method should be called on * every * atom of a partition for <nl> + * the tracker to work as efficiently as possible
NEAREST DIFF (one line): diff - - git a / contrib / word _ count / README . txt b / contrib / word _ count / README . txt <nl> new file mode 100644 <nl> index 0000000 . . f46e2e2 <nl> - - - / dev / null <nl> + + + b / contrib / word _ count / README . txt <nl> @ @ - 0 , 0 + 1 , 18 @ @ <nl> + WordCount hadoop example : Inserts a bunch of words across multiple rows , <nl> + and counts them , with RandomPartitioner . <nl> + <nl> + The scripts in bin / assume you are running with cwd of contrib / word _ count . <nl> + <nl> + First build and start a Cassandra server with the default configuration * , <nl> + then run <nl> + <nl> + contrib / word _ count $ ant <nl> + contrib / word _ count $ bin / word _ count _ setup <nl> + contrib / word _ count $ bin / word _ count <nl> + <nl> + Output will be in / tmp / word _ count * . <nl> + <nl> + Read the code in src / for more details . <nl> + <nl> + * If you want to point wordcount at a real cluster , modify the seed <nl> + and listenaddress settings in storage - conf . xml accordingly . <nl> diff - - git a / contrib / word _ count / bin / word _ count b / contrib / word _ count / bin / word _ count <nl> new file mode 100644 <nl> index 0000000 . . a4eafc6 <nl> - - - / dev / null <nl> + + + b / contrib / word _ count / bin / word _ count <nl> @ @ - 0 , 0 + 1 , 53 @ @ <nl> + # ! / bin / sh <nl> + <nl> + # Licensed to the Apache Software Foundation ( ASF ) under one <nl> + # or more contributor license agreements . See the NOTICE file <nl> + # distributed with this work for additional information <nl> + # regarding copyright ownership . The ASF licenses this file <nl> + # to you under the Apache License , Version 2 . 0 ( the <nl> + # " License " ) ; you may not use this file except in compliance <nl> + # with the License . You may obtain a copy of the License at <nl> + # <nl> + # http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + # <nl> + # Unless required by applicable law or agreed to in writing , software <nl> + # distributed under the License is distributed on an " AS IS " BASIS , <nl> + # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> + # See the License for the specific language governing permissions and <nl> + # limitations under the License . <nl> + <nl> + cwd = ` dirname $ 0 ` <nl> + <nl> + # Cassandra class files . <nl> + if [ ! - d $ cwd / . . / . . / . . / build / classes ] ; then <nl> + echo " Unable to locate cassandra class files " > & 2 <nl> + exit 1 <nl> + fi <nl> + <nl> + # word _ count Jar . <nl> + if [ ! - e $ cwd / . . / build / * . jar ] ; then <nl> + echo " Unable to locate word _ count jar " > & 2 <nl> + exit 1 <nl> + fi <nl> + <nl> + CLASSPATH = $ CLASSPATH : ` ls - 1 $ cwd / . . / build / * . jar ` <nl> + CLASSPATH = $ CLASSPATH : . : $ cwd / . . / . . / . . / build / classes <nl> + for jar in $ cwd / . . / . . / . . / lib / * . jar ; do <nl> + CLASSPATH = $ CLASSPATH : $ jar <nl> + done <nl> + for jar in $ cwd / . . / . . / . . / build / lib / jars / * . jar ; do <nl> + CLASSPATH = $ CLASSPATH : $ jar <nl> + done <nl> + <nl> + if [ - x $ JAVA _ HOME / bin / java ] ; then <nl> + JAVA = $ JAVA _ HOME / bin / java <nl> + else <nl> + JAVA = ` which java ` <nl> + fi <nl> + <nl> + if [ " x $ JAVA " = " x " ] ; then <nl> + echo " Java executable not found ( hint : set JAVA _ HOME ) " > & 2 <nl> + exit 1 <nl> + fi <nl> + <nl> + $ JAVA - Xmx1G - ea - cp $ CLASSPATH WordCount <nl> diff - - git a / contrib / word _ count / bin / word _ count _ setup b / contrib / word _ count / bin / word _ count _ setup <nl> new file mode 100644 <nl> index 0000000 . . 9af6562 <nl> - - - / dev / null <nl> + + + b / contrib / word _ count / bin / word _ count _ setup <nl> @ @ - 0 , 0 + 1 , 53 @ @ <nl> + # ! / bin / sh <nl> + <nl> + # Licensed to the Apache Software Foundation ( ASF ) under one <nl> + # or more contributor license agreements . See the NOTICE file <nl> + # distributed with this work for additional information <nl> + # regarding copyright ownership . The ASF licenses this file <nl> + # to you under the Apache License , Version 2 . 0 ( the <nl> + # " License " ) ; you may not use this file except in compliance <nl> + # with the License . You may obtain a copy of the License at <nl> + # <nl> + # http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + # <nl> + # Unless required by applicable law or agreed to in writing , software <nl> + # distributed under the License is distributed on an " AS IS " BASIS , <nl> + # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> + # See the License for the specific language governing permissions and <nl> + # limitations under the License . <nl> + <nl> + cwd = ` dirname $ 0 ` <nl> + <nl> + # Cassandra class files . <nl> + if [ ! - d $ cwd / . . / . . / . . / build / classes ] ; then <nl> + echo " Unable to locate cassandra class files " > & 2 <nl> + exit 1 <nl> + fi <nl> + <nl> + # word _ count Jar . <nl> + if [ ! - e $ cwd / . . / build / * . jar ] ; then <nl> + echo " Unable to locate word _ count jar " > & 2 <nl> + exit 1 <nl> + fi <nl> + <nl> + CLASSPATH = $ CLASSPATH : ` ls - 1 $ cwd / . . / build / * . jar ` <nl> + CLASSPATH = $ CLASSPATH : . : $ cwd / . . / . . / . . / build / classes <nl> + for jar in $ cwd / . . / . . / . . / lib / * . jar ; do <nl> + CLASSPATH = $ CLASSPATH : $ jar <nl> + done <nl> + for jar in $ cwd / . . / . . / . . / build / lib / jars / * . jar ; do <nl> + CLASSPATH = $ CLASSPATH : $ jar <nl> + done <nl> + <nl> + if [ - x $ JAVA _ HOME / bin / java ] ; then <nl> + JAVA = $ JAVA _ HOME / bin / java <nl> + else <nl> + JAVA = ` which java ` <nl> + fi <nl> + <nl> + if [ " x $ JAVA " = " x " ] ; then <nl> + echo " Java executable not found ( hint : set JAVA _ HOME ) " > & 2 <nl> + exit 1 <nl> + fi <nl> + <nl> + $ JAVA - Xmx1G - ea - cp $ CLASSPATH WordCountSetup <nl> diff - - git a / contrib / word _ count / build . xml b / contrib / word _ count / build . xml <nl> new file mode 100644 <nl> index 0000000 . . e80dd10 <nl> - - - / dev / null <nl> + + + b / contrib / word _ count / build . xml <nl> @ @ - 0 , 0 + 1 , 65 @ @ <nl> + < ? xml version = " 1 . 0 " encoding = " UTF - 8 " ? > <nl> + < ! - - <nl> + ~ Licensed to the Apache Software Foundation ( ASF ) under one <nl> + ~ or more contributor license agreements . See the NOTICE file <nl> + ~ distributed with this work for additional information <nl> + ~ regarding copyright ownership . The ASF licenses this file <nl> + ~ to you under the Apache License , Version 2 . 0 ( the <nl> + ~ " License " ) ; you may not use this file except in compliance <nl> + ~ with the License . You may obtain a copy of the License at <nl> + ~ <nl> + ~ http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + ~ <nl> + ~ Unless required by applicable law or agreed to in writing , <nl> + ~ software distributed under the License is distributed on an <nl> + ~ " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY <nl> + ~ KIND , either express or implied . See the License for the <nl> + ~ specific language governing permissions and limitations <nl> + ~ under the License . <nl> + - - > <nl> + < project basedir = " . " default = " jar " name = " word _ count " > <nl> + < property name = " cassandra . dir " value = " . . / . . " / > <nl> + < property name = " cassandra . lib " value = " " / > <nl> + < property name = " cassandra . classes " value = " $ { cassandra . dir } / build / classes " / > <nl> + < property name = " build . src " value = " $ { basedir } / src " / > <nl> + < property name = " build . out " value = " $ { basedir } / build " / > <nl> + < property name = " build . classes " value = " $ { build . out } / classes " / > <nl> + < property name = " final . name " value = " word _ count " / > <nl> + <nl> + < target name = " init " > <nl> + < mkdir dir = " $ { build . classes } " / > <nl> + < / target > <nl> + <nl> + < target depends = " init " name = " build " > <nl> + < javac destdir = " $ { build . classes } " > <nl> + < src path = " $ { build . src } " / > <nl> + < classpath > <nl> + < path > <nl> + < fileset dir = " $ { cassandra . dir } / lib " > <nl> + < include name = " * * / * . jar " / > <nl> + < / fileset > <nl> + < fileset dir = " $ { cassandra . dir } / build / lib / jars " > <nl> + < include name = " * * / * . jar " / > <nl> + < / fileset > <nl> + < pathelement location = " $ { cassandra . classes } " / > <nl> + < / path > <nl> + < / classpath > <nl> + < / javac > <nl> + < / target > <nl> + <nl> + < target name = " jar " depends = " build " > <nl> + < mkdir dir = " $ { build . classes } / META - INF " / > <nl> + < jar jarfile = " $ { build . out } / $ { final . name } . jar " > <nl> + < fileset dir = " $ { build . classes } " / > <nl> + < fileset dir = " $ { cassandra . classes } " / > <nl> + < fileset dir = " $ { cassandra . dir } " > <nl> + < include name = " lib / * * / * . jar " / > <nl> + < / fileset > <nl> + < fileset file = " $ { basedir } / storage - conf . xml " / > <nl> + < / jar > <nl> + < / target > <nl> + <nl> + < target name = " clean " > <nl> + < delete dir = " $ { build . out } " / > <nl> + < / target > <nl> + < / project > <nl> diff - - git a / contrib / word _ count / src / WordCount . java b / contrib / word _ count / src / WordCount . java <nl> new file mode 100644 <nl> index 0000000 . . bcd30f9 <nl> - - - / dev / null <nl> + + + b / contrib / word _ count / src / WordCount . java <nl> @ @ - 0 , 0 + 1 , 107 @ @ <nl> + import java . io . IOException ; <nl> + import java . util . SortedMap ; <nl> + import java . util . StringTokenizer ; <nl> + <nl> + import org . apache . log4j . Logger ; <nl> + <nl> + import org . apache . cassandra . db . IColumn ; <nl> + import org . apache . cassandra . hadoop . ColumnFamilyInputFormat ; <nl> + import org . apache . hadoop . conf . Configuration ; <nl> + import org . apache . hadoop . conf . Configured ; <nl> + import org . apache . hadoop . fs . Path ; <nl> + import org . apache . hadoop . io . IntWritable ; <nl> + import org . apache . hadoop . io . Text ; <nl> + import org . apache . hadoop . mapreduce . Job ; <nl> + import org . apache . hadoop . mapreduce . Mapper ; <nl> + import org . apache . hadoop . mapreduce . Reducer ; <nl> + import org . apache . hadoop . mapreduce . lib . output . FileOutputFormat ; <nl> + import org . apache . hadoop . util . Tool ; <nl> + import org . apache . hadoop . util . ToolRunner ; <nl> + <nl> + / * * <nl> + * This counts the occurrences of words in ColumnFamily Standard1 , that has a single column ( that we care about ) <nl> + * " text " containing a sequence of words . <nl> + * <nl> + * For each word , we output the total number of occurrences across all texts . <nl> + * / <nl> + public class WordCount extends Configured implements Tool <nl> + { <nl> + private static final Logger logger = Logger . getLogger ( WordCount . class ) ; <nl> + <nl> + static final String KEYSPACE = " Keyspace1 " ; <nl> + static final String COLUMN _ FAMILY = " Standard1 " ; <nl> + private static String columnName ; <nl> + private static final String OUTPUT _ PATH _ PREFIX = " / tmp / word _ count " ; <nl> + static final int RING _ DELAY = 3000 ; / / this is enough for testing a single server node ; may need more for a real cluster <nl> + <nl> + public static void main ( String [ ] args ) throws Exception <nl> + { <nl> + / / Let ToolRunner handle generic command - line options <nl> + ToolRunner . run ( new Configuration ( ) , new WordCount ( ) , args ) ; <nl> + System . exit ( 0 ) ; <nl> + } <nl> + <nl> + public static class TokenizerMapper extends Mapper < String , SortedMap < byte [ ] , IColumn > , Text , IntWritable > <nl> + { <nl> + private final static IntWritable one = new IntWritable ( 1 ) ; <nl> + private Text word = new Text ( ) ; <nl> + <nl> + public void map ( String key , SortedMap < byte [ ] , IColumn > columns , Context context ) throws IOException , InterruptedException <nl> + { <nl> + if ( columns = = null ) <nl> + return ; <nl> + IColumn column = columns . get ( columnName . getBytes ( ) ) ; <nl> + String value = new String ( column . value ( ) ) ; <nl> + logger . debug ( " read " + key + " : " + value + " from " + context . getInputSplit ( ) ) ; <nl> + <nl> + StringTokenizer itr = new StringTokenizer ( value ) ; <nl> + while ( itr . hasMoreTokens ( ) ) <nl> + { <nl> + word . set ( itr . nextToken ( ) ) ; <nl> + context . write ( word , one ) ; <nl> + } <nl> + } <nl> + } <nl> + <nl> + public static class IntSumReducer extends Reducer < Text , IntWritable , Text , IntWritable > <nl> + { <nl> + private IntWritable result = new IntWritable ( ) ; <nl> + <nl> + public void reduce ( Text key , Iterable < IntWritable > values , Context context ) throws IOException , InterruptedException <nl> + { <nl> + int sum = 0 ; <nl> + for ( IntWritable val : values ) <nl> + { <nl> + sum + = val . get ( ) ; <nl> + } <nl> + <nl> + result . set ( sum ) ; <nl> + context . write ( key , result ) ; <nl> + } <nl> + } <nl> + <nl> + public int run ( String [ ] args ) throws Exception <nl> + { <nl> + Configuration conf = getConf ( ) ; <nl> + <nl> + for ( int i = 0 ; i < WordCountSetup . TEST _ COUNT ; i + + ) <nl> + { <nl> + columnName = " text " + i ; <nl> + Job job = new Job ( conf , " wordcount " ) ; <nl> + job . setJarByClass ( WordCount . class ) ; <nl> + job . setMapperClass ( TokenizerMapper . class ) ; <nl> + job . setCombinerClass ( IntSumReducer . class ) ; <nl> + job . setReducerClass ( IntSumReducer . class ) ; <nl> + job . setOutputKeyClass ( Text . class ) ; <nl> + job . setOutputValueClass ( IntWritable . class ) ; <nl> + <nl> + job . setInputFormatClass ( ColumnFamilyInputFormat . class ) ; <nl> + FileOutputFormat . setOutputPath ( job , new Path ( OUTPUT _ PATH _ PREFIX + i ) ) ; <nl> + <nl> + ColumnFamilyInputFormat . setColumnFamily ( job , KEYSPACE , COLUMN _ FAMILY ) ; <nl> + <nl> + job . waitForCompletion ( true ) ; <nl> + } <nl> + return 0 ; <nl> + } <nl> + } <nl> \ No newline at end of file <nl> diff - - git a / contrib / word _ count / src / WordCountSetup . java b / contrib / word _ count / src / WordCountSetup . java <nl> new file mode 100644 <nl> index 0000000 . . 74ab87a <nl> - - - / dev / null <nl> + + + b / contrib / word _ count / src / WordCountSetup . java <nl> @ @ - 0 , 0 + 1 , 61 @ @ <nl> + import java . util . Arrays ; <nl> + <nl> + import org . apache . log4j . Logger ; <nl> + <nl> + import org . apache . cassandra . db . * ; <nl> + import org . apache . cassandra . service . StorageProxy ; <nl> + import org . apache . cassandra . service . StorageService ; <nl> + import org . apache . cassandra . thrift . ConsistencyLevel ; <nl> + <nl> + public class WordCountSetup <nl> + { <nl> + private static final Logger logger = Logger . getLogger ( WordCountSetup . class ) ; <nl> + <nl> + public static final int TEST _ COUNT = 4 ; <nl> + <nl> + public static void main ( String [ ] args ) throws Exception <nl> + { <nl> + StorageService . instance . initClient ( ) ; <nl> + logger . info ( " Sleeping " + WordCount . RING _ DELAY ) ; <nl> + Thread . sleep ( WordCount . RING _ DELAY ) ; <nl> + assert ! StorageService . instance . getLiveNodes ( ) . isEmpty ( ) ; <nl> + <nl> + RowMutation rm ; <nl> + ColumnFamily cf ; <nl> + byte [ ] columnName ; <nl> + <nl> + / / text0 : no rows <nl> + <nl> + / / text1 : 1 row , 1 word <nl> + columnName = " text1 " . getBytes ( ) ; <nl> + rm = new RowMutation ( WordCount . KEYSPACE , " Key0 " ) ; <nl> + cf = ColumnFamily . create ( WordCount . KEYSPACE , WordCount . COLUMN _ FAMILY ) ; <nl> + cf . addColumn ( new Column ( columnName , " word1 " . getBytes ( ) , 0 ) ) ; <nl> + rm . add ( cf ) ; <nl> + StorageProxy . mutateBlocking ( Arrays . asList ( rm ) , ConsistencyLevel . ONE ) ; <nl> + logger . info ( " added text1 " ) ; <nl> + <nl> + / / text2 : 1 row , 2 words <nl> + columnName = " text2 " . getBytes ( ) ; <nl> + rm = new RowMutation ( WordCount . KEYSPACE , " Key0 " ) ; <nl> + cf = ColumnFamily . create ( WordCount . KEYSPACE , WordCount . COLUMN _ FAMILY ) ; <nl> + cf . addColumn ( new Column ( columnName , " word1 word2 " . getBytes ( ) , 0 ) ) ; <nl> + rm . add ( cf ) ; <nl> + StorageProxy . mutateBlocking ( Arrays . asList ( rm ) , ConsistencyLevel . ONE ) ; <nl> + logger . info ( " added text2 " ) ; <nl> + <nl> + / / text3 : 1000 rows , 1 word <nl> + columnName = " text3 " . getBytes ( ) ; <nl> + for ( int i = 0 ; i < 1000 ; i + + ) <nl> + { <nl> + rm = new RowMutation ( WordCount . KEYSPACE , " Key " + i ) ; <nl> + cf = ColumnFamily . create ( WordCount . KEYSPACE , WordCount . COLUMN _ FAMILY ) ; <nl> + cf . addColumn ( new Column ( columnName , " word1 " . getBytes ( ) , 0 ) ) ; <nl> + rm . add ( cf ) ; <nl> + StorageProxy . mutateBlocking ( Arrays . asList ( rm ) , ConsistencyLevel . ONE ) ; <nl> + } <nl> + logger . info ( " added text3 " ) ; <nl> + <nl> + System . exit ( 0 ) ; <nl> + } <nl> + } <nl> diff - - git a / contrib / word _ count / storage - conf . xml b / contrib / word _ count / storage - conf . xml <nl> new file mode 100644 <nl> index 0000000 . . 0d591d9 <nl> - - - / dev / null <nl> + + + b / contrib / word _ count / storage - conf . xml <nl> @ @ - 0 , 0 + 1 , 369 @ @ <nl> + < ! - - <nl> + ~ Licensed to the Apache Software Foundation ( ASF ) under one <nl> + ~ or more contributor license agreements . See the NOTICE file <nl> + ~ distributed with this work for additional information <nl> + ~ regarding copyright ownership . The ASF licenses this file <nl> + ~ to you under the Apache License , Version 2 . 0 ( the <nl> + ~ " License " ) ; you may not use this file except in compliance <nl> + ~ with the License . You may obtain a copy of the License at <nl> + ~ <nl> + ~ http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + ~ <nl> + ~ Unless required by applicable law or agreed to in writing , <nl> + ~ software distributed under the License is distributed on an <nl> + ~ " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY <nl> + ~ KIND , either express or implied . See the License for the <nl> + ~ specific language governing permissions and limitations <nl> + ~ under the License . <nl> + - - > <nl> + < Storage > <nl> + < ! - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - > <nl> + < ! - - Basic Configuration - - > <nl> + < ! - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - > <nl> + <nl> + < ! - - <nl> + ~ The name of this cluster . This is mainly used to prevent machines in <nl> + ~ one logical cluster from joining another . <nl> + - - > <nl> + < ClusterName > Test Cluster < / ClusterName > <nl> + <nl> + < ! - - <nl> + ~ Turn on to make new [ non - seed ] nodes automatically migrate the right data <nl> + ~ to themselves . ( If no InitialToken is specified , they will pick one <nl> + ~ such that they will get half the range of the most - loaded node . ) <nl> + ~ If a node starts up without bootstrapping , it will mark itself bootstrapped <nl> + ~ so that you can ' t subsequently accidently bootstrap a node with <nl> + ~ data on it . ( You can reset this by wiping your data and commitlog <nl> + ~ directories . ) <nl> + ~ <nl> + ~ Off by default so that new clusters and upgraders from 0 . 4 don ' t <nl> + ~ bootstrap immediately . You should turn this on when you start adding <nl> + ~ new nodes to a cluster that already has data on it . ( If you are upgrading <nl> + ~ from 0 . 4 , start your cluster with it off once before changing it to true . <nl> + ~ Otherwise , no data will be lost but you will incur a lot of unnecessary <nl> + ~ I / O before your cluster starts up . ) <nl> + - - > <nl> + < AutoBootstrap > false < / AutoBootstrap > <nl> + <nl> + < ! - - <nl> + ~ Keyspaces and ColumnFamilies : <nl> + ~ A ColumnFamily is the Cassandra concept closest to a relational <nl> + ~ table . Keyspaces are separate groups of ColumnFamilies . Except in <nl> + ~ very unusual circumstances you will have one Keyspace per application . <nl> + <nl> + ~ There is an implicit keyspace named ' system ' for Cassandra internals . <nl> + - - > <nl> + < Keyspaces > <nl> + < Keyspace Name = " Keyspace1 " > <nl> + < ! - - <nl> + ~ ColumnFamily definitions have one required attribute ( Name ) <nl> + ~ and several optional ones . <nl> + ~ <nl> + ~ The CompareWith attribute tells Cassandra how to sort the columns <nl> + ~ for slicing operations . The default is BytesType , which is a <nl> + ~ straightforward lexical comparison of the bytes in each column . <nl> + ~ Other options are AsciiType , UTF8Type , LexicalUUIDType , TimeUUIDType , <nl> + ~ and LongType . You can also specify the fully - qualified class <nl> + ~ name to a class of your choice extending <nl> + ~ org . apache . cassandra . db . marshal . AbstractType . <nl> + ~ <nl> + ~ SuperColumns have a similar CompareSubcolumnsWith attribute . <nl> + ~ <nl> + ~ BytesType : Simple sort by byte value . No validation is performed . <nl> + ~ AsciiType : Like BytesType , but validates that the input can be <nl> + ~ parsed as US - ASCII . <nl> + ~ UTF8Type : A string encoded as UTF8 <nl> + ~ LongType : A 64bit long <nl> + ~ LexicalUUIDType : A 128bit UUID , compared lexically ( by byte value ) <nl> + ~ TimeUUIDType : a 128bit version 1 UUID , compared by timestamp <nl> + ~ <nl> + ~ ( To get the closest approximation to 0 . 3 - style supercolumns , you <nl> + ~ would use CompareWith = UTF8Type CompareSubcolumnsWith = LongType . ) <nl> + ~ <nl> + ~ An optional ` Comment ` attribute may be used to attach additional <nl> + ~ human - readable information about the column family to its definition . <nl> + ~ <nl> + ~ The optional KeysCachedFraction attribute specifies <nl> + ~ The fraction of keys per sstable whose locations we keep in <nl> + ~ memory in " mostly LRU " order . ( JUST the key locations , NOT any <nl> + ~ column values . ) The amount of memory used by the default setting of <nl> + ~ 0 . 01 is comparable to the amount used by the internal per - sstable key <nl> + ~ index . Consider increasing this if you have fewer , wider rows . <nl> + ~ Set to 0 to disable entirely . <nl> + ~ <nl> + ~ The optional RowsCached attribute specifies the number of rows <nl> + ~ whose entire contents we cache in memory , either as a fixed number <nl> + ~ of rows or as a percent of rows in the ColumnFamily . <nl> + ~ Do not use this on ColumnFamilies with large rows , or <nl> + ~ ColumnFamilies with high write : read ratios . As with key caching , <nl> + ~ valid values are from 0 to 1 . The default 0 disables it entirely . <nl> + - - > <nl> + < ColumnFamily CompareWith = " BytesType " <nl> + Name = " Standard1 " <nl> + RowsCached = " 10 % " <nl> + KeysCachedFraction = " 0 " / > <nl> + < ColumnFamily CompareWith = " UTF8Type " Name = " Standard2 " / > <nl> + < ColumnFamily CompareWith = " TimeUUIDType " Name = " StandardByUUID1 " / > <nl> + < ColumnFamily ColumnType = " Super " <nl> + CompareWith = " UTF8Type " <nl> + CompareSubcolumnsWith = " UTF8Type " <nl> + Name = " Super1 " <nl> + RowsCached = " 1000 " <nl> + KeysCachedFraction = " 0 " <nl> + Comment = " A column family with supercolumns , whose column and subcolumn names are UTF8 strings " / > <nl> + <nl> + < ! - - <nl> + ~ Strategy : Setting this to the class that implements <nl> + ~ IReplicaPlacementStrategy will change the way the node picker works . <nl> + ~ Out of the box , Cassandra provides <nl> + ~ org . apache . cassandra . locator . RackUnawareStrategy and <nl> + ~ org . apache . cassandra . locator . RackAwareStrategy ( place one replica in <nl> + ~ a different datacenter , and the others on different racks in the same <nl> + ~ one . ) <nl> + - - > <nl> + < ReplicaPlacementStrategy > org . apache . cassandra . locator . RackUnawareStrategy < / ReplicaPlacementStrategy > <nl> + <nl> + < ! - - Number of replicas of the data - - > <nl> + < ReplicationFactor > 1 < / ReplicationFactor > <nl> + <nl> + < ! - - <nl> + ~ EndPointSnitch : Setting this to the class that implements <nl> + ~ AbstractEndpointSnitch , which lets Cassandra know enough <nl> + ~ about your network topology to route requests efficiently . <nl> + ~ Out of the box , Cassandra provides org . apache . cassandra . locator . EndPointSnitch , <nl> + ~ and PropertyFileEndPointSnitch is available in contrib / . <nl> + - - > <nl> + < EndPointSnitch > org . apache . cassandra . locator . EndPointSnitch < / EndPointSnitch > <nl> + < / Keyspace > <nl> + < / Keyspaces > <nl> + <nl> + < ! - - <nl> + ~ Authenticator : any IAuthenticator may be used , including your own as long <nl> + ~ as it is on the classpath . Out of the box , Cassandra provides <nl> + ~ org . apache . cassandra . auth . AllowAllAuthenticator and , <nl> + ~ org . apache . cassandra . auth . SimpleAuthenticator <nl> + ~ ( SimpleAuthenticator uses access . properties and passwd . properties by <nl> + ~ default ) . <nl> + ~ <nl> + ~ If you don ' t specify an authenticator , AllowAllAuthenticator is used . <nl> + - - > <nl> + < Authenticator > org . apache . cassandra . auth . AllowAllAuthenticator < / Authenticator > <nl> + <nl> + < ! - - <nl> + ~ Partitioner : any IPartitioner may be used , including your own as long <nl> + ~ as it is on the classpath . Out of the box , Cassandra provides <nl> + ~ org . apache . cassandra . dht . RandomPartitioner , <nl> + ~ org . apache . cassandra . dht . OrderPreservingPartitioner , and <nl> + ~ org . apache . cassandra . dht . CollatingOrderPreservingPartitioner . <nl> + ~ ( CollatingOPP colates according to EN , US rules , not naive byte <nl> + ~ ordering . Use this as an example if you need locale - aware collation . ) <nl> + ~ Range queries require using an order - preserving partitioner . <nl> + ~ <nl> + ~ Achtung ! Changing this parameter requires wiping your data <nl> + ~ directories , since the partitioner can modify the sstable on - disk <nl> + ~ format . <nl> + - - > <nl> + < Partitioner > org . apache . cassandra . dht . RandomPartitioner < / Partitioner > <nl> + <nl> + < ! - - <nl> + ~ If you are using an order - preserving partitioner and you know your key <nl> + ~ distribution , you can specify the token for this node to use . ( Keys <nl> + ~ are sent to the node with the " closest " token , so distributing your <nl> + ~ tokens equally along the key distribution space will spread keys <nl> + ~ evenly across your cluster . ) This setting is only checked the first <nl> + ~ time a node is started . <nl> + <nl> + ~ This can also be useful with RandomPartitioner to force equal spacing <nl> + ~ of tokens around the hash space , especially for clusters with a small <nl> + ~ number of nodes . <nl> + - - > <nl> + < InitialToken > < / InitialToken > <nl> + <nl> + < ! - - <nl> + ~ Directories : Specify where Cassandra should store different data on <nl> + ~ disk . Keep the data disks and the CommitLog disks separate for best <nl> + ~ performance <nl> + - - > <nl> + < CommitLogDirectory > / var / lib / cassandra / commitlog < / CommitLogDirectory > <nl> + < DataFileDirectories > <nl> + < DataFileDirectory > / var / lib / cassandra / data < / DataFileDirectory > <nl> + < / DataFileDirectories > <nl> + < CalloutLocation > / var / lib / cassandra / callouts < / CalloutLocation > <nl> + < StagingFileDirectory > / var / lib / cassandra / staging < / StagingFileDirectory > <nl> + <nl> + <nl> + < ! - - <nl> + ~ Addresses of hosts that are deemed contact points . Cassandra nodes <nl> + ~ use this list of hosts to find each other and learn the topology of <nl> + ~ the ring . You must change this if you are running multiple nodes ! <nl> + - - > <nl> + < Seeds > <nl> + < Seed > 127 . 0 . 0 . 1 < / Seed > <nl> + < / Seeds > <nl> + <nl> + <nl> + < ! - - Miscellaneous - - > <nl> + <nl> + < ! - - Time to wait for a reply from other nodes before failing the command - - > <nl> + < RpcTimeoutInMillis > 5000 < / RpcTimeoutInMillis > <nl> + < ! - - Size to allow commitlog to grow to before creating a new segment - - > <nl> + < CommitLogRotationThresholdInMB > 128 < / CommitLogRotationThresholdInMB > <nl> + <nl> + <nl> + < ! - - Local hosts and ports - - > <nl> + <nl> + < ! - - <nl> + ~ Address to bind to and tell other nodes to connect to . You _ must _ <nl> + ~ change this if you want multiple nodes to be able to communicate ! <nl> + ~ <nl> + ~ Leaving it blank leaves it up to InetAddress . getLocalHost ( ) . This <nl> + ~ will always do the Right Thing * if * the node is properly configured <nl> + ~ ( hostname , name resolution , etc ) , and the Right Thing is to use the <nl> + ~ address associated with the hostname ( it might not be ) . <nl> + - - > <nl> + < ListenAddress > 127 . 0 . 0 . 2 < / ListenAddress > <nl> + < ! - - internal communications port - - > <nl> + < StoragePort > 7000 < / StoragePort > <nl> + <nl> + < ! - - <nl> + ~ The address to bind the Thrift RPC service to . Unlike ListenAddress <nl> + ~ above , you * can * specify 0 . 0 . 0 . 0 here if you want Thrift to listen on <nl> + ~ all interfaces . <nl> + ~ <nl> + ~ Leaving this blank has the same effect it does for ListenAddress , <nl> + ~ ( i . e . it will be based on the configured hostname of the node ) . <nl> + - - > <nl> + < ThriftAddress > 127 . 0 . 0 . 2 < / ThriftAddress > <nl> + < ! - - Thrift RPC port ( the port clients connect to ) . - - > <nl> + < ThriftPort > 9160 < / ThriftPort > <nl> + < ! - - <nl> + ~ Whether or not to use a framed transport for Thrift . If this option <nl> + ~ is set to true then you must also use a framed transport on the <nl> + ~ client - side , ( framed and non - framed transports are not compatible ) . <nl> + - - > <nl> + < ThriftFramedTransport > false < / ThriftFramedTransport > <nl> + <nl> + <nl> + < ! - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - > <nl> + < ! - - Memory , Disk , and Performance - - > <nl> + < ! - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - > <nl> + <nl> + < ! - - <nl> + ~ Access mode . mmapped i / o is substantially faster , but only practical on <nl> + ~ a 64bit machine ( which notably does not include EC2 " small " instances ) <nl> + ~ or relatively small datasets . " auto " , the safe choice , will enable <nl> + ~ mmapping on a 64bit JVM . Other values are " mmap " , " mmap _ index _ only " <nl> + ~ ( which may allow you to get part of the benefits of mmap on a 32bit <nl> + ~ machine by mmapping only index files ) and " standard " . <nl> + ~ ( The buffer size settings that follow only apply to standard , <nl> + ~ non - mmapped i / o . ) <nl> + - - > <nl> + < DiskAccessMode > auto < / DiskAccessMode > <nl> + <nl> + < ! - - <nl> + ~ Buffer size to use when performing contiguous column slices . Increase <nl> + ~ this to the size of the column slices you typically perform . <nl> + ~ ( Name - based queries are performed with a buffer size of <nl> + ~ ColumnIndexSizeInKB . ) <nl> + - - > <nl> + < SlicedBufferSizeInKB > 64 < / SlicedBufferSizeInKB > <nl> + <nl> + < ! - - <nl> + ~ Buffer size to use when flushing memtables to disk . ( Only one <nl> + ~ memtable is ever flushed at a time . ) Increase ( decrease ) the index <nl> + ~ buffer size relative to the data buffer if you have few ( many ) <nl> + ~ columns per key . Bigger is only better _ if _ your memtables get large <nl> + ~ enough to use the space . ( Check in your data directory after your <nl> + ~ app has been running long enough . ) - - > <nl> + < FlushDataBufferSizeInMB > 32 < / FlushDataBufferSizeInMB > <nl> + < FlushIndexBufferSizeInMB > 8 < / FlushIndexBufferSizeInMB > <nl> + <nl> + < ! - - <nl> + ~ Add column indexes to a row after its contents reach this size . <nl> + ~ Increase if your column values are large , or if you have a very large <nl> + ~ number of columns . The competing causes are , Cassandra has to <nl> + ~ deserialize this much of the row to read a single column , so you want <nl> + ~ it to be small - at least if you do many partial - row reads - but all <nl> + ~ the index data is read for each access , so you don ' t want to generate <nl> + ~ that wastefully either . <nl> + - - > <nl> + < ColumnIndexSizeInKB > 64 < / ColumnIndexSizeInKB > <nl> + <nl> + < ! - - <nl> + ~ Flush memtable after this much data has been inserted , including <nl> + ~ overwritten data . There is one memtable per column family , and <nl> + ~ this threshold is based solely on the amount of data stored , not <nl> + ~ actual heap memory usage ( there is some overhead in indexing the <nl> + ~ columns ) . <nl> + - - > <nl> + < MemtableThroughputInMB > 64 < / MemtableThroughputInMB > <nl> + < ! - - <nl> + ~ Throughput setting for Binary Memtables . Typically these are <nl> + ~ used for bulk load so you want them to be larger . <nl> + - - > <nl> + < BinaryMemtableThroughputInMB > 256 < / BinaryMemtableThroughputInMB > <nl> + < ! - - <nl> + ~ The maximum number of columns in millions to store in memory per <nl> + ~ ColumnFamily before flushing to disk . This is also a per - memtable <nl> + ~ setting . Use with MemtableThroughputInMB to tune memory usage . <nl> + - - > <nl> + < MemtableOperationsInMillions > 0 . 1 < / MemtableOperationsInMillions > <nl> + < ! - - <nl> + ~ The maximum time to leave a dirty memtable unflushed . <nl> + ~ ( While any affected columnfamilies have unflushed data from a <nl> + ~ commit log segment , that segment cannot be deleted . ) <nl> + ~ This needs to be large enough that it won ' t cause a flush storm <nl> + ~ of all your memtables flushing at once because none has hit <nl> + ~ the size or count thresholds yet . For production , a larger <nl> + ~ value such as 1440 is recommended . <nl> + - - > <nl> + < MemtableFlushAfterMinutes > 60 < / MemtableFlushAfterMinutes > <nl> + <nl> + < ! - - <nl> + ~ Unlike most systems , in Cassandra writes are faster than reads , so <nl> + ~ you can afford more of those in parallel . A good rule of thumb is 2 <nl> + ~ concurrent reads per processor core . Increase ConcurrentWrites to <nl> + ~ the number of clients writing at once if you enable CommitLogSync + <nl> + ~ CommitLogSyncDelay . - - > <nl> + < ConcurrentReads > 8 < / ConcurrentReads > <nl> + < ConcurrentWrites > 32 < / ConcurrentWrites > <nl> + <nl> + < ! - - <nl> + ~ CommitLogSync may be either " periodic " or " batch . " When in batch <nl> + ~ mode , Cassandra won ' t ack writes until the commit log has been <nl> + ~ fsynced to disk . It will wait up to CommitLogSyncBatchWindowInMS <nl> + ~ milliseconds for other writes , before performing the sync . <nl> + <nl> + ~ This is less necessary in Cassandra than in traditional databases <nl> + ~ since replication reduces the odds of losing data from a failure <nl> + ~ after writing the log entry but before it actually reaches the disk . <nl> + ~ So the other option is " timed , " where writes may be acked immediately <nl> + ~ and the CommitLog is simply synced every CommitLogSyncPeriodInMS <nl> + ~ milliseconds . <nl> + - - > <nl> + < CommitLogSync > periodic < / CommitLogSync > <nl> + < ! - - <nl> + ~ Interval at which to perform syncs of the CommitLog in periodic mode . <nl> + ~ Usually the default of 10000ms is fine ; increase it if your i / o <nl> + ~ load is such that syncs are taking excessively long times . <nl> + - - > <nl> + < CommitLogSyncPeriodInMS > 10000 < / CommitLogSyncPeriodInMS > <nl> + < ! - - <nl> + ~ Delay ( in milliseconds ) during which additional commit log entries <nl> + ~ may be written before fsync in batch mode . This will increase <nl> + ~ latency slightly , but can vastly improve throughput where there are <nl> + ~ many writers . Set to zero to disable ( each entry will be synced <nl> + ~ individually ) . Reasonable values range from a minimal 0 . 1 to 10 or <nl> + ~ even more if throughput matters more than latency . <nl> + - - > <nl> + < ! - - < CommitLogSyncBatchWindowInMS > 1 < / CommitLogSyncBatchWindowInMS > - - > <nl> + <nl> + < ! - - <nl> + ~ Time to wait before garbage - collection deletion markers . Set this to <nl> + ~ a large enough value that you are confident that the deletion marker <nl> + ~ will be propagated to all replicas by the time this many seconds has <nl> + ~ elapsed , even in the face of hardware failures . The default value is <nl> + ~ ten days . <nl> + - - > <nl> + < GCGraceSeconds > 864000 < / GCGraceSeconds > <nl> + < / Storage > <nl> diff - - git a / ivy . xml b / ivy . xml <nl> index b85534c . . 02a264d 100644 <nl> - - - a / ivy . xml <nl> + + + b / ivy . xml <nl> @ @ - 19 , 8 + 19 , 11 @ @ <nl> < ivy - module version = " 2 . 0 " > <nl> < info organisation = " apache - cassandra " module = " cassandra " / > <nl> < dependencies > <nl> - < dependency org = " org . apache . mahout . hadoop " <nl> - name = " hadoop - core " rev = " 0 . 20 . 1 " / > <nl> + < ! - - for hadoop - - > <nl> + < dependency org = " commons - logging " name = " commons - logging " rev = " 1 . 1 . 1 " / > <nl> + < dependency org = " org . apache . mahout . hadoop " name = " hadoop - core " rev = " 0 . 20 . 1 " / > <nl> + < dependency org = " commons - httpclient " name = " commons - httpclient " rev = " 3 . 1 " / > <nl> + <nl> < ! - - FIXME : paranamer and jackson can be dropped after we ' re depending <nl> on avro ( since it depends on them ) . - - > <nl> < dependency org = " com . thoughtworks . paranamer " <nl> diff - - git a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> index d3898b1 . . a83bdf0 100644 <nl> - - - a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> + + + b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> @ @ - 42 , 6 + 42 , 7 @ @ import java . lang . reflect . InvocationTargetException ; <nl> import java . util . * ; <nl> import java . net . InetAddress ; <nl> import java . net . UnknownHostException ; <nl> + import java . net . URL ; <nl> <nl> public class DatabaseDescriptor <nl> { <nl> @ @ - 145 , 11 + 146 , 29 @ @ public class DatabaseDescriptor <nl> <nl> private static IAuthenticator authenticator = new AllowAllAuthenticator ( ) ; <nl> <nl> + private final static String STORAGE _ CONF _ FILE = " storage - conf . xml " ; <nl> + <nl> + / * * <nl> + * Try the storage - config system property , and then inspect the classpath . <nl> + * / <nl> + static String getStorageConfigPath ( ) <nl> + { <nl> + String scp = System . getProperty ( " storage - config " ) + File . separator + STORAGE _ CONF _ FILE ; <nl> + if ( new File ( scp ) . exists ( ) ) <nl> + return scp ; <nl> + / / try the classpath <nl> + ClassLoader loader = DatabaseDescriptor . class . getClassLoader ( ) ; <nl> + URL scpurl = loader . getResource ( STORAGE _ CONF _ FILE ) ; <nl> + if ( scpurl ! = null ) <nl> + return scpurl . getFile ( ) ; <nl> + throw new RuntimeException ( " Cannot locate " + STORAGE _ CONF _ FILE + " via storage - config system property or classpath lookup . " ) ; <nl> + } <nl> + <nl> static <nl> { <nl> try <nl> { <nl> - configFileName _ = System . getProperty ( " storage - config " ) + File . separator + " storage - conf . xml " ; <nl> + configFileName _ = getStorageConfigPath ( ) ; <nl> if ( logger _ . isDebugEnabled ( ) ) <nl> logger _ . debug ( " Loading settings from " + configFileName _ ) ; <nl> XMLUtils xmlUtils = new XMLUtils ( configFileName _ ) ; <nl> diff - - git a / src / java / org / apache / cassandra / db / RangeSliceCommand . java b / src / java / org / apache / cassandra / db / RangeSliceCommand . java <nl> index d279f62 . . 07fd5e0 100644 <nl> - - - a / src / java / org / apache / cassandra / db / RangeSliceCommand . java <nl> + + + b / src / java / org / apache / cassandra / db / RangeSliceCommand . java <nl> @ @ - 39 , 7 + 39 , 6 @ @ package org . apache . cassandra . db ; <nl> import org . apache . cassandra . concurrent . StageManager ; <nl> <nl> import org . apache . cassandra . dht . AbstractBounds ; <nl> - import org . apache . cassandra . dht . Bounds ; <nl> import org . apache . cassandra . io . util . DataOutputBuffer ; <nl> import org . apache . cassandra . io . ICompactSerializer ; <nl> import org . apache . cassandra . net . Message ; <nl> @ @ - 129 , 7 + 128 , 7 @ @ class RangeSliceCommandSerializer implements ICompactSerializer < RangeSliceComman <nl> <nl> TSerializer ser = new TSerializer ( new TBinaryProtocol . Factory ( ) ) ; <nl> FBUtilities . serialize ( ser , sliceCommand . predicate , dos ) ; <nl> - Bounds . serializer ( ) . serialize ( sliceCommand . range , dos ) ; <nl> + AbstractBounds . serializer ( ) . serialize ( sliceCommand . range , dos ) ; <nl> dos . writeInt ( sliceCommand . max _ keys ) ; <nl> } <nl>

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index c555a91 . . 16ce060 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 2 . 0 . 16 : 
 + * Don ' t accumulate more range than necessary in RangeTombstone . Tracker ( CASSANDRA - 9486 ) 
 * Add broadcast and rpc addresses to system . local ( CASSANDRA - 9436 ) 
 * Always mark sstable suspect when corrupted ( CASSANDRA - 9478 ) 
 * Add database users and permissions to CQL3 documentation ( CASSANDRA - 7558 ) 
 diff - - git a / src / java / org / apache / cassandra / db / DeletionTime . java b / src / java / org / apache / cassandra / db / DeletionTime . java 
 index dd2ccaf . . b39d681 100644 
 - - - a / src / java / org / apache / cassandra / db / DeletionTime . java 
 + + + b / src / java / org / apache / cassandra / db / DeletionTime . java 
 @ @ - 114 , 6 + 114 , 11 @ @ public class DeletionTime implements Comparable < DeletionTime > 
 return column . timestamp ( ) < = markedForDeleteAt ; 
 } 
 
 + public boolean supersedes ( DeletionTime dt ) 
 + { 
 + return this . markedForDeleteAt > dt . markedForDeleteAt ; 
 + } 
 + 
 public long memorySize ( ) 
 { 
 long fields = TypeSizes . NATIVE . sizeof ( markedForDeleteAt ) + TypeSizes . NATIVE . sizeof ( localDeletionTime ) ; 
 diff - - git a / src / java / org / apache / cassandra / db / RangeTombstone . java b / src / java / org / apache / cassandra / db / RangeTombstone . java 
 index 16fc27a . . fe9da20 100644 
 - - - a / src / java / org / apache / cassandra / db / RangeTombstone . java 
 + + + b / src / java / org / apache / cassandra / db / RangeTombstone . java 
 @ @ - 114 , 52 + 114 , 73 @ @ public class RangeTombstone extends Interval < ByteBuffer , DeletionTime > implement 
 return comparator . compare ( min , rt . min ) < = 0 & & comparator . compare ( max , rt . max ) > = 0 ; 
 } 
 
 + / * * 
 + * Tracks opened RangeTombstones when iterating over a partition . 
 + * < p > 
 + * This tracker must be provided all the atoms of a given partition in 
 + * order ( to the { @ code update } method ) . Given this , it keeps enough 
 + * information to be able to decide if one of an atom is deleted ( shadowed ) 
 + * by a previously open RT . One the tracker can prove a given range 
 + * tombstone cannot be useful anymore ( that is , as soon as we ' ve seen an 
 + * atom that is after the end of that RT ) , it discards this RT . In other 
 + * words , the maximum memory used by this object should be proportional to 
 + * the maximum number of RT that can be simultaneously open ( and this 
 + * should fairly low in practice ) . 
 + * / 
 public static class Tracker 
 { 
 private final Comparator < ByteBuffer > comparator ; 
 - private final Deque < RangeTombstone > ranges = new ArrayDeque < RangeTombstone > ( ) ; 
 - private final SortedSet < RangeTombstone > maxOrderingSet = new TreeSet < RangeTombstone > ( new Comparator < RangeTombstone > ( ) 
 - { 
 - public int compare ( RangeTombstone t1 , RangeTombstone t2 ) 
 - { 
 - return comparator . compare ( t1 . max , t2 . max ) ; 
 - } 
 - } ) ; 
 - public final Set < RangeTombstone > expired = new HashSet < RangeTombstone > ( ) ; 
 + 
 + / / A list the currently open RTs . We keep the list sorted in order of growing end bounds as for a 
 + / / new atom , this allows to efficiently find the RTs that are now useless ( if any ) . Also note that because 
 + / / atom are passed to the tracker in order , any RT that is tracked can be assumed as opened , i . e . we 
 + / / never have to test the RTs start since it ' s always assumed to be less than what we have . 
 + / / Also note that this will store expired RTs ( # 7810 ) . Those will be of type ExpiredRangeTombstone and 
 + / / will be ignored by writeOpenedMarker . 
 + private final List < RangeTombstone > openedTombstones = new LinkedList < RangeTombstone > ( ) ; 
 + 
 + / / Total number of atoms written by writeOpenedMarker ( ) . 
 private int atomCount ; 
 
 + / * * 
 + * Creates a new tracker given the table comparator . 
 + * 
 + * @ param comparator the comparator for the table this will track atoms 
 + * for . The tracker assumes that atoms will be later provided to the 
 + * tracker in { @ code comparator } order . 
 + * / 
 public Tracker ( Comparator < ByteBuffer > comparator ) 
 { 
 this . comparator = comparator ; 
 } 
 
 / * * 
 - * Compute RangeTombstone that are needed at the beginning of an index 
 + * Computes the RangeTombstone that are needed at the beginning of an index 
 * block starting with { @ code firstColumn } . 
 - * Returns the total serialized size of said tombstones and write them 
 - * to { @ code out } it if isn ' t null . 
 + * 
 + * @ return the total serialized size of said tombstones and write them to 
 + * { @ code out } it if isn ' t null . 
 * / 
 public long writeOpenedMarker ( OnDiskAtom firstColumn , DataOutput out , OnDiskAtom . Serializer atomSerializer ) throws IOException 
 { 
 long size = 0 ; 
 - if ( ranges . isEmpty ( ) ) 
 + if ( openedTombstones . isEmpty ( ) ) 
 return size ; 
 
 / * 
 - * Compute the marker that needs to be written at the beginning of 
 - * this block . We need to write one if it the more recent 
 + * Compute the markers that needs to be written at the beginning of 
 + * this block . We need to write one if it is the more recent 
 * ( opened ) tombstone for at least some part of its range . 
 * / 
 List < RangeTombstone > toWrite = new LinkedList < RangeTombstone > ( ) ; 
 outer : 
 - for ( RangeTombstone tombstone : ranges ) 
 + for ( RangeTombstone tombstone : openedTombstones ) 
 { 
 - / / If ever the first column is outside the range , skip it ( in 
 - / / case update ( ) hasn ' t been called yet ) 
 + / / If the first column is outside the range , skip it ( in case update ( ) hasn ' t been called yet ) 
 if ( comparator . compare ( firstColumn . name ( ) , tombstone . max ) > 0 ) 
 continue ; 
 
 - if ( expired . contains ( tombstone ) ) 
 + if ( tombstone instanceof ExpiredRangeTombstone ) 
 continue ; 
 
 RangeTombstone updated = new RangeTombstone ( firstColumn . name ( ) , tombstone . max , tombstone . data ) ; 
 @ @ - 186 , 6 + 207 , 9 @ @ public class RangeTombstone extends Interval < ByteBuffer , DeletionTime > implement 
 return size ; 
 } 
 
 + / * * 
 + * The total number of atoms written by calls to the method { @ link # writeOpenedMarker } . 
 + * / 
 public int writtenAtom ( ) 
 { 
 return atomCount ; 
 @ @ - 193 , 69 + 217 , 129 @ @ public class RangeTombstone extends Interval < ByteBuffer , DeletionTime > implement 
 
 / * * 
 * Update this tracker given an { @ code atom } . 
 - * If column is a Column , check if any tracked range is useless and 
 - * can be removed . If it is a RangeTombstone , add it to this tracker . 
 + * < p > 
 + * This method first test if some range tombstone can be discarded due 
 + * to the knowledge of that new atom . Then , if it ' s a range tombstone , 
 + * it adds it to the tracker . 
 + * < p > 
 + * Note that this method should be called on * every * atom of a partition for 
 + * the tracker to work as efficiently as possible

NEAREST DIFF:
diff - - git a / contrib / word _ count / README . txt b / contrib / word _ count / README . txt 
 new file mode 100644 
 index 0000000 . . f46e2e2 
 - - - / dev / null 
 + + + b / contrib / word _ count / README . txt 
 @ @ - 0 , 0 + 1 , 18 @ @ 
 + WordCount hadoop example : Inserts a bunch of words across multiple rows , 
 + and counts them , with RandomPartitioner . 
 + 
 + The scripts in bin / assume you are running with cwd of contrib / word _ count . 
 + 
 + First build and start a Cassandra server with the default configuration * , 
 + then run 
 + 
 + contrib / word _ count $ ant 
 + contrib / word _ count $ bin / word _ count _ setup 
 + contrib / word _ count $ bin / word _ count 
 + 
 + Output will be in / tmp / word _ count * . 
 + 
 + Read the code in src / for more details . 
 + 
 + * If you want to point wordcount at a real cluster , modify the seed 
 + and listenaddress settings in storage - conf . xml accordingly . 
 diff - - git a / contrib / word _ count / bin / word _ count b / contrib / word _ count / bin / word _ count 
 new file mode 100644 
 index 0000000 . . a4eafc6 
 - - - / dev / null 
 + + + b / contrib / word _ count / bin / word _ count 
 @ @ - 0 , 0 + 1 , 53 @ @ 
 + # ! / bin / sh 
 + 
 + # Licensed to the Apache Software Foundation ( ASF ) under one 
 + # or more contributor license agreements . See the NOTICE file 
 + # distributed with this work for additional information 
 + # regarding copyright ownership . The ASF licenses this file 
 + # to you under the Apache License , Version 2 . 0 ( the 
 + # " License " ) ; you may not use this file except in compliance 
 + # with the License . You may obtain a copy of the License at 
 + # 
 + # http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + # 
 + # Unless required by applicable law or agreed to in writing , software 
 + # distributed under the License is distributed on an " AS IS " BASIS , 
 + # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 + # See the License for the specific language governing permissions and 
 + # limitations under the License . 
 + 
 + cwd = ` dirname $ 0 ` 
 + 
 + # Cassandra class files . 
 + if [ ! - d $ cwd / . . / . . / . . / build / classes ] ; then 
 + echo " Unable to locate cassandra class files " > & 2 
 + exit 1 
 + fi 
 + 
 + # word _ count Jar . 
 + if [ ! - e $ cwd / . . / build / * . jar ] ; then 
 + echo " Unable to locate word _ count jar " > & 2 
 + exit 1 
 + fi 
 + 
 + CLASSPATH = $ CLASSPATH : ` ls - 1 $ cwd / . . / build / * . jar ` 
 + CLASSPATH = $ CLASSPATH : . : $ cwd / . . / . . / . . / build / classes 
 + for jar in $ cwd / . . / . . / . . / lib / * . jar ; do 
 + CLASSPATH = $ CLASSPATH : $ jar 
 + done 
 + for jar in $ cwd / . . / . . / . . / build / lib / jars / * . jar ; do 
 + CLASSPATH = $ CLASSPATH : $ jar 
 + done 
 + 
 + if [ - x $ JAVA _ HOME / bin / java ] ; then 
 + JAVA = $ JAVA _ HOME / bin / java 
 + else 
 + JAVA = ` which java ` 
 + fi 
 + 
 + if [ " x $ JAVA " = " x " ] ; then 
 + echo " Java executable not found ( hint : set JAVA _ HOME ) " > & 2 
 + exit 1 
 + fi 
 + 
 + $ JAVA - Xmx1G - ea - cp $ CLASSPATH WordCount 
 diff - - git a / contrib / word _ count / bin / word _ count _ setup b / contrib / word _ count / bin / word _ count _ setup 
 new file mode 100644 
 index 0000000 . . 9af6562 
 - - - / dev / null 
 + + + b / contrib / word _ count / bin / word _ count _ setup 
 @ @ - 0 , 0 + 1 , 53 @ @ 
 + # ! / bin / sh 
 + 
 + # Licensed to the Apache Software Foundation ( ASF ) under one 
 + # or more contributor license agreements . See the NOTICE file 
 + # distributed with this work for additional information 
 + # regarding copyright ownership . The ASF licenses this file 
 + # to you under the Apache License , Version 2 . 0 ( the 
 + # " License " ) ; you may not use this file except in compliance 
 + # with the License . You may obtain a copy of the License at 
 + # 
 + # http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + # 
 + # Unless required by applicable law or agreed to in writing , software 
 + # distributed under the License is distributed on an " AS IS " BASIS , 
 + # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 + # See the License for the specific language governing permissions and 
 + # limitations under the License . 
 + 
 + cwd = ` dirname $ 0 ` 
 + 
 + # Cassandra class files . 
 + if [ ! - d $ cwd / . . / . . / . . / build / classes ] ; then 
 + echo " Unable to locate cassandra class files " > & 2 
 + exit 1 
 + fi 
 + 
 + # word _ count Jar . 
 + if [ ! - e $ cwd / . . / build / * . jar ] ; then 
 + echo " Unable to locate word _ count jar " > & 2 
 + exit 1 
 + fi 
 + 
 + CLASSPATH = $ CLASSPATH : ` ls - 1 $ cwd / . . / build / * . jar ` 
 + CLASSPATH = $ CLASSPATH : . : $ cwd / . . / . . / . . / build / classes 
 + for jar in $ cwd / . . / . . / . . / lib / * . jar ; do 
 + CLASSPATH = $ CLASSPATH : $ jar 
 + done 
 + for jar in $ cwd / . . / . . / . . / build / lib / jars / * . jar ; do 
 + CLASSPATH = $ CLASSPATH : $ jar 
 + done 
 + 
 + if [ - x $ JAVA _ HOME / bin / java ] ; then 
 + JAVA = $ JAVA _ HOME / bin / java 
 + else 
 + JAVA = ` which java ` 
 + fi 
 + 
 + if [ " x $ JAVA " = " x " ] ; then 
 + echo " Java executable not found ( hint : set JAVA _ HOME ) " > & 2 
 + exit 1 
 + fi 
 + 
 + $ JAVA - Xmx1G - ea - cp $ CLASSPATH WordCountSetup 
 diff - - git a / contrib / word _ count / build . xml b / contrib / word _ count / build . xml 
 new file mode 100644 
 index 0000000 . . e80dd10 
 - - - / dev / null 
 + + + b / contrib / word _ count / build . xml 
 @ @ - 0 , 0 + 1 , 65 @ @ 
 + < ? xml version = " 1 . 0 " encoding = " UTF - 8 " ? > 
 + < ! - - 
 + ~ Licensed to the Apache Software Foundation ( ASF ) under one 
 + ~ or more contributor license agreements . See the NOTICE file 
 + ~ distributed with this work for additional information 
 + ~ regarding copyright ownership . The ASF licenses this file 
 + ~ to you under the Apache License , Version 2 . 0 ( the 
 + ~ " License " ) ; you may not use this file except in compliance 
 + ~ with the License . You may obtain a copy of the License at 
 + ~ 
 + ~ http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + ~ 
 + ~ Unless required by applicable law or agreed to in writing , 
 + ~ software distributed under the License is distributed on an 
 + ~ " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY 
 + ~ KIND , either express or implied . See the License for the 
 + ~ specific language governing permissions and limitations 
 + ~ under the License . 
 + - - > 
 + < project basedir = " . " default = " jar " name = " word _ count " > 
 + < property name = " cassandra . dir " value = " . . / . . " / > 
 + < property name = " cassandra . lib " value = " " / > 
 + < property name = " cassandra . classes " value = " $ { cassandra . dir } / build / classes " / > 
 + < property name = " build . src " value = " $ { basedir } / src " / > 
 + < property name = " build . out " value = " $ { basedir } / build " / > 
 + < property name = " build . classes " value = " $ { build . out } / classes " / > 
 + < property name = " final . name " value = " word _ count " / > 
 + 
 + < target name = " init " > 
 + < mkdir dir = " $ { build . classes } " / > 
 + < / target > 
 + 
 + < target depends = " init " name = " build " > 
 + < javac destdir = " $ { build . classes } " > 
 + < src path = " $ { build . src } " / > 
 + < classpath > 
 + < path > 
 + < fileset dir = " $ { cassandra . dir } / lib " > 
 + < include name = " * * / * . jar " / > 
 + < / fileset > 
 + < fileset dir = " $ { cassandra . dir } / build / lib / jars " > 
 + < include name = " * * / * . jar " / > 
 + < / fileset > 
 + < pathelement location = " $ { cassandra . classes } " / > 
 + < / path > 
 + < / classpath > 
 + < / javac > 
 + < / target > 
 + 
 + < target name = " jar " depends = " build " > 
 + < mkdir dir = " $ { build . classes } / META - INF " / > 
 + < jar jarfile = " $ { build . out } / $ { final . name } . jar " > 
 + < fileset dir = " $ { build . classes } " / > 
 + < fileset dir = " $ { cassandra . classes } " / > 
 + < fileset dir = " $ { cassandra . dir } " > 
 + < include name = " lib / * * / * . jar " / > 
 + < / fileset > 
 + < fileset file = " $ { basedir } / storage - conf . xml " / > 
 + < / jar > 
 + < / target > 
 + 
 + < target name = " clean " > 
 + < delete dir = " $ { build . out } " / > 
 + < / target > 
 + < / project > 
 diff - - git a / contrib / word _ count / src / WordCount . java b / contrib / word _ count / src / WordCount . java 
 new file mode 100644 
 index 0000000 . . bcd30f9 
 - - - / dev / null 
 + + + b / contrib / word _ count / src / WordCount . java 
 @ @ - 0 , 0 + 1 , 107 @ @ 
 + import java . io . IOException ; 
 + import java . util . SortedMap ; 
 + import java . util . StringTokenizer ; 
 + 
 + import org . apache . log4j . Logger ; 
 + 
 + import org . apache . cassandra . db . IColumn ; 
 + import org . apache . cassandra . hadoop . ColumnFamilyInputFormat ; 
 + import org . apache . hadoop . conf . Configuration ; 
 + import org . apache . hadoop . conf . Configured ; 
 + import org . apache . hadoop . fs . Path ; 
 + import org . apache . hadoop . io . IntWritable ; 
 + import org . apache . hadoop . io . Text ; 
 + import org . apache . hadoop . mapreduce . Job ; 
 + import org . apache . hadoop . mapreduce . Mapper ; 
 + import org . apache . hadoop . mapreduce . Reducer ; 
 + import org . apache . hadoop . mapreduce . lib . output . FileOutputFormat ; 
 + import org . apache . hadoop . util . Tool ; 
 + import org . apache . hadoop . util . ToolRunner ; 
 + 
 + / * * 
 + * This counts the occurrences of words in ColumnFamily Standard1 , that has a single column ( that we care about ) 
 + * " text " containing a sequence of words . 
 + * 
 + * For each word , we output the total number of occurrences across all texts . 
 + * / 
 + public class WordCount extends Configured implements Tool 
 + { 
 + private static final Logger logger = Logger . getLogger ( WordCount . class ) ; 
 + 
 + static final String KEYSPACE = " Keyspace1 " ; 
 + static final String COLUMN _ FAMILY = " Standard1 " ; 
 + private static String columnName ; 
 + private static final String OUTPUT _ PATH _ PREFIX = " / tmp / word _ count " ; 
 + static final int RING _ DELAY = 3000 ; / / this is enough for testing a single server node ; may need more for a real cluster 
 + 
 + public static void main ( String [ ] args ) throws Exception 
 + { 
 + / / Let ToolRunner handle generic command - line options 
 + ToolRunner . run ( new Configuration ( ) , new WordCount ( ) , args ) ; 
 + System . exit ( 0 ) ; 
 + } 
 + 
 + public static class TokenizerMapper extends Mapper < String , SortedMap < byte [ ] , IColumn > , Text , IntWritable > 
 + { 
 + private final static IntWritable one = new IntWritable ( 1 ) ; 
 + private Text word = new Text ( ) ; 
 + 
 + public void map ( String key , SortedMap < byte [ ] , IColumn > columns , Context context ) throws IOException , InterruptedException 
 + { 
 + if ( columns = = null ) 
 + return ; 
 + IColumn column = columns . get ( columnName . getBytes ( ) ) ; 
 + String value = new String ( column . value ( ) ) ; 
 + logger . debug ( " read " + key + " : " + value + " from " + context . getInputSplit ( ) ) ; 
 + 
 + StringTokenizer itr = new StringTokenizer ( value ) ; 
 + while ( itr . hasMoreTokens ( ) ) 
 + { 
 + word . set ( itr . nextToken ( ) ) ; 
 + context . write ( word , one ) ; 
 + } 
 + } 
 + } 
 + 
 + public static class IntSumReducer extends Reducer < Text , IntWritable , Text , IntWritable > 
 + { 
 + private IntWritable result = new IntWritable ( ) ; 
 + 
 + public void reduce ( Text key , Iterable < IntWritable > values , Context context ) throws IOException , InterruptedException 
 + { 
 + int sum = 0 ; 
 + for ( IntWritable val : values ) 
 + { 
 + sum + = val . get ( ) ; 
 + } 
 + 
 + result . set ( sum ) ; 
 + context . write ( key , result ) ; 
 + } 
 + } 
 + 
 + public int run ( String [ ] args ) throws Exception 
 + { 
 + Configuration conf = getConf ( ) ; 
 + 
 + for ( int i = 0 ; i < WordCountSetup . TEST _ COUNT ; i + + ) 
 + { 
 + columnName = " text " + i ; 
 + Job job = new Job ( conf , " wordcount " ) ; 
 + job . setJarByClass ( WordCount . class ) ; 
 + job . setMapperClass ( TokenizerMapper . class ) ; 
 + job . setCombinerClass ( IntSumReducer . class ) ; 
 + job . setReducerClass ( IntSumReducer . class ) ; 
 + job . setOutputKeyClass ( Text . class ) ; 
 + job . setOutputValueClass ( IntWritable . class ) ; 
 + 
 + job . setInputFormatClass ( ColumnFamilyInputFormat . class ) ; 
 + FileOutputFormat . setOutputPath ( job , new Path ( OUTPUT _ PATH _ PREFIX + i ) ) ; 
 + 
 + ColumnFamilyInputFormat . setColumnFamily ( job , KEYSPACE , COLUMN _ FAMILY ) ; 
 + 
 + job . waitForCompletion ( true ) ; 
 + } 
 + return 0 ; 
 + } 
 + } 
 \ No newline at end of file 
 diff - - git a / contrib / word _ count / src / WordCountSetup . java b / contrib / word _ count / src / WordCountSetup . java 
 new file mode 100644 
 index 0000000 . . 74ab87a 
 - - - / dev / null 
 + + + b / contrib / word _ count / src / WordCountSetup . java 
 @ @ - 0 , 0 + 1 , 61 @ @ 
 + import java . util . Arrays ; 
 + 
 + import org . apache . log4j . Logger ; 
 + 
 + import org . apache . cassandra . db . * ; 
 + import org . apache . cassandra . service . StorageProxy ; 
 + import org . apache . cassandra . service . StorageService ; 
 + import org . apache . cassandra . thrift . ConsistencyLevel ; 
 + 
 + public class WordCountSetup 
 + { 
 + private static final Logger logger = Logger . getLogger ( WordCountSetup . class ) ; 
 + 
 + public static final int TEST _ COUNT = 4 ; 
 + 
 + public static void main ( String [ ] args ) throws Exception 
 + { 
 + StorageService . instance . initClient ( ) ; 
 + logger . info ( " Sleeping " + WordCount . RING _ DELAY ) ; 
 + Thread . sleep ( WordCount . RING _ DELAY ) ; 
 + assert ! StorageService . instance . getLiveNodes ( ) . isEmpty ( ) ; 
 + 
 + RowMutation rm ; 
 + ColumnFamily cf ; 
 + byte [ ] columnName ; 
 + 
 + / / text0 : no rows 
 + 
 + / / text1 : 1 row , 1 word 
 + columnName = " text1 " . getBytes ( ) ; 
 + rm = new RowMutation ( WordCount . KEYSPACE , " Key0 " ) ; 
 + cf = ColumnFamily . create ( WordCount . KEYSPACE , WordCount . COLUMN _ FAMILY ) ; 
 + cf . addColumn ( new Column ( columnName , " word1 " . getBytes ( ) , 0 ) ) ; 
 + rm . add ( cf ) ; 
 + StorageProxy . mutateBlocking ( Arrays . asList ( rm ) , ConsistencyLevel . ONE ) ; 
 + logger . info ( " added text1 " ) ; 
 + 
 + / / text2 : 1 row , 2 words 
 + columnName = " text2 " . getBytes ( ) ; 
 + rm = new RowMutation ( WordCount . KEYSPACE , " Key0 " ) ; 
 + cf = ColumnFamily . create ( WordCount . KEYSPACE , WordCount . COLUMN _ FAMILY ) ; 
 + cf . addColumn ( new Column ( columnName , " word1 word2 " . getBytes ( ) , 0 ) ) ; 
 + rm . add ( cf ) ; 
 + StorageProxy . mutateBlocking ( Arrays . asList ( rm ) , ConsistencyLevel . ONE ) ; 
 + logger . info ( " added text2 " ) ; 
 + 
 + / / text3 : 1000 rows , 1 word 
 + columnName = " text3 " . getBytes ( ) ; 
 + for ( int i = 0 ; i < 1000 ; i + + ) 
 + { 
 + rm = new RowMutation ( WordCount . KEYSPACE , " Key " + i ) ; 
 + cf = ColumnFamily . create ( WordCount . KEYSPACE , WordCount . COLUMN _ FAMILY ) ; 
 + cf . addColumn ( new Column ( columnName , " word1 " . getBytes ( ) , 0 ) ) ; 
 + rm . add ( cf ) ; 
 + StorageProxy . mutateBlocking ( Arrays . asList ( rm ) , ConsistencyLevel . ONE ) ; 
 + } 
 + logger . info ( " added text3 " ) ; 
 + 
 + System . exit ( 0 ) ; 
 + } 
 + } 
 diff - - git a / contrib / word _ count / storage - conf . xml b / contrib / word _ count / storage - conf . xml 
 new file mode 100644 
 index 0000000 . . 0d591d9 
 - - - / dev / null 
 + + + b / contrib / word _ count / storage - conf . xml 
 @ @ - 0 , 0 + 1 , 369 @ @ 
 + < ! - - 
 + ~ Licensed to the Apache Software Foundation ( ASF ) under one 
 + ~ or more contributor license agreements . See the NOTICE file 
 + ~ distributed with this work for additional information 
 + ~ regarding copyright ownership . The ASF licenses this file 
 + ~ to you under the Apache License , Version 2 . 0 ( the 
 + ~ " License " ) ; you may not use this file except in compliance 
 + ~ with the License . You may obtain a copy of the License at 
 + ~ 
 + ~ http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + ~ 
 + ~ Unless required by applicable law or agreed to in writing , 
 + ~ software distributed under the License is distributed on an 
 + ~ " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY 
 + ~ KIND , either express or implied . See the License for the 
 + ~ specific language governing permissions and limitations 
 + ~ under the License . 
 + - - > 
 + < Storage > 
 + < ! - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - > 
 + < ! - - Basic Configuration - - > 
 + < ! - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - > 
 + 
 + < ! - - 
 + ~ The name of this cluster . This is mainly used to prevent machines in 
 + ~ one logical cluster from joining another . 
 + - - > 
 + < ClusterName > Test Cluster < / ClusterName > 
 + 
 + < ! - - 
 + ~ Turn on to make new [ non - seed ] nodes automatically migrate the right data 
 + ~ to themselves . ( If no InitialToken is specified , they will pick one 
 + ~ such that they will get half the range of the most - loaded node . ) 
 + ~ If a node starts up without bootstrapping , it will mark itself bootstrapped 
 + ~ so that you can ' t subsequently accidently bootstrap a node with 
 + ~ data on it . ( You can reset this by wiping your data and commitlog 
 + ~ directories . ) 
 + ~ 
 + ~ Off by default so that new clusters and upgraders from 0 . 4 don ' t 
 + ~ bootstrap immediately . You should turn this on when you start adding 
 + ~ new nodes to a cluster that already has data on it . ( If you are upgrading 
 + ~ from 0 . 4 , start your cluster with it off once before changing it to true . 
 + ~ Otherwise , no data will be lost but you will incur a lot of unnecessary 
 + ~ I / O before your cluster starts up . ) 
 + - - > 
 + < AutoBootstrap > false < / AutoBootstrap > 
 + 
 + < ! - - 
 + ~ Keyspaces and ColumnFamilies : 
 + ~ A ColumnFamily is the Cassandra concept closest to a relational 
 + ~ table . Keyspaces are separate groups of ColumnFamilies . Except in 
 + ~ very unusual circumstances you will have one Keyspace per application . 
 + 
 + ~ There is an implicit keyspace named ' system ' for Cassandra internals . 
 + - - > 
 + < Keyspaces > 
 + < Keyspace Name = " Keyspace1 " > 
 + < ! - - 
 + ~ ColumnFamily definitions have one required attribute ( Name ) 
 + ~ and several optional ones . 
 + ~ 
 + ~ The CompareWith attribute tells Cassandra how to sort the columns 
 + ~ for slicing operations . The default is BytesType , which is a 
 + ~ straightforward lexical comparison of the bytes in each column . 
 + ~ Other options are AsciiType , UTF8Type , LexicalUUIDType , TimeUUIDType , 
 + ~ and LongType . You can also specify the fully - qualified class 
 + ~ name to a class of your choice extending 
 + ~ org . apache . cassandra . db . marshal . AbstractType . 
 + ~ 
 + ~ SuperColumns have a similar CompareSubcolumnsWith attribute . 
 + ~ 
 + ~ BytesType : Simple sort by byte value . No validation is performed . 
 + ~ AsciiType : Like BytesType , but validates that the input can be 
 + ~ parsed as US - ASCII . 
 + ~ UTF8Type : A string encoded as UTF8 
 + ~ LongType : A 64bit long 
 + ~ LexicalUUIDType : A 128bit UUID , compared lexically ( by byte value ) 
 + ~ TimeUUIDType : a 128bit version 1 UUID , compared by timestamp 
 + ~ 
 + ~ ( To get the closest approximation to 0 . 3 - style supercolumns , you 
 + ~ would use CompareWith = UTF8Type CompareSubcolumnsWith = LongType . ) 
 + ~ 
 + ~ An optional ` Comment ` attribute may be used to attach additional 
 + ~ human - readable information about the column family to its definition . 
 + ~ 
 + ~ The optional KeysCachedFraction attribute specifies 
 + ~ The fraction of keys per sstable whose locations we keep in 
 + ~ memory in " mostly LRU " order . ( JUST the key locations , NOT any 
 + ~ column values . ) The amount of memory used by the default setting of 
 + ~ 0 . 01 is comparable to the amount used by the internal per - sstable key 
 + ~ index . Consider increasing this if you have fewer , wider rows . 
 + ~ Set to 0 to disable entirely . 
 + ~ 
 + ~ The optional RowsCached attribute specifies the number of rows 
 + ~ whose entire contents we cache in memory , either as a fixed number 
 + ~ of rows or as a percent of rows in the ColumnFamily . 
 + ~ Do not use this on ColumnFamilies with large rows , or 
 + ~ ColumnFamilies with high write : read ratios . As with key caching , 
 + ~ valid values are from 0 to 1 . The default 0 disables it entirely . 
 + - - > 
 + < ColumnFamily CompareWith = " BytesType " 
 + Name = " Standard1 " 
 + RowsCached = " 10 % " 
 + KeysCachedFraction = " 0 " / > 
 + < ColumnFamily CompareWith = " UTF8Type " Name = " Standard2 " / > 
 + < ColumnFamily CompareWith = " TimeUUIDType " Name = " StandardByUUID1 " / > 
 + < ColumnFamily ColumnType = " Super " 
 + CompareWith = " UTF8Type " 
 + CompareSubcolumnsWith = " UTF8Type " 
 + Name = " Super1 " 
 + RowsCached = " 1000 " 
 + KeysCachedFraction = " 0 " 
 + Comment = " A column family with supercolumns , whose column and subcolumn names are UTF8 strings " / > 
 + 
 + < ! - - 
 + ~ Strategy : Setting this to the class that implements 
 + ~ IReplicaPlacementStrategy will change the way the node picker works . 
 + ~ Out of the box , Cassandra provides 
 + ~ org . apache . cassandra . locator . RackUnawareStrategy and 
 + ~ org . apache . cassandra . locator . RackAwareStrategy ( place one replica in 
 + ~ a different datacenter , and the others on different racks in the same 
 + ~ one . ) 
 + - - > 
 + < ReplicaPlacementStrategy > org . apache . cassandra . locator . RackUnawareStrategy < / ReplicaPlacementStrategy > 
 + 
 + < ! - - Number of replicas of the data - - > 
 + < ReplicationFactor > 1 < / ReplicationFactor > 
 + 
 + < ! - - 
 + ~ EndPointSnitch : Setting this to the class that implements 
 + ~ AbstractEndpointSnitch , which lets Cassandra know enough 
 + ~ about your network topology to route requests efficiently . 
 + ~ Out of the box , Cassandra provides org . apache . cassandra . locator . EndPointSnitch , 
 + ~ and PropertyFileEndPointSnitch is available in contrib / . 
 + - - > 
 + < EndPointSnitch > org . apache . cassandra . locator . EndPointSnitch < / EndPointSnitch > 
 + < / Keyspace > 
 + < / Keyspaces > 
 + 
 + < ! - - 
 + ~ Authenticator : any IAuthenticator may be used , including your own as long 
 + ~ as it is on the classpath . Out of the box , Cassandra provides 
 + ~ org . apache . cassandra . auth . AllowAllAuthenticator and , 
 + ~ org . apache . cassandra . auth . SimpleAuthenticator 
 + ~ ( SimpleAuthenticator uses access . properties and passwd . properties by 
 + ~ default ) . 
 + ~ 
 + ~ If you don ' t specify an authenticator , AllowAllAuthenticator is used . 
 + - - > 
 + < Authenticator > org . apache . cassandra . auth . AllowAllAuthenticator < / Authenticator > 
 + 
 + < ! - - 
 + ~ Partitioner : any IPartitioner may be used , including your own as long 
 + ~ as it is on the classpath . Out of the box , Cassandra provides 
 + ~ org . apache . cassandra . dht . RandomPartitioner , 
 + ~ org . apache . cassandra . dht . OrderPreservingPartitioner , and 
 + ~ org . apache . cassandra . dht . CollatingOrderPreservingPartitioner . 
 + ~ ( CollatingOPP colates according to EN , US rules , not naive byte 
 + ~ ordering . Use this as an example if you need locale - aware collation . ) 
 + ~ Range queries require using an order - preserving partitioner . 
 + ~ 
 + ~ Achtung ! Changing this parameter requires wiping your data 
 + ~ directories , since the partitioner can modify the sstable on - disk 
 + ~ format . 
 + - - > 
 + < Partitioner > org . apache . cassandra . dht . RandomPartitioner < / Partitioner > 
 + 
 + < ! - - 
 + ~ If you are using an order - preserving partitioner and you know your key 
 + ~ distribution , you can specify the token for this node to use . ( Keys 
 + ~ are sent to the node with the " closest " token , so distributing your 
 + ~ tokens equally along the key distribution space will spread keys 
 + ~ evenly across your cluster . ) This setting is only checked the first 
 + ~ time a node is started . 
 + 
 + ~ This can also be useful with RandomPartitioner to force equal spacing 
 + ~ of tokens around the hash space , especially for clusters with a small 
 + ~ number of nodes . 
 + - - > 
 + < InitialToken > < / InitialToken > 
 + 
 + < ! - - 
 + ~ Directories : Specify where Cassandra should store different data on 
 + ~ disk . Keep the data disks and the CommitLog disks separate for best 
 + ~ performance 
 + - - > 
 + < CommitLogDirectory > / var / lib / cassandra / commitlog < / CommitLogDirectory > 
 + < DataFileDirectories > 
 + < DataFileDirectory > / var / lib / cassandra / data < / DataFileDirectory > 
 + < / DataFileDirectories > 
 + < CalloutLocation > / var / lib / cassandra / callouts < / CalloutLocation > 
 + < StagingFileDirectory > / var / lib / cassandra / staging < / StagingFileDirectory > 
 + 
 + 
 + < ! - - 
 + ~ Addresses of hosts that are deemed contact points . Cassandra nodes 
 + ~ use this list of hosts to find each other and learn the topology of 
 + ~ the ring . You must change this if you are running multiple nodes ! 
 + - - > 
 + < Seeds > 
 + < Seed > 127 . 0 . 0 . 1 < / Seed > 
 + < / Seeds > 
 + 
 + 
 + < ! - - Miscellaneous - - > 
 + 
 + < ! - - Time to wait for a reply from other nodes before failing the command - - > 
 + < RpcTimeoutInMillis > 5000 < / RpcTimeoutInMillis > 
 + < ! - - Size to allow commitlog to grow to before creating a new segment - - > 
 + < CommitLogRotationThresholdInMB > 128 < / CommitLogRotationThresholdInMB > 
 + 
 + 
 + < ! - - Local hosts and ports - - > 
 + 
 + < ! - - 
 + ~ Address to bind to and tell other nodes to connect to . You _ must _ 
 + ~ change this if you want multiple nodes to be able to communicate ! 
 + ~ 
 + ~ Leaving it blank leaves it up to InetAddress . getLocalHost ( ) . This 
 + ~ will always do the Right Thing * if * the node is properly configured 
 + ~ ( hostname , name resolution , etc ) , and the Right Thing is to use the 
 + ~ address associated with the hostname ( it might not be ) . 
 + - - > 
 + < ListenAddress > 127 . 0 . 0 . 2 < / ListenAddress > 
 + < ! - - internal communications port - - > 
 + < StoragePort > 7000 < / StoragePort > 
 + 
 + < ! - - 
 + ~ The address to bind the Thrift RPC service to . Unlike ListenAddress 
 + ~ above , you * can * specify 0 . 0 . 0 . 0 here if you want Thrift to listen on 
 + ~ all interfaces . 
 + ~ 
 + ~ Leaving this blank has the same effect it does for ListenAddress , 
 + ~ ( i . e . it will be based on the configured hostname of the node ) . 
 + - - > 
 + < ThriftAddress > 127 . 0 . 0 . 2 < / ThriftAddress > 
 + < ! - - Thrift RPC port ( the port clients connect to ) . - - > 
 + < ThriftPort > 9160 < / ThriftPort > 
 + < ! - - 
 + ~ Whether or not to use a framed transport for Thrift . If this option 
 + ~ is set to true then you must also use a framed transport on the 
 + ~ client - side , ( framed and non - framed transports are not compatible ) . 
 + - - > 
 + < ThriftFramedTransport > false < / ThriftFramedTransport > 
 + 
 + 
 + < ! - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - > 
 + < ! - - Memory , Disk , and Performance - - > 
 + < ! - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - > 
 + 
 + < ! - - 
 + ~ Access mode . mmapped i / o is substantially faster , but only practical on 
 + ~ a 64bit machine ( which notably does not include EC2 " small " instances ) 
 + ~ or relatively small datasets . " auto " , the safe choice , will enable 
 + ~ mmapping on a 64bit JVM . Other values are " mmap " , " mmap _ index _ only " 
 + ~ ( which may allow you to get part of the benefits of mmap on a 32bit 
 + ~ machine by mmapping only index files ) and " standard " . 
 + ~ ( The buffer size settings that follow only apply to standard , 
 + ~ non - mmapped i / o . ) 
 + - - > 
 + < DiskAccessMode > auto < / DiskAccessMode > 
 + 
 + < ! - - 
 + ~ Buffer size to use when performing contiguous column slices . Increase 
 + ~ this to the size of the column slices you typically perform . 
 + ~ ( Name - based queries are performed with a buffer size of 
 + ~ ColumnIndexSizeInKB . ) 
 + - - > 
 + < SlicedBufferSizeInKB > 64 < / SlicedBufferSizeInKB > 
 + 
 + < ! - - 
 + ~ Buffer size to use when flushing memtables to disk . ( Only one 
 + ~ memtable is ever flushed at a time . ) Increase ( decrease ) the index 
 + ~ buffer size relative to the data buffer if you have few ( many ) 
 + ~ columns per key . Bigger is only better _ if _ your memtables get large 
 + ~ enough to use the space . ( Check in your data directory after your 
 + ~ app has been running long enough . ) - - > 
 + < FlushDataBufferSizeInMB > 32 < / FlushDataBufferSizeInMB > 
 + < FlushIndexBufferSizeInMB > 8 < / FlushIndexBufferSizeInMB > 
 + 
 + < ! - - 
 + ~ Add column indexes to a row after its contents reach this size . 
 + ~ Increase if your column values are large , or if you have a very large 
 + ~ number of columns . The competing causes are , Cassandra has to 
 + ~ deserialize this much of the row to read a single column , so you want 
 + ~ it to be small - at least if you do many partial - row reads - but all 
 + ~ the index data is read for each access , so you don ' t want to generate 
 + ~ that wastefully either . 
 + - - > 
 + < ColumnIndexSizeInKB > 64 < / ColumnIndexSizeInKB > 
 + 
 + < ! - - 
 + ~ Flush memtable after this much data has been inserted , including 
 + ~ overwritten data . There is one memtable per column family , and 
 + ~ this threshold is based solely on the amount of data stored , not 
 + ~ actual heap memory usage ( there is some overhead in indexing the 
 + ~ columns ) . 
 + - - > 
 + < MemtableThroughputInMB > 64 < / MemtableThroughputInMB > 
 + < ! - - 
 + ~ Throughput setting for Binary Memtables . Typically these are 
 + ~ used for bulk load so you want them to be larger . 
 + - - > 
 + < BinaryMemtableThroughputInMB > 256 < / BinaryMemtableThroughputInMB > 
 + < ! - - 
 + ~ The maximum number of columns in millions to store in memory per 
 + ~ ColumnFamily before flushing to disk . This is also a per - memtable 
 + ~ setting . Use with MemtableThroughputInMB to tune memory usage . 
 + - - > 
 + < MemtableOperationsInMillions > 0 . 1 < / MemtableOperationsInMillions > 
 + < ! - - 
 + ~ The maximum time to leave a dirty memtable unflushed . 
 + ~ ( While any affected columnfamilies have unflushed data from a 
 + ~ commit log segment , that segment cannot be deleted . ) 
 + ~ This needs to be large enough that it won ' t cause a flush storm 
 + ~ of all your memtables flushing at once because none has hit 
 + ~ the size or count thresholds yet . For production , a larger 
 + ~ value such as 1440 is recommended . 
 + - - > 
 + < MemtableFlushAfterMinutes > 60 < / MemtableFlushAfterMinutes > 
 + 
 + < ! - - 
 + ~ Unlike most systems , in Cassandra writes are faster than reads , so 
 + ~ you can afford more of those in parallel . A good rule of thumb is 2 
 + ~ concurrent reads per processor core . Increase ConcurrentWrites to 
 + ~ the number of clients writing at once if you enable CommitLogSync + 
 + ~ CommitLogSyncDelay . - - > 
 + < ConcurrentReads > 8 < / ConcurrentReads > 
 + < ConcurrentWrites > 32 < / ConcurrentWrites > 
 + 
 + < ! - - 
 + ~ CommitLogSync may be either " periodic " or " batch . " When in batch 
 + ~ mode , Cassandra won ' t ack writes until the commit log has been 
 + ~ fsynced to disk . It will wait up to CommitLogSyncBatchWindowInMS 
 + ~ milliseconds for other writes , before performing the sync . 
 + 
 + ~ This is less necessary in Cassandra than in traditional databases 
 + ~ since replication reduces the odds of losing data from a failure 
 + ~ after writing the log entry but before it actually reaches the disk . 
 + ~ So the other option is " timed , " where writes may be acked immediately 
 + ~ and the CommitLog is simply synced every CommitLogSyncPeriodInMS 
 + ~ milliseconds . 
 + - - > 
 + < CommitLogSync > periodic < / CommitLogSync > 
 + < ! - - 
 + ~ Interval at which to perform syncs of the CommitLog in periodic mode . 
 + ~ Usually the default of 10000ms is fine ; increase it if your i / o 
 + ~ load is such that syncs are taking excessively long times . 
 + - - > 
 + < CommitLogSyncPeriodInMS > 10000 < / CommitLogSyncPeriodInMS > 
 + < ! - - 
 + ~ Delay ( in milliseconds ) during which additional commit log entries 
 + ~ may be written before fsync in batch mode . This will increase 
 + ~ latency slightly , but can vastly improve throughput where there are 
 + ~ many writers . Set to zero to disable ( each entry will be synced 
 + ~ individually ) . Reasonable values range from a minimal 0 . 1 to 10 or 
 + ~ even more if throughput matters more than latency . 
 + - - > 
 + < ! - - < CommitLogSyncBatchWindowInMS > 1 < / CommitLogSyncBatchWindowInMS > - - > 
 + 
 + < ! - - 
 + ~ Time to wait before garbage - collection deletion markers . Set this to 
 + ~ a large enough value that you are confident that the deletion marker 
 + ~ will be propagated to all replicas by the time this many seconds has 
 + ~ elapsed , even in the face of hardware failures . The default value is 
 + ~ ten days . 
 + - - > 
 + < GCGraceSeconds > 864000 < / GCGraceSeconds > 
 + < / Storage > 
 diff - - git a / ivy . xml b / ivy . xml 
 index b85534c . . 02a264d 100644 
 - - - a / ivy . xml 
 + + + b / ivy . xml 
 @ @ - 19 , 8 + 19 , 11 @ @ 
 < ivy - module version = " 2 . 0 " > 
 < info organisation = " apache - cassandra " module = " cassandra " / > 
 < dependencies > 
 - < dependency org = " org . apache . mahout . hadoop " 
 - name = " hadoop - core " rev = " 0 . 20 . 1 " / > 
 + < ! - - for hadoop - - > 
 + < dependency org = " commons - logging " name = " commons - logging " rev = " 1 . 1 . 1 " / > 
 + < dependency org = " org . apache . mahout . hadoop " name = " hadoop - core " rev = " 0 . 20 . 1 " / > 
 + < dependency org = " commons - httpclient " name = " commons - httpclient " rev = " 3 . 1 " / > 
 + 
 < ! - - FIXME : paranamer and jackson can be dropped after we ' re depending 
 on avro ( since it depends on them ) . - - > 
 < dependency org = " com . thoughtworks . paranamer " 
 diff - - git a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 index d3898b1 . . a83bdf0 100644 
 - - - a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 + + + b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 @ @ - 42 , 6 + 42 , 7 @ @ import java . lang . reflect . InvocationTargetException ; 
 import java . util . * ; 
 import java . net . InetAddress ; 
 import java . net . UnknownHostException ; 
 + import java . net . URL ; 
 
 public class DatabaseDescriptor 
 { 
 @ @ - 145 , 11 + 146 , 29 @ @ public class DatabaseDescriptor 
 
 private static IAuthenticator authenticator = new AllowAllAuthenticator ( ) ; 
 
 + private final static String STORAGE _ CONF _ FILE = " storage - conf . xml " ; 
 + 
 + / * * 
 + * Try the storage - config system property , and then inspect the classpath . 
 + * / 
 + static String getStorageConfigPath ( ) 
 + { 
 + String scp = System . getProperty ( " storage - config " ) + File . separator + STORAGE _ CONF _ FILE ; 
 + if ( new File ( scp ) . exists ( ) ) 
 + return scp ; 
 + / / try the classpath 
 + ClassLoader loader = DatabaseDescriptor . class . getClassLoader ( ) ; 
 + URL scpurl = loader . getResource ( STORAGE _ CONF _ FILE ) ; 
 + if ( scpurl ! = null ) 
 + return scpurl . getFile ( ) ; 
 + throw new RuntimeException ( " Cannot locate " + STORAGE _ CONF _ FILE + " via storage - config system property or classpath lookup . " ) ; 
 + } 
 + 
 static 
 { 
 try 
 { 
 - configFileName _ = System . getProperty ( " storage - config " ) + File . separator + " storage - conf . xml " ; 
 + configFileName _ = getStorageConfigPath ( ) ; 
 if ( logger _ . isDebugEnabled ( ) ) 
 logger _ . debug ( " Loading settings from " + configFileName _ ) ; 
 XMLUtils xmlUtils = new XMLUtils ( configFileName _ ) ; 
 diff - - git a / src / java / org / apache / cassandra / db / RangeSliceCommand . java b / src / java / org / apache / cassandra / db / RangeSliceCommand . java 
 index d279f62 . . 07fd5e0 100644 
 - - - a / src / java / org / apache / cassandra / db / RangeSliceCommand . java 
 + + + b / src / java / org / apache / cassandra / db / RangeSliceCommand . java 
 @ @ - 39 , 7 + 39 , 6 @ @ package org . apache . cassandra . db ; 
 import org . apache . cassandra . concurrent . StageManager ; 
 
 import org . apache . cassandra . dht . AbstractBounds ; 
 - import org . apache . cassandra . dht . Bounds ; 
 import org . apache . cassandra . io . util . DataOutputBuffer ; 
 import org . apache . cassandra . io . ICompactSerializer ; 
 import org . apache . cassandra . net . Message ; 
 @ @ - 129 , 7 + 128 , 7 @ @ class RangeSliceCommandSerializer implements ICompactSerializer < RangeSliceComman 
 
 TSerializer ser = new TSerializer ( new TBinaryProtocol . Factory ( ) ) ; 
 FBUtilities . serialize ( ser , sliceCommand . predicate , dos ) ; 
 - Bounds . serializer ( ) . serialize ( sliceCommand . range , dos ) ; 
 + AbstractBounds . serializer ( ) . serialize ( sliceCommand . range , dos ) ; 
 dos . writeInt ( sliceCommand . max _ keys ) ; 
 } 

