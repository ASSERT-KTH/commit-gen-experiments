BLEU SCORE: 0.027611988917697356

TEST MSG: Fix paging for COMPACT tables without clustering columns
GENERATED MSG: Paging filter empty rows too aggressively

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 50bc894 . . 113da17 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 5 + 1 , 6 @ @ <nl> 2 . 1 . 14 <nl> - * Fix out - of - space error treatment in memtable flushing ( CASSANDRA - 11448 ) . <nl> + * Fix paging for COMPACT tables without clustering columns ( CASSANDRA - 11467 ) <nl> + * Fix out - of - space error treatment in memtable flushing ( CASSANDRA - 11448 ) <nl> * Backport CASSANDRA - 10859 ( CASSANDRA - 11415 ) <nl> * COPY FROM fails when importing blob ( CASSANDRA - 11375 ) <nl> * Backport CASSANDRA - 10679 ( CASSANDRA - 9598 ) <nl> diff - - git a / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java b / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java <nl> index 6056b9a . . 8bbf6d6 100644 <nl> - - - a / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java <nl> + + + b / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java <nl> @ @ - 359 , 6 + 359 , 9 @ @ abstract class AbstractQueryPager implements QueryPager <nl> / / paging and a deletion ( pretty unlikely ) , so this is probably acceptable . <nl> int liveCount = columnCounter ( ) . countAll ( cf ) . live ( ) ; <nl> <nl> + if ( liveCount = = toDiscard ) <nl> + return toDiscard ; <nl> + <nl> ColumnCounter counter = columnCounter ( ) ; <nl> / / Discard the last ' toDiscard ' live ( so stop adding as sound as we ' re past ' liveCount - toDiscard ' ) <nl> while ( iter . hasNext ( ) )
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index fb4f3f4 . . 47ff752 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 3 , 7 + 3 , 6 @ @ <nl> * ( Hadoop ) Fix CQLRW for thrift tables ( CASSANDRA - 6002 ) <nl> * Fix possible divide - by - zero in HHOM ( CASSANDRA - 5990 ) <nl> * Allow local batchlog writes for CL . ANY ( CASSANDRA - 5967 ) <nl> - * Optimize name query performance in wide rows ( CASSANDRA - 5966 ) <nl> * Upgrade metrics - core to version 2 . 2 . 0 ( CASSANDRA - 5947 ) <nl> * Add snitch , schema version , cluster , partitioner to JMX ( CASSANDRA - 5881 ) <nl> * Fix CqlRecordWriter with composite keys ( CASSANDRA - 5949 ) <nl> diff - - git a / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java b / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java <nl> index 40934d4 . . df28c46 100644 <nl> - - - a / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java <nl> + + + b / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java <nl> @ @ - 153 , 7 + 153 , 7 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement <nl> List < OnDiskAtom > result = new ArrayList < OnDiskAtom > ( ) ; <nl> if ( indexList . isEmpty ( ) ) <nl> { <nl> - readSimpleColumns ( sstable . metadata , file , columns , result ) ; <nl> + readSimpleColumns ( file , columns , result ) ; <nl> } <nl> else <nl> { <nl> @ @ - 175 , 37 + 175 , 27 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement <nl> iter = result . iterator ( ) ; <nl> } <nl> <nl> - private void readSimpleColumns ( CFMetaData metadata , <nl> - FileDataInput file , <nl> - SortedSet < ByteBuffer > columnNames , <nl> - List < OnDiskAtom > result ) <nl> - throws IOException <nl> + private void readSimpleColumns ( FileDataInput file , SortedSet < ByteBuffer > columnNames , List < OnDiskAtom > result ) throws IOException <nl> { <nl> - AbstractType < ? > comparator = metadata . comparator ; <nl> OnDiskAtom . Serializer atomSerializer = cf . getOnDiskSerializer ( ) ; <nl> - ByteBuffer maximalColumnName = columnNames . last ( ) ; <nl> int count = file . readInt ( ) ; <nl> - <nl> + int n = 0 ; <nl> for ( int i = 0 ; i < count ; i + + ) <nl> { <nl> OnDiskAtom column = atomSerializer . deserializeFromSSTable ( file , sstable . descriptor . version ) ; <nl> - ByteBuffer columnName = column . name ( ) ; <nl> - <nl> if ( column instanceof IColumn ) <nl> { <nl> - if ( columnNames . contains ( columnName ) ) <nl> + if ( columnNames . contains ( column . name ( ) ) ) <nl> { <nl> result . add ( column ) ; <nl> + if ( + + n > = columns . size ( ) ) <nl> + break ; <nl> } <nl> } <nl> else <nl> { <nl> result . add ( column ) ; <nl> } <nl> - <nl> - / / Already consumed all of this block that ' s going to have columns that apply to this query . <nl> - if ( comparator . compare ( columnName , maximalColumnName ) > = 0 ) <nl> - break ; <nl> } <nl> } <nl> <nl> @ @ - 241 , 12 + 231 , 6 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement <nl> { <nl> long positionToSeek = basePosition + indexInfo . offset ; <nl> <nl> - / / SortedSet . subSet ( ) is end - exclusive , so we special - case that <nl> - / / if it ' s one of the columns we ' re looking for <nl> - ByteBuffer maximalColumnName = columnNames . contains ( indexInfo . lastName ) <nl> - ? indexInfo . lastName <nl> - : columnNames . subSet ( indexInfo . firstName , indexInfo . lastName ) . last ( ) ; <nl> - <nl> / / With new promoted indexes , our first seek in the data file will happen at that point . <nl> if ( file = = null ) <nl> file = createFileDataInput ( positionToSeek ) ; <nl> @ @ - 254 , 20 + 238 , 13 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement <nl> OnDiskAtom . Serializer atomSerializer = cf . getOnDiskSerializer ( ) ; <nl> file . seek ( positionToSeek ) ; <nl> FileMark mark = file . mark ( ) ; <nl> - <nl> / / TODO only completely deserialize columns we are interested in <nl> while ( file . bytesPastMark ( mark ) < indexInfo . width ) <nl> { <nl> OnDiskAtom column = atomSerializer . deserializeFromSSTable ( file , sstable . descriptor . version ) ; <nl> - ByteBuffer columnName = column . name ( ) ; <nl> - <nl> / / we check vs the original Set , not the filtered List , for efficiency <nl> - if ( ! ( column instanceof IColumn ) | | columnNames . contains ( columnName ) ) <nl> + if ( ! ( column instanceof IColumn ) | | columnNames . contains ( column . name ( ) ) ) <nl> result . add ( column ) ; <nl> - <nl> - / / Already consumed all of this block that ' s going to have columns that apply to this query . <nl> - if ( comparator . compare ( columnName , maximalColumnName ) > = 0 ) <nl> - break ; <nl> } <nl> } <nl> }

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 50bc894 . . 113da17 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 5 + 1 , 6 @ @ 
 2 . 1 . 14 
 - * Fix out - of - space error treatment in memtable flushing ( CASSANDRA - 11448 ) . 
 + * Fix paging for COMPACT tables without clustering columns ( CASSANDRA - 11467 ) 
 + * Fix out - of - space error treatment in memtable flushing ( CASSANDRA - 11448 ) 
 * Backport CASSANDRA - 10859 ( CASSANDRA - 11415 ) 
 * COPY FROM fails when importing blob ( CASSANDRA - 11375 ) 
 * Backport CASSANDRA - 10679 ( CASSANDRA - 9598 ) 
 diff - - git a / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java b / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java 
 index 6056b9a . . 8bbf6d6 100644 
 - - - a / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java 
 + + + b / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java 
 @ @ - 359 , 6 + 359 , 9 @ @ abstract class AbstractQueryPager implements QueryPager 
 / / paging and a deletion ( pretty unlikely ) , so this is probably acceptable . 
 int liveCount = columnCounter ( ) . countAll ( cf ) . live ( ) ; 
 
 + if ( liveCount = = toDiscard ) 
 + return toDiscard ; 
 + 
 ColumnCounter counter = columnCounter ( ) ; 
 / / Discard the last ' toDiscard ' live ( so stop adding as sound as we ' re past ' liveCount - toDiscard ' ) 
 while ( iter . hasNext ( ) )

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index fb4f3f4 . . 47ff752 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 3 , 7 + 3 , 6 @ @ 
 * ( Hadoop ) Fix CQLRW for thrift tables ( CASSANDRA - 6002 ) 
 * Fix possible divide - by - zero in HHOM ( CASSANDRA - 5990 ) 
 * Allow local batchlog writes for CL . ANY ( CASSANDRA - 5967 ) 
 - * Optimize name query performance in wide rows ( CASSANDRA - 5966 ) 
 * Upgrade metrics - core to version 2 . 2 . 0 ( CASSANDRA - 5947 ) 
 * Add snitch , schema version , cluster , partitioner to JMX ( CASSANDRA - 5881 ) 
 * Fix CqlRecordWriter with composite keys ( CASSANDRA - 5949 ) 
 diff - - git a / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java b / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java 
 index 40934d4 . . df28c46 100644 
 - - - a / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java 
 + + + b / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java 
 @ @ - 153 , 7 + 153 , 7 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement 
 List < OnDiskAtom > result = new ArrayList < OnDiskAtom > ( ) ; 
 if ( indexList . isEmpty ( ) ) 
 { 
 - readSimpleColumns ( sstable . metadata , file , columns , result ) ; 
 + readSimpleColumns ( file , columns , result ) ; 
 } 
 else 
 { 
 @ @ - 175 , 37 + 175 , 27 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement 
 iter = result . iterator ( ) ; 
 } 
 
 - private void readSimpleColumns ( CFMetaData metadata , 
 - FileDataInput file , 
 - SortedSet < ByteBuffer > columnNames , 
 - List < OnDiskAtom > result ) 
 - throws IOException 
 + private void readSimpleColumns ( FileDataInput file , SortedSet < ByteBuffer > columnNames , List < OnDiskAtom > result ) throws IOException 
 { 
 - AbstractType < ? > comparator = metadata . comparator ; 
 OnDiskAtom . Serializer atomSerializer = cf . getOnDiskSerializer ( ) ; 
 - ByteBuffer maximalColumnName = columnNames . last ( ) ; 
 int count = file . readInt ( ) ; 
 - 
 + int n = 0 ; 
 for ( int i = 0 ; i < count ; i + + ) 
 { 
 OnDiskAtom column = atomSerializer . deserializeFromSSTable ( file , sstable . descriptor . version ) ; 
 - ByteBuffer columnName = column . name ( ) ; 
 - 
 if ( column instanceof IColumn ) 
 { 
 - if ( columnNames . contains ( columnName ) ) 
 + if ( columnNames . contains ( column . name ( ) ) ) 
 { 
 result . add ( column ) ; 
 + if ( + + n > = columns . size ( ) ) 
 + break ; 
 } 
 } 
 else 
 { 
 result . add ( column ) ; 
 } 
 - 
 - / / Already consumed all of this block that ' s going to have columns that apply to this query . 
 - if ( comparator . compare ( columnName , maximalColumnName ) > = 0 ) 
 - break ; 
 } 
 } 
 
 @ @ - 241 , 12 + 231 , 6 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement 
 { 
 long positionToSeek = basePosition + indexInfo . offset ; 
 
 - / / SortedSet . subSet ( ) is end - exclusive , so we special - case that 
 - / / if it ' s one of the columns we ' re looking for 
 - ByteBuffer maximalColumnName = columnNames . contains ( indexInfo . lastName ) 
 - ? indexInfo . lastName 
 - : columnNames . subSet ( indexInfo . firstName , indexInfo . lastName ) . last ( ) ; 
 - 
 / / With new promoted indexes , our first seek in the data file will happen at that point . 
 if ( file = = null ) 
 file = createFileDataInput ( positionToSeek ) ; 
 @ @ - 254 , 20 + 238 , 13 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement 
 OnDiskAtom . Serializer atomSerializer = cf . getOnDiskSerializer ( ) ; 
 file . seek ( positionToSeek ) ; 
 FileMark mark = file . mark ( ) ; 
 - 
 / / TODO only completely deserialize columns we are interested in 
 while ( file . bytesPastMark ( mark ) < indexInfo . width ) 
 { 
 OnDiskAtom column = atomSerializer . deserializeFromSSTable ( file , sstable . descriptor . version ) ; 
 - ByteBuffer columnName = column . name ( ) ; 
 - 
 / / we check vs the original Set , not the filtered List , for efficiency 
 - if ( ! ( column instanceof IColumn ) | | columnNames . contains ( columnName ) ) 
 + if ( ! ( column instanceof IColumn ) | | columnNames . contains ( column . name ( ) ) ) 
 result . add ( column ) ; 
 - 
 - / / Already consumed all of this block that ' s going to have columns that apply to this query . 
 - if ( comparator . compare ( columnName , maximalColumnName ) > = 0 ) 
 - break ; 
 } 
 } 
 }
