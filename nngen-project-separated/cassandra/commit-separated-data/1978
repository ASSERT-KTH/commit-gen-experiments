BLEU SCORE: 0.028465126651392333

TEST MSG: Handle abort ( ) properly in SSTableRewriter
GENERATED MSG: Make sure unfinished compaction files are removed .

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index f022b19 . . e5f7c28 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 2 . 1 . 3 <nl> + * Handle abort ( ) in SSTableRewriter properly ( CASSANDRA - 8320 ) <nl> * Fix high size calculations for prepared statements ( CASSANDRA - 8231 ) <nl> * Centralize shared executors ( CASSANDRA - 8055 ) <nl> * Fix filtering for CONTAINS ( KEY ) relations on frozen collection <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> index a3e3cf5 . . 1fe4330 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> @ @ - 202 , 6 + 202 , 7 @ @ public class SSTableReader extends SSTable <nl> private Object replaceLock = new Object ( ) ; <nl> private SSTableReader replacedBy ; <nl> private SSTableReader replaces ; <nl> + private SSTableReader sharesBfWith ; <nl> private SSTableDeletingTask deletingTask ; <nl> private Runnable runOnClose ; <nl> <nl> @ @ - 594 , 6 + 595 , 14 @ @ public class SSTableReader extends SSTable <nl> deleteFiles & = ! dfile . path . equals ( replaces . dfile . path ) ; <nl> } <nl> <nl> + if ( sharesBfWith ! = null ) <nl> + { <nl> + closeBf & = sharesBfWith . bf ! = bf ; <nl> + closeSummary & = sharesBfWith . indexSummary ! = indexSummary ; <nl> + closeFiles & = sharesBfWith . dfile ! = dfile ; <nl> + deleteFiles & = ! dfile . path . equals ( sharesBfWith . dfile . path ) ; <nl> + } <nl> + <nl> boolean deleteAll = false ; <nl> if ( release & & isCompacted . get ( ) ) <nl> { <nl> @ @ - 928 , 6 + 937 , 19 @ @ public class SSTableReader extends SSTable <nl> } <nl> } <nl> <nl> + / * * <nl> + * this is used to avoid closing the bloom filter multiple times when finishing an SSTableRewriter <nl> + * <nl> + * note that the reason we don ' t use replacedBy is that we are not yet actually replaced <nl> + * <nl> + * @ param newReader <nl> + * / <nl> + public void sharesBfWith ( SSTableReader newReader ) <nl> + { <nl> + assert openReason . equals ( OpenReason . EARLY ) ; <nl> + this . sharesBfWith = newReader ; <nl> + } <nl> + <nl> public SSTableReader cloneWithNewStart ( DecoratedKey newStart , final Runnable runOnClose ) <nl> { <nl> synchronized ( replaceLock ) <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableRewriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableRewriter . java <nl> index 4d5a06f . . d187e9d 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableRewriter . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableRewriter . java <nl> @ @ - 20 , 6 + 20 , 7 @ @ package org . apache . cassandra . io . sstable ; <nl> import java . util . ArrayList ; <nl> import java . util . Collections ; <nl> import java . util . HashMap ; <nl> + import java . util . Iterator ; <nl> import java . util . List ; <nl> import java . util . Map ; <nl> import java . util . Set ; <nl> @ @ - 75 , 6 + 76 , 7 @ @ public class SSTableRewriter <nl> private final ColumnFamilyStore cfs ; <nl> <nl> private final long maxAge ; <nl> + private final List < SSTableReader > finished = new ArrayList < > ( ) ; <nl> private final Set < SSTableReader > rewriting ; / / the readers we are rewriting ( updated as they are replaced ) <nl> private final Map < Descriptor , DecoratedKey > originalStarts = new HashMap < > ( ) ; / / the start key for each reader we are rewriting <nl> private final Map < Descriptor , Integer > fileDescriptors = new HashMap < > ( ) ; / / the file descriptors for each reader descriptor we are rewriting <nl> @ @ - 180 , 16 + 182 , 11 @ @ public class SSTableRewriter <nl> <nl> public void abort ( ) <nl> { <nl> - if ( writer = = null ) <nl> - return ; <nl> - <nl> switchWriter ( null ) ; <nl> <nl> moveStarts ( null , Functions . forMap ( originalStarts ) , true ) ; <nl> <nl> List < SSTableReader > close = Lists . newArrayList ( finishedOpenedEarly ) ; <nl> - if ( currentlyOpenedEarly ! = null ) <nl> - close . add ( currentlyOpenedEarly ) ; <nl> <nl> for ( Pair < SSTableWriter , SSTableReader > w : finishedWriters ) <nl> { <nl> @ @ - 202 , 6 + 199 , 12 @ @ public class SSTableRewriter <nl> for ( SSTableReader sstable : close ) <nl> sstable . markObsolete ( ) ; <nl> <nl> + for ( SSTableReader sstable : finished ) <nl> + { <nl> + sstable . markObsolete ( ) ; <nl> + sstable . releaseReference ( ) ; <nl> + } <nl> + <nl> / / releases reference in replaceReaders <nl> if ( ! isOffline ) <nl> { <nl> @ @ - 210 , 6 + 213 , 7 @ @ public class SSTableRewriter <nl> } <nl> } <nl> <nl> + <nl> / * * <nl> * Replace the readers we are rewriting with cloneWithNewStart , reclaiming any page cache that is no longer <nl> * needed , and transferring any key cache entries over to the new reader , expiring them from the old . if reset <nl> @ @ - 327 , 38 + 331 , 70 @ @ public class SSTableRewriter <nl> * / <nl> public List < SSTableReader > finish ( long repairedAt ) <nl> { <nl> - List < SSTableReader > finished = new ArrayList < > ( ) ; <nl> - if ( writer . getFilePointer ( ) > 0 ) <nl> - { <nl> - SSTableReader reader = repairedAt < 0 ? writer . closeAndOpenReader ( maxAge ) : writer . closeAndOpenReader ( maxAge , repairedAt ) ; <nl> - finished . add ( reader ) ; <nl> - replaceEarlyOpenedFile ( currentlyOpenedEarly , reader ) ; <nl> - moveStarts ( reader , Functions . constant ( reader . last ) , false ) ; <nl> - } <nl> - else <nl> - { <nl> - writer . abort ( true ) ; <nl> - } <nl> + List < Pair < SSTableReader , SSTableReader > > toReplace = new ArrayList < > ( ) ; <nl> + switchWriter ( null ) ; <nl> / / make real sstables of the written ones : <nl> - for ( Pair < SSTableWriter , SSTableReader > w : finishedWriters ) <nl> + Iterator < Pair < SSTableWriter , SSTableReader > > it = finishedWriters . iterator ( ) ; <nl> + while ( it . hasNext ( ) ) <nl> { <nl> + Pair < SSTableWriter , SSTableReader > w = it . next ( ) ; <nl> if ( w . left . getFilePointer ( ) > 0 ) <nl> { <nl> SSTableReader newReader = repairedAt < 0 ? w . left . closeAndOpenReader ( maxAge ) : w . left . closeAndOpenReader ( maxAge , repairedAt ) ; <nl> finished . add ( newReader ) ; <nl> + <nl> + if ( w . right ! = null ) <nl> + w . right . sharesBfWith ( newReader ) ; <nl> / / w . right is the tmplink - reader we added when switching writer , replace with the real sstable . <nl> - replaceEarlyOpenedFile ( w . right , newReader ) ; <nl> + toReplace . add ( Pair . create ( w . right , newReader ) ) ; <nl> } <nl> else <nl> { <nl> assert w . right = = null ; <nl> w . left . abort ( true ) ; <nl> } <nl> + it . remove ( ) ; <nl> } <nl> + <nl> + for ( Pair < SSTableReader , SSTableReader > replace : toReplace ) <nl> + replaceEarlyOpenedFile ( replace . left , replace . right ) ; <nl> + <nl> if ( ! isOffline ) <nl> { <nl> dataTracker . unmarkCompacting ( finished ) ; <nl> } <nl> return finished ; <nl> } <nl> + <nl> + @ VisibleForTesting <nl> + void finishAndThrow ( boolean early ) <nl> + { <nl> + List < Pair < SSTableReader , SSTableReader > > toReplace = new ArrayList < > ( ) ; <nl> + switchWriter ( null ) ; <nl> + if ( early ) <nl> + throw new RuntimeException ( " exception thrown early in finish " ) ; <nl> + / / make real sstables of the written ones : <nl> + Iterator < Pair < SSTableWriter , SSTableReader > > it = finishedWriters . iterator ( ) ; <nl> + while ( it . hasNext ( ) ) <nl> + { <nl> + Pair < SSTableWriter , SSTableReader > w = it . next ( ) ; <nl> + if ( w . left . getFilePointer ( ) > 0 ) <nl> + { <nl> + SSTableReader newReader = w . left . closeAndOpenReader ( maxAge ) ; <nl> + finished . add ( newReader ) ; <nl> + if ( w . right ! = null ) <nl> + w . right . sharesBfWith ( newReader ) ; <nl> + / / w . right is the tmplink - reader we added when switching writer , replace with the real sstable . <nl> + toReplace . add ( Pair . create ( w . right , newReader ) ) ; <nl> + } <nl> + else <nl> + { <nl> + assert w . right = = null ; <nl> + w . left . abort ( true ) ; <nl> + } <nl> + it . remove ( ) ; <nl> + } <nl> + <nl> + throw new RuntimeException ( " exception thrown after all sstables finished " ) ; <nl> + } <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java <nl> index 8a494a6 . . 0a76b66 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java <nl> @ @ - 21 , 6 + 21 , 7 @ @ import java . io . File ; <nl> import java . nio . ByteBuffer ; <nl> import java . util . Arrays ; <nl> import java . util . Collection ; <nl> + import java . util . Collections ; <nl> import java . util . HashSet ; <nl> import java . util . List ; <nl> import java . util . Set ; <nl> @ @ - 29 , 9 + 30 , 11 @ @ import org . junit . Test ; <nl> <nl> import org . apache . cassandra . SchemaLoader ; <nl> import org . apache . cassandra . Util ; <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . db . ArrayBackedSortedColumns ; <nl> import org . apache . cassandra . db . ColumnFamilyStore ; <nl> import org . apache . cassandra . db . DecoratedKey ; <nl> + import org . apache . cassandra . db . Directories ; <nl> import org . apache . cassandra . db . Keyspace ; <nl> import org . apache . cassandra . db . Mutation ; <nl> import org . apache . cassandra . db . compaction . AbstractCompactedRow ; <nl> @ @ - 80 , 12 + 83 , 45 @ @ public class SSTableRewriterTest extends SchemaLoader <nl> writer . append ( row ) ; <nl> } <nl> } <nl> - cfs . getDataTracker ( ) . markCompactedSSTablesReplaced ( sstables , writer . finish ( ) , OperationType . COMPACTION ) ; <nl> - <nl> + Collection < SSTableReader > newsstables = writer . finish ( ) ; <nl> + cfs . getDataTracker ( ) . markCompactedSSTablesReplaced ( sstables , newsstables , OperationType . COMPACTION ) ; <nl> + Thread . sleep ( 100 ) ; <nl> validateCFS ( cfs ) ; <nl> + int filecounts = assertFileCounts ( sstables . iterator ( ) . next ( ) . descriptor . directory . list ( ) , 0 , 0 ) ; <nl> + assertEquals ( 1 , filecounts ) ; <nl> <nl> } <nl> + @ Test <nl> + public void basicTest2 ( ) throws InterruptedException <nl> + { <nl> + Keyspace keyspace = Keyspace . open ( KEYSPACE ) ; <nl> + ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( CF ) ; <nl> + cfs . truncateBlocking ( ) ; <nl> <nl> + SSTableReader s = writeFile ( cfs , 1000 ) ; <nl> + cfs . addSSTable ( s ) ; <nl> + Set < SSTableReader > sstables = new HashSet < > ( cfs . getSSTables ( ) ) ; <nl> + assertEquals ( 1 , sstables . size ( ) ) ; <nl> + SSTableRewriter . overrideOpenInterval ( 10000000 ) ; <nl> + SSTableRewriter writer = new SSTableRewriter ( cfs , sstables , 1000 , false ) ; <nl> + try ( AbstractCompactionStrategy . ScannerList scanners = cfs . getCompactionStrategy ( ) . getScanners ( sstables ) ; ) <nl> + { <nl> + ICompactionScanner scanner = scanners . scanners . get ( 0 ) ; <nl> + CompactionController controller = new CompactionController ( cfs , sstables , cfs . gcBefore ( System . currentTimeMillis ( ) ) ) ; <nl> + writer . switchWriter ( getWriter ( cfs , sstables . iterator ( ) . next ( ) . descriptor . directory ) ) ; <nl> + while ( scanner . hasNext ( ) ) <nl> + { <nl> + AbstractCompactedRow row = new LazilyCompactedRow ( controller , Arrays . asList ( scanner . next ( ) ) ) ; <nl> + writer . append ( row ) ; <nl> + } <nl> + } <nl> + Collection < SSTableReader > newsstables = writer . finish ( ) ; <nl> + cfs . getDataTracker ( ) . markCompactedSSTablesReplaced ( sstables , newsstables , OperationType . COMPACTION ) ; <nl> + Thread . sleep ( 100 ) ; <nl> + validateCFS ( cfs ) ; <nl> + int filecounts = assertFileCounts ( sstables . iterator ( ) . next ( ) . descriptor . directory . list ( ) , 0 , 0 ) ; <nl> + assertEquals ( 1 , filecounts ) ; <nl> + } <nl> <nl> @ Test <nl> public void testFileRemoval ( ) throws InterruptedException <nl> @ @ - 114 , 37 + 150 , 11 @ @ public class SSTableRewriterTest extends SchemaLoader <nl> assertFileCounts ( dir . list ( ) , 0 , 3 ) ; <nl> writer . abort ( false ) ; <nl> Thread . sleep ( 1000 ) ; <nl> - assertFileCounts ( dir . list ( ) , 0 , 0 ) ; <nl> - validateCFS ( cfs ) ; <nl> - } <nl> - <nl> - @ Test <nl> - public void testFileRemovalNoAbort ( ) throws InterruptedException <nl> - { <nl> - Keyspace keyspace = Keyspace . open ( KEYSPACE ) ; <nl> - ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( CF ) ; <nl> - cfs . truncateBlocking ( ) ; <nl> - ArrayBackedSortedColumns cf = ArrayBackedSortedColumns . factory . create ( cfs . metadata ) ; <nl> - for ( int i = 0 ; i < 1000 ; i + + ) <nl> - cf . addColumn ( Util . column ( String . valueOf ( i ) , " a " , 1 ) ) ; <nl> - File dir = cfs . directories . getDirectoryForNewSSTables ( ) ; <nl> - SSTableWriter writer = getWriter ( cfs , dir ) ; <nl> - <nl> - for ( int i = 0 ; i < 500 ; i + + ) <nl> - writer . append ( StorageService . getPartitioner ( ) . decorateKey ( ByteBufferUtil . bytes ( i ) ) , cf ) ; <nl> - SSTableReader s = writer . openEarly ( 1000 ) ; <nl> - / / assertFileCounts ( dir . list ( ) , 2 , 3 ) ; <nl> - for ( int i = 500 ; i < 1000 ; i + + ) <nl> - writer . append ( StorageService . getPartitioner ( ) . decorateKey ( ByteBufferUtil . bytes ( i ) ) , cf ) ; <nl> - writer . closeAndOpenReader ( ) ; <nl> - s . markObsolete ( ) ; <nl> - s . releaseReference ( ) ; <nl> - Thread . sleep ( 1000 ) ; <nl> - assertFileCounts ( dir . list ( ) , 0 , 0 ) ; <nl> + int datafiles = assertFileCounts ( dir . list ( ) , 0 , 0 ) ; <nl> + assertEquals ( datafiles , 0 ) ; <nl> validateCFS ( cfs ) ; <nl> } <nl> <nl> - <nl> @ Test <nl> public void testNumberOfFilesAndSizes ( ) throws Exception <nl> { <nl> @ @ - 446 , 6 + 456 , 95 @ @ public class SSTableRewriterTest extends SchemaLoader <nl> assertFileCounts ( s . descriptor . directory . list ( ) , 0 , 0 ) ; <nl> validateCFS ( cfs ) ; <nl> } <nl> + @ Test <nl> + public void testAbort ( ) throws Exception <nl> + { <nl> + Keyspace keyspace = Keyspace . open ( KEYSPACE ) ; <nl> + ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( CF ) ; <nl> + cfs . truncateBlocking ( ) ; <nl> + SSTableReader s = writeFile ( cfs , 1000 ) ; <nl> + cfs . addSSTable ( s ) ; <nl> + Set < SSTableReader > compacting = Sets . newHashSet ( s ) ; <nl> + SSTableRewriter . overrideOpenInterval ( 10000000 ) ; <nl> + SSTableRewriter rewriter = new SSTableRewriter ( cfs , compacting , 1000 , false ) ; <nl> + SSTableWriter w = getWriter ( cfs , s . descriptor . directory ) ; <nl> + rewriter . switchWriter ( w ) ; <nl> + try ( ICompactionScanner scanner = compacting . iterator ( ) . next ( ) . getScanner ( ) ; <nl> + CompactionController controller = new CompactionController ( cfs , compacting , 0 ) ) <nl> + { <nl> + while ( scanner . hasNext ( ) ) <nl> + { <nl> + rewriter . append ( new LazilyCompactedRow ( controller , Arrays . asList ( scanner . next ( ) ) ) ) ; <nl> + if ( rewriter . currentWriter ( ) . getOnDiskFilePointer ( ) > 25000000 ) <nl> + { <nl> + rewriter . switchWriter ( getWriter ( cfs , s . descriptor . directory ) ) ; <nl> + } <nl> + } <nl> + try <nl> + { <nl> + rewriter . finishAndThrow ( false ) ; <nl> + } <nl> + catch ( Throwable t ) <nl> + { <nl> + rewriter . abort ( ) ; <nl> + } <nl> + } <nl> + Thread . sleep ( 1000 ) ; <nl> + int filecount = assertFileCounts ( s . descriptor . directory . list ( ) , 0 , 0 ) ; <nl> + assertEquals ( filecount , 1 ) ; <nl> + assertEquals ( 1 , cfs . getSSTables ( ) . size ( ) ) ; <nl> + validateCFS ( cfs ) ; <nl> + cfs . truncateBlocking ( ) ; <nl> + Thread . sleep ( 1000 ) ; <nl> + filecount = assertFileCounts ( s . descriptor . directory . list ( ) , 0 , 0 ) ; <nl> + assertEquals ( 0 , filecount ) ; <nl> + <nl> + } <nl> + <nl> + @ Test <nl> + public void testAbort2 ( ) throws Exception <nl> + { <nl> + Keyspace keyspace = Keyspace . open ( KEYSPACE ) ; <nl> + ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( CF ) ; <nl> + cfs . truncateBlocking ( ) ; <nl> + SSTableReader s = writeFile ( cfs , 1000 ) ; <nl> + cfs . addSSTable ( s ) ; <nl> + Set < SSTableReader > compacting = Sets . newHashSet ( s ) ; <nl> + SSTableRewriter . overrideOpenInterval ( 10000000 ) ; <nl> + SSTableRewriter rewriter = new SSTableRewriter ( cfs , compacting , 1000 , false ) ; <nl> + SSTableWriter w = getWriter ( cfs , s . descriptor . directory ) ; <nl> + rewriter . switchWriter ( w ) ; <nl> + try ( ICompactionScanner scanner = compacting . iterator ( ) . next ( ) . getScanner ( ) ; <nl> + CompactionController controller = new CompactionController ( cfs , compacting , 0 ) ) <nl> + { <nl> + while ( scanner . hasNext ( ) ) <nl> + { <nl> + rewriter . append ( new LazilyCompactedRow ( controller , Arrays . asList ( scanner . next ( ) ) ) ) ; <nl> + if ( rewriter . currentWriter ( ) . getOnDiskFilePointer ( ) > 25000000 ) <nl> + { <nl> + rewriter . switchWriter ( getWriter ( cfs , s . descriptor . directory ) ) ; <nl> + } <nl> + } <nl> + try <nl> + { <nl> + rewriter . finishAndThrow ( true ) ; <nl> + } <nl> + catch ( Throwable t ) <nl> + { <nl> + rewriter . abort ( ) ; <nl> + } <nl> + } <nl> + Thread . sleep ( 1000 ) ; <nl> + int filecount = assertFileCounts ( s . descriptor . directory . list ( ) , 0 , 0 ) ; <nl> + assertEquals ( filecount , 1 ) ; <nl> + assertEquals ( 1 , cfs . getSSTables ( ) . size ( ) ) ; <nl> + validateCFS ( cfs ) ; <nl> + cfs . truncateBlocking ( ) ; <nl> + Thread . sleep ( 1000 ) ; <nl> + filecount = assertFileCounts ( s . descriptor . directory . list ( ) , 0 , 0 ) ; <nl> + assertEquals ( 0 , filecount ) ; <nl> + <nl> + } <nl> <nl> private SSTableReader writeFile ( ColumnFamilyStore cfs , int count ) <nl> { <nl> @ @ - 469 , 28 + 568 , 45 @ @ public class SSTableRewriterTest extends SchemaLoader <nl> <nl> private void validateCFS ( ColumnFamilyStore cfs ) <nl> { <nl> + Set < Integer > liveDescriptors = new HashSet < > ( ) ; <nl> for ( SSTableReader sstable : cfs . getSSTables ( ) ) <nl> { <nl> assertFalse ( sstable . isMarkedCompacted ( ) ) ; <nl> assertEquals ( 1 , sstable . referenceCount ( ) ) ; <nl> + liveDescriptors . add ( sstable . descriptor . generation ) ; <nl> + } <nl> + for ( File dir : cfs . directories . getCFDirectories ( ) ) <nl> + { <nl> + for ( String f : dir . list ( ) ) <nl> + { <nl> + if ( f . contains ( " Data " ) ) <nl> + { <nl> + Descriptor d = Descriptor . fromFilename ( f ) ; <nl> + assertTrue ( d . toString ( ) , liveDescriptors . contains ( d . generation ) ) ; <nl> + } <nl> + } <nl> } <nl> assertTrue ( cfs . getDataTracker ( ) . getCompacting ( ) . isEmpty ( ) ) ; <nl> } <nl> <nl> <nl> - private void assertFileCounts ( String [ ] files , int expectedtmplinkCount , int expectedtmpCount ) <nl> + private int assertFileCounts ( String [ ] files , int expectedtmplinkCount , int expectedtmpCount ) <nl> { <nl> int tmplinkcount = 0 ; <nl> int tmpcount = 0 ; <nl> + int datacount = 0 ; <nl> for ( String f : files ) <nl> { <nl> if ( f . contains ( " - tmplink - " ) ) <nl> tmplinkcount + + ; <nl> - if ( f . contains ( " - tmp - " ) ) <nl> + else if ( f . contains ( " - tmp - " ) ) <nl> tmpcount + + ; <nl> + else if ( f . contains ( " Data " ) ) <nl> + datacount + + ; <nl> } <nl> assertEquals ( expectedtmplinkCount , tmplinkcount ) ; <nl> assertEquals ( expectedtmpCount , tmpcount ) ; <nl> + return datacount ; <nl> } <nl> <nl> private SSTableWriter getWriter ( ColumnFamilyStore cfs , File directory )
NEAREST DIFF (one line): ELIMINATEDSENTENCE

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index f022b19 . . e5f7c28 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 2 . 1 . 3 
 + * Handle abort ( ) in SSTableRewriter properly ( CASSANDRA - 8320 ) 
 * Fix high size calculations for prepared statements ( CASSANDRA - 8231 ) 
 * Centralize shared executors ( CASSANDRA - 8055 ) 
 * Fix filtering for CONTAINS ( KEY ) relations on frozen collection 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 index a3e3cf5 . . 1fe4330 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 @ @ - 202 , 6 + 202 , 7 @ @ public class SSTableReader extends SSTable 
 private Object replaceLock = new Object ( ) ; 
 private SSTableReader replacedBy ; 
 private SSTableReader replaces ; 
 + private SSTableReader sharesBfWith ; 
 private SSTableDeletingTask deletingTask ; 
 private Runnable runOnClose ; 
 
 @ @ - 594 , 6 + 595 , 14 @ @ public class SSTableReader extends SSTable 
 deleteFiles & = ! dfile . path . equals ( replaces . dfile . path ) ; 
 } 
 
 + if ( sharesBfWith ! = null ) 
 + { 
 + closeBf & = sharesBfWith . bf ! = bf ; 
 + closeSummary & = sharesBfWith . indexSummary ! = indexSummary ; 
 + closeFiles & = sharesBfWith . dfile ! = dfile ; 
 + deleteFiles & = ! dfile . path . equals ( sharesBfWith . dfile . path ) ; 
 + } 
 + 
 boolean deleteAll = false ; 
 if ( release & & isCompacted . get ( ) ) 
 { 
 @ @ - 928 , 6 + 937 , 19 @ @ public class SSTableReader extends SSTable 
 } 
 } 
 
 + / * * 
 + * this is used to avoid closing the bloom filter multiple times when finishing an SSTableRewriter 
 + * 
 + * note that the reason we don ' t use replacedBy is that we are not yet actually replaced 
 + * 
 + * @ param newReader 
 + * / 
 + public void sharesBfWith ( SSTableReader newReader ) 
 + { 
 + assert openReason . equals ( OpenReason . EARLY ) ; 
 + this . sharesBfWith = newReader ; 
 + } 
 + 
 public SSTableReader cloneWithNewStart ( DecoratedKey newStart , final Runnable runOnClose ) 
 { 
 synchronized ( replaceLock ) 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableRewriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableRewriter . java 
 index 4d5a06f . . d187e9d 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableRewriter . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableRewriter . java 
 @ @ - 20 , 6 + 20 , 7 @ @ package org . apache . cassandra . io . sstable ; 
 import java . util . ArrayList ; 
 import java . util . Collections ; 
 import java . util . HashMap ; 
 + import java . util . Iterator ; 
 import java . util . List ; 
 import java . util . Map ; 
 import java . util . Set ; 
 @ @ - 75 , 6 + 76 , 7 @ @ public class SSTableRewriter 
 private final ColumnFamilyStore cfs ; 
 
 private final long maxAge ; 
 + private final List < SSTableReader > finished = new ArrayList < > ( ) ; 
 private final Set < SSTableReader > rewriting ; / / the readers we are rewriting ( updated as they are replaced ) 
 private final Map < Descriptor , DecoratedKey > originalStarts = new HashMap < > ( ) ; / / the start key for each reader we are rewriting 
 private final Map < Descriptor , Integer > fileDescriptors = new HashMap < > ( ) ; / / the file descriptors for each reader descriptor we are rewriting 
 @ @ - 180 , 16 + 182 , 11 @ @ public class SSTableRewriter 
 
 public void abort ( ) 
 { 
 - if ( writer = = null ) 
 - return ; 
 - 
 switchWriter ( null ) ; 
 
 moveStarts ( null , Functions . forMap ( originalStarts ) , true ) ; 
 
 List < SSTableReader > close = Lists . newArrayList ( finishedOpenedEarly ) ; 
 - if ( currentlyOpenedEarly ! = null ) 
 - close . add ( currentlyOpenedEarly ) ; 
 
 for ( Pair < SSTableWriter , SSTableReader > w : finishedWriters ) 
 { 
 @ @ - 202 , 6 + 199 , 12 @ @ public class SSTableRewriter 
 for ( SSTableReader sstable : close ) 
 sstable . markObsolete ( ) ; 
 
 + for ( SSTableReader sstable : finished ) 
 + { 
 + sstable . markObsolete ( ) ; 
 + sstable . releaseReference ( ) ; 
 + } 
 + 
 / / releases reference in replaceReaders 
 if ( ! isOffline ) 
 { 
 @ @ - 210 , 6 + 213 , 7 @ @ public class SSTableRewriter 
 } 
 } 
 
 + 
 / * * 
 * Replace the readers we are rewriting with cloneWithNewStart , reclaiming any page cache that is no longer 
 * needed , and transferring any key cache entries over to the new reader , expiring them from the old . if reset 
 @ @ - 327 , 38 + 331 , 70 @ @ public class SSTableRewriter 
 * / 
 public List < SSTableReader > finish ( long repairedAt ) 
 { 
 - List < SSTableReader > finished = new ArrayList < > ( ) ; 
 - if ( writer . getFilePointer ( ) > 0 ) 
 - { 
 - SSTableReader reader = repairedAt < 0 ? writer . closeAndOpenReader ( maxAge ) : writer . closeAndOpenReader ( maxAge , repairedAt ) ; 
 - finished . add ( reader ) ; 
 - replaceEarlyOpenedFile ( currentlyOpenedEarly , reader ) ; 
 - moveStarts ( reader , Functions . constant ( reader . last ) , false ) ; 
 - } 
 - else 
 - { 
 - writer . abort ( true ) ; 
 - } 
 + List < Pair < SSTableReader , SSTableReader > > toReplace = new ArrayList < > ( ) ; 
 + switchWriter ( null ) ; 
 / / make real sstables of the written ones : 
 - for ( Pair < SSTableWriter , SSTableReader > w : finishedWriters ) 
 + Iterator < Pair < SSTableWriter , SSTableReader > > it = finishedWriters . iterator ( ) ; 
 + while ( it . hasNext ( ) ) 
 { 
 + Pair < SSTableWriter , SSTableReader > w = it . next ( ) ; 
 if ( w . left . getFilePointer ( ) > 0 ) 
 { 
 SSTableReader newReader = repairedAt < 0 ? w . left . closeAndOpenReader ( maxAge ) : w . left . closeAndOpenReader ( maxAge , repairedAt ) ; 
 finished . add ( newReader ) ; 
 + 
 + if ( w . right ! = null ) 
 + w . right . sharesBfWith ( newReader ) ; 
 / / w . right is the tmplink - reader we added when switching writer , replace with the real sstable . 
 - replaceEarlyOpenedFile ( w . right , newReader ) ; 
 + toReplace . add ( Pair . create ( w . right , newReader ) ) ; 
 } 
 else 
 { 
 assert w . right = = null ; 
 w . left . abort ( true ) ; 
 } 
 + it . remove ( ) ; 
 } 
 + 
 + for ( Pair < SSTableReader , SSTableReader > replace : toReplace ) 
 + replaceEarlyOpenedFile ( replace . left , replace . right ) ; 
 + 
 if ( ! isOffline ) 
 { 
 dataTracker . unmarkCompacting ( finished ) ; 
 } 
 return finished ; 
 } 
 + 
 + @ VisibleForTesting 
 + void finishAndThrow ( boolean early ) 
 + { 
 + List < Pair < SSTableReader , SSTableReader > > toReplace = new ArrayList < > ( ) ; 
 + switchWriter ( null ) ; 
 + if ( early ) 
 + throw new RuntimeException ( " exception thrown early in finish " ) ; 
 + / / make real sstables of the written ones : 
 + Iterator < Pair < SSTableWriter , SSTableReader > > it = finishedWriters . iterator ( ) ; 
 + while ( it . hasNext ( ) ) 
 + { 
 + Pair < SSTableWriter , SSTableReader > w = it . next ( ) ; 
 + if ( w . left . getFilePointer ( ) > 0 ) 
 + { 
 + SSTableReader newReader = w . left . closeAndOpenReader ( maxAge ) ; 
 + finished . add ( newReader ) ; 
 + if ( w . right ! = null ) 
 + w . right . sharesBfWith ( newReader ) ; 
 + / / w . right is the tmplink - reader we added when switching writer , replace with the real sstable . 
 + toReplace . add ( Pair . create ( w . right , newReader ) ) ; 
 + } 
 + else 
 + { 
 + assert w . right = = null ; 
 + w . left . abort ( true ) ; 
 + } 
 + it . remove ( ) ; 
 + } 
 + 
 + throw new RuntimeException ( " exception thrown after all sstables finished " ) ; 
 + } 
 } 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java 
 index 8a494a6 . . 0a76b66 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java 
 @ @ - 21 , 6 + 21 , 7 @ @ import java . io . File ; 
 import java . nio . ByteBuffer ; 
 import java . util . Arrays ; 
 import java . util . Collection ; 
 + import java . util . Collections ; 
 import java . util . HashSet ; 
 import java . util . List ; 
 import java . util . Set ; 
 @ @ - 29 , 9 + 30 , 11 @ @ import org . junit . Test ; 
 
 import org . apache . cassandra . SchemaLoader ; 
 import org . apache . cassandra . Util ; 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . db . ArrayBackedSortedColumns ; 
 import org . apache . cassandra . db . ColumnFamilyStore ; 
 import org . apache . cassandra . db . DecoratedKey ; 
 + import org . apache . cassandra . db . Directories ; 
 import org . apache . cassandra . db . Keyspace ; 
 import org . apache . cassandra . db . Mutation ; 
 import org . apache . cassandra . db . compaction . AbstractCompactedRow ; 
 @ @ - 80 , 12 + 83 , 45 @ @ public class SSTableRewriterTest extends SchemaLoader 
 writer . append ( row ) ; 
 } 
 } 
 - cfs . getDataTracker ( ) . markCompactedSSTablesReplaced ( sstables , writer . finish ( ) , OperationType . COMPACTION ) ; 
 - 
 + Collection < SSTableReader > newsstables = writer . finish ( ) ; 
 + cfs . getDataTracker ( ) . markCompactedSSTablesReplaced ( sstables , newsstables , OperationType . COMPACTION ) ; 
 + Thread . sleep ( 100 ) ; 
 validateCFS ( cfs ) ; 
 + int filecounts = assertFileCounts ( sstables . iterator ( ) . next ( ) . descriptor . directory . list ( ) , 0 , 0 ) ; 
 + assertEquals ( 1 , filecounts ) ; 
 
 } 
 + @ Test 
 + public void basicTest2 ( ) throws InterruptedException 
 + { 
 + Keyspace keyspace = Keyspace . open ( KEYSPACE ) ; 
 + ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( CF ) ; 
 + cfs . truncateBlocking ( ) ; 
 
 + SSTableReader s = writeFile ( cfs , 1000 ) ; 
 + cfs . addSSTable ( s ) ; 
 + Set < SSTableReader > sstables = new HashSet < > ( cfs . getSSTables ( ) ) ; 
 + assertEquals ( 1 , sstables . size ( ) ) ; 
 + SSTableRewriter . overrideOpenInterval ( 10000000 ) ; 
 + SSTableRewriter writer = new SSTableRewriter ( cfs , sstables , 1000 , false ) ; 
 + try ( AbstractCompactionStrategy . ScannerList scanners = cfs . getCompactionStrategy ( ) . getScanners ( sstables ) ; ) 
 + { 
 + ICompactionScanner scanner = scanners . scanners . get ( 0 ) ; 
 + CompactionController controller = new CompactionController ( cfs , sstables , cfs . gcBefore ( System . currentTimeMillis ( ) ) ) ; 
 + writer . switchWriter ( getWriter ( cfs , sstables . iterator ( ) . next ( ) . descriptor . directory ) ) ; 
 + while ( scanner . hasNext ( ) ) 
 + { 
 + AbstractCompactedRow row = new LazilyCompactedRow ( controller , Arrays . asList ( scanner . next ( ) ) ) ; 
 + writer . append ( row ) ; 
 + } 
 + } 
 + Collection < SSTableReader > newsstables = writer . finish ( ) ; 
 + cfs . getDataTracker ( ) . markCompactedSSTablesReplaced ( sstables , newsstables , OperationType . COMPACTION ) ; 
 + Thread . sleep ( 100 ) ; 
 + validateCFS ( cfs ) ; 
 + int filecounts = assertFileCounts ( sstables . iterator ( ) . next ( ) . descriptor . directory . list ( ) , 0 , 0 ) ; 
 + assertEquals ( 1 , filecounts ) ; 
 + } 
 
 @ Test 
 public void testFileRemoval ( ) throws InterruptedException 
 @ @ - 114 , 37 + 150 , 11 @ @ public class SSTableRewriterTest extends SchemaLoader 
 assertFileCounts ( dir . list ( ) , 0 , 3 ) ; 
 writer . abort ( false ) ; 
 Thread . sleep ( 1000 ) ; 
 - assertFileCounts ( dir . list ( ) , 0 , 0 ) ; 
 - validateCFS ( cfs ) ; 
 - } 
 - 
 - @ Test 
 - public void testFileRemovalNoAbort ( ) throws InterruptedException 
 - { 
 - Keyspace keyspace = Keyspace . open ( KEYSPACE ) ; 
 - ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( CF ) ; 
 - cfs . truncateBlocking ( ) ; 
 - ArrayBackedSortedColumns cf = ArrayBackedSortedColumns . factory . create ( cfs . metadata ) ; 
 - for ( int i = 0 ; i < 1000 ; i + + ) 
 - cf . addColumn ( Util . column ( String . valueOf ( i ) , " a " , 1 ) ) ; 
 - File dir = cfs . directories . getDirectoryForNewSSTables ( ) ; 
 - SSTableWriter writer = getWriter ( cfs , dir ) ; 
 - 
 - for ( int i = 0 ; i < 500 ; i + + ) 
 - writer . append ( StorageService . getPartitioner ( ) . decorateKey ( ByteBufferUtil . bytes ( i ) ) , cf ) ; 
 - SSTableReader s = writer . openEarly ( 1000 ) ; 
 - / / assertFileCounts ( dir . list ( ) , 2 , 3 ) ; 
 - for ( int i = 500 ; i < 1000 ; i + + ) 
 - writer . append ( StorageService . getPartitioner ( ) . decorateKey ( ByteBufferUtil . bytes ( i ) ) , cf ) ; 
 - writer . closeAndOpenReader ( ) ; 
 - s . markObsolete ( ) ; 
 - s . releaseReference ( ) ; 
 - Thread . sleep ( 1000 ) ; 
 - assertFileCounts ( dir . list ( ) , 0 , 0 ) ; 
 + int datafiles = assertFileCounts ( dir . list ( ) , 0 , 0 ) ; 
 + assertEquals ( datafiles , 0 ) ; 
 validateCFS ( cfs ) ; 
 } 
 
 - 
 @ Test 
 public void testNumberOfFilesAndSizes ( ) throws Exception 
 { 
 @ @ - 446 , 6 + 456 , 95 @ @ public class SSTableRewriterTest extends SchemaLoader 
 assertFileCounts ( s . descriptor . directory . list ( ) , 0 , 0 ) ; 
 validateCFS ( cfs ) ; 
 } 
 + @ Test 
 + public void testAbort ( ) throws Exception 
 + { 
 + Keyspace keyspace = Keyspace . open ( KEYSPACE ) ; 
 + ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( CF ) ; 
 + cfs . truncateBlocking ( ) ; 
 + SSTableReader s = writeFile ( cfs , 1000 ) ; 
 + cfs . addSSTable ( s ) ; 
 + Set < SSTableReader > compacting = Sets . newHashSet ( s ) ; 
 + SSTableRewriter . overrideOpenInterval ( 10000000 ) ; 
 + SSTableRewriter rewriter = new SSTableRewriter ( cfs , compacting , 1000 , false ) ; 
 + SSTableWriter w = getWriter ( cfs , s . descriptor . directory ) ; 
 + rewriter . switchWriter ( w ) ; 
 + try ( ICompactionScanner scanner = compacting . iterator ( ) . next ( ) . getScanner ( ) ; 
 + CompactionController controller = new CompactionController ( cfs , compacting , 0 ) ) 
 + { 
 + while ( scanner . hasNext ( ) ) 
 + { 
 + rewriter . append ( new LazilyCompactedRow ( controller , Arrays . asList ( scanner . next ( ) ) ) ) ; 
 + if ( rewriter . currentWriter ( ) . getOnDiskFilePointer ( ) > 25000000 ) 
 + { 
 + rewriter . switchWriter ( getWriter ( cfs , s . descriptor . directory ) ) ; 
 + } 
 + } 
 + try 
 + { 
 + rewriter . finishAndThrow ( false ) ; 
 + } 
 + catch ( Throwable t ) 
 + { 
 + rewriter . abort ( ) ; 
 + } 
 + } 
 + Thread . sleep ( 1000 ) ; 
 + int filecount = assertFileCounts ( s . descriptor . directory . list ( ) , 0 , 0 ) ; 
 + assertEquals ( filecount , 1 ) ; 
 + assertEquals ( 1 , cfs . getSSTables ( ) . size ( ) ) ; 
 + validateCFS ( cfs ) ; 
 + cfs . truncateBlocking ( ) ; 
 + Thread . sleep ( 1000 ) ; 
 + filecount = assertFileCounts ( s . descriptor . directory . list ( ) , 0 , 0 ) ; 
 + assertEquals ( 0 , filecount ) ; 
 + 
 + } 
 + 
 + @ Test 
 + public void testAbort2 ( ) throws Exception 
 + { 
 + Keyspace keyspace = Keyspace . open ( KEYSPACE ) ; 
 + ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( CF ) ; 
 + cfs . truncateBlocking ( ) ; 
 + SSTableReader s = writeFile ( cfs , 1000 ) ; 
 + cfs . addSSTable ( s ) ; 
 + Set < SSTableReader > compacting = Sets . newHashSet ( s ) ; 
 + SSTableRewriter . overrideOpenInterval ( 10000000 ) ; 
 + SSTableRewriter rewriter = new SSTableRewriter ( cfs , compacting , 1000 , false ) ; 
 + SSTableWriter w = getWriter ( cfs , s . descriptor . directory ) ; 
 + rewriter . switchWriter ( w ) ; 
 + try ( ICompactionScanner scanner = compacting . iterator ( ) . next ( ) . getScanner ( ) ; 
 + CompactionController controller = new CompactionController ( cfs , compacting , 0 ) ) 
 + { 
 + while ( scanner . hasNext ( ) ) 
 + { 
 + rewriter . append ( new LazilyCompactedRow ( controller , Arrays . asList ( scanner . next ( ) ) ) ) ; 
 + if ( rewriter . currentWriter ( ) . getOnDiskFilePointer ( ) > 25000000 ) 
 + { 
 + rewriter . switchWriter ( getWriter ( cfs , s . descriptor . directory ) ) ; 
 + } 
 + } 
 + try 
 + { 
 + rewriter . finishAndThrow ( true ) ; 
 + } 
 + catch ( Throwable t ) 
 + { 
 + rewriter . abort ( ) ; 
 + } 
 + } 
 + Thread . sleep ( 1000 ) ; 
 + int filecount = assertFileCounts ( s . descriptor . directory . list ( ) , 0 , 0 ) ; 
 + assertEquals ( filecount , 1 ) ; 
 + assertEquals ( 1 , cfs . getSSTables ( ) . size ( ) ) ; 
 + validateCFS ( cfs ) ; 
 + cfs . truncateBlocking ( ) ; 
 + Thread . sleep ( 1000 ) ; 
 + filecount = assertFileCounts ( s . descriptor . directory . list ( ) , 0 , 0 ) ; 
 + assertEquals ( 0 , filecount ) ; 
 + 
 + } 
 
 private SSTableReader writeFile ( ColumnFamilyStore cfs , int count ) 
 { 
 @ @ - 469 , 28 + 568 , 45 @ @ public class SSTableRewriterTest extends SchemaLoader 
 
 private void validateCFS ( ColumnFamilyStore cfs ) 
 { 
 + Set < Integer > liveDescriptors = new HashSet < > ( ) ; 
 for ( SSTableReader sstable : cfs . getSSTables ( ) ) 
 { 
 assertFalse ( sstable . isMarkedCompacted ( ) ) ; 
 assertEquals ( 1 , sstable . referenceCount ( ) ) ; 
 + liveDescriptors . add ( sstable . descriptor . generation ) ; 
 + } 
 + for ( File dir : cfs . directories . getCFDirectories ( ) ) 
 + { 
 + for ( String f : dir . list ( ) ) 
 + { 
 + if ( f . contains ( " Data " ) ) 
 + { 
 + Descriptor d = Descriptor . fromFilename ( f ) ; 
 + assertTrue ( d . toString ( ) , liveDescriptors . contains ( d . generation ) ) ; 
 + } 
 + } 
 } 
 assertTrue ( cfs . getDataTracker ( ) . getCompacting ( ) . isEmpty ( ) ) ; 
 } 
 
 
 - private void assertFileCounts ( String [ ] files , int expectedtmplinkCount , int expectedtmpCount ) 
 + private int assertFileCounts ( String [ ] files , int expectedtmplinkCount , int expectedtmpCount ) 
 { 
 int tmplinkcount = 0 ; 
 int tmpcount = 0 ; 
 + int datacount = 0 ; 
 for ( String f : files ) 
 { 
 if ( f . contains ( " - tmplink - " ) ) 
 tmplinkcount + + ; 
 - if ( f . contains ( " - tmp - " ) ) 
 + else if ( f . contains ( " - tmp - " ) ) 
 tmpcount + + ; 
 + else if ( f . contains ( " Data " ) ) 
 + datacount + + ; 
 } 
 assertEquals ( expectedtmplinkCount , tmplinkcount ) ; 
 assertEquals ( expectedtmpCount , tmpcount ) ; 
 + return datacount ; 
 } 
 
 private SSTableWriter getWriter ( ColumnFamilyStore cfs , File directory )

NEAREST DIFF:
ELIMINATEDSENTENCE
