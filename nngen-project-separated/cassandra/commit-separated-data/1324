BLEU SCORE: 0.007870921666513668

TEST MSG: Small optimizations of sstable index serialization
GENERATED MSG: Revert " Always record row - level tombstones in index component ; this time from the correct feature branch "

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index afd45e5 . . d6ebe7a 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 3 . 0 . 0 - rc1 <nl> + * Small optimizations of sstable index serialization ( CASSANDRA - 10232 ) <nl> * Support for both encrypted and unencrypted native transport connections ( CASSANDRA - 9590 ) <nl> <nl> <nl> diff - - git a / src / java / org / apache / cassandra / db / ColumnIndex . java b / src / java / org / apache / cassandra / db / ColumnIndex . java <nl> index 9eef23e . . b350f90 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnIndex . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnIndex . java <nl> @ @ - 31 , 14 + 31 , 16 @ @ import org . apache . cassandra . utils . ByteBufferUtil ; <nl> <nl> public class ColumnIndex <nl> { <nl> + public final long partitionHeaderLength ; <nl> public final List < IndexHelper . IndexInfo > columnsIndex ; <nl> <nl> - private static final ColumnIndex EMPTY = new ColumnIndex ( Collections . < IndexHelper . IndexInfo > emptyList ( ) ) ; <nl> + private static final ColumnIndex EMPTY = new ColumnIndex ( - 1 , Collections . < IndexHelper . IndexInfo > emptyList ( ) ) ; <nl> <nl> - private ColumnIndex ( List < IndexHelper . IndexInfo > columnsIndex ) <nl> + private ColumnIndex ( long partitionHeaderLength , List < IndexHelper . IndexInfo > columnsIndex ) <nl> { <nl> assert columnsIndex ! = null ; <nl> <nl> + this . partitionHeaderLength = partitionHeaderLength ; <nl> this . columnsIndex = columnsIndex ; <nl> } <nl> <nl> @ @ - 67 , 8 + 69 , 10 @ @ public class ColumnIndex <nl> private final SerializationHeader header ; <nl> private final int version ; <nl> <nl> - private final ColumnIndex result ; <nl> + private final List < IndexHelper . IndexInfo > columnsIndex = new ArrayList < > ( ) ; <nl> private final long initialPosition ; <nl> + private long headerLength = - 1 ; <nl> + <nl> private long startPosition = - 1 ; <nl> <nl> private int written ; <nl> @ @ - 87 , 8 + 91 , 6 @ @ public class ColumnIndex <nl> this . writer = writer ; <nl> this . header = header ; <nl> this . version = version ; <nl> - <nl> - this . result = new ColumnIndex ( new ArrayList < IndexHelper . IndexInfo > ( ) ) ; <nl> this . initialPosition = writer . getFilePointer ( ) ; <nl> } <nl> <nl> @ @ - 103 , 6 + 105 , 7 @ @ public class ColumnIndex <nl> public ColumnIndex build ( ) throws IOException <nl> { <nl> writePartitionHeader ( iterator ) ; <nl> + this . headerLength = writer . getFilePointer ( ) - initialPosition ; <nl> <nl> while ( iterator . hasNext ( ) ) <nl> add ( iterator . next ( ) ) ; <nl> @ @ - 119 , 10 + 122 , 9 @ @ public class ColumnIndex <nl> { <nl> IndexHelper . IndexInfo cIndexInfo = new IndexHelper . IndexInfo ( firstClustering , <nl> lastClustering , <nl> - startPosition , <nl> currentPosition ( ) - startPosition , <nl> openMarker ) ; <nl> - result . columnsIndex . add ( cIndexInfo ) ; <nl> + columnsIndex . add ( cIndexInfo ) ; <nl> firstClustering = null ; <nl> } <nl> <nl> @ @ - 164 , 8 + 166 , 8 @ @ public class ColumnIndex <nl> addIndexBlock ( ) ; <nl> <nl> / / we should always have at least one computed index block , but we only write it out if there is more than that . <nl> - assert result . columnsIndex . size ( ) > 0 ; <nl> - return result ; <nl> + assert columnsIndex . size ( ) > 0 & & headerLength > = 0 ; <nl> + return new ColumnIndex ( headerLength , columnsIndex ) ; <nl> } <nl> } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / RowIndexEntry . java b / src / java / org / apache / cassandra / db / RowIndexEntry . java <nl> index e783508 . . f63e893 100644 <nl> - - - a / src / java / org / apache / cassandra / db / RowIndexEntry . java <nl> + + + b / src / java / org / apache / cassandra / db / RowIndexEntry . java <nl> @ @ - 47 , 7 + 47 , 7 @ @ public class RowIndexEntry < T > implements IMeasurableMemory <nl> this . position = position ; <nl> } <nl> <nl> - public int promotedSize ( CFMetaData metadata , Version version , SerializationHeader header ) <nl> + protected int promotedSize ( IndexHelper . IndexInfo . Serializer idxSerializer ) <nl> { <nl> return 0 ; <nl> } <nl> @ @ - 61 , 7 + 61 , 7 @ @ public class RowIndexEntry < T > implements IMeasurableMemory <nl> / / since if there are insufficient columns to be worth indexing we ' re going to seek to <nl> / / the beginning of the row anyway , so we might as well read the tombstone there as well . <nl> if ( index . columnsIndex . size ( ) > 1 ) <nl> - return new IndexedEntry ( position , deletionTime , index . columnsIndex ) ; <nl> + return new IndexedEntry ( position , deletionTime , index . partitionHeaderLength , index . columnsIndex ) ; <nl> else <nl> return new RowIndexEntry < > ( position ) ; <nl> } <nl> @ @ - 89 , 6 + 89 , 16 @ @ public class RowIndexEntry < T > implements IMeasurableMemory <nl> return 0 ; <nl> } <nl> <nl> + / * * <nl> + * The length of the row header ( partition key , partition deletion and static row ) . <nl> + * This value is only provided for indexed entries and this method will throw <nl> + * { @ code UnsupportedOperationException } if { @ code ! isIndexed ( ) } . <nl> + * / <nl> + public long headerLength ( ) <nl> + { <nl> + throw new UnsupportedOperationException ( ) ; <nl> + } <nl> + <nl> public List < T > columnsIndex ( ) <nl> { <nl> return Collections . emptyList ( ) ; <nl> @ @ - 108 , 48 + 118 , 81 @ @ public class RowIndexEntry < T > implements IMeasurableMemory <nl> <nl> public static class Serializer implements IndexSerializer < IndexHelper . IndexInfo > <nl> { <nl> - private final CFMetaData metadata ; <nl> + private final IndexHelper . IndexInfo . Serializer idxSerializer ; <nl> private final Version version ; <nl> - private final SerializationHeader header ; <nl> <nl> public Serializer ( CFMetaData metadata , Version version , SerializationHeader header ) <nl> { <nl> - this . metadata = metadata ; <nl> + this . idxSerializer = new IndexHelper . IndexInfo . Serializer ( metadata , version , header ) ; <nl> this . version = version ; <nl> - this . header = header ; <nl> } <nl> <nl> public void serialize ( RowIndexEntry < IndexHelper . IndexInfo > rie , DataOutputPlus out ) throws IOException <nl> { <nl> - out . writeLong ( rie . position ) ; <nl> - out . writeInt ( rie . promotedSize ( metadata , version , header ) ) ; <nl> + assert version . storeRows ( ) : " We read old index files but we should never write them " ; <nl> + <nl> + out . writeUnsignedVInt ( rie . position ) ; <nl> + out . writeUnsignedVInt ( rie . promotedSize ( idxSerializer ) ) ; <nl> <nl> if ( rie . isIndexed ( ) ) <nl> { <nl> + out . writeUnsignedVInt ( rie . headerLength ( ) ) ; <nl> DeletionTime . serializer . serialize ( rie . deletionTime ( ) , out ) ; <nl> - out . writeInt ( rie . columnsIndex ( ) . size ( ) ) ; <nl> - IndexHelper . IndexInfo . Serializer idxSerializer = metadata . serializers ( ) . indexSerializer ( version ) ; <nl> + out . writeUnsignedVInt ( rie . columnsIndex ( ) . size ( ) ) ; <nl> for ( IndexHelper . IndexInfo info : rie . columnsIndex ( ) ) <nl> - idxSerializer . serialize ( info , out , header ) ; <nl> + idxSerializer . serialize ( info , out ) ; <nl> } <nl> } <nl> <nl> public RowIndexEntry < IndexHelper . IndexInfo > deserialize ( DataInputPlus in ) throws IOException <nl> { <nl> - long position = in . readLong ( ) ; <nl> + if ( ! version . storeRows ( ) ) <nl> + { <nl> + long position = in . readLong ( ) ; <nl> + <nl> + int size = in . readInt ( ) ; <nl> + if ( size > 0 ) <nl> + { <nl> + DeletionTime deletionTime = DeletionTime . serializer . deserialize ( in ) ; <nl> + <nl> + int entries = in . readInt ( ) ; <nl> + List < IndexHelper . IndexInfo > columnsIndex = new ArrayList < > ( entries ) ; <nl> + <nl> + / / The old format didn ' t saved the partition header length per - se , but rather for each entry it ' s <nl> + / / offset from the beginning of the row . We don ' t use that offset anymore , but we do need the <nl> + / / header length so we basically need the first entry offset . And so we inline the deserialization <nl> + / / of the first index entry to get that information . While this is a bit ugly , we ' ll get rid of that <nl> + / / code once pre - 3 . 0 backward compatibility is dropped so it feels fine as a temporary hack . <nl> + ClusteringPrefix firstName = idxSerializer . clusteringSerializer . deserialize ( in ) ; <nl> + ClusteringPrefix lastName = idxSerializer . clusteringSerializer . deserialize ( in ) ; <nl> + long headerLength = in . readLong ( ) ; <nl> + long width = in . readLong ( ) ; <nl> + <nl> + columnsIndex . add ( new IndexHelper . IndexInfo ( firstName , lastName , width , null ) ) ; <nl> + for ( int i = 1 ; i < entries ; i + + ) <nl> + columnsIndex . add ( idxSerializer . deserialize ( in ) ) ; <nl> + <nl> + return new IndexedEntry ( position , deletionTime , headerLength , columnsIndex ) ; <nl> + } <nl> + else <nl> + { <nl> + return new RowIndexEntry < > ( position ) ; <nl> + } <nl> + } <nl> <nl> - int size = in . readInt ( ) ; <nl> + long position = in . readUnsignedVInt ( ) ; <nl> + <nl> + int size = ( int ) in . readUnsignedVInt ( ) ; <nl> if ( size > 0 ) <nl> { <nl> + long headerLength = in . readUnsignedVInt ( ) ; <nl> DeletionTime deletionTime = DeletionTime . serializer . deserialize ( in ) ; <nl> - <nl> - int entries = in . readInt ( ) ; <nl> - IndexHelper . IndexInfo . Serializer idxSerializer = metadata . serializers ( ) . indexSerializer ( version ) ; <nl> + int entries = ( int ) in . readUnsignedVInt ( ) ; <nl> List < IndexHelper . IndexInfo > columnsIndex = new ArrayList < > ( entries ) ; <nl> for ( int i = 0 ; i < entries ; i + + ) <nl> - columnsIndex . add ( idxSerializer . deserialize ( in , header ) ) ; <nl> + columnsIndex . add ( idxSerializer . deserialize ( in ) ) ; <nl> <nl> - return new IndexedEntry ( position , deletionTime , columnsIndex ) ; <nl> + return new IndexedEntry ( position , deletionTime , headerLength , columnsIndex ) ; <nl> } <nl> else <nl> { <nl> @ @ - 157 , 15 + 200 , 23 @ @ public class RowIndexEntry < T > implements IMeasurableMemory <nl> } <nl> } <nl> <nl> - public static void skip ( DataInput in ) throws IOException <nl> + / / Reads only the data ' position ' of the index entry and returns it . Note that this left ' in ' in the middle <nl> + / / of reading an entry , so this is only useful if you know what you are doing and in most case ' deserialize ' <nl> + / / should be used instead . <nl> + public static long readPosition ( DataInputPlus in , Version version ) throws IOException <nl> { <nl> - in . readLong ( ) ; <nl> - skipPromotedIndex ( in ) ; <nl> + return version . storeRows ( ) ? in . readUnsignedVInt ( ) : in . readLong ( ) ; <nl> } <nl> <nl> - public static void skipPromotedIndex ( DataInput in ) throws IOException <nl> + public static void skip ( DataInputPlus in , Version version ) throws IOException <nl> { <nl> - int size = in . readInt ( ) ; <nl> + readPosition ( in , version ) ; <nl> + skipPromotedIndex ( in , version ) ; <nl> + } <nl> + <nl> + public static void skipPromotedIndex ( DataInputPlus in , Version version ) throws IOException <nl> + { <nl> + int size = version . storeRows ( ) ? ( int ) in . readUnsignedVInt ( ) : in . readInt ( ) ; <nl> if ( size < = 0 ) <nl> return ; <nl> <nl> @ @ - 174 , 21 + 225 , 21 @ @ public class RowIndexEntry < T > implements IMeasurableMemory <nl> <nl> public int serializedSize ( RowIndexEntry < IndexHelper . IndexInfo > rie ) <nl> { <nl> - int size = TypeSizes . sizeof ( rie . position ) + TypeSizes . sizeof ( rie . promotedSize ( metadata , version , header ) ) ; <nl> + assert version . storeRows ( ) : " We read old index files but we should never write them " ; <nl> + <nl> + int size = TypeSizes . sizeofUnsignedVInt ( rie . position ) + TypeSizes . sizeofUnsignedVInt ( rie . promotedSize ( idxSerializer ) ) ; <nl> <nl> if ( rie . isIndexed ( ) ) <nl> { <nl> List < IndexHelper . IndexInfo > index = rie . columnsIndex ( ) ; <nl> <nl> + size + = TypeSizes . sizeofUnsignedVInt ( rie . headerLength ( ) ) ; <nl> size + = DeletionTime . serializer . serializedSize ( rie . deletionTime ( ) ) ; <nl> - size + = TypeSizes . sizeof ( index . size ( ) ) ; <nl> + size + = TypeSizes . sizeofUnsignedVInt ( index . size ( ) ) ; <nl> <nl> - IndexHelper . IndexInfo . Serializer idxSerializer = metadata . serializers ( ) . indexSerializer ( version ) ; <nl> for ( IndexHelper . IndexInfo info : index ) <nl> - size + = idxSerializer . serializedSize ( info , header ) ; <nl> + size + = idxSerializer . serializedSize ( info ) ; <nl> } <nl> - <nl> - <nl> return size ; <nl> } <nl> } <nl> @ @ - 199 , 17 + 250 , 21 @ @ public class RowIndexEntry < T > implements IMeasurableMemory <nl> private static class IndexedEntry extends RowIndexEntry < IndexHelper . IndexInfo > <nl> { <nl> private final DeletionTime deletionTime ; <nl> + <nl> + / / The offset in the file when the index entry end <nl> + private final long headerLength ; <nl> private final List < IndexHelper . IndexInfo > columnsIndex ; <nl> private static final long BASE _ SIZE = <nl> - ObjectSizes . measure ( new IndexedEntry ( 0 , DeletionTime . LIVE , Arrays . < IndexHelper . IndexInfo > asList ( null , null ) ) ) <nl> + ObjectSizes . measure ( new IndexedEntry ( 0 , DeletionTime . LIVE , 0 , Arrays . < IndexHelper . IndexInfo > asList ( null , null ) ) ) <nl> + ObjectSizes . measure ( new ArrayList < > ( 1 ) ) ; <nl> <nl> - private IndexedEntry ( long position , DeletionTime deletionTime , List < IndexHelper . IndexInfo > columnsIndex ) <nl> + private IndexedEntry ( long position , DeletionTime deletionTime , long headerLength , List < IndexHelper . IndexInfo > columnsIndex ) <nl> { <nl> super ( position ) ; <nl> assert deletionTime ! = null ; <nl> assert columnsIndex ! = null & & columnsIndex . size ( ) > 1 ; <nl> this . deletionTime = deletionTime ; <nl> + this . headerLength = headerLength ; <nl> this . columnsIndex = columnsIndex ; <nl> } <nl> <nl> @ @ - 220 , 19 + 275 , 25 @ @ public class RowIndexEntry < T > implements IMeasurableMemory <nl> } <nl> <nl> @ Override <nl> + public long headerLength ( ) <nl> + { <nl> + return headerLength ; <nl> + } <nl> + <nl> + @ Override <nl> public List < IndexHelper . IndexInfo > columnsIndex ( ) <nl> { <nl> return columnsIndex ; <nl> } <nl> <nl> @ Override <nl> - public int promotedSize ( CFMetaData metadata , Version version , SerializationHeader header ) <nl> + protected int promotedSize ( IndexHelper . IndexInfo . Serializer idxSerializer ) <nl> { <nl> - long size = DeletionTime . serializer . serializedSize ( deletionTime ) ; <nl> - size + = TypeSizes . sizeof ( columnsIndex . size ( ) ) ; / / number of entries <nl> - IndexHelper . IndexInfo . Serializer idxSerializer = metadata . serializers ( ) . indexSerializer ( version ) ; <nl> + long size = TypeSizes . sizeofUnsignedVInt ( headerLength ) <nl> + + DeletionTime . serializer . serializedSize ( deletionTime ) <nl> + + TypeSizes . sizeofUnsignedVInt ( columnsIndex . size ( ) ) ; / / number of entries <nl> for ( IndexHelper . IndexInfo info : columnsIndex ) <nl> - size + = idxSerializer . serializedSize ( info , header ) ; <nl> + size + = idxSerializer . serializedSize ( info ) ; <nl> <nl> return Ints . checkedCast ( size ) ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / Serializers . java b / src / java / org / apache / cassandra / db / Serializers . java <nl> index 2561bbe . . 9b29d89 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Serializers . java <nl> + + + b / src / java / org / apache / cassandra / db / Serializers . java <nl> @ @ - 43 , 11 + 43 , 6 @ @ public class Serializers <nl> this . metadata = metadata ; <nl> } <nl> <nl> - public IndexInfo . Serializer indexSerializer ( Version version ) <nl> - { <nl> - return new IndexInfo . Serializer ( metadata , version ) ; <nl> - } <nl> - <nl> / / TODO : Once we drop support for old ( pre - 3 . 0 ) sstables , we can drop this method and inline the calls to <nl> / / ClusteringPrefix . serializer in IndexHelper directly . At which point this whole class probably becomes <nl> / / unecessary ( since IndexInfo . Serializer won ' t depend on the metadata either ) . <nl> diff - - git a / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java b / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java <nl> index 87a57c6 . . c075a2b 100644 <nl> - - - a / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java <nl> + + + b / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java <nl> @ @ - 412 , 6 + 412 , 7 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator <nl> <nl> private final RowIndexEntry indexEntry ; <nl> private final List < IndexHelper . IndexInfo > indexes ; <nl> + private final long [ ] blockOffsets ; <nl> private final boolean reversed ; <nl> <nl> private int currentIndexIdx ; <nl> @ @ - 427 , 6 + 428 , 14 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator <nl> this . indexes = indexEntry . columnsIndex ( ) ; <nl> this . reversed = reversed ; <nl> this . currentIndexIdx = reversed ? indexEntry . columnsIndex ( ) . size ( ) : - 1 ; <nl> + <nl> + this . blockOffsets = new long [ indexes . size ( ) ] ; <nl> + long offset = indexEntry . position + indexEntry . headerLength ( ) ; <nl> + for ( int i = 0 ; i < blockOffsets . length ; i + + ) <nl> + { <nl> + blockOffsets [ i ] = offset ; <nl> + offset + = indexes . get ( i ) . width ; <nl> + } <nl> } <nl> <nl> public boolean isDone ( ) <nl> @ @ - 438 , 7 + 447 , 7 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator <nl> public void setToBlock ( int blockIdx ) throws IOException <nl> { <nl> if ( blockIdx > = 0 & & blockIdx < indexes . size ( ) ) <nl> - reader . seekToPosition ( indexEntry . position + indexes . get ( blockIdx ) . offset ) ; <nl> + reader . seekToPosition ( blockOffsets [ blockIdx ] ) ; <nl> <nl> currentIndexIdx = blockIdx ; <nl> reader . openMarker = blockIdx > 0 ? indexes . get ( blockIdx - 1 ) . endOpenMarker : null ; <nl> @ @ - 461 , 7 + 470 , 7 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator <nl> <nl> / / We have to set the mark , and we have to set it at the beginning of the block . So if we ' re not at the beginning of the block , this forces us to a weird seek dance . <nl> / / This can only happen when reading old file however . <nl> - long startOfBlock = indexEntry . position + indexes . get ( currentIndexIdx ) . offset ; <nl> + long startOfBlock = blockOffsets [ currentIndexIdx ] ; <nl> long currentFilePointer = reader . file . getFilePointer ( ) ; <nl> if ( startOfBlock = = currentFilePointer ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java <nl> index 4dabe69 . . e95af29 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java <nl> @ @ - 37 , 34 + 37 , 6 @ @ import org . apache . cassandra . utils . * ; <nl> * / <nl> public class IndexHelper <nl> { <nl> - public static void skipBloomFilter ( DataInput in ) throws IOException <nl> - { <nl> - int size = in . readInt ( ) ; <nl> - FileUtils . skipBytesFully ( in , size ) ; <nl> - } <nl> - <nl> - / * * <nl> - * Skip the index <nl> - * @ param in the data input from which the index should be skipped <nl> - * @ throws IOException if an I / O error occurs . <nl> - * / <nl> - public static void skipIndex ( DataInput in ) throws IOException <nl> - { <nl> - / * read only the column index list * / <nl> - int columnIndexSize = in . readInt ( ) ; <nl> - / * skip the column index data * / <nl> - if ( in instanceof FileDataInput ) <nl> - { <nl> - FileUtils . skipBytesFully ( in , columnIndexSize ) ; <nl> - } <nl> - else <nl> - { <nl> - / / skip bytes <nl> - byte [ ] skip = new byte [ columnIndexSize ] ; <nl> - in . readFully ( skip ) ; <nl> - } <nl> - } <nl> - <nl> / * * <nl> * The index of the IndexInfo in which a scan starting with @ name should begin . <nl> * <nl> @ @ - 78 , 7 + 50 , 7 @ @ public class IndexHelper <nl> * / <nl> public static int indexFor ( ClusteringPrefix name , List < IndexInfo > indexList , ClusteringComparator comparator , boolean reversed , int lastIndex ) <nl> { <nl> - IndexInfo target = new IndexInfo ( name , name , 0 , 0 , null ) ; <nl> + IndexInfo target = new IndexInfo ( name , name , 0 , null ) ; <nl> / * <nl> Take the example from the unit test , and say your index looks like this : <nl> [ 0 . . 5 ] [ 10 . . 15 ] [ 20 . . 25 ] <nl> @ @ - 115 , 12 + 87 , 11 @ @ public class IndexHelper <nl> <nl> public static class IndexInfo <nl> { <nl> - private static final long EMPTY _ SIZE = ObjectSizes . measure ( new IndexInfo ( null , null , 0 , 0 , null ) ) ; <nl> + private static final long EMPTY _ SIZE = ObjectSizes . measure ( new IndexInfo ( null , null , 0 , null ) ) ; <nl> <nl> public final long width ; <nl> - public final ClusteringPrefix lastName ; <nl> public final ClusteringPrefix firstName ; <nl> - public final long offset ; <nl> + public final ClusteringPrefix lastName ; <nl> <nl> / / If at the end of the index block there is an open range tombstone marker , this marker <nl> / / deletion infos . null otherwise . <nl> @ @ - 128 , 73 + 99 , 77 @ @ public class IndexHelper <nl> <nl> public IndexInfo ( ClusteringPrefix firstName , <nl> ClusteringPrefix lastName , <nl> - long offset , <nl> long width , <nl> DeletionTime endOpenMarker ) <nl> { <nl> this . firstName = firstName ; <nl> this . lastName = lastName ; <nl> - this . offset = offset ; <nl> this . width = width ; <nl> this . endOpenMarker = endOpenMarker ; <nl> } <nl> <nl> public static class Serializer <nl> { <nl> - private final CFMetaData metadata ; <nl> + / / This is the default index size that we use to delta - encode width when serializing so we get better vint - encoding . <nl> + / / This is imperfect as user can change the index size and ideally we would save the index size used with each index file <nl> + / / to use as base . However , that ' s a bit more involved a change that we want for now and very seldom do use change the index <nl> + / / size so using the default is almost surely better than using no base at all . <nl> + private static final long WIDTH _ BASE = 64 * 1024 ; <nl> + <nl> + / / TODO : Only public for use in RowIndexEntry for backward compatibility code . Can be made private once backward compatibility is dropped . <nl> + public final ISerializer < ClusteringPrefix > clusteringSerializer ; <nl> private final Version version ; <nl> <nl> - public Serializer ( CFMetaData metadata , Version version ) <nl> + public Serializer ( CFMetaData metadata , Version version , SerializationHeader header ) <nl> { <nl> - this . metadata = metadata ; <nl> + this . clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; <nl> this . version = version ; <nl> } <nl> <nl> - public void serialize ( IndexInfo info , DataOutputPlus out , SerializationHeader header ) throws IOException <nl> + public void serialize ( IndexInfo info , DataOutputPlus out ) throws IOException <nl> { <nl> - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; <nl> + assert version . storeRows ( ) : " We read old index files but we should never write them " ; <nl> + <nl> clusteringSerializer . serialize ( info . firstName , out ) ; <nl> clusteringSerializer . serialize ( info . lastName , out ) ; <nl> - out . writeLong ( info . offset ) ; <nl> - out . writeLong ( info . width ) ; <nl> + out . writeVInt ( info . width - WIDTH _ BASE ) ; <nl> <nl> - if ( version . storeRows ( ) ) <nl> - { <nl> - out . writeBoolean ( info . endOpenMarker ! = null ) ; <nl> - if ( info . endOpenMarker ! = null ) <nl> - DeletionTime . serializer . serialize ( info . endOpenMarker , out ) ; <nl> - } <nl> + out . writeBoolean ( info . endOpenMarker ! = null ) ; <nl> + if ( info . endOpenMarker ! = null ) <nl> + DeletionTime . serializer . serialize ( info . endOpenMarker , out ) ; <nl> } <nl> <nl> - public IndexInfo deserialize ( DataInputPlus in , SerializationHeader header ) throws IOException <nl> + public IndexInfo deserialize ( DataInputPlus in ) throws IOException <nl> { <nl> - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; <nl> - <nl> ClusteringPrefix firstName = clusteringSerializer . deserialize ( in ) ; <nl> ClusteringPrefix lastName = clusteringSerializer . deserialize ( in ) ; <nl> - long offset = in . readLong ( ) ; <nl> - long width = in . readLong ( ) ; <nl> - DeletionTime endOpenMarker = version . storeRows ( ) & & in . readBoolean ( ) <nl> - ? DeletionTime . serializer . deserialize ( in ) <nl> - : null ; <nl> - <nl> - return new IndexInfo ( firstName , lastName , offset , width , endOpenMarker ) ; <nl> + long width ; <nl> + DeletionTime endOpenMarker = null ; <nl> + if ( version . storeRows ( ) ) <nl> + { <nl> + width = in . readVInt ( ) + WIDTH _ BASE ; <nl> + if ( in . readBoolean ( ) ) <nl> + endOpenMarker = DeletionTime . serializer . deserialize ( in ) ; <nl> + } <nl> + else <nl> + { <nl> + in . readLong ( ) ; / / skip offset <nl> + width = in . readLong ( ) ; <nl> + } <nl> + return new IndexInfo ( firstName , lastName , width , endOpenMarker ) ; <nl> } <nl> <nl> - public long serializedSize ( IndexInfo info , SerializationHeader header ) <nl> + public long serializedSize ( IndexInfo info ) <nl> { <nl> - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; <nl> + assert version . storeRows ( ) : " We read old index files but we should never write them " ; <nl> + <nl> long size = clusteringSerializer . serializedSize ( info . firstName ) <nl> + clusteringSerializer . serializedSize ( info . lastName ) <nl> - + TypeSizes . sizeof ( info . offset ) <nl> - + TypeSizes . sizeof ( info . width ) ; <nl> + + TypeSizes . sizeofVInt ( info . width - WIDTH _ BASE ) <nl> + + TypeSizes . sizeof ( info . endOpenMarker ! = null ) ; <nl> <nl> - if ( version . storeRows ( ) ) <nl> - { <nl> - size + = TypeSizes . sizeof ( info . endOpenMarker ! = null ) ; <nl> - if ( info . endOpenMarker ! = null ) <nl> - size + = DeletionTime . serializer . serializedSize ( info . endOpenMarker ) ; <nl> - } <nl> + if ( info . endOpenMarker ! = null ) <nl> + size + = DeletionTime . serializer . serializedSize ( info . endOpenMarker ) ; <nl> return size ; <nl> } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> index 6f1e2f4 . . f02b9d1 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> @ @ - 17 , 7 + 17 , 6 @ @ <nl> * / <nl> package org . apache . cassandra . io . sstable ; <nl> <nl> - import java . io . DataInput ; <nl> import java . io . File ; <nl> import java . io . IOException ; <nl> <nl> @ @ - 27 , 6 + 26 , 7 @ @ import org . apache . cassandra . config . CFMetaData ; <nl> import org . apache . cassandra . db . DecoratedKey ; <nl> import org . apache . cassandra . db . RowIndexEntry ; <nl> import org . apache . cassandra . dht . IPartitioner ; <nl> + import org . apache . cassandra . io . util . DataInputPlus ; <nl> import org . apache . cassandra . io . util . RandomAccessReader ; <nl> import org . apache . cassandra . utils . ByteBufferUtil ; <nl> import org . apache . cassandra . utils . CloseableIterator ; <nl> @ @ - 49 , 7 + 49 , 7 @ @ public class KeyIterator extends AbstractIterator < DecoratedKey > implements Close <nl> in = RandomAccessReader . open ( path ) ; <nl> } <nl> <nl> - public DataInput get ( ) <nl> + public DataInputPlus get ( ) <nl> { <nl> maybeInit ( ) ; <nl> return in ; <nl> @ @ - 80 , 12 + 80 , 14 @ @ public class KeyIterator extends AbstractIterator < DecoratedKey > implements Close <nl> } <nl> } <nl> <nl> + private final Descriptor desc ; <nl> private final In in ; <nl> private final IPartitioner partitioner ; <nl> <nl> <nl> public KeyIterator ( Descriptor desc , CFMetaData metadata ) <nl> { <nl> + this . desc = desc ; <nl> in = new In ( new File ( desc . filenameFor ( Component . PRIMARY _ INDEX ) ) ) ; <nl> partitioner = metadata . partitioner ; <nl> } <nl> @ @ - 98 , 7 + 100 , 7 @ @ public class KeyIterator extends AbstractIterator < DecoratedKey > implements Close <nl> return endOfData ( ) ; <nl> <nl> DecoratedKey key = partitioner . decorateKey ( ByteBufferUtil . readWithShortLength ( in . get ( ) ) ) ; <nl> - RowIndexEntry . Serializer . skip ( in . get ( ) ) ; / / skip remainder of the entry <nl> + RowIndexEntry . Serializer . skip ( in . get ( ) , desc . version ) ; / / skip remainder of the entry <nl> return key ; <nl> } <nl> catch ( IOException e ) <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTable . java b / src / java / org / apache / cassandra / io / sstable / SSTable . java <nl> index b86d9b4 . . 63b8f3e 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTable . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTable . java <nl> @ @ - 237 , 7 + 237 , 7 @ @ public abstract class SSTable <nl> while ( ifile . getFilePointer ( ) < BYTES _ CAP & & keys < SAMPLES _ CAP ) <nl> { <nl> ByteBufferUtil . skipShortLength ( ifile ) ; <nl> - RowIndexEntry . Serializer . skip ( ifile ) ; <nl> + RowIndexEntry . Serializer . skip ( ifile , descriptor . version ) ; <nl> keys + + ; <nl> } <nl> assert keys > 0 & & ifile . getFilePointer ( ) > 0 & & ifile . length ( ) > 0 : " Unexpected empty index file : " + ifile ; <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / format / SSTableReader . java b / src / java / org / apache / cassandra / io / sstable / format / SSTableReader . java <nl> index 5d8ab50 . . b958240 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / format / SSTableReader . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / format / SSTableReader . java <nl> @ @ - 922 , 7 + 922 , 7 @ @ public abstract class SSTableReader extends SSTable implements SelfRefCounted < SS <nl> if ( summaryEntriesChecked = = Downsampling . BASE _ SAMPLING _ LEVEL ) <nl> return true ; <nl> } <nl> - RowIndexEntry . Serializer . skip ( in ) ; <nl> + RowIndexEntry . Serializer . skip ( in , descriptor . version ) ; <nl> i + + ; <nl> } <nl> } <nl> @ @ - 1199 , 7 + 1199 , 7 @ @ public abstract class SSTableReader extends SSTable implements SelfRefCounted < SS <nl> while ( ( indexPosition = primaryIndex . getFilePointer ( ) ) ! = indexSize ) <nl> { <nl> summaryBuilder . maybeAddEntry ( decorateKey ( ByteBufferUtil . readWithShortLength ( primaryIndex ) ) , indexPosition ) ; <nl> - RowIndexEntry . Serializer . skip ( primaryIndex ) ; <nl> + RowIndexEntry . Serializer . skip ( primaryIndex , descriptor . version ) ; <nl> } <nl> <nl> return summaryBuilder . build ( getPartitioner ( ) ) ; <nl> @ @ - 1605 , 7 + 1605 , 7 @ @ public abstract class SSTableReader extends SSTable implements SelfRefCounted < SS <nl> if ( indexDecoratedKey . compareTo ( token ) > 0 ) <nl> return indexDecoratedKey ; <nl> <nl> - RowIndexEntry . Serializer . skip ( in ) ; <nl> + RowIndexEntry . Serializer . skip ( in , descriptor . version ) ; <nl> } <nl> } <nl> catch ( IOException e ) <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / format / big / BigTableReader . java b / src / java / org / apache / cassandra / io / sstable / format / big / BigTableReader . java <nl> index 4b66942 . . efd1057 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / format / big / BigTableReader . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / format / big / BigTableReader . java <nl> @ @ - 239 , 7 + 239 , 7 @ @ public class BigTableReader extends SSTableReader <nl> return indexEntry ; <nl> } <nl> <nl> - RowIndexEntry . Serializer . skip ( in ) ; <nl> + RowIndexEntry . Serializer . skip ( in , descriptor . version ) ; <nl> } <nl> } <nl> catch ( IOException e ) <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / format / big / BigTableScanner . java b / src / java / org / apache / cassandra / io / sstable / format / big / BigTableScanner . java <nl> index d135df0 . . 1a4ac21 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / format / big / BigTableScanner . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / format / big / BigTableScanner . java <nl> @ @ - 176 , 14 + 176 , 14 @ @ public class BigTableScanner implements ISSTableScanner <nl> if ( indexDecoratedKey . compareTo ( currentRange . left ) > 0 | | currentRange . contains ( indexDecoratedKey ) ) <nl> { <nl> / / Found , just read the dataPosition and seek into index and data files <nl> - long dataPosition = ifile . readLong ( ) ; <nl> + long dataPosition = RowIndexEntry . Serializer . readPosition ( ifile , sstable . descriptor . version ) ; <nl> ifile . seek ( indexPosition ) ; <nl> dfile . seek ( dataPosition ) ; <nl> break ; <nl> } <nl> else <nl> { <nl> - RowIndexEntry . Serializer . skip ( ifile ) ; <nl> + RowIndexEntry . Serializer . skip ( ifile , sstable . descriptor . version ) ; <nl> } <nl> } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / service / CacheService . java b / src / java / org / apache / cassandra / service / CacheService . java <nl> index 9213b20 . . a48466a 100644 <nl> - - - a / src / java / org / apache / cassandra / service / CacheService . java <nl> + + + b / src / java / org / apache / cassandra / service / CacheService . java <nl> @ @ - 53 , 6 + 53 , 7 @ @ import org . apache . cassandra . db . partitions . CachedPartition ; <nl> import org . apache . cassandra . db . context . CounterContext ; <nl> import org . apache . cassandra . io . util . DataInputPlus ; <nl> import org . apache . cassandra . io . util . DataOutputPlus ; <nl> + import org . apache . cassandra . io . sstable . format . big . BigFormat ; <nl> import org . apache . cassandra . utils . ByteBufferUtil ; <nl> import org . apache . cassandra . utils . FBUtilities ; <nl> import org . apache . cassandra . utils . Pair ; <nl> @ @ - 469 , 7 + 470 , 11 @ @ public class CacheService implements CacheServiceMBean <nl> input . readBoolean ( ) ; / / backwards compatibility for " promoted indexes " boolean <nl> if ( reader = = null ) <nl> { <nl> - RowIndexEntry . Serializer . skipPromotedIndex ( input ) ; <nl> + / / The sstable doesn ' t exist anymore , so we can ' t be sure of the exact version and assume its the current version . The only case where we ' ll be <nl> + / / wrong is during upgrade , in which case we fail at deserialization . This is not a huge deal however since 1 ) this is unlikely enough that <nl> + / / this won ' t affect many users ( if any ) and only once , 2 ) this doesn ' t prevent the node from starting and 3 ) CASSANDRA - 10219 shows that this <nl> + / / part of the code has been broken for a while without anyone noticing ( it is , btw , still broken until CASSANDRA - 10219 is fixed ) . <nl> + RowIndexEntry . Serializer . skipPromotedIndex ( input , BigFormat . instance . getLatestVersion ( ) ) ; <nl> return null ; <nl> } <nl> RowIndexEntry . IndexSerializer < ? > indexSerializer = reader . descriptor . getFormat ( ) . getIndexSerializer ( reader . metadata , <nl> diff - - git a / test / unit / org / apache / cassandra / db / SinglePartitionSliceCommandTest . java b / test / unit / org / apache / cassandra / db / SinglePartitionSliceCommandTest . java <nl> index 4b7f15a . . 9f80023 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / SinglePartitionSliceCommandTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / SinglePartitionSliceCommandTest . java <nl> @ @ - 132 , 10 + 132 , 10 @ @ public class SinglePartitionSliceCommandTest <nl> @ Test <nl> public void staticColumnsAreReturned ( ) throws IOException <nl> { <nl> - DecoratedKey key = cfm . decorateKey ( ByteBufferUtil . bytes ( " k " ) ) ; <nl> + DecoratedKey key = cfm . decorateKey ( ByteBufferUtil . bytes ( " k1 " ) ) ; <nl> <nl> - QueryProcessor . executeInternal ( " INSERT INTO ks . tbl ( k , s ) VALUES ( ' k ' , ' s ' ) " ) ; <nl> - Assert . assertFalse ( QueryProcessor . executeInternal ( " SELECT s FROM ks . tbl WHERE k = ' k ' " ) . isEmpty ( ) ) ; <nl> + QueryProcessor . executeInternal ( " INSERT INTO ks . tbl ( k , s ) VALUES ( ' k1 ' , ' s ' ) " ) ; <nl> + Assert . assertFalse ( QueryProcessor . executeInternal ( " SELECT s FROM ks . tbl WHERE k = ' k1 ' " ) . isEmpty ( ) ) ; <nl> <nl> ColumnFilter columnFilter = ColumnFilter . selection ( PartitionColumns . of ( s ) ) ; <nl> ClusteringIndexSliceFilter sliceFilter = new ClusteringIndexSliceFilter ( Slices . NONE , false ) ; <nl> @ @ - 147 , 11 + 147 , 11 @ @ public class SinglePartitionSliceCommandTest <nl> key , <nl> sliceFilter ) ; <nl> <nl> - UnfilteredPartitionIterator pi ; <nl> - <nl> / / check raw iterator for static cell <nl> - pi = cmd . executeLocally ( ReadOrderGroup . emptyGroup ( ) ) ; <nl> - checkForS ( pi ) ; <nl> + try ( ReadOrderGroup orderGroup = cmd . startOrderGroup ( ) ; UnfilteredPartitionIterator pi = cmd . executeLocally ( orderGroup ) ) <nl> + { <nl> + checkForS ( pi ) ; <nl> + } <nl> <nl> ReadResponse response ; <nl> DataOutputBuffer out ; <nl> @ @ - 159 , 24 + 159 , 33 @ @ public class SinglePartitionSliceCommandTest <nl> ReadResponse dst ; <nl> <nl> / / check ( de ) serialized iterator for memtable static cell <nl> - pi = cmd . executeLocally ( ReadOrderGroup . emptyGroup ( ) ) ; <nl> - response = ReadResponse . createDataResponse ( pi , cmd . columnFilter ( ) ) ; <nl> + try ( ReadOrderGroup orderGroup = cmd . startOrderGroup ( ) ; UnfilteredPartitionIterator pi = cmd . executeLocally ( orderGroup ) ) <nl> + { <nl> + response = ReadResponse . createDataResponse ( pi , cmd . columnFilter ( ) ) ; <nl> + } <nl> + <nl> out = new DataOutputBuffer ( ( int ) ReadResponse . serializer . serializedSize ( response , MessagingService . VERSION _ 30 ) ) ; <nl> ReadResponse . serializer . serialize ( response , out , MessagingService . VERSION _ 30 ) ; <nl> in = new DataInputBuffer ( out . buffer ( ) , true ) ; <nl> dst = ReadResponse . serializer . deserialize ( in , MessagingService . VERSION _ 30 ) ; <nl> - pi = dst . makeIterator ( cfm , cmd ) ; <nl> - checkForS ( pi ) ; <nl> + try ( UnfilteredPartitionIterator pi = dst . makeIterator ( cfm , cmd ) ) <nl> + { <nl> + checkForS ( pi ) ; <nl> + } <nl> <nl> / / check ( de ) serialized iterator for sstable static cell <nl> Schema . instance . getColumnFamilyStoreInstance ( cfm . cfId ) . forceBlockingFlush ( ) ; <nl> - pi = cmd . executeLocally ( ReadOrderGroup . emptyGroup ( ) ) ; <nl> - response = ReadResponse . createDataResponse ( pi , cmd . columnFilter ( ) ) ; <nl> + try ( ReadOrderGroup orderGroup = cmd . startOrderGroup ( ) ; UnfilteredPartitionIterator pi = cmd . executeLocally ( orderGroup ) ) <nl> + { <nl> + response = ReadResponse . createDataResponse ( pi , cmd . columnFilter ( ) ) ; <nl> + } <nl> out = new DataOutputBuffer ( ( int ) ReadResponse . serializer . serializedSize ( response , MessagingService . VERSION _ 30 ) ) ; <nl> ReadResponse . serializer . serialize ( response , out , MessagingService . VERSION _ 30 ) ; <nl> in = new DataInputBuffer ( out . buffer ( ) , true ) ; <nl> dst = ReadResponse . serializer . deserialize ( in , MessagingService . VERSION _ 30 ) ; <nl> - pi = dst . makeIterator ( cfm , cmd ) ; <nl> - checkForS ( pi ) ; <nl> + try ( UnfilteredPartitionIterator pi = dst . makeIterator ( cfm , cmd ) ) <nl> + { <nl> + checkForS ( pi ) ; <nl> + } <nl> } <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / IndexHelperTest . java b / test / unit / org / apache / cassandra / io / sstable / IndexHelperTest . java <nl> index c9f268a . . 2c967d0 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / IndexHelperTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / IndexHelperTest . java <nl> @ @ - 51 , 9 + 51 , 9 @ @ public class IndexHelperTest <nl> DeletionTime deletionInfo = new DeletionTime ( FBUtilities . timestampMicros ( ) , FBUtilities . nowInSeconds ( ) ) ; <nl> <nl> List < IndexInfo > indexes = new ArrayList < > ( ) ; <nl> - indexes . add ( new IndexInfo ( cn ( 0L ) , cn ( 5L ) , 0 , 0 , deletionInfo ) ) ; <nl> - indexes . add ( new IndexInfo ( cn ( 10L ) , cn ( 15L ) , 0 , 0 , deletionInfo ) ) ; <nl> - indexes . add ( new IndexInfo ( cn ( 20L ) , cn ( 25L ) , 0 , 0 , deletionInfo ) ) ; <nl> + indexes . add ( new IndexInfo ( cn ( 0L ) , cn ( 5L ) , 0 , deletionInfo ) ) ; <nl> + indexes . add ( new IndexInfo ( cn ( 10L ) , cn ( 15L ) , 0 , deletionInfo ) ) ; <nl> + indexes . add ( new IndexInfo ( cn ( 20L ) , cn ( 25L ) , 0 , deletionInfo ) ) ; <nl> <nl> <nl> assertEquals ( 0 , IndexHelper . indexFor ( cn ( - 1L ) , indexes , comp , false , - 1 ) ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableLoaderTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableLoaderTest . java <nl> index 4eebdeb . . faa9c3e 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableLoaderTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableLoaderTest . java <nl> @ @ - 167 , 7 + 167 , 9 @ @ public class SSTableLoaderTest <nl> . withBufferSizeInMB ( 1 ) <nl> . build ( ) ; <nl> <nl> - for ( int i = 0 ; i < 1000 ; i + + ) / / make sure to write more than 1 MB <nl> + int NB _ PARTITIONS = 5000 ; / / Enough to write > 1MB and get at least one completed sstable before we ' ve closed the writer <nl> + <nl> + for ( int i = 0 ; i < NB _ PARTITIONS ; i + + ) <nl> { <nl> for ( int j = 0 ; j < 100 ; j + + ) <nl> writer . addRow ( String . format ( " key % d " , i ) , String . format ( " col % d " , j ) , " 100 " ) ; <nl> @ @ - 183 , 7 + 185 , 7 @ @ public class SSTableLoaderTest <nl> <nl> List < FilteredPartition > partitions = Util . getAll ( Util . cmd ( Keyspace . open ( KEYSPACE1 ) . getColumnFamilyStore ( CF _ STANDARD2 ) ) . build ( ) ) ; <nl> <nl> - assertTrue ( partitions . size ( ) > 0 & & partitions . size ( ) < 1000 ) ; <nl> + assertTrue ( partitions . size ( ) > 0 & & partitions . size ( ) < NB _ PARTITIONS ) ; <nl> <nl> / / now we complete the write and the second loader should load the last sstable as well <nl> writer . close ( ) ; <nl> @ @ - 192 , 7 + 194 , 7 @ @ public class SSTableLoaderTest <nl> loader . stream ( Collections . emptySet ( ) , completionStreamListener ( latch ) ) . get ( ) ; <nl> <nl> partitions = Util . getAll ( Util . cmd ( Keyspace . open ( KEYSPACE1 ) . getColumnFamilyStore ( CF _ STANDARD2 ) ) . build ( ) ) ; <nl> - assertEquals ( 1000 , partitions . size ( ) ) ; <nl> + assertEquals ( NB _ PARTITIONS , partitions . size ( ) ) ; <nl> <nl> / / The stream future is signalled when the work is complete but before releasing references . Wait for release <nl> / / before cleanup ( CASSANDRA - 10118 ) .
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 9551c4c . . a3ebacd 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 45 , 6 + 45 , 8 @ @ <nl> * Fix streaming RangeTombstones at column index boundary ( CASSANDRA - 5418 ) <nl> * Fix preparing statements when current keyspace is not set ( CASSANDRA - 5468 ) <nl> * Fix SemanticVersion . isSupportedBy minor / patch handling ( CASSANDRA - 5496 ) <nl> + * Don ' t provide oldCfId for post - 1 . 1 system cfs ( CASSANDRA - 5490 ) <nl> + * Fix primary range ignores replication strategy ( CASSANDRA - 5424 ) <nl> Merged from 1 . 1 <nl> * Add retry mechanism to OTC for non - droppable _ verbs ( CASSANDRA - 5393 ) <nl> * Use allocator information to improve memtable memory usage estimate <nl> diff - - git a / conf / cassandra . yaml b / conf / cassandra . yaml <nl> index 7ccbe81 . . c4a6f84 100644 <nl> - - - a / conf / cassandra . yaml <nl> + + + b / conf / cassandra . yaml <nl> @ @ - 42 , 7 + 42 , 7 @ @ hinted _ handoff _ enabled : true <nl> # generated . After it has been dead this long , new hints for it will not be <nl> # created until it has been seen alive and gone down again . <nl> max _ hint _ window _ in _ ms : 10800000 # 3 hours <nl> - # throttle in KB ' s per second , per delivery thread <nl> + # throttle in KBs per second , per delivery thread <nl> hinted _ handoff _ throttle _ in _ kb : 1024 <nl> # Number of threads with which to deliver hints ; <nl> # Consider increasing this number when you have multi - dc deployments , since <nl> @ @ - 95 , 7 + 95 , 7 @ @ permissions _ validity _ in _ ms : 2000 <nl> # - OrderPreservingPartitioner is an obsolete form of BOP , that stores <nl> # - keys in a less - efficient format and only works with keys that are <nl> # UTF8 - encoded Strings . <nl> - # - CollatingOPP colates according to EN , US rules rather than lexical byte <nl> + # - CollatingOPP collates according to EN , US rules rather than lexical byte <nl> # ordering . Use this as an example if you need custom collation . <nl> # <nl> # See http : / / wiki . apache . org / cassandra / Operations for more on <nl> @ @ - 113 , 7 + 113 , 7 @ @ commitlog _ directory : / var / lib / cassandra / commitlog <nl> <nl> # policy for data disk failures : <nl> # stop : shut down gossip and Thrift , leaving the node effectively dead , but <nl> - # still inspectable via JMX . <nl> + # can still be inspected via JMX . <nl> # best _ effort : stop using the failed disk and respond to requests based on <nl> # remaining available sstables . This means you WILL see obsolete <nl> # data at CL . ONE ! <nl> @ @ - 125 , 8 + 125 , 8 @ @ disk _ failure _ policy : stop <nl> # Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the <nl> # minimum , sometimes more . The key cache is fairly tiny for the amount of <nl> # time it saves , so it ' s worthwhile to use it at large numbers . <nl> - # The row cache saves even more time , but must store the whole values of <nl> - # its rows , so it is extremely space - intensive . It ' s best to only use the <nl> + # The row cache saves even more time , but must contain the entire row , <nl> + # so it is extremely space - intensive . It ' s best to only use the <nl> # row cache if you have hot rows or static rows . <nl> # <nl> # NOTE : if you reduce the size , you may not get you hottest keys loaded on startup . <nl> @ @ - 135 , 7 + 135 , 7 @ @ disk _ failure _ policy : stop <nl> key _ cache _ size _ in _ mb : <nl> <nl> # Duration in seconds after which Cassandra should <nl> - # safe the keys cache . Caches are saved to saved _ caches _ directory as <nl> + # save the key cache . Caches are saved to saved _ caches _ directory as <nl> # specified in this configuration file . <nl> # <nl> # Saved caches greatly improve cold - start speeds , and is relatively cheap in <nl> @ @ - 179 , 6 + 179 , 8 @ @ row _ cache _ save _ period : 0 <nl> # significantly less memory than " live " rows in the JVM , so you can cache <nl> # more rows in a given memory footprint . And storing the cache off - heap <nl> # means you can use smaller heap sizes , reducing the impact of GC pauses . <nl> + # Note however that when a row is requested from the row cache , it must be <nl> + # deserialized into the heap for use . <nl> # <nl> # It is also valid to specify the fully - qualified class name to a class <nl> # that implements org . apache . cassandra . cache . IRowCacheProvider . <nl> @ @ - 217 , 7 + 219 , 7 @ @ commitlog _ sync _ period _ in _ ms : 10000 <nl> <nl> # The size of the individual commitlog file segments . A commitlog <nl> # segment may be archived , deleted , or recycled once all the data <nl> - # in it ( potentally from each columnfamily in the system ) has been <nl> + # in it ( potentially from each columnfamily in the system ) has been <nl> # flushed to sstables . <nl> # <nl> # The default size is 32 , which is almost always fine , but if you are <nl> @ @ - 281 , 7 + 283 , 7 @ @ memtable _ flush _ queue _ size : 4 <nl> # Whether to , when doing sequential writing , fsync ( ) at intervals in <nl> # order to force the operating system to flush the dirty <nl> # buffers . Enable this to avoid sudden dirty buffer flushing from <nl> - # impacting read latencies . Almost always a good idea on SSD : s ; not <nl> + # impacting read latencies . Almost always a good idea on SSDs ; not <nl> # necessarily on platters . <nl> trickle _ fsync : false <nl> trickle _ fsync _ interval _ in _ kb : 10240 <nl> @ @ - 298 , 7 + 300 , 7 @ @ ssl _ storage _ port : 7001 <nl> # communicate ! <nl> # <nl> # Leaving it blank leaves it up to InetAddress . getLocalHost ( ) . This <nl> - # will always do the Right Thing * if * the node is properly configured <nl> + # will always do the Right Thing _ if _ the node is properly configured <nl> # ( hostname , name resolution , etc ) , and the Right Thing is to use the <nl> # address associated with the hostname ( it might not be ) . <nl> # <nl> @ @ - 322 , 9 + 324 , 8 @ @ start _ native _ transport : false <nl> # port for the CQL native transport to listen for clients on <nl> native _ transport _ port : 9042 <nl> # The minimum and maximum threads for handling requests when the native <nl> - # transport is used . The meaning is those is similar to the one of <nl> - # rpc _ min _ threads and rpc _ max _ threads , though the default differ slightly and <nl> - # are the ones below : <nl> + # transport is used . They are similar to rpc _ min _ threads and rpc _ max _ threads , <nl> + # though the defaults differ slightly . <nl> # native _ transport _ min _ threads : 16 <nl> # native _ transport _ max _ threads : 128 <nl> <nl> @ @ - 332 , 7 + 333 , 7 @ @ native _ transport _ port : 9042 <nl> start _ rpc : true <nl> <nl> # The address to bind the Thrift RPC service to - - clients connect <nl> - # here . Unlike ListenAddress above , you * can * specify 0 . 0 . 0 . 0 here if <nl> + # here . Unlike ListenAddress above , you _ can _ specify 0 . 0 . 0 . 0 here if <nl> # you want Thrift to listen on all interfaces . <nl> # <nl> # Leaving this blank has the same effect it does for ListenAddress , <nl> @ @ - 347 , 7 + 348 , 7 @ @ rpc _ keepalive : true <nl> # Cassandra provides three out - of - the - box options for the RPC Server : <nl> # <nl> # sync - > One thread per thrift connection . For a very large number of clients , memory <nl> - # will be your limiting factor . On a 64 bit JVM , 128KB is the minimum stack size <nl> + # will be your limiting factor . On a 64 bit JVM , 180KB is the minimum stack size <nl> # per thread , and that will correspond to your use of virtual memory ( but physical memory <nl> # may be limited depending on use of stack space ) . <nl> # <nl> @ @ - 369 , 7 + 370 , 7 @ @ rpc _ server _ type : sync <nl> # RPC thread pool dictates how many concurrent requests are possible ( but if you are using the sync <nl> # RPC server , it also dictates the number of clients that can be connected at all ) . <nl> # <nl> - # The default is unlimited and thus provide no protection against clients overwhelming the server . You are <nl> + # The default is unlimited and thus provides no protection against clients overwhelming the server . You are <nl> # encouraged to set a maximum that makes sense for you in production , but do keep in mind that <nl> # rpc _ max _ threads represents the maximum number of client requests this server may execute concurrently . <nl> # <nl> @ @ - 401 , 7 + 402 , 7 @ @ thrift _ max _ message _ length _ in _ mb : 16 <nl> <nl> # Set to true to have Cassandra create a hard link to each sstable <nl> # flushed or streamed locally in a backups / subdirectory of the <nl> - # Keyspace data . Removing these links is the operator ' s <nl> + # keyspace data . Removing these links is the operator ' s <nl> # responsibility . <nl> incremental _ backups : false <nl> <nl> @ @ - 499 , 7 + 500 , 7 @ @ cross _ node _ timeout : false <nl> <nl> # Enable socket timeout for streaming operation . <nl> # When a timeout occurs during streaming , streaming is retried from the start <nl> - # of the current file . This * can * involve re - streaming an important amount of <nl> + # of the current file . This _ can _ involve re - streaming an important amount of <nl> # data , so you should avoid setting the value too low . <nl> # Default value is 0 , which never timeout streams . <nl> # streaming _ socket _ timeout _ in _ ms : 0 <nl> @ @ - 542 , 9 + 543 , 9 @ @ cross _ node _ timeout : false <nl> # deployment conventions ( as it did Facebook ' s ) , this is best used <nl> # as an example of writing a custom Snitch class . <nl> # - Ec2Snitch : <nl> - # Appropriate for EC2 deployments in a single Region . Loads Region <nl> + # Appropriate for EC2 deployments in a single Region . Loads Region <nl> # and Availability Zone information from the EC2 API . The Region is <nl> - # treated as the Datacenter , and the Availability Zone as the rack . <nl> + # treated as the datacenter , and the Availability Zone as the rack . <nl> # Only private IPs are used , so this will not work across multiple <nl> # Regions . <nl> # - Ec2MultiRegionSnitch : <nl> @ @ - 610 , 7 + 611 , 7 @ @ request _ scheduler : org . apache . cassandra . scheduler . NoScheduler <nl> # Keyspace1 : 1 <nl> # Keyspace2 : 5 <nl> <nl> - # request _ scheduler _ id - - An identifer based on which to perform <nl> + # request _ scheduler _ id - - An identifier based on which to perform <nl> # the request scheduling . Currently the only valid option is keyspace . <nl> # request _ scheduler _ id : keyspace <nl> <nl> diff - - git a / src / java / org / apache / cassandra / config / CFMetaData . java b / src / java / org / apache / cassandra / config / CFMetaData . java <nl> index 2f27b87 . . ae210e6 100644 <nl> - - - a / src / java / org / apache / cassandra / config / CFMetaData . java <nl> + + + b / src / java / org / apache / cassandra / config / CFMetaData . java <nl> @ @ - 165 , 7 + 165 , 7 @ @ public final class CFMetaData <nl> + " PRIMARY KEY ( keyspace _ name , columnfamily _ name , column _ name ) " <nl> + " ) WITH COMMENT = ' ColumnFamily column attributes ' AND gc _ grace _ seconds = 8640 " ) ; <nl> <nl> - public static final CFMetaData HintsCf = compile ( 11 , " CREATE TABLE " + SystemTable . HINTS _ CF + " ( " <nl> + public static final CFMetaData HintsCf = compile ( " CREATE TABLE " + SystemTable . HINTS _ CF + " ( " <nl> + " target _ id uuid , " <nl> + " hint _ id timeuuid , " <nl> + " message _ version int , " <nl> @ @ - 176 , 7 + 176 , 7 @ @ public final class CFMetaData <nl> + " AND COMMENT = ' hints awaiting delivery ' " <nl> + " AND gc _ grace _ seconds = 0 " ) ; <nl> <nl> - public static final CFMetaData PeersCf = compile ( 12 , " CREATE TABLE " + SystemTable . PEERS _ CF + " ( " <nl> + public static final CFMetaData PeersCf = compile ( " CREATE TABLE " + SystemTable . PEERS _ CF + " ( " <nl> + " peer inet PRIMARY KEY , " <nl> + " host _ id uuid , " <nl> + " tokens set < varchar > , " <nl> @ @ - 187 , 12 + 187 , 12 @ @ public final class CFMetaData <nl> + " rack text " <nl> + " ) WITH COMMENT = ' known peers in the cluster ' " ) ; <nl> <nl> - public static final CFMetaData PeerEventsCf = compile ( 12 , " CREATE TABLE " + SystemTable . PEER _ EVENTS _ CF + " ( " <nl> + public static final CFMetaData PeerEventsCf = compile ( " CREATE TABLE " + SystemTable . PEER _ EVENTS _ CF + " ( " <nl> + " peer inet PRIMARY KEY , " <nl> + " hints _ dropped map < uuid , int > " <nl> + " ) WITH COMMENT = ' cf contains events related to peers ' " ) ; <nl> <nl> - public static final CFMetaData LocalCf = compile ( 13 , " CREATE TABLE " + SystemTable . LOCAL _ CF + " ( " <nl> + public static final CFMetaData LocalCf = compile ( " CREATE TABLE " + SystemTable . LOCAL _ CF + " ( " <nl> + " key text PRIMARY KEY , " <nl> + " tokens set < varchar > , " <nl> + " cluster _ name text , " <nl> @ @ - 209 , 7 + 209 , 7 @ @ public final class CFMetaData <nl> + " truncated _ at map < uuid , blob > " <nl> + " ) WITH COMMENT = ' information about the local node ' " ) ; <nl> <nl> - public static final CFMetaData TraceSessionsCf = compile ( 14 , " CREATE TABLE " + Tracing . SESSIONS _ CF + " ( " <nl> + public static final CFMetaData TraceSessionsCf = compile ( " CREATE TABLE " + Tracing . SESSIONS _ CF + " ( " <nl> + " session _ id uuid PRIMARY KEY , " <nl> + " coordinator inet , " <nl> + " request text , " <nl> @ @ - 218 , 7 + 218 , 7 @ @ public final class CFMetaData <nl> + " duration int " <nl> + " ) WITH COMMENT = ' traced sessions ' " , Tracing . TRACE _ KS ) ; <nl> <nl> - public static final CFMetaData TraceEventsCf = compile ( 15 , " CREATE TABLE " + Tracing . EVENTS _ CF + " ( " <nl> + public static final CFMetaData TraceEventsCf = compile ( " CREATE TABLE " + Tracing . EVENTS _ CF + " ( " <nl> + " session _ id uuid , " <nl> + " event _ id timeuuid , " <nl> + " source inet , " <nl> @ @ - 228 , 26 + 228 , 26 @ @ public final class CFMetaData <nl> + " PRIMARY KEY ( session _ id , event _ id ) " <nl> + " ) ; " , Tracing . TRACE _ KS ) ; <nl> <nl> - public static final CFMetaData BatchlogCf = compile ( 16 , " CREATE TABLE " + SystemTable . BATCHLOG _ CF + " ( " <nl> + public static final CFMetaData BatchlogCf = compile ( " CREATE TABLE " + SystemTable . BATCHLOG _ CF + " ( " <nl> + " id uuid PRIMARY KEY , " <nl> + " written _ at timestamp , " <nl> + " data blob " <nl> + " ) WITH COMMENT = ' uncommited batches ' AND gc _ grace _ seconds = 0 " <nl> + " AND COMPACTION = { ' class ' : ' SizeTieredCompactionStrategy ' , ' min _ threshold ' : 2 } " ) ; <nl> <nl> - public static final CFMetaData RangeXfersCf = compile ( 17 , " CREATE TABLE " + SystemTable . RANGE _ XFERS _ CF + " ( " <nl> + public static final CFMetaData RangeXfersCf = compile ( " CREATE TABLE " + SystemTable . RANGE _ XFERS _ CF + " ( " <nl> + " token _ bytes blob PRIMARY KEY , " <nl> + " requested _ at timestamp " <nl> + " ) WITH COMMENT = ' ranges requested for transfer here ' " ) ; <nl> <nl> - public static final CFMetaData CompactionLogCf = compile ( 18 , " CREATE TABLE " + SystemTable . COMPACTION _ LOG + " ( " <nl> + public static final CFMetaData CompactionLogCf = compile ( " CREATE TABLE " + SystemTable . COMPACTION _ LOG + " ( " <nl> + " id uuid PRIMARY KEY , " <nl> + " keyspace _ name text , " <nl> + " columnfamily _ name text , " <nl> + " inputs set < int > " <nl> + " ) WITH COMMENT = ' unfinished compactions ' " ) ; <nl> <nl> - public static final CFMetaData PaxosCf = compile ( 18 , " CREATE TABLE " + SystemTable . PAXOS _ CF + " ( " <nl> + public static final CFMetaData PaxosCf = compile ( " CREATE TABLE " + SystemTable . PAXOS _ CF + " ( " <nl> + " row _ key blob , " <nl> + " cf _ id UUID , " <nl> + " in _ progress _ ballot timeuuid , " <nl> @ @ - 437 , 7 + 437 , 12 @ @ public final class CFMetaData <nl> updateCfDef ( ) ; / / init cqlCfDef <nl> } <nl> <nl> - private static CFMetaData compile ( int id , String cql , String keyspace ) <nl> + private static CFMetaData compile ( String cql , String keyspace ) <nl> + { <nl> + return compile ( null , cql , keyspace ) ; <nl> + } <nl> + <nl> + private static CFMetaData compile ( Integer id , String cql , String keyspace ) <nl> { <nl> try <nl> { <nl> @ @ - 452 , 6 + 457 , 11 @ @ public final class CFMetaData <nl> } <nl> } <nl> <nl> + private static CFMetaData compile ( String cql ) <nl> + { <nl> + return compile ( null , cql , Table . SYSTEM _ KS ) ; <nl> + } <nl> + <nl> private static CFMetaData compile ( int id , String cql ) <nl> { <nl> return compile ( id , cql , Table . SYSTEM _ KS ) ; <nl> diff - - git a / src / java / org / apache / cassandra / locator / NetworkTopologyStrategy . java b / src / java / org / apache / cassandra / locator / NetworkTopologyStrategy . java <nl> index 91478e8 . . d354019 100644 <nl> - - - a / src / java / org / apache / cassandra / locator / NetworkTopologyStrategy . java <nl> + + + b / src / java / org / apache / cassandra / locator / NetworkTopologyStrategy . java <nl> @ @ - 78 , 7 + 78 , 8 @ @ public class NetworkTopologyStrategy extends AbstractReplicationStrategy <nl> @ SuppressWarnings ( " serial " ) <nl> public List < InetAddress > calculateNaturalEndpoints ( Token searchToken , TokenMetadata tokenMetadata ) <nl> { <nl> - Set < InetAddress > replicas = new HashSet < InetAddress > ( ) ; <nl> + / / we want to preserve insertion order so that the first added endpoint becomes primary <nl> + Set < InetAddress > replicas = new LinkedHashSet < InetAddress > ( ) ; <nl> / / replicas we have found in each DC <nl> Map < String , Set < InetAddress > > dcReplicas = new HashMap < String , Set < InetAddress > > ( datacenters . size ( ) ) <nl> { { <nl> diff - - git a / src / java / org / apache / cassandra / service / StorageService . java b / src / java / org / apache / cassandra / service / StorageService . java <nl> index c1acb44 . . b0d62dc 100644 <nl> - - - a / src / java / org / apache / cassandra / service / StorageService . java <nl> + + + b / src / java / org / apache / cassandra / service / StorageService . java <nl> @ @ - 150 , 9 + 150 , 9 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE <nl> return getRangesForEndpoint ( table , FBUtilities . getBroadcastAddress ( ) ) ; <nl> } <nl> <nl> - public Collection < Range < Token > > getLocalPrimaryRanges ( ) <nl> + public Collection < Range < Token > > getLocalPrimaryRanges ( String keyspace ) <nl> { <nl> - return getPrimaryRangesForEndpoint ( FBUtilities . getBroadcastAddress ( ) ) ; <nl> + return getPrimaryRangesForEndpoint ( keyspace , FBUtilities . getBroadcastAddress ( ) ) ; <nl> } <nl> <nl> @ Deprecated <nl> @ @ - 2342 , 13 + 2342 , 13 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE <nl> } <nl> public int forceRepairAsync ( final String keyspace , final boolean isSequential , final boolean isLocal , final boolean primaryRange , final String . . . columnFamilies ) <nl> { <nl> - final Collection < Range < Token > > ranges = primaryRange ? getLocalPrimaryRanges ( ) : getLocalRanges ( keyspace ) ; <nl> + final Collection < Range < Token > > ranges = primaryRange ? getLocalPrimaryRanges ( keyspace ) : getLocalRanges ( keyspace ) ; <nl> return forceRepairAsync ( keyspace , isSequential , isLocal , ranges , columnFamilies ) ; <nl> } <nl> <nl> public int forceRepairAsync ( final String keyspace , final boolean isSequential , final boolean isLocal , final Collection < Range < Token > > ranges , final String . . . columnFamilies ) <nl> { <nl> - if ( Table . SYSTEM _ KS . equals ( keyspace ) | | Tracing . TRACE _ KS . equals ( keyspace ) ) <nl> + if ( Table . SYSTEM _ KS . equals ( keyspace ) | | Tracing . TRACE _ KS . equals ( keyspace ) | | ranges . isEmpty ( ) ) <nl> return 0 ; <nl> <nl> final int cmd = nextRepairCommand . incrementAndGet ( ) ; <nl> @ @ - 2383 , 7 + 2383 , 7 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE <nl> <nl> public void forceTableRepairPrimaryRange ( final String tableName , boolean isSequential , boolean isLocal , final String . . . columnFamilies ) throws IOException <nl> { <nl> - forceTableRepairRange ( tableName , getLocalPrimaryRanges ( ) , isSequential , isLocal , columnFamilies ) ; <nl> + forceTableRepairRange ( tableName , getLocalPrimaryRanges ( tableName ) , isSequential , isLocal , columnFamilies ) ; <nl> } <nl> <nl> public void forceTableRepairRange ( String beginToken , String endToken , final String tableName , boolean isSequential , boolean isLocal , final String . . . columnFamilies ) throws IOException <nl> @ @ - 2511 , 17 + 2511 , 36 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE <nl> } <nl> <nl> / * * <nl> - * Get the primary ranges for the specified endpoint . <nl> + * Get the " primary ranges " for the specified keyspace and endpoint . <nl> + * " Primary ranges " are the ranges that the node is responsible for storing replica primarily . <nl> + * The node that stores replica primarily is defined as the first node returned <nl> + * by { @ link AbstractReplicationStrategy # calculateNaturalEndpoints } . <nl> + * <nl> + * @ param keyspace <nl> * @ param ep endpoint we are interested in . <nl> - * @ return collection of ranges for the specified endpoint . <nl> + * @ return primary ranges for the specified endpoint . <nl> * / <nl> - public Collection < Range < Token > > getPrimaryRangesForEndpoint ( InetAddress ep ) <nl> + public Collection < Range < Token > > getPrimaryRangesForEndpoint ( String keyspace , InetAddress ep ) <nl> { <nl> - return tokenMetadata . getPrimaryRangesFor ( tokenMetadata . getTokens ( ep ) ) ; <nl> + AbstractReplicationStrategy strategy = Table . open ( keyspace ) . getReplicationStrategy ( ) ; <nl> + Collection < Range < Token > > primaryRanges = new HashSet < Range < Token > > ( ) ; <nl> + TokenMetadata metadata = tokenMetadata . cloneOnlyTokenMap ( ) ; <nl> + for ( Token token : metadata . sortedTokens ( ) ) <nl> + { <nl> + List < InetAddress > endpoints = strategy . calculateNaturalEndpoints ( token , metadata ) ; <nl> + if ( endpoints . size ( ) > 0 & & endpoints . get ( 0 ) . equals ( ep ) ) <nl> + primaryRanges . add ( new Range < Token > ( metadata . getPredecessor ( token ) , token ) ) ; <nl> + } <nl> + return primaryRanges ; <nl> } <nl> <nl> / * * <nl> - * Get the primary range for the specified endpoint . <nl> + * Previously , primary range is the range that the node is responsible for and calculated <nl> + * only from the token assigned to the node . <nl> + * But this does not take replication strategy into account , and therefore returns insufficient <nl> + * range especially using NTS with replication only to certain DC ( see CASSANDRA - 5424 ) . <nl> + * <nl> + * @ deprecated <nl> * @ param ep endpoint we are interested in . <nl> * @ return range for the specified endpoint . <nl> * / <nl> @ @ - 3814 , 8 + 3833 , 11 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE <nl> public List < String > sampleKeyRange ( ) / / do not rename to getter - see CASSANDRA - 4452 for details <nl> { <nl> List < DecoratedKey > keys = new ArrayList < DecoratedKey > ( ) ; <nl> - for ( Range < Token > range : getLocalPrimaryRanges ( ) ) <nl> - keys . addAll ( keySamples ( ColumnFamilyStore . allUserDefined ( ) , range ) ) ; <nl> + for ( Table keyspace : Table . nonSystem ( ) ) <nl> + { <nl> + for ( Range < Token > range : getPrimaryRangesForEndpoint ( keyspace . name , FBUtilities . getBroadcastAddress ( ) ) ) <nl> + keys . addAll ( keySamples ( keyspace . getColumnFamilyStores ( ) , range ) ) ; <nl> + } <nl> <nl> List < String > sampledKeys = new ArrayList < String > ( keys . size ( ) ) ; <nl> for ( DecoratedKey key : keys ) <nl> diff - - git a / src / java / org / apache / cassandra / utils / SlabAllocator . java b / src / java / org / apache / cassandra / utils / SlabAllocator . java <nl> index c50e8c4 . . 6ff66f8 100644 <nl> - - - a / src / java / org / apache / cassandra / utils / SlabAllocator . java <nl> + + + b / src / java / org / apache / cassandra / utils / SlabAllocator . java <nl> @ @ - 113 , 7 + 113 , 7 @ @ public class SlabAllocator extends Allocator <nl> * / <nl> public long getMinimumSize ( ) <nl> { <nl> - return unslabbed . get ( ) + ( regionCount - 1 ) * REGION _ SIZE ; <nl> + return unslabbed . get ( ) + ( regionCount - 1 ) * ( long ) REGION _ SIZE ; <nl> } <nl> <nl> / * * <nl> @ @ - 121 , 7 + 121 , 7 @ @ public class SlabAllocator extends Allocator <nl> * / <nl> public long getMaximumSize ( ) <nl> { <nl> - return unslabbed . get ( ) + regionCount * REGION _ SIZE ; <nl> + return unslabbed . get ( ) + regionCount * ( long ) REGION _ SIZE ; <nl> } <nl> <nl> / * * <nl> diff - - git a / test / unit / org / apache / cassandra / service / AntiEntropyServiceTestAbstract . java b / test / unit / org / apache / cassandra / service / AntiEntropyServiceTestAbstract . java <nl> index 50ae5e4 . . 04930d3 100644 <nl> - - - a / test / unit / org / apache / cassandra / service / AntiEntropyServiceTestAbstract . java <nl> + + + b / test / unit / org / apache / cassandra / service / AntiEntropyServiceTestAbstract . java <nl> @ @ - 102 , 7 + 102 , 7 @ @ public abstract class AntiEntropyServiceTestAbstract extends SchemaLoader <nl> <nl> Gossiper . instance . initializeNodeUnsafe ( REMOTE , UUID . randomUUID ( ) , 1 ) ; <nl> <nl> - local _ range = StorageService . instance . getLocalPrimaryRange ( ) ; <nl> + local _ range = StorageService . instance . getPrimaryRangesForEndpoint ( tablename , LOCAL ) . iterator ( ) . next ( ) ; <nl> <nl> / / ( we use REMOTE instead of LOCAL so that the reponses for the validator . complete ( ) get lost ) <nl> int gcBefore = ( int ) ( System . currentTimeMillis ( ) / 1000 ) - store . metadata . getGcGraceSeconds ( ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / service / StorageServiceServerTest . java b / test / unit / org / apache / cassandra / service / StorageServiceServerTest . java <nl> index 39fbb4a . . 5ce9160 100644 <nl> - - - a / test / unit / org / apache / cassandra / service / StorageServiceServerTest . java <nl> + + + b / test / unit / org / apache / cassandra / service / StorageServiceServerTest . java <nl> @ @ - 21 , 18 + 21 , 28 @ @ package org . apache . cassandra . service ; <nl> <nl> import java . io . File ; <nl> import java . io . IOException ; <nl> - import java . util . Collections ; <nl> - import java . util . List ; <nl> + import java . net . InetAddress ; <nl> + import java . util . * ; <nl> <nl> + import com . google . common . collect . HashMultimap ; <nl> + import com . google . common . collect . Multimap ; <nl> + import org . junit . BeforeClass ; <nl> import org . junit . Test ; <nl> import org . junit . runner . RunWith ; <nl> <nl> import org . apache . cassandra . OrderedJUnit4ClassRunner ; <nl> + import org . apache . cassandra . config . KSMetaData ; <nl> + import org . apache . cassandra . config . Schema ; <nl> + import org . apache . cassandra . dht . Range ; <nl> + import org . apache . cassandra . dht . StringToken ; <nl> import org . apache . cassandra . exceptions . ConfigurationException ; <nl> import org . apache . cassandra . db . Table ; <nl> import org . apache . cassandra . dht . Token ; <nl> import org . apache . cassandra . SchemaLoader ; <nl> import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + import org . apache . cassandra . locator . IEndpointSnitch ; <nl> + import org . apache . cassandra . locator . PropertyFileSnitch ; <nl> + import org . apache . cassandra . locator . TokenMetadata ; <nl> <nl> import static org . junit . Assert . assertEquals ; <nl> import static org . junit . Assert . assertTrue ; <nl> @ @ - 40 , 6 + 50 , 13 @ @ import static org . junit . Assert . assertTrue ; <nl> @ RunWith ( OrderedJUnit4ClassRunner . class ) <nl> public class StorageServiceServerTest <nl> { <nl> + @ BeforeClass <nl> + public static void setUp ( ) throws ConfigurationException <nl> + { <nl> + IEndpointSnitch snitch = new PropertyFileSnitch ( ) ; <nl> + DatabaseDescriptor . setEndpointSnitch ( snitch ) ; <nl> + } <nl> + <nl> @ Test <nl> public void testRegularMode ( ) throws IOException , InterruptedException , ConfigurationException <nl> { <nl> @ @ - 79 , 4 + 96 , 170 @ @ public class StorageServiceServerTest <nl> StorageService . instance . takeColumnFamilySnapshot ( Table . SYSTEM _ KS , " Schema " , " cf _ snapshot " ) ; <nl> } <nl> <nl> + @ Test <nl> + public void testPrimaryRangesWithNetworkTopologyStrategy ( ) throws Exception <nl> + { <nl> + TokenMetadata metadata = StorageService . instance . getTokenMetadata ( ) ; <nl> + metadata . clearUnsafe ( ) ; <nl> + / / DC1 <nl> + metadata . updateNormalToken ( new StringToken ( " A " ) , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; <nl> + metadata . updateNormalToken ( new StringToken ( " C " ) , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; <nl> + / / DC2 <nl> + metadata . updateNormalToken ( new StringToken ( " B " ) , InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) ) ; <nl> + metadata . updateNormalToken ( new StringToken ( " D " ) , InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) ) ; <nl> + <nl> + Map < String , String > configOptions = new HashMap < String , String > ( ) ; <nl> + configOptions . put ( " DC1 " , " 1 " ) ; <nl> + configOptions . put ( " DC2 " , " 1 " ) ; <nl> + <nl> + Table . clear ( " Keyspace1 " ) ; <nl> + KSMetaData meta = KSMetaData . newKeyspace ( " Keyspace1 " , " NetworkTopologyStrategy " , configOptions , false ) ; <nl> + Schema . instance . setTableDefinition ( meta ) ; <nl> + <nl> + Collection < Range < Token > > primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; <nl> + assert primaryRanges . size ( ) = = 1 ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " D " ) , new StringToken ( " A " ) ) ) ; <nl> + <nl> + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; <nl> + assert primaryRanges . size ( ) = = 1 ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " B " ) , new StringToken ( " C " ) ) ) ; <nl> + <nl> + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) ) ; <nl> + assert primaryRanges . size ( ) = = 1 ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " A " ) , new StringToken ( " B " ) ) ) ; <nl> + <nl> + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) ) ; <nl> + assert primaryRanges . size ( ) = = 1 ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " C " ) , new StringToken ( " D " ) ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testPrimaryRangesWithNetworkTopologyStrategyOneDCOnly ( ) throws Exception <nl> + { <nl> + TokenMetadata metadata = StorageService . instance . getTokenMetadata ( ) ; <nl> + metadata . clearUnsafe ( ) ; <nl> + / / DC1 <nl> + metadata . updateNormalToken ( new StringToken ( " A " ) , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; <nl> + metadata . updateNormalToken ( new StringToken ( " C " ) , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; <nl> + / / DC2 <nl> + metadata . updateNormalToken ( new StringToken ( " B " ) , InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) ) ; <nl> + metadata . updateNormalToken ( new StringToken ( " D " ) , InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) ) ; <nl> + <nl> + Map < String , String > configOptions = new HashMap < String , String > ( ) ; <nl> + configOptions . put ( " DC2 " , " 2 " ) ; <nl> + <nl> + Table . clear ( " Keyspace1 " ) ; <nl> + KSMetaData meta = KSMetaData . newKeyspace ( " Keyspace1 " , " NetworkTopologyStrategy " , configOptions , false ) ; <nl> + Schema . instance . setTableDefinition ( meta ) ; <nl> + <nl> + / / endpoints in DC1 should not have primary range <nl> + Collection < Range < Token > > primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; <nl> + assert primaryRanges . isEmpty ( ) ; <nl> + <nl> + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; <nl> + assert primaryRanges . isEmpty ( ) ; <nl> + <nl> + / / endpoints in DC2 should have primary ranges which also cover DC1 <nl> + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) ) ; <nl> + assert primaryRanges . size ( ) = = 2 ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " D " ) , new StringToken ( " A " ) ) ) ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " A " ) , new StringToken ( " B " ) ) ) ; <nl> + <nl> + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) ) ; <nl> + assert primaryRanges . size ( ) = = 2 ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " C " ) , new StringToken ( " D " ) ) ) ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " B " ) , new StringToken ( " C " ) ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testPrimaryRangesWithVnodes ( ) throws Exception <nl> + { <nl> + TokenMetadata metadata = StorageService . instance . getTokenMetadata ( ) ; <nl> + metadata . clearUnsafe ( ) ; <nl> + / / DC1 <nl> + Multimap < InetAddress , Token > dc1 = HashMultimap . create ( ) ; <nl> + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) , new StringToken ( " A " ) ) ; <nl> + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) , new StringToken ( " E " ) ) ; <nl> + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) , new StringToken ( " H " ) ) ; <nl> + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) , new StringToken ( " C " ) ) ; <nl> + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) , new StringToken ( " I " ) ) ; <nl> + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) , new StringToken ( " J " ) ) ; <nl> + metadata . updateNormalTokens ( dc1 ) ; <nl> + / / DC2 <nl> + Multimap < InetAddress , Token > dc2 = HashMultimap . create ( ) ; <nl> + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) , new StringToken ( " B " ) ) ; <nl> + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) , new StringToken ( " G " ) ) ; <nl> + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) , new StringToken ( " L " ) ) ; <nl> + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) , new StringToken ( " D " ) ) ; <nl> + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) , new StringToken ( " F " ) ) ; <nl> + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) , new StringToken ( " K " ) ) ; <nl> + metadata . updateNormalTokens ( dc2 ) ; <nl> + <nl> + Map < String , String > configOptions = new HashMap < String , String > ( ) ; <nl> + configOptions . put ( " DC2 " , " 2 " ) ; <nl> + <nl> + Table . clear ( " Keyspace1 " ) ; <nl> + KSMetaData meta = KSMetaData . newKeyspace ( " Keyspace1 " , " NetworkTopologyStrategy " , configOptions , false ) ; <nl> + Schema . instance . setTableDefinition ( meta ) ; <nl> + <nl> + / / endpoints in DC1 should not have primary range <nl> + Collection < Range < Token > > primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; <nl> + assert primaryRanges . isEmpty ( ) ; <nl> + <nl> + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; <nl> + assert primaryRanges . isEmpty ( ) ; <nl> + <nl> + / / endpoints in DC2 should have primary ranges which also cover DC1 <nl> + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) ) ; <nl> + assert primaryRanges . size ( ) = = 4 ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " A " ) , new StringToken ( " B " ) ) ) ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " F " ) , new StringToken ( " G " ) ) ) ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " K " ) , new StringToken ( " L " ) ) ) ; <nl> + / / because / 127 . 0 . 0 . 4 holds token " B " which is the next to token " A " from / 127 . 0 . 0 . 1 , <nl> + / / the node covers range ( L , A ] <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " L " ) , new StringToken ( " A " ) ) ) ; <nl> + <nl> + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) ) ; <nl> + assert primaryRanges . size ( ) = = 8 ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " C " ) , new StringToken ( " D " ) ) ) ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " E " ) , new StringToken ( " F " ) ) ) ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " J " ) , new StringToken ( " K " ) ) ) ; <nl> + / / ranges from / 127 . 0 . 0 . 1 <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " D " ) , new StringToken ( " E " ) ) ) ; <nl> + / / the next token to " H " in DC2 is " K " in / 127 . 0 . 0 . 5 , so ( G , H ] goes to / 127 . 0 . 0 . 5 <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " G " ) , new StringToken ( " H " ) ) ) ; <nl> + / / ranges from / 127 . 0 . 0 . 2 <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " B " ) , new StringToken ( " C " ) ) ) ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " H " ) , new StringToken ( " I " ) ) ) ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " I " ) , new StringToken ( " J " ) ) ) ; <nl> + } <nl> + @ Test <nl> + public void testPrimaryRangesWithSimpleStrategy ( ) throws Exception <nl> + { <nl> + TokenMetadata metadata = StorageService . instance . getTokenMetadata ( ) ; <nl> + metadata . clearUnsafe ( ) ; <nl> + <nl> + metadata . updateNormalToken ( new StringToken ( " A " ) , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; <nl> + metadata . updateNormalToken ( new StringToken ( " B " ) , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; <nl> + metadata . updateNormalToken ( new StringToken ( " C " ) , InetAddress . getByName ( " 127 . 0 . 0 . 3 " ) ) ; <nl> + <nl> + Map < String , String > configOptions = new HashMap < String , String > ( ) ; <nl> + configOptions . put ( " replication _ factor " , " 2 " ) ; <nl> + <nl> + Table . clear ( " Keyspace1 " ) ; <nl> + KSMetaData meta = KSMetaData . newKeyspace ( " Keyspace1 " , " SimpleStrategy " , configOptions , false ) ; <nl> + Schema . instance . setTableDefinition ( meta ) ; <nl> + <nl> + Collection < Range < Token > > primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; <nl> + assert primaryRanges . size ( ) = = 1 ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " C " ) , new StringToken ( " A " ) ) ) ; <nl> + <nl> + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; <nl> + assert primaryRanges . size ( ) = = 1 ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " A " ) , new StringToken ( " B " ) ) ) ; <nl> + <nl> + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 3 " ) ) ; <nl> + assert primaryRanges . size ( ) = = 1 ; <nl> + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " B " ) , new StringToken ( " C " ) ) ) ; <nl> + } <nl> }

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index afd45e5 . . d6ebe7a 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 3 . 0 . 0 - rc1 
 + * Small optimizations of sstable index serialization ( CASSANDRA - 10232 ) 
 * Support for both encrypted and unencrypted native transport connections ( CASSANDRA - 9590 ) 
 
 
 diff - - git a / src / java / org / apache / cassandra / db / ColumnIndex . java b / src / java / org / apache / cassandra / db / ColumnIndex . java 
 index 9eef23e . . b350f90 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnIndex . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnIndex . java 
 @ @ - 31 , 14 + 31 , 16 @ @ import org . apache . cassandra . utils . ByteBufferUtil ; 
 
 public class ColumnIndex 
 { 
 + public final long partitionHeaderLength ; 
 public final List < IndexHelper . IndexInfo > columnsIndex ; 
 
 - private static final ColumnIndex EMPTY = new ColumnIndex ( Collections . < IndexHelper . IndexInfo > emptyList ( ) ) ; 
 + private static final ColumnIndex EMPTY = new ColumnIndex ( - 1 , Collections . < IndexHelper . IndexInfo > emptyList ( ) ) ; 
 
 - private ColumnIndex ( List < IndexHelper . IndexInfo > columnsIndex ) 
 + private ColumnIndex ( long partitionHeaderLength , List < IndexHelper . IndexInfo > columnsIndex ) 
 { 
 assert columnsIndex ! = null ; 
 
 + this . partitionHeaderLength = partitionHeaderLength ; 
 this . columnsIndex = columnsIndex ; 
 } 
 
 @ @ - 67 , 8 + 69 , 10 @ @ public class ColumnIndex 
 private final SerializationHeader header ; 
 private final int version ; 
 
 - private final ColumnIndex result ; 
 + private final List < IndexHelper . IndexInfo > columnsIndex = new ArrayList < > ( ) ; 
 private final long initialPosition ; 
 + private long headerLength = - 1 ; 
 + 
 private long startPosition = - 1 ; 
 
 private int written ; 
 @ @ - 87 , 8 + 91 , 6 @ @ public class ColumnIndex 
 this . writer = writer ; 
 this . header = header ; 
 this . version = version ; 
 - 
 - this . result = new ColumnIndex ( new ArrayList < IndexHelper . IndexInfo > ( ) ) ; 
 this . initialPosition = writer . getFilePointer ( ) ; 
 } 
 
 @ @ - 103 , 6 + 105 , 7 @ @ public class ColumnIndex 
 public ColumnIndex build ( ) throws IOException 
 { 
 writePartitionHeader ( iterator ) ; 
 + this . headerLength = writer . getFilePointer ( ) - initialPosition ; 
 
 while ( iterator . hasNext ( ) ) 
 add ( iterator . next ( ) ) ; 
 @ @ - 119 , 10 + 122 , 9 @ @ public class ColumnIndex 
 { 
 IndexHelper . IndexInfo cIndexInfo = new IndexHelper . IndexInfo ( firstClustering , 
 lastClustering , 
 - startPosition , 
 currentPosition ( ) - startPosition , 
 openMarker ) ; 
 - result . columnsIndex . add ( cIndexInfo ) ; 
 + columnsIndex . add ( cIndexInfo ) ; 
 firstClustering = null ; 
 } 
 
 @ @ - 164 , 8 + 166 , 8 @ @ public class ColumnIndex 
 addIndexBlock ( ) ; 
 
 / / we should always have at least one computed index block , but we only write it out if there is more than that . 
 - assert result . columnsIndex . size ( ) > 0 ; 
 - return result ; 
 + assert columnsIndex . size ( ) > 0 & & headerLength > = 0 ; 
 + return new ColumnIndex ( headerLength , columnsIndex ) ; 
 } 
 } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / RowIndexEntry . java b / src / java / org / apache / cassandra / db / RowIndexEntry . java 
 index e783508 . . f63e893 100644 
 - - - a / src / java / org / apache / cassandra / db / RowIndexEntry . java 
 + + + b / src / java / org / apache / cassandra / db / RowIndexEntry . java 
 @ @ - 47 , 7 + 47 , 7 @ @ public class RowIndexEntry < T > implements IMeasurableMemory 
 this . position = position ; 
 } 
 
 - public int promotedSize ( CFMetaData metadata , Version version , SerializationHeader header ) 
 + protected int promotedSize ( IndexHelper . IndexInfo . Serializer idxSerializer ) 
 { 
 return 0 ; 
 } 
 @ @ - 61 , 7 + 61 , 7 @ @ public class RowIndexEntry < T > implements IMeasurableMemory 
 / / since if there are insufficient columns to be worth indexing we ' re going to seek to 
 / / the beginning of the row anyway , so we might as well read the tombstone there as well . 
 if ( index . columnsIndex . size ( ) > 1 ) 
 - return new IndexedEntry ( position , deletionTime , index . columnsIndex ) ; 
 + return new IndexedEntry ( position , deletionTime , index . partitionHeaderLength , index . columnsIndex ) ; 
 else 
 return new RowIndexEntry < > ( position ) ; 
 } 
 @ @ - 89 , 6 + 89 , 16 @ @ public class RowIndexEntry < T > implements IMeasurableMemory 
 return 0 ; 
 } 
 
 + / * * 
 + * The length of the row header ( partition key , partition deletion and static row ) . 
 + * This value is only provided for indexed entries and this method will throw 
 + * { @ code UnsupportedOperationException } if { @ code ! isIndexed ( ) } . 
 + * / 
 + public long headerLength ( ) 
 + { 
 + throw new UnsupportedOperationException ( ) ; 
 + } 
 + 
 public List < T > columnsIndex ( ) 
 { 
 return Collections . emptyList ( ) ; 
 @ @ - 108 , 48 + 118 , 81 @ @ public class RowIndexEntry < T > implements IMeasurableMemory 
 
 public static class Serializer implements IndexSerializer < IndexHelper . IndexInfo > 
 { 
 - private final CFMetaData metadata ; 
 + private final IndexHelper . IndexInfo . Serializer idxSerializer ; 
 private final Version version ; 
 - private final SerializationHeader header ; 
 
 public Serializer ( CFMetaData metadata , Version version , SerializationHeader header ) 
 { 
 - this . metadata = metadata ; 
 + this . idxSerializer = new IndexHelper . IndexInfo . Serializer ( metadata , version , header ) ; 
 this . version = version ; 
 - this . header = header ; 
 } 
 
 public void serialize ( RowIndexEntry < IndexHelper . IndexInfo > rie , DataOutputPlus out ) throws IOException 
 { 
 - out . writeLong ( rie . position ) ; 
 - out . writeInt ( rie . promotedSize ( metadata , version , header ) ) ; 
 + assert version . storeRows ( ) : " We read old index files but we should never write them " ; 
 + 
 + out . writeUnsignedVInt ( rie . position ) ; 
 + out . writeUnsignedVInt ( rie . promotedSize ( idxSerializer ) ) ; 
 
 if ( rie . isIndexed ( ) ) 
 { 
 + out . writeUnsignedVInt ( rie . headerLength ( ) ) ; 
 DeletionTime . serializer . serialize ( rie . deletionTime ( ) , out ) ; 
 - out . writeInt ( rie . columnsIndex ( ) . size ( ) ) ; 
 - IndexHelper . IndexInfo . Serializer idxSerializer = metadata . serializers ( ) . indexSerializer ( version ) ; 
 + out . writeUnsignedVInt ( rie . columnsIndex ( ) . size ( ) ) ; 
 for ( IndexHelper . IndexInfo info : rie . columnsIndex ( ) ) 
 - idxSerializer . serialize ( info , out , header ) ; 
 + idxSerializer . serialize ( info , out ) ; 
 } 
 } 
 
 public RowIndexEntry < IndexHelper . IndexInfo > deserialize ( DataInputPlus in ) throws IOException 
 { 
 - long position = in . readLong ( ) ; 
 + if ( ! version . storeRows ( ) ) 
 + { 
 + long position = in . readLong ( ) ; 
 + 
 + int size = in . readInt ( ) ; 
 + if ( size > 0 ) 
 + { 
 + DeletionTime deletionTime = DeletionTime . serializer . deserialize ( in ) ; 
 + 
 + int entries = in . readInt ( ) ; 
 + List < IndexHelper . IndexInfo > columnsIndex = new ArrayList < > ( entries ) ; 
 + 
 + / / The old format didn ' t saved the partition header length per - se , but rather for each entry it ' s 
 + / / offset from the beginning of the row . We don ' t use that offset anymore , but we do need the 
 + / / header length so we basically need the first entry offset . And so we inline the deserialization 
 + / / of the first index entry to get that information . While this is a bit ugly , we ' ll get rid of that 
 + / / code once pre - 3 . 0 backward compatibility is dropped so it feels fine as a temporary hack . 
 + ClusteringPrefix firstName = idxSerializer . clusteringSerializer . deserialize ( in ) ; 
 + ClusteringPrefix lastName = idxSerializer . clusteringSerializer . deserialize ( in ) ; 
 + long headerLength = in . readLong ( ) ; 
 + long width = in . readLong ( ) ; 
 + 
 + columnsIndex . add ( new IndexHelper . IndexInfo ( firstName , lastName , width , null ) ) ; 
 + for ( int i = 1 ; i < entries ; i + + ) 
 + columnsIndex . add ( idxSerializer . deserialize ( in ) ) ; 
 + 
 + return new IndexedEntry ( position , deletionTime , headerLength , columnsIndex ) ; 
 + } 
 + else 
 + { 
 + return new RowIndexEntry < > ( position ) ; 
 + } 
 + } 
 
 - int size = in . readInt ( ) ; 
 + long position = in . readUnsignedVInt ( ) ; 
 + 
 + int size = ( int ) in . readUnsignedVInt ( ) ; 
 if ( size > 0 ) 
 { 
 + long headerLength = in . readUnsignedVInt ( ) ; 
 DeletionTime deletionTime = DeletionTime . serializer . deserialize ( in ) ; 
 - 
 - int entries = in . readInt ( ) ; 
 - IndexHelper . IndexInfo . Serializer idxSerializer = metadata . serializers ( ) . indexSerializer ( version ) ; 
 + int entries = ( int ) in . readUnsignedVInt ( ) ; 
 List < IndexHelper . IndexInfo > columnsIndex = new ArrayList < > ( entries ) ; 
 for ( int i = 0 ; i < entries ; i + + ) 
 - columnsIndex . add ( idxSerializer . deserialize ( in , header ) ) ; 
 + columnsIndex . add ( idxSerializer . deserialize ( in ) ) ; 
 
 - return new IndexedEntry ( position , deletionTime , columnsIndex ) ; 
 + return new IndexedEntry ( position , deletionTime , headerLength , columnsIndex ) ; 
 } 
 else 
 { 
 @ @ - 157 , 15 + 200 , 23 @ @ public class RowIndexEntry < T > implements IMeasurableMemory 
 } 
 } 
 
 - public static void skip ( DataInput in ) throws IOException 
 + / / Reads only the data ' position ' of the index entry and returns it . Note that this left ' in ' in the middle 
 + / / of reading an entry , so this is only useful if you know what you are doing and in most case ' deserialize ' 
 + / / should be used instead . 
 + public static long readPosition ( DataInputPlus in , Version version ) throws IOException 
 { 
 - in . readLong ( ) ; 
 - skipPromotedIndex ( in ) ; 
 + return version . storeRows ( ) ? in . readUnsignedVInt ( ) : in . readLong ( ) ; 
 } 
 
 - public static void skipPromotedIndex ( DataInput in ) throws IOException 
 + public static void skip ( DataInputPlus in , Version version ) throws IOException 
 { 
 - int size = in . readInt ( ) ; 
 + readPosition ( in , version ) ; 
 + skipPromotedIndex ( in , version ) ; 
 + } 
 + 
 + public static void skipPromotedIndex ( DataInputPlus in , Version version ) throws IOException 
 + { 
 + int size = version . storeRows ( ) ? ( int ) in . readUnsignedVInt ( ) : in . readInt ( ) ; 
 if ( size < = 0 ) 
 return ; 
 
 @ @ - 174 , 21 + 225 , 21 @ @ public class RowIndexEntry < T > implements IMeasurableMemory 
 
 public int serializedSize ( RowIndexEntry < IndexHelper . IndexInfo > rie ) 
 { 
 - int size = TypeSizes . sizeof ( rie . position ) + TypeSizes . sizeof ( rie . promotedSize ( metadata , version , header ) ) ; 
 + assert version . storeRows ( ) : " We read old index files but we should never write them " ; 
 + 
 + int size = TypeSizes . sizeofUnsignedVInt ( rie . position ) + TypeSizes . sizeofUnsignedVInt ( rie . promotedSize ( idxSerializer ) ) ; 
 
 if ( rie . isIndexed ( ) ) 
 { 
 List < IndexHelper . IndexInfo > index = rie . columnsIndex ( ) ; 
 
 + size + = TypeSizes . sizeofUnsignedVInt ( rie . headerLength ( ) ) ; 
 size + = DeletionTime . serializer . serializedSize ( rie . deletionTime ( ) ) ; 
 - size + = TypeSizes . sizeof ( index . size ( ) ) ; 
 + size + = TypeSizes . sizeofUnsignedVInt ( index . size ( ) ) ; 
 
 - IndexHelper . IndexInfo . Serializer idxSerializer = metadata . serializers ( ) . indexSerializer ( version ) ; 
 for ( IndexHelper . IndexInfo info : index ) 
 - size + = idxSerializer . serializedSize ( info , header ) ; 
 + size + = idxSerializer . serializedSize ( info ) ; 
 } 
 - 
 - 
 return size ; 
 } 
 } 
 @ @ - 199 , 17 + 250 , 21 @ @ public class RowIndexEntry < T > implements IMeasurableMemory 
 private static class IndexedEntry extends RowIndexEntry < IndexHelper . IndexInfo > 
 { 
 private final DeletionTime deletionTime ; 
 + 
 + / / The offset in the file when the index entry end 
 + private final long headerLength ; 
 private final List < IndexHelper . IndexInfo > columnsIndex ; 
 private static final long BASE _ SIZE = 
 - ObjectSizes . measure ( new IndexedEntry ( 0 , DeletionTime . LIVE , Arrays . < IndexHelper . IndexInfo > asList ( null , null ) ) ) 
 + ObjectSizes . measure ( new IndexedEntry ( 0 , DeletionTime . LIVE , 0 , Arrays . < IndexHelper . IndexInfo > asList ( null , null ) ) ) 
 + ObjectSizes . measure ( new ArrayList < > ( 1 ) ) ; 
 
 - private IndexedEntry ( long position , DeletionTime deletionTime , List < IndexHelper . IndexInfo > columnsIndex ) 
 + private IndexedEntry ( long position , DeletionTime deletionTime , long headerLength , List < IndexHelper . IndexInfo > columnsIndex ) 
 { 
 super ( position ) ; 
 assert deletionTime ! = null ; 
 assert columnsIndex ! = null & & columnsIndex . size ( ) > 1 ; 
 this . deletionTime = deletionTime ; 
 + this . headerLength = headerLength ; 
 this . columnsIndex = columnsIndex ; 
 } 
 
 @ @ - 220 , 19 + 275 , 25 @ @ public class RowIndexEntry < T > implements IMeasurableMemory 
 } 
 
 @ Override 
 + public long headerLength ( ) 
 + { 
 + return headerLength ; 
 + } 
 + 
 + @ Override 
 public List < IndexHelper . IndexInfo > columnsIndex ( ) 
 { 
 return columnsIndex ; 
 } 
 
 @ Override 
 - public int promotedSize ( CFMetaData metadata , Version version , SerializationHeader header ) 
 + protected int promotedSize ( IndexHelper . IndexInfo . Serializer idxSerializer ) 
 { 
 - long size = DeletionTime . serializer . serializedSize ( deletionTime ) ; 
 - size + = TypeSizes . sizeof ( columnsIndex . size ( ) ) ; / / number of entries 
 - IndexHelper . IndexInfo . Serializer idxSerializer = metadata . serializers ( ) . indexSerializer ( version ) ; 
 + long size = TypeSizes . sizeofUnsignedVInt ( headerLength ) 
 + + DeletionTime . serializer . serializedSize ( deletionTime ) 
 + + TypeSizes . sizeofUnsignedVInt ( columnsIndex . size ( ) ) ; / / number of entries 
 for ( IndexHelper . IndexInfo info : columnsIndex ) 
 - size + = idxSerializer . serializedSize ( info , header ) ; 
 + size + = idxSerializer . serializedSize ( info ) ; 
 
 return Ints . checkedCast ( size ) ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / Serializers . java b / src / java / org / apache / cassandra / db / Serializers . java 
 index 2561bbe . . 9b29d89 100644 
 - - - a / src / java / org / apache / cassandra / db / Serializers . java 
 + + + b / src / java / org / apache / cassandra / db / Serializers . java 
 @ @ - 43 , 11 + 43 , 6 @ @ public class Serializers 
 this . metadata = metadata ; 
 } 
 
 - public IndexInfo . Serializer indexSerializer ( Version version ) 
 - { 
 - return new IndexInfo . Serializer ( metadata , version ) ; 
 - } 
 - 
 / / TODO : Once we drop support for old ( pre - 3 . 0 ) sstables , we can drop this method and inline the calls to 
 / / ClusteringPrefix . serializer in IndexHelper directly . At which point this whole class probably becomes 
 / / unecessary ( since IndexInfo . Serializer won ' t depend on the metadata either ) . 
 diff - - git a / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java b / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java 
 index 87a57c6 . . c075a2b 100644 
 - - - a / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java 
 + + + b / src / java / org / apache / cassandra / db / columniterator / AbstractSSTableIterator . java 
 @ @ - 412 , 6 + 412 , 7 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator 
 
 private final RowIndexEntry indexEntry ; 
 private final List < IndexHelper . IndexInfo > indexes ; 
 + private final long [ ] blockOffsets ; 
 private final boolean reversed ; 
 
 private int currentIndexIdx ; 
 @ @ - 427 , 6 + 428 , 14 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator 
 this . indexes = indexEntry . columnsIndex ( ) ; 
 this . reversed = reversed ; 
 this . currentIndexIdx = reversed ? indexEntry . columnsIndex ( ) . size ( ) : - 1 ; 
 + 
 + this . blockOffsets = new long [ indexes . size ( ) ] ; 
 + long offset = indexEntry . position + indexEntry . headerLength ( ) ; 
 + for ( int i = 0 ; i < blockOffsets . length ; i + + ) 
 + { 
 + blockOffsets [ i ] = offset ; 
 + offset + = indexes . get ( i ) . width ; 
 + } 
 } 
 
 public boolean isDone ( ) 
 @ @ - 438 , 7 + 447 , 7 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator 
 public void setToBlock ( int blockIdx ) throws IOException 
 { 
 if ( blockIdx > = 0 & & blockIdx < indexes . size ( ) ) 
 - reader . seekToPosition ( indexEntry . position + indexes . get ( blockIdx ) . offset ) ; 
 + reader . seekToPosition ( blockOffsets [ blockIdx ] ) ; 
 
 currentIndexIdx = blockIdx ; 
 reader . openMarker = blockIdx > 0 ? indexes . get ( blockIdx - 1 ) . endOpenMarker : null ; 
 @ @ - 461 , 7 + 470 , 7 @ @ abstract class AbstractSSTableIterator implements SliceableUnfilteredRowIterator 
 
 / / We have to set the mark , and we have to set it at the beginning of the block . So if we ' re not at the beginning of the block , this forces us to a weird seek dance . 
 / / This can only happen when reading old file however . 
 - long startOfBlock = indexEntry . position + indexes . get ( currentIndexIdx ) . offset ; 
 + long startOfBlock = blockOffsets [ currentIndexIdx ] ; 
 long currentFilePointer = reader . file . getFilePointer ( ) ; 
 if ( startOfBlock = = currentFilePointer ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java 
 index 4dabe69 . . e95af29 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java 
 @ @ - 37 , 34 + 37 , 6 @ @ import org . apache . cassandra . utils . * ; 
 * / 
 public class IndexHelper 
 { 
 - public static void skipBloomFilter ( DataInput in ) throws IOException 
 - { 
 - int size = in . readInt ( ) ; 
 - FileUtils . skipBytesFully ( in , size ) ; 
 - } 
 - 
 - / * * 
 - * Skip the index 
 - * @ param in the data input from which the index should be skipped 
 - * @ throws IOException if an I / O error occurs . 
 - * / 
 - public static void skipIndex ( DataInput in ) throws IOException 
 - { 
 - / * read only the column index list * / 
 - int columnIndexSize = in . readInt ( ) ; 
 - / * skip the column index data * / 
 - if ( in instanceof FileDataInput ) 
 - { 
 - FileUtils . skipBytesFully ( in , columnIndexSize ) ; 
 - } 
 - else 
 - { 
 - / / skip bytes 
 - byte [ ] skip = new byte [ columnIndexSize ] ; 
 - in . readFully ( skip ) ; 
 - } 
 - } 
 - 
 / * * 
 * The index of the IndexInfo in which a scan starting with @ name should begin . 
 * 
 @ @ - 78 , 7 + 50 , 7 @ @ public class IndexHelper 
 * / 
 public static int indexFor ( ClusteringPrefix name , List < IndexInfo > indexList , ClusteringComparator comparator , boolean reversed , int lastIndex ) 
 { 
 - IndexInfo target = new IndexInfo ( name , name , 0 , 0 , null ) ; 
 + IndexInfo target = new IndexInfo ( name , name , 0 , null ) ; 
 / * 
 Take the example from the unit test , and say your index looks like this : 
 [ 0 . . 5 ] [ 10 . . 15 ] [ 20 . . 25 ] 
 @ @ - 115 , 12 + 87 , 11 @ @ public class IndexHelper 
 
 public static class IndexInfo 
 { 
 - private static final long EMPTY _ SIZE = ObjectSizes . measure ( new IndexInfo ( null , null , 0 , 0 , null ) ) ; 
 + private static final long EMPTY _ SIZE = ObjectSizes . measure ( new IndexInfo ( null , null , 0 , null ) ) ; 
 
 public final long width ; 
 - public final ClusteringPrefix lastName ; 
 public final ClusteringPrefix firstName ; 
 - public final long offset ; 
 + public final ClusteringPrefix lastName ; 
 
 / / If at the end of the index block there is an open range tombstone marker , this marker 
 / / deletion infos . null otherwise . 
 @ @ - 128 , 73 + 99 , 77 @ @ public class IndexHelper 
 
 public IndexInfo ( ClusteringPrefix firstName , 
 ClusteringPrefix lastName , 
 - long offset , 
 long width , 
 DeletionTime endOpenMarker ) 
 { 
 this . firstName = firstName ; 
 this . lastName = lastName ; 
 - this . offset = offset ; 
 this . width = width ; 
 this . endOpenMarker = endOpenMarker ; 
 } 
 
 public static class Serializer 
 { 
 - private final CFMetaData metadata ; 
 + / / This is the default index size that we use to delta - encode width when serializing so we get better vint - encoding . 
 + / / This is imperfect as user can change the index size and ideally we would save the index size used with each index file 
 + / / to use as base . However , that ' s a bit more involved a change that we want for now and very seldom do use change the index 
 + / / size so using the default is almost surely better than using no base at all . 
 + private static final long WIDTH _ BASE = 64 * 1024 ; 
 + 
 + / / TODO : Only public for use in RowIndexEntry for backward compatibility code . Can be made private once backward compatibility is dropped . 
 + public final ISerializer < ClusteringPrefix > clusteringSerializer ; 
 private final Version version ; 
 
 - public Serializer ( CFMetaData metadata , Version version ) 
 + public Serializer ( CFMetaData metadata , Version version , SerializationHeader header ) 
 { 
 - this . metadata = metadata ; 
 + this . clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; 
 this . version = version ; 
 } 
 
 - public void serialize ( IndexInfo info , DataOutputPlus out , SerializationHeader header ) throws IOException 
 + public void serialize ( IndexInfo info , DataOutputPlus out ) throws IOException 
 { 
 - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; 
 + assert version . storeRows ( ) : " We read old index files but we should never write them " ; 
 + 
 clusteringSerializer . serialize ( info . firstName , out ) ; 
 clusteringSerializer . serialize ( info . lastName , out ) ; 
 - out . writeLong ( info . offset ) ; 
 - out . writeLong ( info . width ) ; 
 + out . writeVInt ( info . width - WIDTH _ BASE ) ; 
 
 - if ( version . storeRows ( ) ) 
 - { 
 - out . writeBoolean ( info . endOpenMarker ! = null ) ; 
 - if ( info . endOpenMarker ! = null ) 
 - DeletionTime . serializer . serialize ( info . endOpenMarker , out ) ; 
 - } 
 + out . writeBoolean ( info . endOpenMarker ! = null ) ; 
 + if ( info . endOpenMarker ! = null ) 
 + DeletionTime . serializer . serialize ( info . endOpenMarker , out ) ; 
 } 
 
 - public IndexInfo deserialize ( DataInputPlus in , SerializationHeader header ) throws IOException 
 + public IndexInfo deserialize ( DataInputPlus in ) throws IOException 
 { 
 - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; 
 - 
 ClusteringPrefix firstName = clusteringSerializer . deserialize ( in ) ; 
 ClusteringPrefix lastName = clusteringSerializer . deserialize ( in ) ; 
 - long offset = in . readLong ( ) ; 
 - long width = in . readLong ( ) ; 
 - DeletionTime endOpenMarker = version . storeRows ( ) & & in . readBoolean ( ) 
 - ? DeletionTime . serializer . deserialize ( in ) 
 - : null ; 
 - 
 - return new IndexInfo ( firstName , lastName , offset , width , endOpenMarker ) ; 
 + long width ; 
 + DeletionTime endOpenMarker = null ; 
 + if ( version . storeRows ( ) ) 
 + { 
 + width = in . readVInt ( ) + WIDTH _ BASE ; 
 + if ( in . readBoolean ( ) ) 
 + endOpenMarker = DeletionTime . serializer . deserialize ( in ) ; 
 + } 
 + else 
 + { 
 + in . readLong ( ) ; / / skip offset 
 + width = in . readLong ( ) ; 
 + } 
 + return new IndexInfo ( firstName , lastName , width , endOpenMarker ) ; 
 } 
 
 - public long serializedSize ( IndexInfo info , SerializationHeader header ) 
 + public long serializedSize ( IndexInfo info ) 
 { 
 - ISerializer < ClusteringPrefix > clusteringSerializer = metadata . serializers ( ) . indexEntryClusteringPrefixSerializer ( version , header ) ; 
 + assert version . storeRows ( ) : " We read old index files but we should never write them " ; 
 + 
 long size = clusteringSerializer . serializedSize ( info . firstName ) 
 + clusteringSerializer . serializedSize ( info . lastName ) 
 - + TypeSizes . sizeof ( info . offset ) 
 - + TypeSizes . sizeof ( info . width ) ; 
 + + TypeSizes . sizeofVInt ( info . width - WIDTH _ BASE ) 
 + + TypeSizes . sizeof ( info . endOpenMarker ! = null ) ; 
 
 - if ( version . storeRows ( ) ) 
 - { 
 - size + = TypeSizes . sizeof ( info . endOpenMarker ! = null ) ; 
 - if ( info . endOpenMarker ! = null ) 
 - size + = DeletionTime . serializer . serializedSize ( info . endOpenMarker ) ; 
 - } 
 + if ( info . endOpenMarker ! = null ) 
 + size + = DeletionTime . serializer . serializedSize ( info . endOpenMarker ) ; 
 return size ; 
 } 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 index 6f1e2f4 . . f02b9d1 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 @ @ - 17 , 7 + 17 , 6 @ @ 
 * / 
 package org . apache . cassandra . io . sstable ; 
 
 - import java . io . DataInput ; 
 import java . io . File ; 
 import java . io . IOException ; 
 
 @ @ - 27 , 6 + 26 , 7 @ @ import org . apache . cassandra . config . CFMetaData ; 
 import org . apache . cassandra . db . DecoratedKey ; 
 import org . apache . cassandra . db . RowIndexEntry ; 
 import org . apache . cassandra . dht . IPartitioner ; 
 + import org . apache . cassandra . io . util . DataInputPlus ; 
 import org . apache . cassandra . io . util . RandomAccessReader ; 
 import org . apache . cassandra . utils . ByteBufferUtil ; 
 import org . apache . cassandra . utils . CloseableIterator ; 
 @ @ - 49 , 7 + 49 , 7 @ @ public class KeyIterator extends AbstractIterator < DecoratedKey > implements Close 
 in = RandomAccessReader . open ( path ) ; 
 } 
 
 - public DataInput get ( ) 
 + public DataInputPlus get ( ) 
 { 
 maybeInit ( ) ; 
 return in ; 
 @ @ - 80 , 12 + 80 , 14 @ @ public class KeyIterator extends AbstractIterator < DecoratedKey > implements Close 
 } 
 } 
 
 + private final Descriptor desc ; 
 private final In in ; 
 private final IPartitioner partitioner ; 
 
 
 public KeyIterator ( Descriptor desc , CFMetaData metadata ) 
 { 
 + this . desc = desc ; 
 in = new In ( new File ( desc . filenameFor ( Component . PRIMARY _ INDEX ) ) ) ; 
 partitioner = metadata . partitioner ; 
 } 
 @ @ - 98 , 7 + 100 , 7 @ @ public class KeyIterator extends AbstractIterator < DecoratedKey > implements Close 
 return endOfData ( ) ; 
 
 DecoratedKey key = partitioner . decorateKey ( ByteBufferUtil . readWithShortLength ( in . get ( ) ) ) ; 
 - RowIndexEntry . Serializer . skip ( in . get ( ) ) ; / / skip remainder of the entry 
 + RowIndexEntry . Serializer . skip ( in . get ( ) , desc . version ) ; / / skip remainder of the entry 
 return key ; 
 } 
 catch ( IOException e ) 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTable . java b / src / java / org / apache / cassandra / io / sstable / SSTable . java 
 index b86d9b4 . . 63b8f3e 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTable . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTable . java 
 @ @ - 237 , 7 + 237 , 7 @ @ public abstract class SSTable 
 while ( ifile . getFilePointer ( ) < BYTES _ CAP & & keys < SAMPLES _ CAP ) 
 { 
 ByteBufferUtil . skipShortLength ( ifile ) ; 
 - RowIndexEntry . Serializer . skip ( ifile ) ; 
 + RowIndexEntry . Serializer . skip ( ifile , descriptor . version ) ; 
 keys + + ; 
 } 
 assert keys > 0 & & ifile . getFilePointer ( ) > 0 & & ifile . length ( ) > 0 : " Unexpected empty index file : " + ifile ; 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / format / SSTableReader . java b / src / java / org / apache / cassandra / io / sstable / format / SSTableReader . java 
 index 5d8ab50 . . b958240 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / format / SSTableReader . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / format / SSTableReader . java 
 @ @ - 922 , 7 + 922 , 7 @ @ public abstract class SSTableReader extends SSTable implements SelfRefCounted < SS 
 if ( summaryEntriesChecked = = Downsampling . BASE _ SAMPLING _ LEVEL ) 
 return true ; 
 } 
 - RowIndexEntry . Serializer . skip ( in ) ; 
 + RowIndexEntry . Serializer . skip ( in , descriptor . version ) ; 
 i + + ; 
 } 
 } 
 @ @ - 1199 , 7 + 1199 , 7 @ @ public abstract class SSTableReader extends SSTable implements SelfRefCounted < SS 
 while ( ( indexPosition = primaryIndex . getFilePointer ( ) ) ! = indexSize ) 
 { 
 summaryBuilder . maybeAddEntry ( decorateKey ( ByteBufferUtil . readWithShortLength ( primaryIndex ) ) , indexPosition ) ; 
 - RowIndexEntry . Serializer . skip ( primaryIndex ) ; 
 + RowIndexEntry . Serializer . skip ( primaryIndex , descriptor . version ) ; 
 } 
 
 return summaryBuilder . build ( getPartitioner ( ) ) ; 
 @ @ - 1605 , 7 + 1605 , 7 @ @ public abstract class SSTableReader extends SSTable implements SelfRefCounted < SS 
 if ( indexDecoratedKey . compareTo ( token ) > 0 ) 
 return indexDecoratedKey ; 
 
 - RowIndexEntry . Serializer . skip ( in ) ; 
 + RowIndexEntry . Serializer . skip ( in , descriptor . version ) ; 
 } 
 } 
 catch ( IOException e ) 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / format / big / BigTableReader . java b / src / java / org / apache / cassandra / io / sstable / format / big / BigTableReader . java 
 index 4b66942 . . efd1057 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / format / big / BigTableReader . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / format / big / BigTableReader . java 
 @ @ - 239 , 7 + 239 , 7 @ @ public class BigTableReader extends SSTableReader 
 return indexEntry ; 
 } 
 
 - RowIndexEntry . Serializer . skip ( in ) ; 
 + RowIndexEntry . Serializer . skip ( in , descriptor . version ) ; 
 } 
 } 
 catch ( IOException e ) 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / format / big / BigTableScanner . java b / src / java / org / apache / cassandra / io / sstable / format / big / BigTableScanner . java 
 index d135df0 . . 1a4ac21 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / format / big / BigTableScanner . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / format / big / BigTableScanner . java 
 @ @ - 176 , 14 + 176 , 14 @ @ public class BigTableScanner implements ISSTableScanner 
 if ( indexDecoratedKey . compareTo ( currentRange . left ) > 0 | | currentRange . contains ( indexDecoratedKey ) ) 
 { 
 / / Found , just read the dataPosition and seek into index and data files 
 - long dataPosition = ifile . readLong ( ) ; 
 + long dataPosition = RowIndexEntry . Serializer . readPosition ( ifile , sstable . descriptor . version ) ; 
 ifile . seek ( indexPosition ) ; 
 dfile . seek ( dataPosition ) ; 
 break ; 
 } 
 else 
 { 
 - RowIndexEntry . Serializer . skip ( ifile ) ; 
 + RowIndexEntry . Serializer . skip ( ifile , sstable . descriptor . version ) ; 
 } 
 } 
 } 
 diff - - git a / src / java / org / apache / cassandra / service / CacheService . java b / src / java / org / apache / cassandra / service / CacheService . java 
 index 9213b20 . . a48466a 100644 
 - - - a / src / java / org / apache / cassandra / service / CacheService . java 
 + + + b / src / java / org / apache / cassandra / service / CacheService . java 
 @ @ - 53 , 6 + 53 , 7 @ @ import org . apache . cassandra . db . partitions . CachedPartition ; 
 import org . apache . cassandra . db . context . CounterContext ; 
 import org . apache . cassandra . io . util . DataInputPlus ; 
 import org . apache . cassandra . io . util . DataOutputPlus ; 
 + import org . apache . cassandra . io . sstable . format . big . BigFormat ; 
 import org . apache . cassandra . utils . ByteBufferUtil ; 
 import org . apache . cassandra . utils . FBUtilities ; 
 import org . apache . cassandra . utils . Pair ; 
 @ @ - 469 , 7 + 470 , 11 @ @ public class CacheService implements CacheServiceMBean 
 input . readBoolean ( ) ; / / backwards compatibility for " promoted indexes " boolean 
 if ( reader = = null ) 
 { 
 - RowIndexEntry . Serializer . skipPromotedIndex ( input ) ; 
 + / / The sstable doesn ' t exist anymore , so we can ' t be sure of the exact version and assume its the current version . The only case where we ' ll be 
 + / / wrong is during upgrade , in which case we fail at deserialization . This is not a huge deal however since 1 ) this is unlikely enough that 
 + / / this won ' t affect many users ( if any ) and only once , 2 ) this doesn ' t prevent the node from starting and 3 ) CASSANDRA - 10219 shows that this 
 + / / part of the code has been broken for a while without anyone noticing ( it is , btw , still broken until CASSANDRA - 10219 is fixed ) . 
 + RowIndexEntry . Serializer . skipPromotedIndex ( input , BigFormat . instance . getLatestVersion ( ) ) ; 
 return null ; 
 } 
 RowIndexEntry . IndexSerializer < ? > indexSerializer = reader . descriptor . getFormat ( ) . getIndexSerializer ( reader . metadata , 
 diff - - git a / test / unit / org / apache / cassandra / db / SinglePartitionSliceCommandTest . java b / test / unit / org / apache / cassandra / db / SinglePartitionSliceCommandTest . java 
 index 4b7f15a . . 9f80023 100644 
 - - - a / test / unit / org / apache / cassandra / db / SinglePartitionSliceCommandTest . java 
 + + + b / test / unit / org / apache / cassandra / db / SinglePartitionSliceCommandTest . java 
 @ @ - 132 , 10 + 132 , 10 @ @ public class SinglePartitionSliceCommandTest 
 @ Test 
 public void staticColumnsAreReturned ( ) throws IOException 
 { 
 - DecoratedKey key = cfm . decorateKey ( ByteBufferUtil . bytes ( " k " ) ) ; 
 + DecoratedKey key = cfm . decorateKey ( ByteBufferUtil . bytes ( " k1 " ) ) ; 
 
 - QueryProcessor . executeInternal ( " INSERT INTO ks . tbl ( k , s ) VALUES ( ' k ' , ' s ' ) " ) ; 
 - Assert . assertFalse ( QueryProcessor . executeInternal ( " SELECT s FROM ks . tbl WHERE k = ' k ' " ) . isEmpty ( ) ) ; 
 + QueryProcessor . executeInternal ( " INSERT INTO ks . tbl ( k , s ) VALUES ( ' k1 ' , ' s ' ) " ) ; 
 + Assert . assertFalse ( QueryProcessor . executeInternal ( " SELECT s FROM ks . tbl WHERE k = ' k1 ' " ) . isEmpty ( ) ) ; 
 
 ColumnFilter columnFilter = ColumnFilter . selection ( PartitionColumns . of ( s ) ) ; 
 ClusteringIndexSliceFilter sliceFilter = new ClusteringIndexSliceFilter ( Slices . NONE , false ) ; 
 @ @ - 147 , 11 + 147 , 11 @ @ public class SinglePartitionSliceCommandTest 
 key , 
 sliceFilter ) ; 
 
 - UnfilteredPartitionIterator pi ; 
 - 
 / / check raw iterator for static cell 
 - pi = cmd . executeLocally ( ReadOrderGroup . emptyGroup ( ) ) ; 
 - checkForS ( pi ) ; 
 + try ( ReadOrderGroup orderGroup = cmd . startOrderGroup ( ) ; UnfilteredPartitionIterator pi = cmd . executeLocally ( orderGroup ) ) 
 + { 
 + checkForS ( pi ) ; 
 + } 
 
 ReadResponse response ; 
 DataOutputBuffer out ; 
 @ @ - 159 , 24 + 159 , 33 @ @ public class SinglePartitionSliceCommandTest 
 ReadResponse dst ; 
 
 / / check ( de ) serialized iterator for memtable static cell 
 - pi = cmd . executeLocally ( ReadOrderGroup . emptyGroup ( ) ) ; 
 - response = ReadResponse . createDataResponse ( pi , cmd . columnFilter ( ) ) ; 
 + try ( ReadOrderGroup orderGroup = cmd . startOrderGroup ( ) ; UnfilteredPartitionIterator pi = cmd . executeLocally ( orderGroup ) ) 
 + { 
 + response = ReadResponse . createDataResponse ( pi , cmd . columnFilter ( ) ) ; 
 + } 
 + 
 out = new DataOutputBuffer ( ( int ) ReadResponse . serializer . serializedSize ( response , MessagingService . VERSION _ 30 ) ) ; 
 ReadResponse . serializer . serialize ( response , out , MessagingService . VERSION _ 30 ) ; 
 in = new DataInputBuffer ( out . buffer ( ) , true ) ; 
 dst = ReadResponse . serializer . deserialize ( in , MessagingService . VERSION _ 30 ) ; 
 - pi = dst . makeIterator ( cfm , cmd ) ; 
 - checkForS ( pi ) ; 
 + try ( UnfilteredPartitionIterator pi = dst . makeIterator ( cfm , cmd ) ) 
 + { 
 + checkForS ( pi ) ; 
 + } 
 
 / / check ( de ) serialized iterator for sstable static cell 
 Schema . instance . getColumnFamilyStoreInstance ( cfm . cfId ) . forceBlockingFlush ( ) ; 
 - pi = cmd . executeLocally ( ReadOrderGroup . emptyGroup ( ) ) ; 
 - response = ReadResponse . createDataResponse ( pi , cmd . columnFilter ( ) ) ; 
 + try ( ReadOrderGroup orderGroup = cmd . startOrderGroup ( ) ; UnfilteredPartitionIterator pi = cmd . executeLocally ( orderGroup ) ) 
 + { 
 + response = ReadResponse . createDataResponse ( pi , cmd . columnFilter ( ) ) ; 
 + } 
 out = new DataOutputBuffer ( ( int ) ReadResponse . serializer . serializedSize ( response , MessagingService . VERSION _ 30 ) ) ; 
 ReadResponse . serializer . serialize ( response , out , MessagingService . VERSION _ 30 ) ; 
 in = new DataInputBuffer ( out . buffer ( ) , true ) ; 
 dst = ReadResponse . serializer . deserialize ( in , MessagingService . VERSION _ 30 ) ; 
 - pi = dst . makeIterator ( cfm , cmd ) ; 
 - checkForS ( pi ) ; 
 + try ( UnfilteredPartitionIterator pi = dst . makeIterator ( cfm , cmd ) ) 
 + { 
 + checkForS ( pi ) ; 
 + } 
 } 
 } 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / IndexHelperTest . java b / test / unit / org / apache / cassandra / io / sstable / IndexHelperTest . java 
 index c9f268a . . 2c967d0 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / IndexHelperTest . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / IndexHelperTest . java 
 @ @ - 51 , 9 + 51 , 9 @ @ public class IndexHelperTest 
 DeletionTime deletionInfo = new DeletionTime ( FBUtilities . timestampMicros ( ) , FBUtilities . nowInSeconds ( ) ) ; 
 
 List < IndexInfo > indexes = new ArrayList < > ( ) ; 
 - indexes . add ( new IndexInfo ( cn ( 0L ) , cn ( 5L ) , 0 , 0 , deletionInfo ) ) ; 
 - indexes . add ( new IndexInfo ( cn ( 10L ) , cn ( 15L ) , 0 , 0 , deletionInfo ) ) ; 
 - indexes . add ( new IndexInfo ( cn ( 20L ) , cn ( 25L ) , 0 , 0 , deletionInfo ) ) ; 
 + indexes . add ( new IndexInfo ( cn ( 0L ) , cn ( 5L ) , 0 , deletionInfo ) ) ; 
 + indexes . add ( new IndexInfo ( cn ( 10L ) , cn ( 15L ) , 0 , deletionInfo ) ) ; 
 + indexes . add ( new IndexInfo ( cn ( 20L ) , cn ( 25L ) , 0 , deletionInfo ) ) ; 
 
 
 assertEquals ( 0 , IndexHelper . indexFor ( cn ( - 1L ) , indexes , comp , false , - 1 ) ) ; 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableLoaderTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableLoaderTest . java 
 index 4eebdeb . . faa9c3e 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableLoaderTest . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableLoaderTest . java 
 @ @ - 167 , 7 + 167 , 9 @ @ public class SSTableLoaderTest 
 . withBufferSizeInMB ( 1 ) 
 . build ( ) ; 
 
 - for ( int i = 0 ; i < 1000 ; i + + ) / / make sure to write more than 1 MB 
 + int NB _ PARTITIONS = 5000 ; / / Enough to write > 1MB and get at least one completed sstable before we ' ve closed the writer 
 + 
 + for ( int i = 0 ; i < NB _ PARTITIONS ; i + + ) 
 { 
 for ( int j = 0 ; j < 100 ; j + + ) 
 writer . addRow ( String . format ( " key % d " , i ) , String . format ( " col % d " , j ) , " 100 " ) ; 
 @ @ - 183 , 7 + 185 , 7 @ @ public class SSTableLoaderTest 
 
 List < FilteredPartition > partitions = Util . getAll ( Util . cmd ( Keyspace . open ( KEYSPACE1 ) . getColumnFamilyStore ( CF _ STANDARD2 ) ) . build ( ) ) ; 
 
 - assertTrue ( partitions . size ( ) > 0 & & partitions . size ( ) < 1000 ) ; 
 + assertTrue ( partitions . size ( ) > 0 & & partitions . size ( ) < NB _ PARTITIONS ) ; 
 
 / / now we complete the write and the second loader should load the last sstable as well 
 writer . close ( ) ; 
 @ @ - 192 , 7 + 194 , 7 @ @ public class SSTableLoaderTest 
 loader . stream ( Collections . emptySet ( ) , completionStreamListener ( latch ) ) . get ( ) ; 
 
 partitions = Util . getAll ( Util . cmd ( Keyspace . open ( KEYSPACE1 ) . getColumnFamilyStore ( CF _ STANDARD2 ) ) . build ( ) ) ; 
 - assertEquals ( 1000 , partitions . size ( ) ) ; 
 + assertEquals ( NB _ PARTITIONS , partitions . size ( ) ) ; 
 
 / / The stream future is signalled when the work is complete but before releasing references . Wait for release 
 / / before cleanup ( CASSANDRA - 10118 ) .

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 9551c4c . . a3ebacd 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 45 , 6 + 45 , 8 @ @ 
 * Fix streaming RangeTombstones at column index boundary ( CASSANDRA - 5418 ) 
 * Fix preparing statements when current keyspace is not set ( CASSANDRA - 5468 ) 
 * Fix SemanticVersion . isSupportedBy minor / patch handling ( CASSANDRA - 5496 ) 
 + * Don ' t provide oldCfId for post - 1 . 1 system cfs ( CASSANDRA - 5490 ) 
 + * Fix primary range ignores replication strategy ( CASSANDRA - 5424 ) 
 Merged from 1 . 1 
 * Add retry mechanism to OTC for non - droppable _ verbs ( CASSANDRA - 5393 ) 
 * Use allocator information to improve memtable memory usage estimate 
 diff - - git a / conf / cassandra . yaml b / conf / cassandra . yaml 
 index 7ccbe81 . . c4a6f84 100644 
 - - - a / conf / cassandra . yaml 
 + + + b / conf / cassandra . yaml 
 @ @ - 42 , 7 + 42 , 7 @ @ hinted _ handoff _ enabled : true 
 # generated . After it has been dead this long , new hints for it will not be 
 # created until it has been seen alive and gone down again . 
 max _ hint _ window _ in _ ms : 10800000 # 3 hours 
 - # throttle in KB ' s per second , per delivery thread 
 + # throttle in KBs per second , per delivery thread 
 hinted _ handoff _ throttle _ in _ kb : 1024 
 # Number of threads with which to deliver hints ; 
 # Consider increasing this number when you have multi - dc deployments , since 
 @ @ - 95 , 7 + 95 , 7 @ @ permissions _ validity _ in _ ms : 2000 
 # - OrderPreservingPartitioner is an obsolete form of BOP , that stores 
 # - keys in a less - efficient format and only works with keys that are 
 # UTF8 - encoded Strings . 
 - # - CollatingOPP colates according to EN , US rules rather than lexical byte 
 + # - CollatingOPP collates according to EN , US rules rather than lexical byte 
 # ordering . Use this as an example if you need custom collation . 
 # 
 # See http : / / wiki . apache . org / cassandra / Operations for more on 
 @ @ - 113 , 7 + 113 , 7 @ @ commitlog _ directory : / var / lib / cassandra / commitlog 
 
 # policy for data disk failures : 
 # stop : shut down gossip and Thrift , leaving the node effectively dead , but 
 - # still inspectable via JMX . 
 + # can still be inspected via JMX . 
 # best _ effort : stop using the failed disk and respond to requests based on 
 # remaining available sstables . This means you WILL see obsolete 
 # data at CL . ONE ! 
 @ @ - 125 , 8 + 125 , 8 @ @ disk _ failure _ policy : stop 
 # Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the 
 # minimum , sometimes more . The key cache is fairly tiny for the amount of 
 # time it saves , so it ' s worthwhile to use it at large numbers . 
 - # The row cache saves even more time , but must store the whole values of 
 - # its rows , so it is extremely space - intensive . It ' s best to only use the 
 + # The row cache saves even more time , but must contain the entire row , 
 + # so it is extremely space - intensive . It ' s best to only use the 
 # row cache if you have hot rows or static rows . 
 # 
 # NOTE : if you reduce the size , you may not get you hottest keys loaded on startup . 
 @ @ - 135 , 7 + 135 , 7 @ @ disk _ failure _ policy : stop 
 key _ cache _ size _ in _ mb : 
 
 # Duration in seconds after which Cassandra should 
 - # safe the keys cache . Caches are saved to saved _ caches _ directory as 
 + # save the key cache . Caches are saved to saved _ caches _ directory as 
 # specified in this configuration file . 
 # 
 # Saved caches greatly improve cold - start speeds , and is relatively cheap in 
 @ @ - 179 , 6 + 179 , 8 @ @ row _ cache _ save _ period : 0 
 # significantly less memory than " live " rows in the JVM , so you can cache 
 # more rows in a given memory footprint . And storing the cache off - heap 
 # means you can use smaller heap sizes , reducing the impact of GC pauses . 
 + # Note however that when a row is requested from the row cache , it must be 
 + # deserialized into the heap for use . 
 # 
 # It is also valid to specify the fully - qualified class name to a class 
 # that implements org . apache . cassandra . cache . IRowCacheProvider . 
 @ @ - 217 , 7 + 219 , 7 @ @ commitlog _ sync _ period _ in _ ms : 10000 
 
 # The size of the individual commitlog file segments . A commitlog 
 # segment may be archived , deleted , or recycled once all the data 
 - # in it ( potentally from each columnfamily in the system ) has been 
 + # in it ( potentially from each columnfamily in the system ) has been 
 # flushed to sstables . 
 # 
 # The default size is 32 , which is almost always fine , but if you are 
 @ @ - 281 , 7 + 283 , 7 @ @ memtable _ flush _ queue _ size : 4 
 # Whether to , when doing sequential writing , fsync ( ) at intervals in 
 # order to force the operating system to flush the dirty 
 # buffers . Enable this to avoid sudden dirty buffer flushing from 
 - # impacting read latencies . Almost always a good idea on SSD : s ; not 
 + # impacting read latencies . Almost always a good idea on SSDs ; not 
 # necessarily on platters . 
 trickle _ fsync : false 
 trickle _ fsync _ interval _ in _ kb : 10240 
 @ @ - 298 , 7 + 300 , 7 @ @ ssl _ storage _ port : 7001 
 # communicate ! 
 # 
 # Leaving it blank leaves it up to InetAddress . getLocalHost ( ) . This 
 - # will always do the Right Thing * if * the node is properly configured 
 + # will always do the Right Thing _ if _ the node is properly configured 
 # ( hostname , name resolution , etc ) , and the Right Thing is to use the 
 # address associated with the hostname ( it might not be ) . 
 # 
 @ @ - 322 , 9 + 324 , 8 @ @ start _ native _ transport : false 
 # port for the CQL native transport to listen for clients on 
 native _ transport _ port : 9042 
 # The minimum and maximum threads for handling requests when the native 
 - # transport is used . The meaning is those is similar to the one of 
 - # rpc _ min _ threads and rpc _ max _ threads , though the default differ slightly and 
 - # are the ones below : 
 + # transport is used . They are similar to rpc _ min _ threads and rpc _ max _ threads , 
 + # though the defaults differ slightly . 
 # native _ transport _ min _ threads : 16 
 # native _ transport _ max _ threads : 128 
 
 @ @ - 332 , 7 + 333 , 7 @ @ native _ transport _ port : 9042 
 start _ rpc : true 
 
 # The address to bind the Thrift RPC service to - - clients connect 
 - # here . Unlike ListenAddress above , you * can * specify 0 . 0 . 0 . 0 here if 
 + # here . Unlike ListenAddress above , you _ can _ specify 0 . 0 . 0 . 0 here if 
 # you want Thrift to listen on all interfaces . 
 # 
 # Leaving this blank has the same effect it does for ListenAddress , 
 @ @ - 347 , 7 + 348 , 7 @ @ rpc _ keepalive : true 
 # Cassandra provides three out - of - the - box options for the RPC Server : 
 # 
 # sync - > One thread per thrift connection . For a very large number of clients , memory 
 - # will be your limiting factor . On a 64 bit JVM , 128KB is the minimum stack size 
 + # will be your limiting factor . On a 64 bit JVM , 180KB is the minimum stack size 
 # per thread , and that will correspond to your use of virtual memory ( but physical memory 
 # may be limited depending on use of stack space ) . 
 # 
 @ @ - 369 , 7 + 370 , 7 @ @ rpc _ server _ type : sync 
 # RPC thread pool dictates how many concurrent requests are possible ( but if you are using the sync 
 # RPC server , it also dictates the number of clients that can be connected at all ) . 
 # 
 - # The default is unlimited and thus provide no protection against clients overwhelming the server . You are 
 + # The default is unlimited and thus provides no protection against clients overwhelming the server . You are 
 # encouraged to set a maximum that makes sense for you in production , but do keep in mind that 
 # rpc _ max _ threads represents the maximum number of client requests this server may execute concurrently . 
 # 
 @ @ - 401 , 7 + 402 , 7 @ @ thrift _ max _ message _ length _ in _ mb : 16 
 
 # Set to true to have Cassandra create a hard link to each sstable 
 # flushed or streamed locally in a backups / subdirectory of the 
 - # Keyspace data . Removing these links is the operator ' s 
 + # keyspace data . Removing these links is the operator ' s 
 # responsibility . 
 incremental _ backups : false 
 
 @ @ - 499 , 7 + 500 , 7 @ @ cross _ node _ timeout : false 
 
 # Enable socket timeout for streaming operation . 
 # When a timeout occurs during streaming , streaming is retried from the start 
 - # of the current file . This * can * involve re - streaming an important amount of 
 + # of the current file . This _ can _ involve re - streaming an important amount of 
 # data , so you should avoid setting the value too low . 
 # Default value is 0 , which never timeout streams . 
 # streaming _ socket _ timeout _ in _ ms : 0 
 @ @ - 542 , 9 + 543 , 9 @ @ cross _ node _ timeout : false 
 # deployment conventions ( as it did Facebook ' s ) , this is best used 
 # as an example of writing a custom Snitch class . 
 # - Ec2Snitch : 
 - # Appropriate for EC2 deployments in a single Region . Loads Region 
 + # Appropriate for EC2 deployments in a single Region . Loads Region 
 # and Availability Zone information from the EC2 API . The Region is 
 - # treated as the Datacenter , and the Availability Zone as the rack . 
 + # treated as the datacenter , and the Availability Zone as the rack . 
 # Only private IPs are used , so this will not work across multiple 
 # Regions . 
 # - Ec2MultiRegionSnitch : 
 @ @ - 610 , 7 + 611 , 7 @ @ request _ scheduler : org . apache . cassandra . scheduler . NoScheduler 
 # Keyspace1 : 1 
 # Keyspace2 : 5 
 
 - # request _ scheduler _ id - - An identifer based on which to perform 
 + # request _ scheduler _ id - - An identifier based on which to perform 
 # the request scheduling . Currently the only valid option is keyspace . 
 # request _ scheduler _ id : keyspace 
 
 diff - - git a / src / java / org / apache / cassandra / config / CFMetaData . java b / src / java / org / apache / cassandra / config / CFMetaData . java 
 index 2f27b87 . . ae210e6 100644 
 - - - a / src / java / org / apache / cassandra / config / CFMetaData . java 
 + + + b / src / java / org / apache / cassandra / config / CFMetaData . java 
 @ @ - 165 , 7 + 165 , 7 @ @ public final class CFMetaData 
 + " PRIMARY KEY ( keyspace _ name , columnfamily _ name , column _ name ) " 
 + " ) WITH COMMENT = ' ColumnFamily column attributes ' AND gc _ grace _ seconds = 8640 " ) ; 
 
 - public static final CFMetaData HintsCf = compile ( 11 , " CREATE TABLE " + SystemTable . HINTS _ CF + " ( " 
 + public static final CFMetaData HintsCf = compile ( " CREATE TABLE " + SystemTable . HINTS _ CF + " ( " 
 + " target _ id uuid , " 
 + " hint _ id timeuuid , " 
 + " message _ version int , " 
 @ @ - 176 , 7 + 176 , 7 @ @ public final class CFMetaData 
 + " AND COMMENT = ' hints awaiting delivery ' " 
 + " AND gc _ grace _ seconds = 0 " ) ; 
 
 - public static final CFMetaData PeersCf = compile ( 12 , " CREATE TABLE " + SystemTable . PEERS _ CF + " ( " 
 + public static final CFMetaData PeersCf = compile ( " CREATE TABLE " + SystemTable . PEERS _ CF + " ( " 
 + " peer inet PRIMARY KEY , " 
 + " host _ id uuid , " 
 + " tokens set < varchar > , " 
 @ @ - 187 , 12 + 187 , 12 @ @ public final class CFMetaData 
 + " rack text " 
 + " ) WITH COMMENT = ' known peers in the cluster ' " ) ; 
 
 - public static final CFMetaData PeerEventsCf = compile ( 12 , " CREATE TABLE " + SystemTable . PEER _ EVENTS _ CF + " ( " 
 + public static final CFMetaData PeerEventsCf = compile ( " CREATE TABLE " + SystemTable . PEER _ EVENTS _ CF + " ( " 
 + " peer inet PRIMARY KEY , " 
 + " hints _ dropped map < uuid , int > " 
 + " ) WITH COMMENT = ' cf contains events related to peers ' " ) ; 
 
 - public static final CFMetaData LocalCf = compile ( 13 , " CREATE TABLE " + SystemTable . LOCAL _ CF + " ( " 
 + public static final CFMetaData LocalCf = compile ( " CREATE TABLE " + SystemTable . LOCAL _ CF + " ( " 
 + " key text PRIMARY KEY , " 
 + " tokens set < varchar > , " 
 + " cluster _ name text , " 
 @ @ - 209 , 7 + 209 , 7 @ @ public final class CFMetaData 
 + " truncated _ at map < uuid , blob > " 
 + " ) WITH COMMENT = ' information about the local node ' " ) ; 
 
 - public static final CFMetaData TraceSessionsCf = compile ( 14 , " CREATE TABLE " + Tracing . SESSIONS _ CF + " ( " 
 + public static final CFMetaData TraceSessionsCf = compile ( " CREATE TABLE " + Tracing . SESSIONS _ CF + " ( " 
 + " session _ id uuid PRIMARY KEY , " 
 + " coordinator inet , " 
 + " request text , " 
 @ @ - 218 , 7 + 218 , 7 @ @ public final class CFMetaData 
 + " duration int " 
 + " ) WITH COMMENT = ' traced sessions ' " , Tracing . TRACE _ KS ) ; 
 
 - public static final CFMetaData TraceEventsCf = compile ( 15 , " CREATE TABLE " + Tracing . EVENTS _ CF + " ( " 
 + public static final CFMetaData TraceEventsCf = compile ( " CREATE TABLE " + Tracing . EVENTS _ CF + " ( " 
 + " session _ id uuid , " 
 + " event _ id timeuuid , " 
 + " source inet , " 
 @ @ - 228 , 26 + 228 , 26 @ @ public final class CFMetaData 
 + " PRIMARY KEY ( session _ id , event _ id ) " 
 + " ) ; " , Tracing . TRACE _ KS ) ; 
 
 - public static final CFMetaData BatchlogCf = compile ( 16 , " CREATE TABLE " + SystemTable . BATCHLOG _ CF + " ( " 
 + public static final CFMetaData BatchlogCf = compile ( " CREATE TABLE " + SystemTable . BATCHLOG _ CF + " ( " 
 + " id uuid PRIMARY KEY , " 
 + " written _ at timestamp , " 
 + " data blob " 
 + " ) WITH COMMENT = ' uncommited batches ' AND gc _ grace _ seconds = 0 " 
 + " AND COMPACTION = { ' class ' : ' SizeTieredCompactionStrategy ' , ' min _ threshold ' : 2 } " ) ; 
 
 - public static final CFMetaData RangeXfersCf = compile ( 17 , " CREATE TABLE " + SystemTable . RANGE _ XFERS _ CF + " ( " 
 + public static final CFMetaData RangeXfersCf = compile ( " CREATE TABLE " + SystemTable . RANGE _ XFERS _ CF + " ( " 
 + " token _ bytes blob PRIMARY KEY , " 
 + " requested _ at timestamp " 
 + " ) WITH COMMENT = ' ranges requested for transfer here ' " ) ; 
 
 - public static final CFMetaData CompactionLogCf = compile ( 18 , " CREATE TABLE " + SystemTable . COMPACTION _ LOG + " ( " 
 + public static final CFMetaData CompactionLogCf = compile ( " CREATE TABLE " + SystemTable . COMPACTION _ LOG + " ( " 
 + " id uuid PRIMARY KEY , " 
 + " keyspace _ name text , " 
 + " columnfamily _ name text , " 
 + " inputs set < int > " 
 + " ) WITH COMMENT = ' unfinished compactions ' " ) ; 
 
 - public static final CFMetaData PaxosCf = compile ( 18 , " CREATE TABLE " + SystemTable . PAXOS _ CF + " ( " 
 + public static final CFMetaData PaxosCf = compile ( " CREATE TABLE " + SystemTable . PAXOS _ CF + " ( " 
 + " row _ key blob , " 
 + " cf _ id UUID , " 
 + " in _ progress _ ballot timeuuid , " 
 @ @ - 437 , 7 + 437 , 12 @ @ public final class CFMetaData 
 updateCfDef ( ) ; / / init cqlCfDef 
 } 
 
 - private static CFMetaData compile ( int id , String cql , String keyspace ) 
 + private static CFMetaData compile ( String cql , String keyspace ) 
 + { 
 + return compile ( null , cql , keyspace ) ; 
 + } 
 + 
 + private static CFMetaData compile ( Integer id , String cql , String keyspace ) 
 { 
 try 
 { 
 @ @ - 452 , 6 + 457 , 11 @ @ public final class CFMetaData 
 } 
 } 
 
 + private static CFMetaData compile ( String cql ) 
 + { 
 + return compile ( null , cql , Table . SYSTEM _ KS ) ; 
 + } 
 + 
 private static CFMetaData compile ( int id , String cql ) 
 { 
 return compile ( id , cql , Table . SYSTEM _ KS ) ; 
 diff - - git a / src / java / org / apache / cassandra / locator / NetworkTopologyStrategy . java b / src / java / org / apache / cassandra / locator / NetworkTopologyStrategy . java 
 index 91478e8 . . d354019 100644 
 - - - a / src / java / org / apache / cassandra / locator / NetworkTopologyStrategy . java 
 + + + b / src / java / org / apache / cassandra / locator / NetworkTopologyStrategy . java 
 @ @ - 78 , 7 + 78 , 8 @ @ public class NetworkTopologyStrategy extends AbstractReplicationStrategy 
 @ SuppressWarnings ( " serial " ) 
 public List < InetAddress > calculateNaturalEndpoints ( Token searchToken , TokenMetadata tokenMetadata ) 
 { 
 - Set < InetAddress > replicas = new HashSet < InetAddress > ( ) ; 
 + / / we want to preserve insertion order so that the first added endpoint becomes primary 
 + Set < InetAddress > replicas = new LinkedHashSet < InetAddress > ( ) ; 
 / / replicas we have found in each DC 
 Map < String , Set < InetAddress > > dcReplicas = new HashMap < String , Set < InetAddress > > ( datacenters . size ( ) ) 
 { { 
 diff - - git a / src / java / org / apache / cassandra / service / StorageService . java b / src / java / org / apache / cassandra / service / StorageService . java 
 index c1acb44 . . b0d62dc 100644 
 - - - a / src / java / org / apache / cassandra / service / StorageService . java 
 + + + b / src / java / org / apache / cassandra / service / StorageService . java 
 @ @ - 150 , 9 + 150 , 9 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE 
 return getRangesForEndpoint ( table , FBUtilities . getBroadcastAddress ( ) ) ; 
 } 
 
 - public Collection < Range < Token > > getLocalPrimaryRanges ( ) 
 + public Collection < Range < Token > > getLocalPrimaryRanges ( String keyspace ) 
 { 
 - return getPrimaryRangesForEndpoint ( FBUtilities . getBroadcastAddress ( ) ) ; 
 + return getPrimaryRangesForEndpoint ( keyspace , FBUtilities . getBroadcastAddress ( ) ) ; 
 } 
 
 @ Deprecated 
 @ @ - 2342 , 13 + 2342 , 13 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE 
 } 
 public int forceRepairAsync ( final String keyspace , final boolean isSequential , final boolean isLocal , final boolean primaryRange , final String . . . columnFamilies ) 
 { 
 - final Collection < Range < Token > > ranges = primaryRange ? getLocalPrimaryRanges ( ) : getLocalRanges ( keyspace ) ; 
 + final Collection < Range < Token > > ranges = primaryRange ? getLocalPrimaryRanges ( keyspace ) : getLocalRanges ( keyspace ) ; 
 return forceRepairAsync ( keyspace , isSequential , isLocal , ranges , columnFamilies ) ; 
 } 
 
 public int forceRepairAsync ( final String keyspace , final boolean isSequential , final boolean isLocal , final Collection < Range < Token > > ranges , final String . . . columnFamilies ) 
 { 
 - if ( Table . SYSTEM _ KS . equals ( keyspace ) | | Tracing . TRACE _ KS . equals ( keyspace ) ) 
 + if ( Table . SYSTEM _ KS . equals ( keyspace ) | | Tracing . TRACE _ KS . equals ( keyspace ) | | ranges . isEmpty ( ) ) 
 return 0 ; 
 
 final int cmd = nextRepairCommand . incrementAndGet ( ) ; 
 @ @ - 2383 , 7 + 2383 , 7 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE 
 
 public void forceTableRepairPrimaryRange ( final String tableName , boolean isSequential , boolean isLocal , final String . . . columnFamilies ) throws IOException 
 { 
 - forceTableRepairRange ( tableName , getLocalPrimaryRanges ( ) , isSequential , isLocal , columnFamilies ) ; 
 + forceTableRepairRange ( tableName , getLocalPrimaryRanges ( tableName ) , isSequential , isLocal , columnFamilies ) ; 
 } 
 
 public void forceTableRepairRange ( String beginToken , String endToken , final String tableName , boolean isSequential , boolean isLocal , final String . . . columnFamilies ) throws IOException 
 @ @ - 2511 , 17 + 2511 , 36 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE 
 } 
 
 / * * 
 - * Get the primary ranges for the specified endpoint . 
 + * Get the " primary ranges " for the specified keyspace and endpoint . 
 + * " Primary ranges " are the ranges that the node is responsible for storing replica primarily . 
 + * The node that stores replica primarily is defined as the first node returned 
 + * by { @ link AbstractReplicationStrategy # calculateNaturalEndpoints } . 
 + * 
 + * @ param keyspace 
 * @ param ep endpoint we are interested in . 
 - * @ return collection of ranges for the specified endpoint . 
 + * @ return primary ranges for the specified endpoint . 
 * / 
 - public Collection < Range < Token > > getPrimaryRangesForEndpoint ( InetAddress ep ) 
 + public Collection < Range < Token > > getPrimaryRangesForEndpoint ( String keyspace , InetAddress ep ) 
 { 
 - return tokenMetadata . getPrimaryRangesFor ( tokenMetadata . getTokens ( ep ) ) ; 
 + AbstractReplicationStrategy strategy = Table . open ( keyspace ) . getReplicationStrategy ( ) ; 
 + Collection < Range < Token > > primaryRanges = new HashSet < Range < Token > > ( ) ; 
 + TokenMetadata metadata = tokenMetadata . cloneOnlyTokenMap ( ) ; 
 + for ( Token token : metadata . sortedTokens ( ) ) 
 + { 
 + List < InetAddress > endpoints = strategy . calculateNaturalEndpoints ( token , metadata ) ; 
 + if ( endpoints . size ( ) > 0 & & endpoints . get ( 0 ) . equals ( ep ) ) 
 + primaryRanges . add ( new Range < Token > ( metadata . getPredecessor ( token ) , token ) ) ; 
 + } 
 + return primaryRanges ; 
 } 
 
 / * * 
 - * Get the primary range for the specified endpoint . 
 + * Previously , primary range is the range that the node is responsible for and calculated 
 + * only from the token assigned to the node . 
 + * But this does not take replication strategy into account , and therefore returns insufficient 
 + * range especially using NTS with replication only to certain DC ( see CASSANDRA - 5424 ) . 
 + * 
 + * @ deprecated 
 * @ param ep endpoint we are interested in . 
 * @ return range for the specified endpoint . 
 * / 
 @ @ - 3814 , 8 + 3833 , 11 @ @ public class StorageService extends NotificationBroadcasterSupport implements IE 
 public List < String > sampleKeyRange ( ) / / do not rename to getter - see CASSANDRA - 4452 for details 
 { 
 List < DecoratedKey > keys = new ArrayList < DecoratedKey > ( ) ; 
 - for ( Range < Token > range : getLocalPrimaryRanges ( ) ) 
 - keys . addAll ( keySamples ( ColumnFamilyStore . allUserDefined ( ) , range ) ) ; 
 + for ( Table keyspace : Table . nonSystem ( ) ) 
 + { 
 + for ( Range < Token > range : getPrimaryRangesForEndpoint ( keyspace . name , FBUtilities . getBroadcastAddress ( ) ) ) 
 + keys . addAll ( keySamples ( keyspace . getColumnFamilyStores ( ) , range ) ) ; 
 + } 
 
 List < String > sampledKeys = new ArrayList < String > ( keys . size ( ) ) ; 
 for ( DecoratedKey key : keys ) 
 diff - - git a / src / java / org / apache / cassandra / utils / SlabAllocator . java b / src / java / org / apache / cassandra / utils / SlabAllocator . java 
 index c50e8c4 . . 6ff66f8 100644 
 - - - a / src / java / org / apache / cassandra / utils / SlabAllocator . java 
 + + + b / src / java / org / apache / cassandra / utils / SlabAllocator . java 
 @ @ - 113 , 7 + 113 , 7 @ @ public class SlabAllocator extends Allocator 
 * / 
 public long getMinimumSize ( ) 
 { 
 - return unslabbed . get ( ) + ( regionCount - 1 ) * REGION _ SIZE ; 
 + return unslabbed . get ( ) + ( regionCount - 1 ) * ( long ) REGION _ SIZE ; 
 } 
 
 / * * 
 @ @ - 121 , 7 + 121 , 7 @ @ public class SlabAllocator extends Allocator 
 * / 
 public long getMaximumSize ( ) 
 { 
 - return unslabbed . get ( ) + regionCount * REGION _ SIZE ; 
 + return unslabbed . get ( ) + regionCount * ( long ) REGION _ SIZE ; 
 } 
 
 / * * 
 diff - - git a / test / unit / org / apache / cassandra / service / AntiEntropyServiceTestAbstract . java b / test / unit / org / apache / cassandra / service / AntiEntropyServiceTestAbstract . java 
 index 50ae5e4 . . 04930d3 100644 
 - - - a / test / unit / org / apache / cassandra / service / AntiEntropyServiceTestAbstract . java 
 + + + b / test / unit / org / apache / cassandra / service / AntiEntropyServiceTestAbstract . java 
 @ @ - 102 , 7 + 102 , 7 @ @ public abstract class AntiEntropyServiceTestAbstract extends SchemaLoader 
 
 Gossiper . instance . initializeNodeUnsafe ( REMOTE , UUID . randomUUID ( ) , 1 ) ; 
 
 - local _ range = StorageService . instance . getLocalPrimaryRange ( ) ; 
 + local _ range = StorageService . instance . getPrimaryRangesForEndpoint ( tablename , LOCAL ) . iterator ( ) . next ( ) ; 
 
 / / ( we use REMOTE instead of LOCAL so that the reponses for the validator . complete ( ) get lost ) 
 int gcBefore = ( int ) ( System . currentTimeMillis ( ) / 1000 ) - store . metadata . getGcGraceSeconds ( ) ; 
 diff - - git a / test / unit / org / apache / cassandra / service / StorageServiceServerTest . java b / test / unit / org / apache / cassandra / service / StorageServiceServerTest . java 
 index 39fbb4a . . 5ce9160 100644 
 - - - a / test / unit / org / apache / cassandra / service / StorageServiceServerTest . java 
 + + + b / test / unit / org / apache / cassandra / service / StorageServiceServerTest . java 
 @ @ - 21 , 18 + 21 , 28 @ @ package org . apache . cassandra . service ; 
 
 import java . io . File ; 
 import java . io . IOException ; 
 - import java . util . Collections ; 
 - import java . util . List ; 
 + import java . net . InetAddress ; 
 + import java . util . * ; 
 
 + import com . google . common . collect . HashMultimap ; 
 + import com . google . common . collect . Multimap ; 
 + import org . junit . BeforeClass ; 
 import org . junit . Test ; 
 import org . junit . runner . RunWith ; 
 
 import org . apache . cassandra . OrderedJUnit4ClassRunner ; 
 + import org . apache . cassandra . config . KSMetaData ; 
 + import org . apache . cassandra . config . Schema ; 
 + import org . apache . cassandra . dht . Range ; 
 + import org . apache . cassandra . dht . StringToken ; 
 import org . apache . cassandra . exceptions . ConfigurationException ; 
 import org . apache . cassandra . db . Table ; 
 import org . apache . cassandra . dht . Token ; 
 import org . apache . cassandra . SchemaLoader ; 
 import org . apache . cassandra . config . DatabaseDescriptor ; 
 + import org . apache . cassandra . locator . IEndpointSnitch ; 
 + import org . apache . cassandra . locator . PropertyFileSnitch ; 
 + import org . apache . cassandra . locator . TokenMetadata ; 
 
 import static org . junit . Assert . assertEquals ; 
 import static org . junit . Assert . assertTrue ; 
 @ @ - 40 , 6 + 50 , 13 @ @ import static org . junit . Assert . assertTrue ; 
 @ RunWith ( OrderedJUnit4ClassRunner . class ) 
 public class StorageServiceServerTest 
 { 
 + @ BeforeClass 
 + public static void setUp ( ) throws ConfigurationException 
 + { 
 + IEndpointSnitch snitch = new PropertyFileSnitch ( ) ; 
 + DatabaseDescriptor . setEndpointSnitch ( snitch ) ; 
 + } 
 + 
 @ Test 
 public void testRegularMode ( ) throws IOException , InterruptedException , ConfigurationException 
 { 
 @ @ - 79 , 4 + 96 , 170 @ @ public class StorageServiceServerTest 
 StorageService . instance . takeColumnFamilySnapshot ( Table . SYSTEM _ KS , " Schema " , " cf _ snapshot " ) ; 
 } 
 
 + @ Test 
 + public void testPrimaryRangesWithNetworkTopologyStrategy ( ) throws Exception 
 + { 
 + TokenMetadata metadata = StorageService . instance . getTokenMetadata ( ) ; 
 + metadata . clearUnsafe ( ) ; 
 + / / DC1 
 + metadata . updateNormalToken ( new StringToken ( " A " ) , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; 
 + metadata . updateNormalToken ( new StringToken ( " C " ) , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; 
 + / / DC2 
 + metadata . updateNormalToken ( new StringToken ( " B " ) , InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) ) ; 
 + metadata . updateNormalToken ( new StringToken ( " D " ) , InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) ) ; 
 + 
 + Map < String , String > configOptions = new HashMap < String , String > ( ) ; 
 + configOptions . put ( " DC1 " , " 1 " ) ; 
 + configOptions . put ( " DC2 " , " 1 " ) ; 
 + 
 + Table . clear ( " Keyspace1 " ) ; 
 + KSMetaData meta = KSMetaData . newKeyspace ( " Keyspace1 " , " NetworkTopologyStrategy " , configOptions , false ) ; 
 + Schema . instance . setTableDefinition ( meta ) ; 
 + 
 + Collection < Range < Token > > primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; 
 + assert primaryRanges . size ( ) = = 1 ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " D " ) , new StringToken ( " A " ) ) ) ; 
 + 
 + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; 
 + assert primaryRanges . size ( ) = = 1 ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " B " ) , new StringToken ( " C " ) ) ) ; 
 + 
 + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) ) ; 
 + assert primaryRanges . size ( ) = = 1 ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " A " ) , new StringToken ( " B " ) ) ) ; 
 + 
 + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) ) ; 
 + assert primaryRanges . size ( ) = = 1 ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " C " ) , new StringToken ( " D " ) ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testPrimaryRangesWithNetworkTopologyStrategyOneDCOnly ( ) throws Exception 
 + { 
 + TokenMetadata metadata = StorageService . instance . getTokenMetadata ( ) ; 
 + metadata . clearUnsafe ( ) ; 
 + / / DC1 
 + metadata . updateNormalToken ( new StringToken ( " A " ) , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; 
 + metadata . updateNormalToken ( new StringToken ( " C " ) , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; 
 + / / DC2 
 + metadata . updateNormalToken ( new StringToken ( " B " ) , InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) ) ; 
 + metadata . updateNormalToken ( new StringToken ( " D " ) , InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) ) ; 
 + 
 + Map < String , String > configOptions = new HashMap < String , String > ( ) ; 
 + configOptions . put ( " DC2 " , " 2 " ) ; 
 + 
 + Table . clear ( " Keyspace1 " ) ; 
 + KSMetaData meta = KSMetaData . newKeyspace ( " Keyspace1 " , " NetworkTopologyStrategy " , configOptions , false ) ; 
 + Schema . instance . setTableDefinition ( meta ) ; 
 + 
 + / / endpoints in DC1 should not have primary range 
 + Collection < Range < Token > > primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; 
 + assert primaryRanges . isEmpty ( ) ; 
 + 
 + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; 
 + assert primaryRanges . isEmpty ( ) ; 
 + 
 + / / endpoints in DC2 should have primary ranges which also cover DC1 
 + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) ) ; 
 + assert primaryRanges . size ( ) = = 2 ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " D " ) , new StringToken ( " A " ) ) ) ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " A " ) , new StringToken ( " B " ) ) ) ; 
 + 
 + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) ) ; 
 + assert primaryRanges . size ( ) = = 2 ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " C " ) , new StringToken ( " D " ) ) ) ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " B " ) , new StringToken ( " C " ) ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testPrimaryRangesWithVnodes ( ) throws Exception 
 + { 
 + TokenMetadata metadata = StorageService . instance . getTokenMetadata ( ) ; 
 + metadata . clearUnsafe ( ) ; 
 + / / DC1 
 + Multimap < InetAddress , Token > dc1 = HashMultimap . create ( ) ; 
 + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) , new StringToken ( " A " ) ) ; 
 + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) , new StringToken ( " E " ) ) ; 
 + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) , new StringToken ( " H " ) ) ; 
 + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) , new StringToken ( " C " ) ) ; 
 + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) , new StringToken ( " I " ) ) ; 
 + dc1 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) , new StringToken ( " J " ) ) ; 
 + metadata . updateNormalTokens ( dc1 ) ; 
 + / / DC2 
 + Multimap < InetAddress , Token > dc2 = HashMultimap . create ( ) ; 
 + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) , new StringToken ( " B " ) ) ; 
 + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) , new StringToken ( " G " ) ) ; 
 + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) , new StringToken ( " L " ) ) ; 
 + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) , new StringToken ( " D " ) ) ; 
 + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) , new StringToken ( " F " ) ) ; 
 + dc2 . put ( InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) , new StringToken ( " K " ) ) ; 
 + metadata . updateNormalTokens ( dc2 ) ; 
 + 
 + Map < String , String > configOptions = new HashMap < String , String > ( ) ; 
 + configOptions . put ( " DC2 " , " 2 " ) ; 
 + 
 + Table . clear ( " Keyspace1 " ) ; 
 + KSMetaData meta = KSMetaData . newKeyspace ( " Keyspace1 " , " NetworkTopologyStrategy " , configOptions , false ) ; 
 + Schema . instance . setTableDefinition ( meta ) ; 
 + 
 + / / endpoints in DC1 should not have primary range 
 + Collection < Range < Token > > primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; 
 + assert primaryRanges . isEmpty ( ) ; 
 + 
 + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; 
 + assert primaryRanges . isEmpty ( ) ; 
 + 
 + / / endpoints in DC2 should have primary ranges which also cover DC1 
 + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 4 " ) ) ; 
 + assert primaryRanges . size ( ) = = 4 ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " A " ) , new StringToken ( " B " ) ) ) ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " F " ) , new StringToken ( " G " ) ) ) ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " K " ) , new StringToken ( " L " ) ) ) ; 
 + / / because / 127 . 0 . 0 . 4 holds token " B " which is the next to token " A " from / 127 . 0 . 0 . 1 , 
 + / / the node covers range ( L , A ] 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " L " ) , new StringToken ( " A " ) ) ) ; 
 + 
 + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 5 " ) ) ; 
 + assert primaryRanges . size ( ) = = 8 ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " C " ) , new StringToken ( " D " ) ) ) ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " E " ) , new StringToken ( " F " ) ) ) ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " J " ) , new StringToken ( " K " ) ) ) ; 
 + / / ranges from / 127 . 0 . 0 . 1 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " D " ) , new StringToken ( " E " ) ) ) ; 
 + / / the next token to " H " in DC2 is " K " in / 127 . 0 . 0 . 5 , so ( G , H ] goes to / 127 . 0 . 0 . 5 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " G " ) , new StringToken ( " H " ) ) ) ; 
 + / / ranges from / 127 . 0 . 0 . 2 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " B " ) , new StringToken ( " C " ) ) ) ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " H " ) , new StringToken ( " I " ) ) ) ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " I " ) , new StringToken ( " J " ) ) ) ; 
 + } 
 + @ Test 
 + public void testPrimaryRangesWithSimpleStrategy ( ) throws Exception 
 + { 
 + TokenMetadata metadata = StorageService . instance . getTokenMetadata ( ) ; 
 + metadata . clearUnsafe ( ) ; 
 + 
 + metadata . updateNormalToken ( new StringToken ( " A " ) , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; 
 + metadata . updateNormalToken ( new StringToken ( " B " ) , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; 
 + metadata . updateNormalToken ( new StringToken ( " C " ) , InetAddress . getByName ( " 127 . 0 . 0 . 3 " ) ) ; 
 + 
 + Map < String , String > configOptions = new HashMap < String , String > ( ) ; 
 + configOptions . put ( " replication _ factor " , " 2 " ) ; 
 + 
 + Table . clear ( " Keyspace1 " ) ; 
 + KSMetaData meta = KSMetaData . newKeyspace ( " Keyspace1 " , " SimpleStrategy " , configOptions , false ) ; 
 + Schema . instance . setTableDefinition ( meta ) ; 
 + 
 + Collection < Range < Token > > primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 1 " ) ) ; 
 + assert primaryRanges . size ( ) = = 1 ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " C " ) , new StringToken ( " A " ) ) ) ; 
 + 
 + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 2 " ) ) ; 
 + assert primaryRanges . size ( ) = = 1 ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " A " ) , new StringToken ( " B " ) ) ) ; 
 + 
 + primaryRanges = StorageService . instance . getPrimaryRangesForEndpoint ( meta . name , InetAddress . getByName ( " 127 . 0 . 0 . 3 " ) ) ; 
 + assert primaryRanges . size ( ) = = 1 ; 
 + assert primaryRanges . contains ( new Range < Token > ( new StringToken ( " B " ) , new StringToken ( " C " ) ) ) ; 
 + } 
 }
