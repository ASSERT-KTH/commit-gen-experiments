BLEU SCORE: 1.0

TEST MSG: update cassandra . yaml comments post - CASSANDRA - 10243
GENERATED MSG: update cassandra . yaml comments post - CASSANDRA - 10243

TEST DIFF (one line): diff - - git a / conf / cassandra . yaml b / conf / cassandra . yaml <nl> index 09d2094 . . 085b68e 100644 <nl> - - - a / conf / cassandra . yaml <nl> + + + b / conf / cassandra . yaml <nl> @ @ - 762 , 12 + 762 , 15 @ @ cross _ node _ timeout : false <nl> # more than one replica on the same " rack " ( which may not actually <nl> # be a physical location ) <nl> # <nl> - # IF YOU CHANGE THE SNITCH AFTER DATA IS INSERTED INTO THE CLUSTER , <nl> - # YOU MUST RUN A FULL REPAIR , SINCE THE SNITCH AFFECTS WHERE REPLICAS <nl> - # ARE PLACED . <nl> - # <nl> - # IF THE RACK A REPLICA IS PLACED IN CHANGES AFTER THE REPLICA HAS BEEN <nl> - # ADDED TO A RING , THE NODE MUST BE DECOMMISSIONED AND REBOOTSTRAPPED . <nl> + # CASSANDRA WILL NOT ALLOW YOU TO SWITCH TO AN INCOMPATIBLE SNITCH <nl> + # ONCE DATA IS INSERTED INTO THE CLUSTER . This would cause data loss . <nl> + # This means that if you start with the default SimpleSnitch , which <nl> + # locates every node on " rack1 " in " datacenter1 " , your only options <nl> + # if you need to add another datacenter are GossipingPropertyFileSnitch <nl> + # ( and the older PFS ) . From there , if you want to migrate to an <nl> + # incompatible snitch like Ec2Snitch you can do it by adding new nodes <nl> + # under Ec2Snitch ( which will locate them in a new " datacenter " ) and <nl> + # decommissioning the old ones . <nl> # <nl> # Out of the box , Cassandra provides <nl> # - SimpleSnitch :
NEAREST DIFF (one line): ELIMINATEDSENTENCE

TEST DIFF:
diff - - git a / conf / cassandra . yaml b / conf / cassandra . yaml 
 index 09d2094 . . 085b68e 100644 
 - - - a / conf / cassandra . yaml 
 + + + b / conf / cassandra . yaml 
 @ @ - 762 , 12 + 762 , 15 @ @ cross _ node _ timeout : false 
 # more than one replica on the same " rack " ( which may not actually 
 # be a physical location ) 
 # 
 - # IF YOU CHANGE THE SNITCH AFTER DATA IS INSERTED INTO THE CLUSTER , 
 - # YOU MUST RUN A FULL REPAIR , SINCE THE SNITCH AFFECTS WHERE REPLICAS 
 - # ARE PLACED . 
 - # 
 - # IF THE RACK A REPLICA IS PLACED IN CHANGES AFTER THE REPLICA HAS BEEN 
 - # ADDED TO A RING , THE NODE MUST BE DECOMMISSIONED AND REBOOTSTRAPPED . 
 + # CASSANDRA WILL NOT ALLOW YOU TO SWITCH TO AN INCOMPATIBLE SNITCH 
 + # ONCE DATA IS INSERTED INTO THE CLUSTER . This would cause data loss . 
 + # This means that if you start with the default SimpleSnitch , which 
 + # locates every node on " rack1 " in " datacenter1 " , your only options 
 + # if you need to add another datacenter are GossipingPropertyFileSnitch 
 + # ( and the older PFS ) . From there , if you want to migrate to an 
 + # incompatible snitch like Ec2Snitch you can do it by adding new nodes 
 + # under Ec2Snitch ( which will locate them in a new " datacenter " ) and 
 + # decommissioning the old ones . 
 # 
 # Out of the box , Cassandra provides 
 # - SimpleSnitch :

NEAREST DIFF:
ELIMINATEDSENTENCE
