BLEU SCORE: 0.02431326188030181

TEST MSG: fix CLTest post - # 6764
GENERATED MSG: fix commitlog tests post - 1135 . patch by mdennis ; reviewed by jbellis for CASSANDRA - 1318

TEST DIFF (one line): diff - - git a / test / unit / org / apache / cassandra / db / CommitLogTest . java b / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> index 577692d . . ddab9ea 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> @ @ - 30 , 9 + 30 , 11 @ @ import org . junit . Test ; <nl> <nl> import org . apache . cassandra . SchemaLoader ; <nl> import org . apache . cassandra . Util ; <nl> + import org . apache . cassandra . config . Config ; <nl> import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . db . commitlog . CommitLog ; <nl> import org . apache . cassandra . db . commitlog . CommitLogDescriptor ; <nl> + import org . apache . cassandra . db . composites . CellName ; <nl> import org . apache . cassandra . net . MessagingService ; <nl> <nl> import static org . apache . cassandra . utils . ByteBufferUtil . bytes ; <nl> @ @ - 166 , 17 + 168 , 49 @ @ public class CommitLogTest extends SchemaLoader <nl> assert CommitLog . instance . activeSegments ( ) = = 1 : " Expecting 1 segment , got " + CommitLog . instance . activeSegments ( ) ; <nl> } <nl> <nl> + private static int getMaxRecordDataSize ( String keyspace , ByteBuffer key , String table , CellName column ) <nl> + { <nl> + Mutation rm = new Mutation ( " Keyspace1 " , bytes ( " k " ) ) ; <nl> + rm . add ( " Standard1 " , Util . cellname ( " c1 " ) , ByteBuffer . allocate ( 0 ) , 0 ) ; <nl> + <nl> + int max = ( DatabaseDescriptor . getCommitLogSegmentSize ( ) / 2 ) ; <nl> + max - = ( 4 + 8 + 8 ) ; / / log entry overhead <nl> + return max - ( int ) Mutation . serializer . serializedSize ( rm , MessagingService . current _ version ) ; <nl> + } <nl> + <nl> + private static int getMaxRecordDataSize ( ) <nl> + { <nl> + return getMaxRecordDataSize ( " Keyspace1 " , bytes ( " k " ) , " Standard1 " , Util . cellname ( " c1 " ) ) ; <nl> + } <nl> + <nl> / / CASSANDRA - 3615 <nl> @ Test <nl> - public void testExceedSegmentSizeWithOverhead ( ) throws Exception <nl> + public void testEqualRecordLimit ( ) throws Exception <nl> { <nl> CommitLog . instance . resetUnsafe ( ) ; <nl> <nl> Mutation rm = new Mutation ( " Keyspace1 " , bytes ( " k " ) ) ; <nl> - rm . add ( " Standard1 " , Util . cellname ( " c1 " ) , ByteBuffer . allocate ( ( DatabaseDescriptor . getCommitLogSegmentSize ( ) ) - 83 ) , 0 ) ; <nl> + rm . add ( " Standard1 " , Util . cellname ( " c1 " ) , ByteBuffer . allocate ( getMaxRecordDataSize ( ) ) , 0 ) ; <nl> CommitLog . instance . add ( rm ) ; <nl> } <nl> <nl> + @ Test <nl> + public void testExceedRecordLimit ( ) throws Exception <nl> + { <nl> + CommitLog . instance . resetUnsafe ( ) ; <nl> + try <nl> + { <nl> + Mutation rm = new Mutation ( " Keyspace1 " , bytes ( " k " ) ) ; <nl> + rm . add ( " Standard1 " , Util . cellname ( " c1 " ) , ByteBuffer . allocate ( 1 + getMaxRecordDataSize ( ) ) , 0 ) ; <nl> + CommitLog . instance . add ( rm ) ; <nl> + throw new AssertionError ( " mutation larger than limit was accepted " ) ; <nl> + } <nl> + catch ( IllegalArgumentException e ) <nl> + { <nl> + / / IAE is thrown on too - large mutations <nl> + } <nl> + } <nl> + <nl> protected void testRecoveryWithBadSizeArgument ( int size , int dataSize ) throws Exception <nl> { <nl> Checksum checksum = new CRC32 ( ) ;
NEAREST DIFF (one line): diff - - git a / test / unit / org / apache / cassandra / db / CommitLogTest . java b / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> index 44a0161 . . 042a7d4 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> @ @ - 20 , 7 + 20 , 6 @ @ <nl> package org . apache . cassandra . db ; <nl> <nl> import java . io . * ; <nl> - import java . util . concurrent . ExecutionException ; <nl> import java . util . zip . CRC32 ; <nl> import java . util . zip . Checksum ; <nl> <nl> @ @ - 28 , 7 + 27 , 6 @ @ import org . junit . Test ; <nl> <nl> import org . apache . cassandra . CleanupHelper ; <nl> import org . apache . cassandra . db . commitlog . CommitLog ; <nl> - <nl> import org . apache . cassandra . db . commitlog . CommitLogHeader ; <nl> import org . apache . cassandra . db . filter . QueryPath ; <nl> import org . apache . cassandra . utils . Pair ; <nl> @ @ - 36 , 18 + 34 , 21 @ @ import org . apache . cassandra . utils . Pair ; <nl> public class CommitLogTest extends CleanupHelper <nl> { <nl> @ Test <nl> - public void testCleanup ( ) throws IOException , ExecutionException , InterruptedException <nl> + public void testCleanup ( ) throws Exception <nl> { <nl> - assert CommitLog . instance ( ) . getSegmentCount ( ) = = 1 ; <nl> - CommitLog . setSegmentSize ( 1000 ) ; <nl> + int segmentCount = CommitLog . instance ( ) . getSegmentCount ( ) ; <nl> + assert segmentCount = = 1 : segmentCount + " ! = 1 " ; <nl> + <nl> + / / must me large enough to hold persistent _ stats <nl> + CommitLog . setSegmentSize ( 10000 ) ; <nl> <nl> Table table = Table . open ( " Keyspace1 " ) ; <nl> ColumnFamilyStore store1 = table . getColumnFamilyStore ( " Standard1 " ) ; <nl> ColumnFamilyStore store2 = table . getColumnFamilyStore ( " Standard2 " ) ; <nl> RowMutation rm ; <nl> - byte [ ] value = new byte [ 501 ] ; <nl> + byte [ ] value = new byte [ 5001 ] ; <nl> <nl> - / / add data . use relatively large values to force quick segment creation since we have a low flush threshold in the test config . <nl> + / / add data , one each of Standard1 / Standard2 per segment <nl> for ( int i = 0 ; i < 10 ; i + + ) <nl> { <nl> rm = new RowMutation ( " Keyspace1 " , " key1 " . getBytes ( ) ) ; <nl> @ @ - 59 , 11 + 60 , 13 @ @ public class CommitLogTest extends CleanupHelper <nl> <nl> / / nothing should get removed after flushing just Standard1 <nl> store1 . forceBlockingFlush ( ) ; <nl> - assert CommitLog . instance ( ) . getSegmentCount ( ) > 1 ; <nl> + segmentCount = CommitLog . instance ( ) . getSegmentCount ( ) ; <nl> + assert segmentCount > 1 : segmentCount + " ! > 1 " ; <nl> <nl> / / after flushing Standard2 we should be able to clean out all segments <nl> store2 . forceBlockingFlush ( ) ; <nl> - assert CommitLog . instance ( ) . getSegmentCount ( ) = = 1 ; <nl> + segmentCount = CommitLog . instance ( ) . getSegmentCount ( ) ; <nl> + assert segmentCount = = 1 : segmentCount + " ! = 1 " ; <nl> } <nl> <nl> @ Test <nl> diff - - git a / test / unit / org / apache / cassandra / db / RecoveryManager2Test . java b / test / unit / org / apache / cassandra / db / RecoveryManager2Test . java <nl> index 11370db . . 94bee64 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / RecoveryManager2Test . java <nl> + + + b / test / unit / org / apache / cassandra / db / RecoveryManager2Test . java <nl> @ @ - 22 , 21 + 22 , 19 @ @ package org . apache . cassandra . db ; <nl> <nl> <nl> import java . io . IOException ; <nl> - import java . util . concurrent . ExecutionException ; <nl> - <nl> - import org . apache . cassandra . Util ; <nl> <nl> import org . junit . Test ; <nl> <nl> + import static org . apache . cassandra . Util . column ; <nl> import org . apache . cassandra . CleanupHelper ; <nl> + import org . apache . cassandra . Util ; <nl> import org . apache . cassandra . db . commitlog . CommitLog ; <nl> <nl> - import static org . apache . cassandra . Util . column ; <nl> - <nl> public class RecoveryManager2Test extends CleanupHelper <nl> { <nl> @ Test <nl> - public void testWithFlush ( ) throws IOException , ExecutionException , InterruptedException <nl> + / * test that commit logs do not replay flushed data * / <nl> + public void testWithFlush ( ) throws Exception <nl> { <nl> CompactionManager . instance . disableAutoCompaction ( ) ; <nl> <nl> @ @ - 50 , 9 + 48 , 19 @ @ public class RecoveryManager2Test extends CleanupHelper <nl> ColumnFamilyStore cfs = table1 . getColumnFamilyStore ( " Standard1 " ) ; <nl> cfs . forceBlockingFlush ( ) ; <nl> <nl> + / / forceBlockingFlush above adds persistent stats to the current commit log segment <nl> + / / it ends up in the same segment as key99 meaning that segment still has unwritten data <nl> + / / thus the commit log replays it when recover is called below <nl> + Table . open ( Table . SYSTEM _ TABLE ) . getColumnFamilyStore ( StatisticsTable . STATISTICS _ CF ) . forceBlockingFlush ( ) ; <nl> + <nl> + / / remove all SSTable / MemTables <nl> cfs . clearUnsafe ( ) ; <nl> - CommitLog . recover ( ) ; / / this is a no - op . is testing this useful ? <nl> <nl> + / / replay the commit log ( nothing should be replayed since everything was flushed ) <nl> + CommitLog . recover ( ) ; <nl> + <nl> + / / since everything that was flushed was removed ( i . e . clearUnsafe ) <nl> + / / and the commit shouldn ' t have replayed anything , there should be no data <nl> assert Util . getRangeSlice ( cfs ) . isEmpty ( ) ; <nl> } <nl>

TEST DIFF:
diff - - git a / test / unit / org / apache / cassandra / db / CommitLogTest . java b / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 index 577692d . . ddab9ea 100644 
 - - - a / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 + + + b / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 @ @ - 30 , 9 + 30 , 11 @ @ import org . junit . Test ; 
 
 import org . apache . cassandra . SchemaLoader ; 
 import org . apache . cassandra . Util ; 
 + import org . apache . cassandra . config . Config ; 
 import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . db . commitlog . CommitLog ; 
 import org . apache . cassandra . db . commitlog . CommitLogDescriptor ; 
 + import org . apache . cassandra . db . composites . CellName ; 
 import org . apache . cassandra . net . MessagingService ; 
 
 import static org . apache . cassandra . utils . ByteBufferUtil . bytes ; 
 @ @ - 166 , 17 + 168 , 49 @ @ public class CommitLogTest extends SchemaLoader 
 assert CommitLog . instance . activeSegments ( ) = = 1 : " Expecting 1 segment , got " + CommitLog . instance . activeSegments ( ) ; 
 } 
 
 + private static int getMaxRecordDataSize ( String keyspace , ByteBuffer key , String table , CellName column ) 
 + { 
 + Mutation rm = new Mutation ( " Keyspace1 " , bytes ( " k " ) ) ; 
 + rm . add ( " Standard1 " , Util . cellname ( " c1 " ) , ByteBuffer . allocate ( 0 ) , 0 ) ; 
 + 
 + int max = ( DatabaseDescriptor . getCommitLogSegmentSize ( ) / 2 ) ; 
 + max - = ( 4 + 8 + 8 ) ; / / log entry overhead 
 + return max - ( int ) Mutation . serializer . serializedSize ( rm , MessagingService . current _ version ) ; 
 + } 
 + 
 + private static int getMaxRecordDataSize ( ) 
 + { 
 + return getMaxRecordDataSize ( " Keyspace1 " , bytes ( " k " ) , " Standard1 " , Util . cellname ( " c1 " ) ) ; 
 + } 
 + 
 / / CASSANDRA - 3615 
 @ Test 
 - public void testExceedSegmentSizeWithOverhead ( ) throws Exception 
 + public void testEqualRecordLimit ( ) throws Exception 
 { 
 CommitLog . instance . resetUnsafe ( ) ; 
 
 Mutation rm = new Mutation ( " Keyspace1 " , bytes ( " k " ) ) ; 
 - rm . add ( " Standard1 " , Util . cellname ( " c1 " ) , ByteBuffer . allocate ( ( DatabaseDescriptor . getCommitLogSegmentSize ( ) ) - 83 ) , 0 ) ; 
 + rm . add ( " Standard1 " , Util . cellname ( " c1 " ) , ByteBuffer . allocate ( getMaxRecordDataSize ( ) ) , 0 ) ; 
 CommitLog . instance . add ( rm ) ; 
 } 
 
 + @ Test 
 + public void testExceedRecordLimit ( ) throws Exception 
 + { 
 + CommitLog . instance . resetUnsafe ( ) ; 
 + try 
 + { 
 + Mutation rm = new Mutation ( " Keyspace1 " , bytes ( " k " ) ) ; 
 + rm . add ( " Standard1 " , Util . cellname ( " c1 " ) , ByteBuffer . allocate ( 1 + getMaxRecordDataSize ( ) ) , 0 ) ; 
 + CommitLog . instance . add ( rm ) ; 
 + throw new AssertionError ( " mutation larger than limit was accepted " ) ; 
 + } 
 + catch ( IllegalArgumentException e ) 
 + { 
 + / / IAE is thrown on too - large mutations 
 + } 
 + } 
 + 
 protected void testRecoveryWithBadSizeArgument ( int size , int dataSize ) throws Exception 
 { 
 Checksum checksum = new CRC32 ( ) ;

NEAREST DIFF:
diff - - git a / test / unit / org / apache / cassandra / db / CommitLogTest . java b / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 index 44a0161 . . 042a7d4 100644 
 - - - a / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 + + + b / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 @ @ - 20 , 7 + 20 , 6 @ @ 
 package org . apache . cassandra . db ; 
 
 import java . io . * ; 
 - import java . util . concurrent . ExecutionException ; 
 import java . util . zip . CRC32 ; 
 import java . util . zip . Checksum ; 
 
 @ @ - 28 , 7 + 27 , 6 @ @ import org . junit . Test ; 
 
 import org . apache . cassandra . CleanupHelper ; 
 import org . apache . cassandra . db . commitlog . CommitLog ; 
 - 
 import org . apache . cassandra . db . commitlog . CommitLogHeader ; 
 import org . apache . cassandra . db . filter . QueryPath ; 
 import org . apache . cassandra . utils . Pair ; 
 @ @ - 36 , 18 + 34 , 21 @ @ import org . apache . cassandra . utils . Pair ; 
 public class CommitLogTest extends CleanupHelper 
 { 
 @ Test 
 - public void testCleanup ( ) throws IOException , ExecutionException , InterruptedException 
 + public void testCleanup ( ) throws Exception 
 { 
 - assert CommitLog . instance ( ) . getSegmentCount ( ) = = 1 ; 
 - CommitLog . setSegmentSize ( 1000 ) ; 
 + int segmentCount = CommitLog . instance ( ) . getSegmentCount ( ) ; 
 + assert segmentCount = = 1 : segmentCount + " ! = 1 " ; 
 + 
 + / / must me large enough to hold persistent _ stats 
 + CommitLog . setSegmentSize ( 10000 ) ; 
 
 Table table = Table . open ( " Keyspace1 " ) ; 
 ColumnFamilyStore store1 = table . getColumnFamilyStore ( " Standard1 " ) ; 
 ColumnFamilyStore store2 = table . getColumnFamilyStore ( " Standard2 " ) ; 
 RowMutation rm ; 
 - byte [ ] value = new byte [ 501 ] ; 
 + byte [ ] value = new byte [ 5001 ] ; 
 
 - / / add data . use relatively large values to force quick segment creation since we have a low flush threshold in the test config . 
 + / / add data , one each of Standard1 / Standard2 per segment 
 for ( int i = 0 ; i < 10 ; i + + ) 
 { 
 rm = new RowMutation ( " Keyspace1 " , " key1 " . getBytes ( ) ) ; 
 @ @ - 59 , 11 + 60 , 13 @ @ public class CommitLogTest extends CleanupHelper 
 
 / / nothing should get removed after flushing just Standard1 
 store1 . forceBlockingFlush ( ) ; 
 - assert CommitLog . instance ( ) . getSegmentCount ( ) > 1 ; 
 + segmentCount = CommitLog . instance ( ) . getSegmentCount ( ) ; 
 + assert segmentCount > 1 : segmentCount + " ! > 1 " ; 
 
 / / after flushing Standard2 we should be able to clean out all segments 
 store2 . forceBlockingFlush ( ) ; 
 - assert CommitLog . instance ( ) . getSegmentCount ( ) = = 1 ; 
 + segmentCount = CommitLog . instance ( ) . getSegmentCount ( ) ; 
 + assert segmentCount = = 1 : segmentCount + " ! = 1 " ; 
 } 
 
 @ Test 
 diff - - git a / test / unit / org / apache / cassandra / db / RecoveryManager2Test . java b / test / unit / org / apache / cassandra / db / RecoveryManager2Test . java 
 index 11370db . . 94bee64 100644 
 - - - a / test / unit / org / apache / cassandra / db / RecoveryManager2Test . java 
 + + + b / test / unit / org / apache / cassandra / db / RecoveryManager2Test . java 
 @ @ - 22 , 21 + 22 , 19 @ @ package org . apache . cassandra . db ; 
 
 
 import java . io . IOException ; 
 - import java . util . concurrent . ExecutionException ; 
 - 
 - import org . apache . cassandra . Util ; 
 
 import org . junit . Test ; 
 
 + import static org . apache . cassandra . Util . column ; 
 import org . apache . cassandra . CleanupHelper ; 
 + import org . apache . cassandra . Util ; 
 import org . apache . cassandra . db . commitlog . CommitLog ; 
 
 - import static org . apache . cassandra . Util . column ; 
 - 
 public class RecoveryManager2Test extends CleanupHelper 
 { 
 @ Test 
 - public void testWithFlush ( ) throws IOException , ExecutionException , InterruptedException 
 + / * test that commit logs do not replay flushed data * / 
 + public void testWithFlush ( ) throws Exception 
 { 
 CompactionManager . instance . disableAutoCompaction ( ) ; 
 
 @ @ - 50 , 9 + 48 , 19 @ @ public class RecoveryManager2Test extends CleanupHelper 
 ColumnFamilyStore cfs = table1 . getColumnFamilyStore ( " Standard1 " ) ; 
 cfs . forceBlockingFlush ( ) ; 
 
 + / / forceBlockingFlush above adds persistent stats to the current commit log segment 
 + / / it ends up in the same segment as key99 meaning that segment still has unwritten data 
 + / / thus the commit log replays it when recover is called below 
 + Table . open ( Table . SYSTEM _ TABLE ) . getColumnFamilyStore ( StatisticsTable . STATISTICS _ CF ) . forceBlockingFlush ( ) ; 
 + 
 + / / remove all SSTable / MemTables 
 cfs . clearUnsafe ( ) ; 
 - CommitLog . recover ( ) ; / / this is a no - op . is testing this useful ? 
 
 + / / replay the commit log ( nothing should be replayed since everything was flushed ) 
 + CommitLog . recover ( ) ; 
 + 
 + / / since everything that was flushed was removed ( i . e . clearUnsafe ) 
 + / / and the commit shouldn ' t have replayed anything , there should be no data 
 assert Util . getRangeSlice ( cfs ) . isEmpty ( ) ; 
 } 

