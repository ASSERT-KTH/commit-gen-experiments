BLEU SCORE: 0.4111336169005197

TEST MSG: Revert " Expand use of vints "
GENERATED MSG: Expand use of vints

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 4608492 . . 2db4115 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 5 , 8 + 5 , 8 @ @ <nl> * Metrics should use up to date nomenclature ( CASSANDRA - 9448 ) <nl> * Change CREATE / ALTER TABLE syntax for compression ( CASSANDRA - 8384 ) <nl> * Cleanup crc and adler code for java 8 ( CASSANDRA - 9650 ) <nl> - * Storage engine refactor ( CASSANDRA - 8099 , 9743 , 9746 , 9759 , 9781 , 9808 , 9825 , <nl> - 9848 , 9705 , 9859 , 9867 , 9874 , 9828 , 9801 ) <nl> + * Storage engine refactor ( CASSANDRA - 8099 , 9743 , 9746 , 9759 , 9781 , 9808 , 9825 , 9848 , <nl> + 9705 , 9859 , 9867 , 9874 , 9828 , 9801 ) <nl> * Update Guava to 18 . 0 ( CASSANDRA - 9653 ) <nl> * Bloom filter false positive ratio is not honoured ( CASSANDRA - 8413 ) <nl> * New option for cassandra - stress to leave a ratio of columns null ( CASSANDRA - 9522 ) <nl> diff - - git a / src / java / org / apache / cassandra / db / Columns . java b / src / java / org / apache / cassandra / db / Columns . java <nl> index 03d2e14 . . 48a4504 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Columns . java <nl> + + + b / src / java / org / apache / cassandra / db / Columns . java <nl> @ @ - 17 , 6 + 17 , 7 @ @ <nl> * / <nl> package org . apache . cassandra . db ; <nl> <nl> + import java . io . DataInput ; <nl> import java . io . IOException ; <nl> import java . util . * ; <nl> import java . util . function . Predicate ; <nl> @ @ - 30 , 7 + 31 , 6 @ @ import org . apache . cassandra . config . CFMetaData ; <nl> import org . apache . cassandra . config . ColumnDefinition ; <nl> import org . apache . cassandra . db . marshal . UTF8Type ; <nl> import org . apache . cassandra . db . marshal . MapType ; <nl> - import org . apache . cassandra . io . util . DataInputPlus ; <nl> import org . apache . cassandra . io . util . DataOutputPlus ; <nl> import org . apache . cassandra . utils . ByteBufferUtil ; <nl> <nl> @ @ - 526 , 26 + 526 , 26 @ @ public class Columns implements Iterable < ColumnDefinition > <nl> { <nl> public void serialize ( Columns columns , DataOutputPlus out ) throws IOException <nl> { <nl> - out . writeVInt ( columns . columnCount ( ) ) ; <nl> + out . writeShort ( columns . columnCount ( ) ) ; <nl> for ( ColumnDefinition column : columns ) <nl> - ByteBufferUtil . writeWithVIntLength ( column . name . bytes , out ) ; <nl> + ByteBufferUtil . writeWithShortLength ( column . name . bytes , out ) ; <nl> } <nl> <nl> public long serializedSize ( Columns columns ) <nl> { <nl> - long size = TypeSizes . sizeofVInt ( columns . columnCount ( ) ) ; <nl> + long size = TypeSizes . sizeof ( ( short ) columns . columnCount ( ) ) ; <nl> for ( ColumnDefinition column : columns ) <nl> - size + = ByteBufferUtil . serializedSizeWithVIntLength ( column . name . bytes ) ; <nl> + size + = TypeSizes . sizeofWithShortLength ( column . name . bytes ) ; <nl> return size ; <nl> } <nl> <nl> - public Columns deserialize ( DataInputPlus in , CFMetaData metadata ) throws IOException <nl> + public Columns deserialize ( DataInput in , CFMetaData metadata ) throws IOException <nl> { <nl> - int length = ( int ) in . readVInt ( ) ; <nl> + int length = in . readUnsignedShort ( ) ; <nl> ColumnDefinition [ ] columns = new ColumnDefinition [ length ] ; <nl> for ( int i = 0 ; i < length ; i + + ) <nl> { <nl> - ByteBuffer name = ByteBufferUtil . readWithVIntLength ( in ) ; <nl> + ByteBuffer name = ByteBufferUtil . readWithShortLength ( in ) ; <nl> ColumnDefinition column = metadata . getColumnDefinition ( name ) ; <nl> if ( column = = null ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / db / Mutation . java b / src / java / org / apache / cassandra / db / Mutation . java <nl> index 3d49ca6 . . aca6622 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Mutation . java <nl> + + + b / src / java / org / apache / cassandra / db / Mutation . java <nl> @ @ - 248 , 19 + 248 , 12 @ @ public class Mutation implements IMutation <nl> if ( version < MessagingService . VERSION _ 20 ) <nl> out . writeUTF ( mutation . getKeyspaceName ( ) ) ; <nl> <nl> - / * serialize the modifications in the mutation * / <nl> - int size = mutation . modifications . size ( ) ; <nl> - <nl> if ( version < MessagingService . VERSION _ 30 ) <nl> - { <nl> ByteBufferUtil . writeWithShortLength ( mutation . key ( ) . getKey ( ) , out ) ; <nl> - out . writeInt ( size ) ; <nl> - } <nl> - else <nl> - { <nl> - out . writeVInt ( size ) ; <nl> - } <nl> <nl> + / * serialize the modifications in the mutation * / <nl> + int size = mutation . modifications . size ( ) ; <nl> + out . writeInt ( size ) ; <nl> assert size > 0 ; <nl> for ( Map . Entry < UUID , PartitionUpdate > entry : mutation . modifications . entrySet ( ) ) <nl> PartitionUpdate . serializer . serialize ( entry . getValue ( ) , out , version ) ; <nl> @ @ - 273 , 17 + 266 , 10 @ @ public class Mutation implements IMutation <nl> keyspaceName = in . readUTF ( ) ; <nl> <nl> DecoratedKey key = null ; <nl> - int size ; <nl> if ( version < MessagingService . VERSION _ 30 ) <nl> - { <nl> key = StorageService . getPartitioner ( ) . decorateKey ( ByteBufferUtil . readWithShortLength ( in ) ) ; <nl> - size = in . readInt ( ) ; <nl> - } <nl> - else <nl> - { <nl> - size = ( int ) in . readVInt ( ) ; <nl> - } <nl> <nl> + int size = in . readInt ( ) ; <nl> assert size > 0 ; <nl> <nl> if ( size = = 1 ) <nl> @ @ - 321 , 13 + 307 , 9 @ @ public class Mutation implements IMutation <nl> { <nl> int keySize = mutation . key ( ) . getKey ( ) . remaining ( ) ; <nl> size + = TypeSizes . sizeof ( ( short ) keySize ) + keySize ; <nl> - size + = TypeSizes . sizeof ( mutation . modifications . size ( ) ) ; <nl> - } <nl> - else <nl> - { <nl> - size + = TypeSizes . sizeofVInt ( mutation . modifications . size ( ) ) ; <nl> } <nl> <nl> + size + = TypeSizes . sizeof ( mutation . modifications . size ( ) ) ; <nl> for ( Map . Entry < UUID , PartitionUpdate > entry : mutation . modifications . entrySet ( ) ) <nl> size + = PartitionUpdate . serializer . serializedSize ( entry . getValue ( ) , version ) ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / db / ReadResponse . java b / src / java / org / apache / cassandra / db / ReadResponse . java <nl> index 90bd21d . . 740423a 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ReadResponse . java <nl> + + + b / src / java / org / apache / cassandra / db / ReadResponse . java <nl> @ @ - 20 , 6 + 20 , 8 @ @ package org . apache . cassandra . db ; <nl> import java . io . * ; <nl> import java . nio . ByteBuffer ; <nl> import java . security . MessageDigest ; <nl> + import java . util . ArrayList ; <nl> + import java . util . List ; <nl> <nl> import org . apache . cassandra . config . CFMetaData ; <nl> import org . apache . cassandra . db . rows . * ; <nl> @ @ - 163 , 14 + 165 , 14 @ @ public abstract class ReadResponse <nl> } <nl> <nl> boolean isDigest = response . isDigestQuery ( ) ; <nl> - ByteBufferUtil . writeWithVIntLength ( isDigest ? response . digest ( response . metadata ) : ByteBufferUtil . EMPTY _ BYTE _ BUFFER , out ) ; <nl> + ByteBufferUtil . writeWithShortLength ( isDigest ? response . digest ( response . metadata ) : ByteBufferUtil . EMPTY _ BYTE _ BUFFER , out ) ; <nl> if ( ! isDigest ) <nl> { <nl> / / Note that we can only get there if version = = 3 . 0 , which is the current _ version . When we ' ll change the <nl> / / version , we ' ll have to deserialize / re - serialize the data to be in the proper version . <nl> assert version = = MessagingService . VERSION _ 30 ; <nl> ByteBuffer data = ( ( DataResponse ) response ) . data ; <nl> - ByteBufferUtil . writeWithVIntLength ( data , out ) ; <nl> + ByteBufferUtil . writeWithLength ( data , out ) ; <nl> } <nl> } <nl> <nl> @ @ - 182 , 12 + 184 , 12 @ @ public abstract class ReadResponse <nl> throw new UnsupportedOperationException ( ) ; <nl> } <nl> <nl> - ByteBuffer digest = ByteBufferUtil . readWithVIntLength ( in ) ; <nl> + ByteBuffer digest = ByteBufferUtil . readWithShortLength ( in ) ; <nl> if ( digest . hasRemaining ( ) ) <nl> return new DigestResponse ( digest ) ; <nl> <nl> assert version = = MessagingService . VERSION _ 30 ; <nl> - ByteBuffer data = ByteBufferUtil . readWithVIntLength ( in ) ; <nl> + ByteBuffer data = ByteBufferUtil . readWithLength ( in ) ; <nl> return new DataResponse ( data ) ; <nl> } <nl> <nl> @ @ - 200 , 14 + 202 , 15 @ @ public abstract class ReadResponse <nl> } <nl> <nl> boolean isDigest = response . isDigestQuery ( ) ; <nl> - long size = ByteBufferUtil . serializedSizeWithVIntLength ( isDigest ? response . digest ( response . metadata ) : ByteBufferUtil . EMPTY _ BYTE _ BUFFER ) ; <nl> + long size = ByteBufferUtil . serializedSizeWithShortLength ( isDigest ? response . digest ( response . metadata ) : ByteBufferUtil . EMPTY _ BYTE _ BUFFER ) ; <nl> + <nl> if ( ! isDigest ) <nl> { <nl> / / Note that we can only get there if version = = 3 . 0 , which is the current _ version . When we ' ll change the <nl> / / version , we ' ll have to deserialize / re - serialize the data to be in the proper version . <nl> assert version = = MessagingService . VERSION _ 30 ; <nl> ByteBuffer data = ( ( DataResponse ) response ) . data ; <nl> - size + = ByteBufferUtil . serializedSizeWithVIntLength ( data ) ; <nl> + size + = ByteBufferUtil . serializedSizeWithLength ( data ) ; <nl> } <nl> return size ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / SerializationHeader . java b / src / java / org / apache / cassandra / db / SerializationHeader . java <nl> index 2326f1e . . c054b25 100644 <nl> - - - a / src / java / org / apache / cassandra / db / SerializationHeader . java <nl> + + + b / src / java / org / apache / cassandra / db / SerializationHeader . java <nl> @ @ - 410 , 7 + 410 , 7 @ @ public class SerializationHeader <nl> EncodingStats . serializer . serialize ( header . stats , out ) ; <nl> <nl> writeType ( header . keyType , out ) ; <nl> - out . writeVInt ( header . clusteringTypes . size ( ) ) ; <nl> + out . writeShort ( header . clusteringTypes . size ( ) ) ; <nl> for ( AbstractType < ? > type : header . clusteringTypes ) <nl> writeType ( type , out ) ; <nl> <nl> @ @ - 424 , 7 + 424 , 7 @ @ public class SerializationHeader <nl> EncodingStats stats = EncodingStats . serializer . deserialize ( in ) ; <nl> <nl> AbstractType < ? > keyType = readType ( in ) ; <nl> - int size = ( int ) in . readVInt ( ) ; <nl> + int size = in . readUnsignedShort ( ) ; <nl> List < AbstractType < ? > > clusteringTypes = new ArrayList < > ( size ) ; <nl> for ( int i = 0 ; i < size ; i + + ) <nl> clusteringTypes . add ( readType ( in ) ) ; <nl> @ @ - 444 , 7 + 444 , 7 @ @ public class SerializationHeader <nl> int size = EncodingStats . serializer . serializedSize ( header . stats ) ; <nl> <nl> size + = sizeofType ( header . keyType ) ; <nl> - size + = TypeSizes . sizeofVInt ( header . clusteringTypes . size ( ) ) ; <nl> + size + = TypeSizes . sizeof ( ( short ) header . clusteringTypes . size ( ) ) ; <nl> for ( AbstractType < ? > type : header . clusteringTypes ) <nl> size + = sizeofType ( type ) ; <nl> <nl> @ @ - 455 , 20 + 455 , 20 @ @ public class SerializationHeader <nl> <nl> private void writeColumnsWithTypes ( Map < ByteBuffer , AbstractType < ? > > columns , DataOutputPlus out ) throws IOException <nl> { <nl> - out . writeVInt ( columns . size ( ) ) ; <nl> + out . writeShort ( columns . size ( ) ) ; <nl> for ( Map . Entry < ByteBuffer , AbstractType < ? > > entry : columns . entrySet ( ) ) <nl> { <nl> - ByteBufferUtil . writeWithVIntLength ( entry . getKey ( ) , out ) ; <nl> + ByteBufferUtil . writeWithShortLength ( entry . getKey ( ) , out ) ; <nl> writeType ( entry . getValue ( ) , out ) ; <nl> } <nl> } <nl> <nl> private long sizeofColumnsWithTypes ( Map < ByteBuffer , AbstractType < ? > > columns ) <nl> { <nl> - long size = TypeSizes . sizeofVInt ( columns . size ( ) ) ; <nl> + long size = TypeSizes . sizeof ( ( short ) columns . size ( ) ) ; <nl> for ( Map . Entry < ByteBuffer , AbstractType < ? > > entry : columns . entrySet ( ) ) <nl> { <nl> - size + = ByteBufferUtil . serializedSizeWithVIntLength ( entry . getKey ( ) ) ; <nl> + size + = TypeSizes . sizeofWithShortLength ( entry . getKey ( ) ) ; <nl> size + = sizeofType ( entry . getValue ( ) ) ; <nl> } <nl> return size ; <nl> @ @ - 476 , 10 + 476 , 10 @ @ public class SerializationHeader <nl> <nl> private void readColumnsWithType ( DataInputPlus in , Map < ByteBuffer , AbstractType < ? > > typeMap ) throws IOException <nl> { <nl> - int length = ( int ) in . readVInt ( ) ; <nl> + int length = in . readUnsignedShort ( ) ; <nl> for ( int i = 0 ; i < length ; i + + ) <nl> { <nl> - ByteBuffer name = ByteBufferUtil . readWithVIntLength ( in ) ; <nl> + ByteBuffer name = ByteBufferUtil . readWithShortLength ( in ) ; <nl> typeMap . put ( name , readType ( in ) ) ; <nl> } <nl> } <nl> @ @ - 487 , 18 + 487 , 18 @ @ public class SerializationHeader <nl> private void writeType ( AbstractType < ? > type , DataOutputPlus out ) throws IOException <nl> { <nl> / / TODO : we should have a terser serializaion format . Not a big deal though <nl> - ByteBufferUtil . writeWithVIntLength ( UTF8Type . instance . decompose ( type . toString ( ) ) , out ) ; <nl> + ByteBufferUtil . writeWithLength ( UTF8Type . instance . decompose ( type . toString ( ) ) , out ) ; <nl> } <nl> <nl> private AbstractType < ? > readType ( DataInputPlus in ) throws IOException <nl> { <nl> - ByteBuffer raw = ByteBufferUtil . readWithVIntLength ( in ) ; <nl> + ByteBuffer raw = ByteBufferUtil . readWithLength ( in ) ; <nl> return TypeParser . parse ( UTF8Type . instance . compose ( raw ) ) ; <nl> } <nl> <nl> private int sizeofType ( AbstractType < ? > type ) <nl> { <nl> - return ByteBufferUtil . serializedSizeWithVIntLength ( UTF8Type . instance . decompose ( type . toString ( ) ) ) ; <nl> + return TypeSizes . sizeofWithLength ( UTF8Type . instance . decompose ( type . toString ( ) ) ) ; <nl> } <nl> } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / Slices . java b / src / java / org / apache / cassandra / db / Slices . java <nl> index 9dd4a48 . . 32ca06d 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Slices . java <nl> + + + b / src / java / org / apache / cassandra / db / Slices . java <nl> @ @ - 288 , 7 + 288 , 7 @ @ public abstract class Slices implements Iterable < Slice > <nl> public void serialize ( Slices slices , DataOutputPlus out , int version ) throws IOException <nl> { <nl> int size = slices . size ( ) ; <nl> - out . writeVInt ( size ) ; <nl> + out . writeInt ( size ) ; <nl> <nl> if ( size = = 0 ) <nl> return ; <nl> @ @ - 303 , 7 + 303 , 7 @ @ public abstract class Slices implements Iterable < Slice > <nl> <nl> public long serializedSize ( Slices slices , int version ) <nl> { <nl> - long size = TypeSizes . sizeofVInt ( slices . size ( ) ) ; <nl> + long size = TypeSizes . sizeof ( slices . size ( ) ) ; <nl> <nl> if ( slices . size ( ) = = 0 ) <nl> return size ; <nl> @ @ - 320 , 7 + 320 , 7 @ @ public abstract class Slices implements Iterable < Slice > <nl> <nl> public Slices deserialize ( DataInputPlus in , int version , CFMetaData metadata ) throws IOException <nl> { <nl> - int size = ( int ) in . readVInt ( ) ; <nl> + int size = in . readInt ( ) ; <nl> <nl> if ( size = = 0 ) <nl> return NONE ; <nl> diff - - git a / src / java / org / apache / cassandra / db / filter / ClusteringIndexNamesFilter . java b / src / java / org / apache / cassandra / db / filter / ClusteringIndexNamesFilter . java <nl> index a6f2179 . . 13329f3 100644 <nl> - - - a / src / java / org / apache / cassandra / db / filter / ClusteringIndexNamesFilter . java <nl> + + + b / src / java / org / apache / cassandra / db / filter / ClusteringIndexNamesFilter . java <nl> @ @ - 245 , 15 + 245 , 15 @ @ public class ClusteringIndexNamesFilter extends AbstractClusteringIndexFilter <nl> protected void serializeInternal ( DataOutputPlus out , int version ) throws IOException <nl> { <nl> ClusteringComparator comparator = ( ClusteringComparator ) clusterings . comparator ( ) ; <nl> - out . writeVInt ( clusterings . size ( ) ) ; <nl> + out . writeInt ( clusterings . size ( ) ) ; <nl> for ( Clustering clustering : clusterings ) <nl> Clustering . serializer . serialize ( clustering , out , version , comparator . subtypes ( ) ) ; <nl> } <nl> <nl> protected long serializedSizeInternal ( int version ) <nl> { <nl> + long size = 0 ; <nl> ClusteringComparator comparator = ( ClusteringComparator ) clusterings . comparator ( ) ; <nl> - long size = TypeSizes . sizeofVInt ( clusterings . size ( ) ) ; <nl> for ( Clustering clustering : clusterings ) <nl> size + = Clustering . serializer . serializedSize ( clustering , version , comparator . subtypes ( ) ) ; <nl> return size ; <nl> @ @ - 265 , 7 + 265 , 7 @ @ public class ClusteringIndexNamesFilter extends AbstractClusteringIndexFilter <nl> { <nl> ClusteringComparator comparator = metadata . comparator ; <nl> BTreeSet . Builder < Clustering > clusterings = BTreeSet . builder ( comparator ) ; <nl> - int size = ( int ) in . readVInt ( ) ; <nl> + int size = in . readInt ( ) ; <nl> for ( int i = 0 ; i < size ; i + + ) <nl> clusterings . add ( Clustering . serializer . deserialize ( in , version , comparator . subtypes ( ) ) ) ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / db / filter / ColumnFilter . java b / src / java / org / apache / cassandra / db / filter / ColumnFilter . java <nl> index d2cb87d . . 084bad6 100644 <nl> - - - a / src / java / org / apache / cassandra / db / filter / ColumnFilter . java <nl> + + + b / src / java / org / apache / cassandra / db / filter / ColumnFilter . java <nl> @ @ - 365 , 7 + 365 , 7 @ @ public class ColumnFilter <nl> <nl> if ( selection . subSelections ! = null ) <nl> { <nl> - out . writeVInt ( selection . subSelections . size ( ) ) ; <nl> + out . writeShort ( selection . subSelections . size ( ) ) ; <nl> for ( ColumnSubselection subSel : selection . subSelections . values ( ) ) <nl> ColumnSubselection . serializer . serialize ( subSel , out , version ) ; <nl> } <nl> @ @ - 390 , 7 + 390 , 7 @ @ public class ColumnFilter <nl> if ( hasSubSelections ) <nl> { <nl> subSelections = TreeMultimap . create ( Comparator . < ColumnIdentifier > naturalOrder ( ) , Comparator . < ColumnSubselection > naturalOrder ( ) ) ; <nl> - int size = ( int ) in . readVInt ( ) ; <nl> + int size = in . readUnsignedShort ( ) ; <nl> for ( int i = 0 ; i < size ; i + + ) <nl> { <nl> ColumnSubselection subSel = ColumnSubselection . serializer . deserialize ( in , version , metadata ) ; <nl> @ @ - 414 , 7 + 414 , 7 @ @ public class ColumnFilter <nl> if ( selection . subSelections ! = null ) <nl> { <nl> <nl> - size + = TypeSizes . sizeofVInt ( selection . subSelections . size ( ) ) ; <nl> + size + = TypeSizes . sizeof ( ( short ) selection . subSelections . size ( ) ) ; <nl> for ( ColumnSubselection subSel : selection . subSelections . values ( ) ) <nl> size + = ColumnSubselection . serializer . serializedSize ( subSel , version ) ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / filter / DataLimits . java b / src / java / org / apache / cassandra / db / filter / DataLimits . java <nl> index 458ee30 . . 206afa4 100644 <nl> - - - a / src / java / org / apache / cassandra / db / filter / DataLimits . java <nl> + + + b / src / java / org / apache / cassandra / db / filter / DataLimits . java <nl> @ @ - 17 , 13 + 17 , 13 @ @ <nl> * / <nl> package org . apache . cassandra . db . filter ; <nl> <nl> + import java . io . DataInput ; <nl> import java . io . IOException ; <nl> import java . nio . ByteBuffer ; <nl> <nl> import org . apache . cassandra . db . * ; <nl> import org . apache . cassandra . db . rows . * ; <nl> import org . apache . cassandra . db . partitions . * ; <nl> - import org . apache . cassandra . io . util . DataInputPlus ; <nl> import org . apache . cassandra . io . util . DataOutputPlus ; <nl> import org . apache . cassandra . utils . ByteBufferUtil ; <nl> <nl> @ @ - 644 , 45 + 644 , 45 @ @ public abstract class DataLimits <nl> case CQL _ LIMIT : <nl> case CQL _ PAGING _ LIMIT : <nl> CQLLimits cqlLimits = ( CQLLimits ) limits ; <nl> - out . writeVInt ( cqlLimits . rowLimit ) ; <nl> - out . writeVInt ( cqlLimits . perPartitionLimit ) ; <nl> + out . writeInt ( cqlLimits . rowLimit ) ; <nl> + out . writeInt ( cqlLimits . perPartitionLimit ) ; <nl> out . writeBoolean ( cqlLimits . isDistinct ) ; <nl> if ( limits . kind ( ) = = Kind . CQL _ PAGING _ LIMIT ) <nl> { <nl> CQLPagingLimits pagingLimits = ( CQLPagingLimits ) cqlLimits ; <nl> - ByteBufferUtil . writeWithVIntLength ( pagingLimits . lastReturnedKey , out ) ; <nl> - out . writeVInt ( pagingLimits . lastReturnedKeyRemaining ) ; <nl> + ByteBufferUtil . writeWithShortLength ( pagingLimits . lastReturnedKey , out ) ; <nl> + out . writeInt ( pagingLimits . lastReturnedKeyRemaining ) ; <nl> } <nl> break ; <nl> case THRIFT _ LIMIT : <nl> case SUPER _ COLUMN _ COUNTING _ LIMIT : <nl> ThriftLimits thriftLimits = ( ThriftLimits ) limits ; <nl> - out . writeVInt ( thriftLimits . partitionLimit ) ; <nl> - out . writeVInt ( thriftLimits . cellPerPartitionLimit ) ; <nl> + out . writeInt ( thriftLimits . partitionLimit ) ; <nl> + out . writeInt ( thriftLimits . cellPerPartitionLimit ) ; <nl> break ; <nl> } <nl> } <nl> <nl> - public DataLimits deserialize ( DataInputPlus in , int version ) throws IOException <nl> + public DataLimits deserialize ( DataInput in , int version ) throws IOException <nl> { <nl> Kind kind = Kind . values ( ) [ in . readUnsignedByte ( ) ] ; <nl> switch ( kind ) <nl> { <nl> case CQL _ LIMIT : <nl> case CQL _ PAGING _ LIMIT : <nl> - int rowLimit = ( int ) in . readVInt ( ) ; <nl> - int perPartitionLimit = ( int ) in . readVInt ( ) ; <nl> + int rowLimit = in . readInt ( ) ; <nl> + int perPartitionLimit = in . readInt ( ) ; <nl> boolean isDistinct = in . readBoolean ( ) ; <nl> if ( kind = = Kind . CQL _ LIMIT ) <nl> return new CQLLimits ( rowLimit , perPartitionLimit , isDistinct ) ; <nl> <nl> - ByteBuffer lastKey = ByteBufferUtil . readWithVIntLength ( in ) ; <nl> - int lastRemaining = ( int ) in . readVInt ( ) ; <nl> + ByteBuffer lastKey = ByteBufferUtil . readWithShortLength ( in ) ; <nl> + int lastRemaining = in . readInt ( ) ; <nl> return new CQLPagingLimits ( rowLimit , perPartitionLimit , isDistinct , lastKey , lastRemaining ) ; <nl> case THRIFT _ LIMIT : <nl> case SUPER _ COLUMN _ COUNTING _ LIMIT : <nl> - int partitionLimit = ( int ) in . readVInt ( ) ; <nl> - int cellPerPartitionLimit = ( int ) in . readVInt ( ) ; <nl> + int partitionLimit = in . readInt ( ) ; <nl> + int cellPerPartitionLimit = in . readInt ( ) ; <nl> return kind = = Kind . THRIFT _ LIMIT <nl> ? new ThriftLimits ( partitionLimit , cellPerPartitionLimit ) <nl> : new SuperColumnCountingLimits ( partitionLimit , cellPerPartitionLimit ) ; <nl> @ @ - 698 , 21 + 698 , 21 @ @ public abstract class DataLimits <nl> case CQL _ LIMIT : <nl> case CQL _ PAGING _ LIMIT : <nl> CQLLimits cqlLimits = ( CQLLimits ) limits ; <nl> - size + = TypeSizes . sizeofVInt ( cqlLimits . rowLimit ) ; <nl> - size + = TypeSizes . sizeofVInt ( cqlLimits . perPartitionLimit ) ; <nl> + size + = TypeSizes . sizeof ( cqlLimits . rowLimit ) ; <nl> + size + = TypeSizes . sizeof ( cqlLimits . perPartitionLimit ) ; <nl> size + = TypeSizes . sizeof ( cqlLimits . isDistinct ) ; <nl> if ( limits . kind ( ) = = Kind . CQL _ PAGING _ LIMIT ) <nl> { <nl> CQLPagingLimits pagingLimits = ( CQLPagingLimits ) cqlLimits ; <nl> - size + = ByteBufferUtil . serializedSizeWithVIntLength ( pagingLimits . lastReturnedKey ) ; <nl> - size + = TypeSizes . sizeofVInt ( pagingLimits . lastReturnedKeyRemaining ) ; <nl> + size + = ByteBufferUtil . serializedSizeWithShortLength ( pagingLimits . lastReturnedKey ) ; <nl> + size + = TypeSizes . sizeof ( pagingLimits . lastReturnedKeyRemaining ) ; <nl> } <nl> break ; <nl> case THRIFT _ LIMIT : <nl> case SUPER _ COLUMN _ COUNTING _ LIMIT : <nl> ThriftLimits thriftLimits = ( ThriftLimits ) limits ; <nl> - size + = TypeSizes . sizeofVInt ( thriftLimits . partitionLimit ) ; <nl> - size + = TypeSizes . sizeofVInt ( thriftLimits . cellPerPartitionLimit ) ; <nl> + size + = TypeSizes . sizeof ( thriftLimits . partitionLimit ) ; <nl> + size + = TypeSizes . sizeof ( thriftLimits . cellPerPartitionLimit ) ; <nl> break ; <nl> default : <nl> throw new AssertionError ( ) ; <nl> diff - - git a / src / java / org / apache / cassandra / db / filter / RowFilter . java b / src / java / org / apache / cassandra / db / filter / RowFilter . java <nl> index 881e154 . . 5a49bca 100644 <nl> - - - a / src / java / org / apache / cassandra / db / filter / RowFilter . java <nl> + + + b / src / java / org / apache / cassandra / db / filter / RowFilter . java <nl> @ @ - 17 , 6 + 17 , 7 @ @ <nl> * / <nl> package org . apache . cassandra . db . filter ; <nl> <nl> + import java . io . DataInput ; <nl> import java . io . IOException ; <nl> import java . nio . ByteBuffer ; <nl> import java . util . * ; <nl> @ @ - 32 , 7 + 33 , 6 @ @ import org . apache . cassandra . db . partitions . * ; <nl> import org . apache . cassandra . db . marshal . * ; <nl> import org . apache . cassandra . db . partitions . UnfilteredPartitionIterator ; <nl> import org . apache . cassandra . exceptions . InvalidRequestException ; <nl> - import org . apache . cassandra . io . util . DataInputPlus ; <nl> import org . apache . cassandra . io . util . DataOutputPlus ; <nl> import org . apache . cassandra . net . MessagingService ; <nl> import org . apache . cassandra . utils . ByteBufferUtil ; <nl> @ @ - 392 , 7 + 392 , 7 @ @ public abstract class RowFilter implements Iterable < RowFilter . Expression > <nl> } <nl> } <nl> <nl> - public Expression deserialize ( DataInputPlus in , int version , CFMetaData metadata ) throws IOException <nl> + public Expression deserialize ( DataInput in , int version , CFMetaData metadata ) throws IOException <nl> { <nl> ByteBuffer name = ByteBufferUtil . readWithShortLength ( in ) ; <nl> Operator operator = Operator . readFrom ( in ) ; <nl> @ @ - 742 , 15 + 742 , 15 @ @ public abstract class RowFilter implements Iterable < RowFilter . Expression > <nl> public void serialize ( RowFilter filter , DataOutputPlus out , int version ) throws IOException <nl> { <nl> out . writeBoolean ( filter instanceof ThriftFilter ) ; <nl> - out . writeVInt ( filter . expressions . size ( ) ) ; <nl> + out . writeShort ( filter . expressions . size ( ) ) ; <nl> for ( Expression expr : filter . expressions ) <nl> Expression . serializer . serialize ( expr , out , version ) ; <nl> } <nl> <nl> - public RowFilter deserialize ( DataInputPlus in , int version , CFMetaData metadata ) throws IOException <nl> + public RowFilter deserialize ( DataInput in , int version , CFMetaData metadata ) throws IOException <nl> { <nl> boolean forThrift = in . readBoolean ( ) ; <nl> - int size = ( int ) in . readVInt ( ) ; <nl> + int size = in . readUnsignedShort ( ) ; <nl> List < Expression > expressions = new ArrayList < > ( size ) ; <nl> for ( int i = 0 ; i < size ; i + + ) <nl> expressions . add ( Expression . serializer . deserialize ( in , version , metadata ) ) ; <nl> @ @ - 762 , 7 + 762 , 7 @ @ public abstract class RowFilter implements Iterable < RowFilter . Expression > <nl> public long serializedSize ( RowFilter filter , int version ) <nl> { <nl> long size = 1 / / forThrift <nl> - + TypeSizes . sizeofVInt ( filter . expressions . size ( ) ) ; <nl> + + TypeSizes . sizeof ( ( short ) filter . expressions . size ( ) ) ; <nl> for ( Expression expr : filter . expressions ) <nl> size + = Expression . serializer . serializedSize ( expr , version ) ; <nl> return size ; <nl> diff - - git a / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java b / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java <nl> index b96e0b1 . . 3a12584 100644 <nl> - - - a / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java <nl> + + + b / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java <nl> @ @ - 89 , 7 + 89 , 7 @ @ public class UnfilteredRowIteratorSerializer <nl> / / Should only be used for the on - wire format . <nl> public void serialize ( UnfilteredRowIterator iterator , DataOutputPlus out , SerializationHeader header , int version , int rowEstimate ) throws IOException <nl> { <nl> - ByteBufferUtil . writeWithVIntLength ( iterator . partitionKey ( ) . getKey ( ) , out ) ; <nl> + ByteBufferUtil . writeWithLength ( iterator . partitionKey ( ) . getKey ( ) , out ) ; <nl> <nl> int flags = 0 ; <nl> if ( iterator . isReverseOrder ( ) ) <nl> @ @ - 140 , 7 + 140 , 7 @ @ public class UnfilteredRowIteratorSerializer <nl> <nl> assert rowEstimate > = 0 ; <nl> <nl> - long size = ByteBufferUtil . serializedSizeWithVIntLength ( iterator . partitionKey ( ) . getKey ( ) ) <nl> + long size = TypeSizes . sizeofWithLength ( iterator . partitionKey ( ) . getKey ( ) ) <nl> + 1 ; / / flags <nl> <nl> if ( iterator . isEmpty ( ) ) <nl> @ @ - 170 , 7 + 170 , 7 @ @ public class UnfilteredRowIteratorSerializer <nl> <nl> public Header deserializeHeader ( DataInputPlus in , int version , CFMetaData metadata , SerializationHelper . Flag flag ) throws IOException <nl> { <nl> - DecoratedKey key = StorageService . getPartitioner ( ) . decorateKey ( ByteBufferUtil . readWithVIntLength ( in ) ) ; <nl> + DecoratedKey key = StorageService . getPartitioner ( ) . decorateKey ( ByteBufferUtil . readWithLength ( in ) ) ; <nl> int flags = in . readUnsignedByte ( ) ; <nl> boolean isReversed = ( flags & IS _ REVERSED ) ! = 0 ; <nl> if ( ( flags & IS _ EMPTY ) ! = 0 ) <nl> diff - - git a / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java b / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java <nl> index f306e6d . . f6eb62a 100644 <nl> - - - a / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java <nl> + + + b / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java <nl> @ @ - 146 , 7 + 146 , 7 @ @ public class UnfilteredSerializer <nl> writeComplexColumn ( i , ( ComplexColumnData ) cells . next ( columns . getComplex ( i - simpleCount ) ) , hasComplexDeletion , pkLiveness , header , out , useSparse ) ; <nl> <nl> if ( useSparse ) <nl> - out . writeVInt ( - 1 ) ; <nl> + out . writeShort ( - 1 ) ; <nl> } <nl> <nl> private void writeSimpleColumn ( int idx , Cell cell , LivenessInfo rowLiveness , SerializationHeader header , DataOutputPlus out , boolean useSparse ) <nl> @ @ - 157 , 7 + 157 , 7 @ @ public class UnfilteredSerializer <nl> if ( cell = = null ) <nl> return ; <nl> <nl> - out . writeVInt ( idx ) ; <nl> + out . writeShort ( idx ) ; <nl> } <nl> Cell . serializer . serialize ( cell , out , rowLiveness , header ) ; <nl> } <nl> @ @ - 170 , 7 + 170 , 7 @ @ public class UnfilteredSerializer <nl> if ( data = = null ) <nl> return ; <nl> <nl> - out . writeVInt ( idx ) ; <nl> + out . writeShort ( idx ) ; <nl> } <nl> <nl> if ( hasComplexDeletion ) <nl> @ @ - 244 , 7 + 244 , 7 @ @ public class UnfilteredSerializer <nl> size + = sizeOfComplexColumn ( i , ( ComplexColumnData ) cells . next ( columns . getComplex ( i - simpleCount ) ) , hasComplexDeletion , pkLiveness , header , useSparse ) ; <nl> <nl> if ( useSparse ) <nl> - size + = TypeSizes . sizeofVInt ( - 1 ) ; <nl> + size + = TypeSizes . sizeof ( ( short ) - 1 ) ; <nl> <nl> return size ; <nl> } <nl> @ @ - 257 , 7 + 257 , 7 @ @ public class UnfilteredSerializer <nl> if ( cell = = null ) <nl> return size ; <nl> <nl> - size + = TypeSizes . sizeofVInt ( idx ) ; <nl> + size + = TypeSizes . sizeof ( ( short ) idx ) ; <nl> } <nl> return size + Cell . serializer . serializedSize ( cell , rowLiveness , header ) ; <nl> } <nl> @ @ - 270 , 7 + 270 , 7 @ @ public class UnfilteredSerializer <nl> if ( data = = null ) <nl> return size ; <nl> <nl> - size + = TypeSizes . sizeofVInt ( idx ) ; <nl> + size + = TypeSizes . sizeof ( ( short ) idx ) ; <nl> } <nl> <nl> if ( hasComplexDeletion ) <nl> @ @ - 388 , 7 + 388 , 7 @ @ public class UnfilteredSerializer <nl> int count = columns . columnCount ( ) ; <nl> int simpleCount = columns . simpleColumnCount ( ) ; <nl> int i ; <nl> - while ( ( i = ( int ) in . readVInt ( ) ) > = 0 ) <nl> + while ( ( i = in . readShort ( ) ) > = 0 ) <nl> { <nl> if ( i > count ) <nl> throw new IOException ( String . format ( " Impossible column index % d , the header has only % d columns defined " , i , count ) ) ; <nl> @ @ - 489 , 7 + 489 , 7 @ @ public class UnfilteredSerializer <nl> int count = columns . columnCount ( ) ; <nl> int simpleCount = columns . simpleColumnCount ( ) ; <nl> int i ; <nl> - while ( ( i = ( int ) in . readVInt ( ) ) > = 0 ) <nl> + while ( ( i = in . readShort ( ) ) > = 0 ) <nl> { <nl> if ( i > count ) <nl> throw new IOException ( String . format ( " Impossible column index % d , the header has only % d columns defined " , i , count ) ) ;
NEAREST DIFF (one line): ELIMINATEDSENTENCE

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 4608492 . . 2db4115 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 5 , 8 + 5 , 8 @ @ 
 * Metrics should use up to date nomenclature ( CASSANDRA - 9448 ) 
 * Change CREATE / ALTER TABLE syntax for compression ( CASSANDRA - 8384 ) 
 * Cleanup crc and adler code for java 8 ( CASSANDRA - 9650 ) 
 - * Storage engine refactor ( CASSANDRA - 8099 , 9743 , 9746 , 9759 , 9781 , 9808 , 9825 , 
 - 9848 , 9705 , 9859 , 9867 , 9874 , 9828 , 9801 ) 
 + * Storage engine refactor ( CASSANDRA - 8099 , 9743 , 9746 , 9759 , 9781 , 9808 , 9825 , 9848 , 
 + 9705 , 9859 , 9867 , 9874 , 9828 , 9801 ) 
 * Update Guava to 18 . 0 ( CASSANDRA - 9653 ) 
 * Bloom filter false positive ratio is not honoured ( CASSANDRA - 8413 ) 
 * New option for cassandra - stress to leave a ratio of columns null ( CASSANDRA - 9522 ) 
 diff - - git a / src / java / org / apache / cassandra / db / Columns . java b / src / java / org / apache / cassandra / db / Columns . java 
 index 03d2e14 . . 48a4504 100644 
 - - - a / src / java / org / apache / cassandra / db / Columns . java 
 + + + b / src / java / org / apache / cassandra / db / Columns . java 
 @ @ - 17 , 6 + 17 , 7 @ @ 
 * / 
 package org . apache . cassandra . db ; 
 
 + import java . io . DataInput ; 
 import java . io . IOException ; 
 import java . util . * ; 
 import java . util . function . Predicate ; 
 @ @ - 30 , 7 + 31 , 6 @ @ import org . apache . cassandra . config . CFMetaData ; 
 import org . apache . cassandra . config . ColumnDefinition ; 
 import org . apache . cassandra . db . marshal . UTF8Type ; 
 import org . apache . cassandra . db . marshal . MapType ; 
 - import org . apache . cassandra . io . util . DataInputPlus ; 
 import org . apache . cassandra . io . util . DataOutputPlus ; 
 import org . apache . cassandra . utils . ByteBufferUtil ; 
 
 @ @ - 526 , 26 + 526 , 26 @ @ public class Columns implements Iterable < ColumnDefinition > 
 { 
 public void serialize ( Columns columns , DataOutputPlus out ) throws IOException 
 { 
 - out . writeVInt ( columns . columnCount ( ) ) ; 
 + out . writeShort ( columns . columnCount ( ) ) ; 
 for ( ColumnDefinition column : columns ) 
 - ByteBufferUtil . writeWithVIntLength ( column . name . bytes , out ) ; 
 + ByteBufferUtil . writeWithShortLength ( column . name . bytes , out ) ; 
 } 
 
 public long serializedSize ( Columns columns ) 
 { 
 - long size = TypeSizes . sizeofVInt ( columns . columnCount ( ) ) ; 
 + long size = TypeSizes . sizeof ( ( short ) columns . columnCount ( ) ) ; 
 for ( ColumnDefinition column : columns ) 
 - size + = ByteBufferUtil . serializedSizeWithVIntLength ( column . name . bytes ) ; 
 + size + = TypeSizes . sizeofWithShortLength ( column . name . bytes ) ; 
 return size ; 
 } 
 
 - public Columns deserialize ( DataInputPlus in , CFMetaData metadata ) throws IOException 
 + public Columns deserialize ( DataInput in , CFMetaData metadata ) throws IOException 
 { 
 - int length = ( int ) in . readVInt ( ) ; 
 + int length = in . readUnsignedShort ( ) ; 
 ColumnDefinition [ ] columns = new ColumnDefinition [ length ] ; 
 for ( int i = 0 ; i < length ; i + + ) 
 { 
 - ByteBuffer name = ByteBufferUtil . readWithVIntLength ( in ) ; 
 + ByteBuffer name = ByteBufferUtil . readWithShortLength ( in ) ; 
 ColumnDefinition column = metadata . getColumnDefinition ( name ) ; 
 if ( column = = null ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / db / Mutation . java b / src / java / org / apache / cassandra / db / Mutation . java 
 index 3d49ca6 . . aca6622 100644 
 - - - a / src / java / org / apache / cassandra / db / Mutation . java 
 + + + b / src / java / org / apache / cassandra / db / Mutation . java 
 @ @ - 248 , 19 + 248 , 12 @ @ public class Mutation implements IMutation 
 if ( version < MessagingService . VERSION _ 20 ) 
 out . writeUTF ( mutation . getKeyspaceName ( ) ) ; 
 
 - / * serialize the modifications in the mutation * / 
 - int size = mutation . modifications . size ( ) ; 
 - 
 if ( version < MessagingService . VERSION _ 30 ) 
 - { 
 ByteBufferUtil . writeWithShortLength ( mutation . key ( ) . getKey ( ) , out ) ; 
 - out . writeInt ( size ) ; 
 - } 
 - else 
 - { 
 - out . writeVInt ( size ) ; 
 - } 
 
 + / * serialize the modifications in the mutation * / 
 + int size = mutation . modifications . size ( ) ; 
 + out . writeInt ( size ) ; 
 assert size > 0 ; 
 for ( Map . Entry < UUID , PartitionUpdate > entry : mutation . modifications . entrySet ( ) ) 
 PartitionUpdate . serializer . serialize ( entry . getValue ( ) , out , version ) ; 
 @ @ - 273 , 17 + 266 , 10 @ @ public class Mutation implements IMutation 
 keyspaceName = in . readUTF ( ) ; 
 
 DecoratedKey key = null ; 
 - int size ; 
 if ( version < MessagingService . VERSION _ 30 ) 
 - { 
 key = StorageService . getPartitioner ( ) . decorateKey ( ByteBufferUtil . readWithShortLength ( in ) ) ; 
 - size = in . readInt ( ) ; 
 - } 
 - else 
 - { 
 - size = ( int ) in . readVInt ( ) ; 
 - } 
 
 + int size = in . readInt ( ) ; 
 assert size > 0 ; 
 
 if ( size = = 1 ) 
 @ @ - 321 , 13 + 307 , 9 @ @ public class Mutation implements IMutation 
 { 
 int keySize = mutation . key ( ) . getKey ( ) . remaining ( ) ; 
 size + = TypeSizes . sizeof ( ( short ) keySize ) + keySize ; 
 - size + = TypeSizes . sizeof ( mutation . modifications . size ( ) ) ; 
 - } 
 - else 
 - { 
 - size + = TypeSizes . sizeofVInt ( mutation . modifications . size ( ) ) ; 
 } 
 
 + size + = TypeSizes . sizeof ( mutation . modifications . size ( ) ) ; 
 for ( Map . Entry < UUID , PartitionUpdate > entry : mutation . modifications . entrySet ( ) ) 
 size + = PartitionUpdate . serializer . serializedSize ( entry . getValue ( ) , version ) ; 
 
 diff - - git a / src / java / org / apache / cassandra / db / ReadResponse . java b / src / java / org / apache / cassandra / db / ReadResponse . java 
 index 90bd21d . . 740423a 100644 
 - - - a / src / java / org / apache / cassandra / db / ReadResponse . java 
 + + + b / src / java / org / apache / cassandra / db / ReadResponse . java 
 @ @ - 20 , 6 + 20 , 8 @ @ package org . apache . cassandra . db ; 
 import java . io . * ; 
 import java . nio . ByteBuffer ; 
 import java . security . MessageDigest ; 
 + import java . util . ArrayList ; 
 + import java . util . List ; 
 
 import org . apache . cassandra . config . CFMetaData ; 
 import org . apache . cassandra . db . rows . * ; 
 @ @ - 163 , 14 + 165 , 14 @ @ public abstract class ReadResponse 
 } 
 
 boolean isDigest = response . isDigestQuery ( ) ; 
 - ByteBufferUtil . writeWithVIntLength ( isDigest ? response . digest ( response . metadata ) : ByteBufferUtil . EMPTY _ BYTE _ BUFFER , out ) ; 
 + ByteBufferUtil . writeWithShortLength ( isDigest ? response . digest ( response . metadata ) : ByteBufferUtil . EMPTY _ BYTE _ BUFFER , out ) ; 
 if ( ! isDigest ) 
 { 
 / / Note that we can only get there if version = = 3 . 0 , which is the current _ version . When we ' ll change the 
 / / version , we ' ll have to deserialize / re - serialize the data to be in the proper version . 
 assert version = = MessagingService . VERSION _ 30 ; 
 ByteBuffer data = ( ( DataResponse ) response ) . data ; 
 - ByteBufferUtil . writeWithVIntLength ( data , out ) ; 
 + ByteBufferUtil . writeWithLength ( data , out ) ; 
 } 
 } 
 
 @ @ - 182 , 12 + 184 , 12 @ @ public abstract class ReadResponse 
 throw new UnsupportedOperationException ( ) ; 
 } 
 
 - ByteBuffer digest = ByteBufferUtil . readWithVIntLength ( in ) ; 
 + ByteBuffer digest = ByteBufferUtil . readWithShortLength ( in ) ; 
 if ( digest . hasRemaining ( ) ) 
 return new DigestResponse ( digest ) ; 
 
 assert version = = MessagingService . VERSION _ 30 ; 
 - ByteBuffer data = ByteBufferUtil . readWithVIntLength ( in ) ; 
 + ByteBuffer data = ByteBufferUtil . readWithLength ( in ) ; 
 return new DataResponse ( data ) ; 
 } 
 
 @ @ - 200 , 14 + 202 , 15 @ @ public abstract class ReadResponse 
 } 
 
 boolean isDigest = response . isDigestQuery ( ) ; 
 - long size = ByteBufferUtil . serializedSizeWithVIntLength ( isDigest ? response . digest ( response . metadata ) : ByteBufferUtil . EMPTY _ BYTE _ BUFFER ) ; 
 + long size = ByteBufferUtil . serializedSizeWithShortLength ( isDigest ? response . digest ( response . metadata ) : ByteBufferUtil . EMPTY _ BYTE _ BUFFER ) ; 
 + 
 if ( ! isDigest ) 
 { 
 / / Note that we can only get there if version = = 3 . 0 , which is the current _ version . When we ' ll change the 
 / / version , we ' ll have to deserialize / re - serialize the data to be in the proper version . 
 assert version = = MessagingService . VERSION _ 30 ; 
 ByteBuffer data = ( ( DataResponse ) response ) . data ; 
 - size + = ByteBufferUtil . serializedSizeWithVIntLength ( data ) ; 
 + size + = ByteBufferUtil . serializedSizeWithLength ( data ) ; 
 } 
 return size ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / SerializationHeader . java b / src / java / org / apache / cassandra / db / SerializationHeader . java 
 index 2326f1e . . c054b25 100644 
 - - - a / src / java / org / apache / cassandra / db / SerializationHeader . java 
 + + + b / src / java / org / apache / cassandra / db / SerializationHeader . java 
 @ @ - 410 , 7 + 410 , 7 @ @ public class SerializationHeader 
 EncodingStats . serializer . serialize ( header . stats , out ) ; 
 
 writeType ( header . keyType , out ) ; 
 - out . writeVInt ( header . clusteringTypes . size ( ) ) ; 
 + out . writeShort ( header . clusteringTypes . size ( ) ) ; 
 for ( AbstractType < ? > type : header . clusteringTypes ) 
 writeType ( type , out ) ; 
 
 @ @ - 424 , 7 + 424 , 7 @ @ public class SerializationHeader 
 EncodingStats stats = EncodingStats . serializer . deserialize ( in ) ; 
 
 AbstractType < ? > keyType = readType ( in ) ; 
 - int size = ( int ) in . readVInt ( ) ; 
 + int size = in . readUnsignedShort ( ) ; 
 List < AbstractType < ? > > clusteringTypes = new ArrayList < > ( size ) ; 
 for ( int i = 0 ; i < size ; i + + ) 
 clusteringTypes . add ( readType ( in ) ) ; 
 @ @ - 444 , 7 + 444 , 7 @ @ public class SerializationHeader 
 int size = EncodingStats . serializer . serializedSize ( header . stats ) ; 
 
 size + = sizeofType ( header . keyType ) ; 
 - size + = TypeSizes . sizeofVInt ( header . clusteringTypes . size ( ) ) ; 
 + size + = TypeSizes . sizeof ( ( short ) header . clusteringTypes . size ( ) ) ; 
 for ( AbstractType < ? > type : header . clusteringTypes ) 
 size + = sizeofType ( type ) ; 
 
 @ @ - 455 , 20 + 455 , 20 @ @ public class SerializationHeader 
 
 private void writeColumnsWithTypes ( Map < ByteBuffer , AbstractType < ? > > columns , DataOutputPlus out ) throws IOException 
 { 
 - out . writeVInt ( columns . size ( ) ) ; 
 + out . writeShort ( columns . size ( ) ) ; 
 for ( Map . Entry < ByteBuffer , AbstractType < ? > > entry : columns . entrySet ( ) ) 
 { 
 - ByteBufferUtil . writeWithVIntLength ( entry . getKey ( ) , out ) ; 
 + ByteBufferUtil . writeWithShortLength ( entry . getKey ( ) , out ) ; 
 writeType ( entry . getValue ( ) , out ) ; 
 } 
 } 
 
 private long sizeofColumnsWithTypes ( Map < ByteBuffer , AbstractType < ? > > columns ) 
 { 
 - long size = TypeSizes . sizeofVInt ( columns . size ( ) ) ; 
 + long size = TypeSizes . sizeof ( ( short ) columns . size ( ) ) ; 
 for ( Map . Entry < ByteBuffer , AbstractType < ? > > entry : columns . entrySet ( ) ) 
 { 
 - size + = ByteBufferUtil . serializedSizeWithVIntLength ( entry . getKey ( ) ) ; 
 + size + = TypeSizes . sizeofWithShortLength ( entry . getKey ( ) ) ; 
 size + = sizeofType ( entry . getValue ( ) ) ; 
 } 
 return size ; 
 @ @ - 476 , 10 + 476 , 10 @ @ public class SerializationHeader 
 
 private void readColumnsWithType ( DataInputPlus in , Map < ByteBuffer , AbstractType < ? > > typeMap ) throws IOException 
 { 
 - int length = ( int ) in . readVInt ( ) ; 
 + int length = in . readUnsignedShort ( ) ; 
 for ( int i = 0 ; i < length ; i + + ) 
 { 
 - ByteBuffer name = ByteBufferUtil . readWithVIntLength ( in ) ; 
 + ByteBuffer name = ByteBufferUtil . readWithShortLength ( in ) ; 
 typeMap . put ( name , readType ( in ) ) ; 
 } 
 } 
 @ @ - 487 , 18 + 487 , 18 @ @ public class SerializationHeader 
 private void writeType ( AbstractType < ? > type , DataOutputPlus out ) throws IOException 
 { 
 / / TODO : we should have a terser serializaion format . Not a big deal though 
 - ByteBufferUtil . writeWithVIntLength ( UTF8Type . instance . decompose ( type . toString ( ) ) , out ) ; 
 + ByteBufferUtil . writeWithLength ( UTF8Type . instance . decompose ( type . toString ( ) ) , out ) ; 
 } 
 
 private AbstractType < ? > readType ( DataInputPlus in ) throws IOException 
 { 
 - ByteBuffer raw = ByteBufferUtil . readWithVIntLength ( in ) ; 
 + ByteBuffer raw = ByteBufferUtil . readWithLength ( in ) ; 
 return TypeParser . parse ( UTF8Type . instance . compose ( raw ) ) ; 
 } 
 
 private int sizeofType ( AbstractType < ? > type ) 
 { 
 - return ByteBufferUtil . serializedSizeWithVIntLength ( UTF8Type . instance . decompose ( type . toString ( ) ) ) ; 
 + return TypeSizes . sizeofWithLength ( UTF8Type . instance . decompose ( type . toString ( ) ) ) ; 
 } 
 } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / Slices . java b / src / java / org / apache / cassandra / db / Slices . java 
 index 9dd4a48 . . 32ca06d 100644 
 - - - a / src / java / org / apache / cassandra / db / Slices . java 
 + + + b / src / java / org / apache / cassandra / db / Slices . java 
 @ @ - 288 , 7 + 288 , 7 @ @ public abstract class Slices implements Iterable < Slice > 
 public void serialize ( Slices slices , DataOutputPlus out , int version ) throws IOException 
 { 
 int size = slices . size ( ) ; 
 - out . writeVInt ( size ) ; 
 + out . writeInt ( size ) ; 
 
 if ( size = = 0 ) 
 return ; 
 @ @ - 303 , 7 + 303 , 7 @ @ public abstract class Slices implements Iterable < Slice > 
 
 public long serializedSize ( Slices slices , int version ) 
 { 
 - long size = TypeSizes . sizeofVInt ( slices . size ( ) ) ; 
 + long size = TypeSizes . sizeof ( slices . size ( ) ) ; 
 
 if ( slices . size ( ) = = 0 ) 
 return size ; 
 @ @ - 320 , 7 + 320 , 7 @ @ public abstract class Slices implements Iterable < Slice > 
 
 public Slices deserialize ( DataInputPlus in , int version , CFMetaData metadata ) throws IOException 
 { 
 - int size = ( int ) in . readVInt ( ) ; 
 + int size = in . readInt ( ) ; 
 
 if ( size = = 0 ) 
 return NONE ; 
 diff - - git a / src / java / org / apache / cassandra / db / filter / ClusteringIndexNamesFilter . java b / src / java / org / apache / cassandra / db / filter / ClusteringIndexNamesFilter . java 
 index a6f2179 . . 13329f3 100644 
 - - - a / src / java / org / apache / cassandra / db / filter / ClusteringIndexNamesFilter . java 
 + + + b / src / java / org / apache / cassandra / db / filter / ClusteringIndexNamesFilter . java 
 @ @ - 245 , 15 + 245 , 15 @ @ public class ClusteringIndexNamesFilter extends AbstractClusteringIndexFilter 
 protected void serializeInternal ( DataOutputPlus out , int version ) throws IOException 
 { 
 ClusteringComparator comparator = ( ClusteringComparator ) clusterings . comparator ( ) ; 
 - out . writeVInt ( clusterings . size ( ) ) ; 
 + out . writeInt ( clusterings . size ( ) ) ; 
 for ( Clustering clustering : clusterings ) 
 Clustering . serializer . serialize ( clustering , out , version , comparator . subtypes ( ) ) ; 
 } 
 
 protected long serializedSizeInternal ( int version ) 
 { 
 + long size = 0 ; 
 ClusteringComparator comparator = ( ClusteringComparator ) clusterings . comparator ( ) ; 
 - long size = TypeSizes . sizeofVInt ( clusterings . size ( ) ) ; 
 for ( Clustering clustering : clusterings ) 
 size + = Clustering . serializer . serializedSize ( clustering , version , comparator . subtypes ( ) ) ; 
 return size ; 
 @ @ - 265 , 7 + 265 , 7 @ @ public class ClusteringIndexNamesFilter extends AbstractClusteringIndexFilter 
 { 
 ClusteringComparator comparator = metadata . comparator ; 
 BTreeSet . Builder < Clustering > clusterings = BTreeSet . builder ( comparator ) ; 
 - int size = ( int ) in . readVInt ( ) ; 
 + int size = in . readInt ( ) ; 
 for ( int i = 0 ; i < size ; i + + ) 
 clusterings . add ( Clustering . serializer . deserialize ( in , version , comparator . subtypes ( ) ) ) ; 
 
 diff - - git a / src / java / org / apache / cassandra / db / filter / ColumnFilter . java b / src / java / org / apache / cassandra / db / filter / ColumnFilter . java 
 index d2cb87d . . 084bad6 100644 
 - - - a / src / java / org / apache / cassandra / db / filter / ColumnFilter . java 
 + + + b / src / java / org / apache / cassandra / db / filter / ColumnFilter . java 
 @ @ - 365 , 7 + 365 , 7 @ @ public class ColumnFilter 
 
 if ( selection . subSelections ! = null ) 
 { 
 - out . writeVInt ( selection . subSelections . size ( ) ) ; 
 + out . writeShort ( selection . subSelections . size ( ) ) ; 
 for ( ColumnSubselection subSel : selection . subSelections . values ( ) ) 
 ColumnSubselection . serializer . serialize ( subSel , out , version ) ; 
 } 
 @ @ - 390 , 7 + 390 , 7 @ @ public class ColumnFilter 
 if ( hasSubSelections ) 
 { 
 subSelections = TreeMultimap . create ( Comparator . < ColumnIdentifier > naturalOrder ( ) , Comparator . < ColumnSubselection > naturalOrder ( ) ) ; 
 - int size = ( int ) in . readVInt ( ) ; 
 + int size = in . readUnsignedShort ( ) ; 
 for ( int i = 0 ; i < size ; i + + ) 
 { 
 ColumnSubselection subSel = ColumnSubselection . serializer . deserialize ( in , version , metadata ) ; 
 @ @ - 414 , 7 + 414 , 7 @ @ public class ColumnFilter 
 if ( selection . subSelections ! = null ) 
 { 
 
 - size + = TypeSizes . sizeofVInt ( selection . subSelections . size ( ) ) ; 
 + size + = TypeSizes . sizeof ( ( short ) selection . subSelections . size ( ) ) ; 
 for ( ColumnSubselection subSel : selection . subSelections . values ( ) ) 
 size + = ColumnSubselection . serializer . serializedSize ( subSel , version ) ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / filter / DataLimits . java b / src / java / org / apache / cassandra / db / filter / DataLimits . java 
 index 458ee30 . . 206afa4 100644 
 - - - a / src / java / org / apache / cassandra / db / filter / DataLimits . java 
 + + + b / src / java / org / apache / cassandra / db / filter / DataLimits . java 
 @ @ - 17 , 13 + 17 , 13 @ @ 
 * / 
 package org . apache . cassandra . db . filter ; 
 
 + import java . io . DataInput ; 
 import java . io . IOException ; 
 import java . nio . ByteBuffer ; 
 
 import org . apache . cassandra . db . * ; 
 import org . apache . cassandra . db . rows . * ; 
 import org . apache . cassandra . db . partitions . * ; 
 - import org . apache . cassandra . io . util . DataInputPlus ; 
 import org . apache . cassandra . io . util . DataOutputPlus ; 
 import org . apache . cassandra . utils . ByteBufferUtil ; 
 
 @ @ - 644 , 45 + 644 , 45 @ @ public abstract class DataLimits 
 case CQL _ LIMIT : 
 case CQL _ PAGING _ LIMIT : 
 CQLLimits cqlLimits = ( CQLLimits ) limits ; 
 - out . writeVInt ( cqlLimits . rowLimit ) ; 
 - out . writeVInt ( cqlLimits . perPartitionLimit ) ; 
 + out . writeInt ( cqlLimits . rowLimit ) ; 
 + out . writeInt ( cqlLimits . perPartitionLimit ) ; 
 out . writeBoolean ( cqlLimits . isDistinct ) ; 
 if ( limits . kind ( ) = = Kind . CQL _ PAGING _ LIMIT ) 
 { 
 CQLPagingLimits pagingLimits = ( CQLPagingLimits ) cqlLimits ; 
 - ByteBufferUtil . writeWithVIntLength ( pagingLimits . lastReturnedKey , out ) ; 
 - out . writeVInt ( pagingLimits . lastReturnedKeyRemaining ) ; 
 + ByteBufferUtil . writeWithShortLength ( pagingLimits . lastReturnedKey , out ) ; 
 + out . writeInt ( pagingLimits . lastReturnedKeyRemaining ) ; 
 } 
 break ; 
 case THRIFT _ LIMIT : 
 case SUPER _ COLUMN _ COUNTING _ LIMIT : 
 ThriftLimits thriftLimits = ( ThriftLimits ) limits ; 
 - out . writeVInt ( thriftLimits . partitionLimit ) ; 
 - out . writeVInt ( thriftLimits . cellPerPartitionLimit ) ; 
 + out . writeInt ( thriftLimits . partitionLimit ) ; 
 + out . writeInt ( thriftLimits . cellPerPartitionLimit ) ; 
 break ; 
 } 
 } 
 
 - public DataLimits deserialize ( DataInputPlus in , int version ) throws IOException 
 + public DataLimits deserialize ( DataInput in , int version ) throws IOException 
 { 
 Kind kind = Kind . values ( ) [ in . readUnsignedByte ( ) ] ; 
 switch ( kind ) 
 { 
 case CQL _ LIMIT : 
 case CQL _ PAGING _ LIMIT : 
 - int rowLimit = ( int ) in . readVInt ( ) ; 
 - int perPartitionLimit = ( int ) in . readVInt ( ) ; 
 + int rowLimit = in . readInt ( ) ; 
 + int perPartitionLimit = in . readInt ( ) ; 
 boolean isDistinct = in . readBoolean ( ) ; 
 if ( kind = = Kind . CQL _ LIMIT ) 
 return new CQLLimits ( rowLimit , perPartitionLimit , isDistinct ) ; 
 
 - ByteBuffer lastKey = ByteBufferUtil . readWithVIntLength ( in ) ; 
 - int lastRemaining = ( int ) in . readVInt ( ) ; 
 + ByteBuffer lastKey = ByteBufferUtil . readWithShortLength ( in ) ; 
 + int lastRemaining = in . readInt ( ) ; 
 return new CQLPagingLimits ( rowLimit , perPartitionLimit , isDistinct , lastKey , lastRemaining ) ; 
 case THRIFT _ LIMIT : 
 case SUPER _ COLUMN _ COUNTING _ LIMIT : 
 - int partitionLimit = ( int ) in . readVInt ( ) ; 
 - int cellPerPartitionLimit = ( int ) in . readVInt ( ) ; 
 + int partitionLimit = in . readInt ( ) ; 
 + int cellPerPartitionLimit = in . readInt ( ) ; 
 return kind = = Kind . THRIFT _ LIMIT 
 ? new ThriftLimits ( partitionLimit , cellPerPartitionLimit ) 
 : new SuperColumnCountingLimits ( partitionLimit , cellPerPartitionLimit ) ; 
 @ @ - 698 , 21 + 698 , 21 @ @ public abstract class DataLimits 
 case CQL _ LIMIT : 
 case CQL _ PAGING _ LIMIT : 
 CQLLimits cqlLimits = ( CQLLimits ) limits ; 
 - size + = TypeSizes . sizeofVInt ( cqlLimits . rowLimit ) ; 
 - size + = TypeSizes . sizeofVInt ( cqlLimits . perPartitionLimit ) ; 
 + size + = TypeSizes . sizeof ( cqlLimits . rowLimit ) ; 
 + size + = TypeSizes . sizeof ( cqlLimits . perPartitionLimit ) ; 
 size + = TypeSizes . sizeof ( cqlLimits . isDistinct ) ; 
 if ( limits . kind ( ) = = Kind . CQL _ PAGING _ LIMIT ) 
 { 
 CQLPagingLimits pagingLimits = ( CQLPagingLimits ) cqlLimits ; 
 - size + = ByteBufferUtil . serializedSizeWithVIntLength ( pagingLimits . lastReturnedKey ) ; 
 - size + = TypeSizes . sizeofVInt ( pagingLimits . lastReturnedKeyRemaining ) ; 
 + size + = ByteBufferUtil . serializedSizeWithShortLength ( pagingLimits . lastReturnedKey ) ; 
 + size + = TypeSizes . sizeof ( pagingLimits . lastReturnedKeyRemaining ) ; 
 } 
 break ; 
 case THRIFT _ LIMIT : 
 case SUPER _ COLUMN _ COUNTING _ LIMIT : 
 ThriftLimits thriftLimits = ( ThriftLimits ) limits ; 
 - size + = TypeSizes . sizeofVInt ( thriftLimits . partitionLimit ) ; 
 - size + = TypeSizes . sizeofVInt ( thriftLimits . cellPerPartitionLimit ) ; 
 + size + = TypeSizes . sizeof ( thriftLimits . partitionLimit ) ; 
 + size + = TypeSizes . sizeof ( thriftLimits . cellPerPartitionLimit ) ; 
 break ; 
 default : 
 throw new AssertionError ( ) ; 
 diff - - git a / src / java / org / apache / cassandra / db / filter / RowFilter . java b / src / java / org / apache / cassandra / db / filter / RowFilter . java 
 index 881e154 . . 5a49bca 100644 
 - - - a / src / java / org / apache / cassandra / db / filter / RowFilter . java 
 + + + b / src / java / org / apache / cassandra / db / filter / RowFilter . java 
 @ @ - 17 , 6 + 17 , 7 @ @ 
 * / 
 package org . apache . cassandra . db . filter ; 
 
 + import java . io . DataInput ; 
 import java . io . IOException ; 
 import java . nio . ByteBuffer ; 
 import java . util . * ; 
 @ @ - 32 , 7 + 33 , 6 @ @ import org . apache . cassandra . db . partitions . * ; 
 import org . apache . cassandra . db . marshal . * ; 
 import org . apache . cassandra . db . partitions . UnfilteredPartitionIterator ; 
 import org . apache . cassandra . exceptions . InvalidRequestException ; 
 - import org . apache . cassandra . io . util . DataInputPlus ; 
 import org . apache . cassandra . io . util . DataOutputPlus ; 
 import org . apache . cassandra . net . MessagingService ; 
 import org . apache . cassandra . utils . ByteBufferUtil ; 
 @ @ - 392 , 7 + 392 , 7 @ @ public abstract class RowFilter implements Iterable < RowFilter . Expression > 
 } 
 } 
 
 - public Expression deserialize ( DataInputPlus in , int version , CFMetaData metadata ) throws IOException 
 + public Expression deserialize ( DataInput in , int version , CFMetaData metadata ) throws IOException 
 { 
 ByteBuffer name = ByteBufferUtil . readWithShortLength ( in ) ; 
 Operator operator = Operator . readFrom ( in ) ; 
 @ @ - 742 , 15 + 742 , 15 @ @ public abstract class RowFilter implements Iterable < RowFilter . Expression > 
 public void serialize ( RowFilter filter , DataOutputPlus out , int version ) throws IOException 
 { 
 out . writeBoolean ( filter instanceof ThriftFilter ) ; 
 - out . writeVInt ( filter . expressions . size ( ) ) ; 
 + out . writeShort ( filter . expressions . size ( ) ) ; 
 for ( Expression expr : filter . expressions ) 
 Expression . serializer . serialize ( expr , out , version ) ; 
 } 
 
 - public RowFilter deserialize ( DataInputPlus in , int version , CFMetaData metadata ) throws IOException 
 + public RowFilter deserialize ( DataInput in , int version , CFMetaData metadata ) throws IOException 
 { 
 boolean forThrift = in . readBoolean ( ) ; 
 - int size = ( int ) in . readVInt ( ) ; 
 + int size = in . readUnsignedShort ( ) ; 
 List < Expression > expressions = new ArrayList < > ( size ) ; 
 for ( int i = 0 ; i < size ; i + + ) 
 expressions . add ( Expression . serializer . deserialize ( in , version , metadata ) ) ; 
 @ @ - 762 , 7 + 762 , 7 @ @ public abstract class RowFilter implements Iterable < RowFilter . Expression > 
 public long serializedSize ( RowFilter filter , int version ) 
 { 
 long size = 1 / / forThrift 
 - + TypeSizes . sizeofVInt ( filter . expressions . size ( ) ) ; 
 + + TypeSizes . sizeof ( ( short ) filter . expressions . size ( ) ) ; 
 for ( Expression expr : filter . expressions ) 
 size + = Expression . serializer . serializedSize ( expr , version ) ; 
 return size ; 
 diff - - git a / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java b / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java 
 index b96e0b1 . . 3a12584 100644 
 - - - a / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java 
 + + + b / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java 
 @ @ - 89 , 7 + 89 , 7 @ @ public class UnfilteredRowIteratorSerializer 
 / / Should only be used for the on - wire format . 
 public void serialize ( UnfilteredRowIterator iterator , DataOutputPlus out , SerializationHeader header , int version , int rowEstimate ) throws IOException 
 { 
 - ByteBufferUtil . writeWithVIntLength ( iterator . partitionKey ( ) . getKey ( ) , out ) ; 
 + ByteBufferUtil . writeWithLength ( iterator . partitionKey ( ) . getKey ( ) , out ) ; 
 
 int flags = 0 ; 
 if ( iterator . isReverseOrder ( ) ) 
 @ @ - 140 , 7 + 140 , 7 @ @ public class UnfilteredRowIteratorSerializer 
 
 assert rowEstimate > = 0 ; 
 
 - long size = ByteBufferUtil . serializedSizeWithVIntLength ( iterator . partitionKey ( ) . getKey ( ) ) 
 + long size = TypeSizes . sizeofWithLength ( iterator . partitionKey ( ) . getKey ( ) ) 
 + 1 ; / / flags 
 
 if ( iterator . isEmpty ( ) ) 
 @ @ - 170 , 7 + 170 , 7 @ @ public class UnfilteredRowIteratorSerializer 
 
 public Header deserializeHeader ( DataInputPlus in , int version , CFMetaData metadata , SerializationHelper . Flag flag ) throws IOException 
 { 
 - DecoratedKey key = StorageService . getPartitioner ( ) . decorateKey ( ByteBufferUtil . readWithVIntLength ( in ) ) ; 
 + DecoratedKey key = StorageService . getPartitioner ( ) . decorateKey ( ByteBufferUtil . readWithLength ( in ) ) ; 
 int flags = in . readUnsignedByte ( ) ; 
 boolean isReversed = ( flags & IS _ REVERSED ) ! = 0 ; 
 if ( ( flags & IS _ EMPTY ) ! = 0 ) 
 diff - - git a / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java b / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java 
 index f306e6d . . f6eb62a 100644 
 - - - a / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java 
 + + + b / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java 
 @ @ - 146 , 7 + 146 , 7 @ @ public class UnfilteredSerializer 
 writeComplexColumn ( i , ( ComplexColumnData ) cells . next ( columns . getComplex ( i - simpleCount ) ) , hasComplexDeletion , pkLiveness , header , out , useSparse ) ; 
 
 if ( useSparse ) 
 - out . writeVInt ( - 1 ) ; 
 + out . writeShort ( - 1 ) ; 
 } 
 
 private void writeSimpleColumn ( int idx , Cell cell , LivenessInfo rowLiveness , SerializationHeader header , DataOutputPlus out , boolean useSparse ) 
 @ @ - 157 , 7 + 157 , 7 @ @ public class UnfilteredSerializer 
 if ( cell = = null ) 
 return ; 
 
 - out . writeVInt ( idx ) ; 
 + out . writeShort ( idx ) ; 
 } 
 Cell . serializer . serialize ( cell , out , rowLiveness , header ) ; 
 } 
 @ @ - 170 , 7 + 170 , 7 @ @ public class UnfilteredSerializer 
 if ( data = = null ) 
 return ; 
 
 - out . writeVInt ( idx ) ; 
 + out . writeShort ( idx ) ; 
 } 
 
 if ( hasComplexDeletion ) 
 @ @ - 244 , 7 + 244 , 7 @ @ public class UnfilteredSerializer 
 size + = sizeOfComplexColumn ( i , ( ComplexColumnData ) cells . next ( columns . getComplex ( i - simpleCount ) ) , hasComplexDeletion , pkLiveness , header , useSparse ) ; 
 
 if ( useSparse ) 
 - size + = TypeSizes . sizeofVInt ( - 1 ) ; 
 + size + = TypeSizes . sizeof ( ( short ) - 1 ) ; 
 
 return size ; 
 } 
 @ @ - 257 , 7 + 257 , 7 @ @ public class UnfilteredSerializer 
 if ( cell = = null ) 
 return size ; 
 
 - size + = TypeSizes . sizeofVInt ( idx ) ; 
 + size + = TypeSizes . sizeof ( ( short ) idx ) ; 
 } 
 return size + Cell . serializer . serializedSize ( cell , rowLiveness , header ) ; 
 } 
 @ @ - 270 , 7 + 270 , 7 @ @ public class UnfilteredSerializer 
 if ( data = = null ) 
 return size ; 
 
 - size + = TypeSizes . sizeofVInt ( idx ) ; 
 + size + = TypeSizes . sizeof ( ( short ) idx ) ; 
 } 
 
 if ( hasComplexDeletion ) 
 @ @ - 388 , 7 + 388 , 7 @ @ public class UnfilteredSerializer 
 int count = columns . columnCount ( ) ; 
 int simpleCount = columns . simpleColumnCount ( ) ; 
 int i ; 
 - while ( ( i = ( int ) in . readVInt ( ) ) > = 0 ) 
 + while ( ( i = in . readShort ( ) ) > = 0 ) 
 { 
 if ( i > count ) 
 throw new IOException ( String . format ( " Impossible column index % d , the header has only % d columns defined " , i , count ) ) ; 
 @ @ - 489 , 7 + 489 , 7 @ @ public class UnfilteredSerializer 
 int count = columns . columnCount ( ) ; 
 int simpleCount = columns . simpleColumnCount ( ) ; 
 int i ; 
 - while ( ( i = ( int ) in . readVInt ( ) ) > = 0 ) 
 + while ( ( i = in . readShort ( ) ) > = 0 ) 
 { 
 if ( i > count ) 
 throw new IOException ( String . format ( " Impossible column index % d , the header has only % d columns defined " , i , count ) ) ;

NEAREST DIFF:
ELIMINATEDSENTENCE
