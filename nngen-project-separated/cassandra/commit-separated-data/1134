BLEU SCORE: 0.005913578458639753

TEST MSG: Fix regression in split size on CqlInputFormat
GENERATED MSG: Add paging to Hadoop InputFormat range queries . Patch by johan , review by jbellis . CASSANDRA - 789

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index cd6b92e . . 30a76a9 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 2 . 2 . 5 <nl> + * Fix regression in split size on CqlInputFormat ( CASSANDRA - 10835 ) <nl> * Better handling of SSL connection errors inter - node ( CASSANDRA - 10816 ) <nl> * Disable reloading of GossipingPropertyFileSnitch ( CASSANDRA - 9474 ) <nl> * Verify tables in pseudo - system keyspaces at startup ( CASSANDRA - 10761 ) <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> index 3c088c2 . . d55f205 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> @ @ - 18 , 8 + 18 , 19 @ @ <nl> package org . apache . cassandra . hadoop ; <nl> <nl> import java . io . IOException ; <nl> - import java . util . * ; <nl> - import java . util . concurrent . * ; <nl> + import java . util . ArrayList ; <nl> + import java . util . Collections ; <nl> + import java . util . HashMap ; <nl> + import java . util . List ; <nl> + import java . util . Map ; <nl> + import java . util . Random ; <nl> + import java . util . Set ; <nl> + import java . util . concurrent . Callable ; <nl> + import java . util . concurrent . ExecutorService ; <nl> + import java . util . concurrent . Future ; <nl> + import java . util . concurrent . LinkedBlockingQueue ; <nl> + import java . util . concurrent . ThreadPoolExecutor ; <nl> + import java . util . concurrent . TimeUnit ; <nl> <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> @ @ - 31 , 18 + 42 , 19 @ @ import com . datastax . driver . core . ResultSet ; <nl> import com . datastax . driver . core . Row ; <nl> import com . datastax . driver . core . Session ; <nl> import com . datastax . driver . core . TokenRange ; <nl> - <nl> import org . apache . cassandra . db . SystemKeyspace ; <nl> - import org . apache . cassandra . dht . ByteOrderedPartitioner ; <nl> import org . apache . cassandra . dht . IPartitioner ; <nl> - import org . apache . cassandra . dht . OrderPreservingPartitioner ; <nl> import org . apache . cassandra . dht . Range ; <nl> import org . apache . cassandra . dht . Token ; <nl> - import org . apache . cassandra . hadoop . cql3 . * ; <nl> + import org . apache . cassandra . hadoop . cql3 . CqlConfigHelper ; <nl> import org . apache . cassandra . thrift . KeyRange ; <nl> import org . apache . hadoop . conf . Configuration ; <nl> import org . apache . hadoop . mapred . JobConf ; <nl> - import org . apache . hadoop . mapreduce . * ; <nl> + import org . apache . hadoop . mapreduce . InputFormat ; <nl> + import org . apache . hadoop . mapreduce . InputSplit ; <nl> + import org . apache . hadoop . mapreduce . JobContext ; <nl> + import org . apache . hadoop . mapreduce . TaskAttemptContext ; <nl> + import org . apache . hadoop . mapreduce . TaskAttemptID ; <nl> <nl> public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < K , Y > implements org . apache . hadoop . mapred . InputFormat < K , Y > <nl> { <nl> @ @ - 230 , 9 + 242 , 10 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < <nl> private Map < TokenRange , Long > getSubSplits ( String keyspace , String cfName , TokenRange range , Configuration conf , Session session ) throws IOException <nl> { <nl> int splitSize = ConfigHelper . getInputSplitSize ( conf ) ; <nl> + int splitSizeMb = ConfigHelper . getInputSplitSizeInMb ( conf ) ; <nl> try <nl> { <nl> - return describeSplits ( keyspace , cfName , range , splitSize , session ) ; <nl> + return describeSplits ( keyspace , cfName , range , splitSize , splitSizeMb , session ) ; <nl> } <nl> catch ( Exception e ) <nl> { <nl> @ @ - 252 , 7 + 265 , 7 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < <nl> } <nl> } <nl> <nl> - private Map < TokenRange , Long > describeSplits ( String keyspace , String table , TokenRange tokenRange , int splitSize , Session session ) <nl> + private Map < TokenRange , Long > describeSplits ( String keyspace , String table , TokenRange tokenRange , int splitSize , int splitSizeMb , Session session ) <nl> { <nl> String query = String . format ( " SELECT mean _ partition _ size , partitions _ count " + <nl> " FROM % s . % s " + <nl> @ @ - 275 , 7 + 288 , 10 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < <nl> long meanPartitionSize = row . getLong ( " mean _ partition _ size " ) ; <nl> long partitionCount = row . getLong ( " partitions _ count " ) ; <nl> <nl> - int splitCount = ( int ) ( ( meanPartitionSize * partitionCount ) / splitSize ) ; <nl> + int splitCount = splitSizeMb > 0 <nl> + ? ( int ) ( meanPartitionSize * partitionCount / splitSizeMb / 1024 / 1024 ) <nl> + : ( int ) ( partitionCount / splitSize ) ; <nl> + <nl> if ( splitCount < = 0 ) splitCount = 1 ; <nl> List < TokenRange > splitRanges = tokenRange . splitEvenly ( splitCount ) ; <nl> Map < TokenRange , Long > rangesWithLength = new HashMap < > ( ) ; <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ConfigHelper . java b / src / java / org / apache / cassandra / hadoop / ConfigHelper . java <nl> index e81860d . . 376c250 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ConfigHelper . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ConfigHelper . java <nl> @ @ - 54 , 6 + 54 , 7 @ @ public class ConfigHelper <nl> private static final String INPUT _ PREDICATE _ CONFIG = " cassandra . input . predicate " ; <nl> private static final String INPUT _ KEYRANGE _ CONFIG = " cassandra . input . keyRange " ; <nl> private static final String INPUT _ SPLIT _ SIZE _ CONFIG = " cassandra . input . split . size " ; <nl> + private static final String INPUT _ SPLIT _ SIZE _ IN _ MB _ CONFIG = " cassandra . input . split . size _ mb " ; <nl> private static final String INPUT _ WIDEROWS _ CONFIG = " cassandra . input . widerows " ; <nl> private static final int DEFAULT _ SPLIT _ SIZE = 64 * 1024 ; <nl> private static final String RANGE _ BATCH _ SIZE _ CONFIG = " cassandra . range . batch . size " ; <nl> @ @ - 176 , 7 + 177 , 7 @ @ public class ConfigHelper <nl> * the overhead of each map will take up the bulk of the job time . <nl> * <nl> * @ param conf Job configuration you are about to run <nl> - * @ param splitsize Size of the input split <nl> + * @ param splitsize Number of partitions in the input split <nl> * / <nl> public static void setInputSplitSize ( Configuration conf , int splitsize ) <nl> { <nl> @ @ - 189 , 6 + 190 , 29 @ @ public class ConfigHelper <nl> } <nl> <nl> / * * <nl> + * Set the size of the input split . getInputSplitSize value is used if this is not set . <nl> + * This affects the number of maps created , if the number is too small <nl> + * the overhead of each map will take up the bulk of the job time . <nl> + * <nl> + * @ param conf Job configuration you are about to run <nl> + * @ param splitSizeMb Input split size in MB <nl> + * / <nl> + public static void setInputSplitSizeInMb ( Configuration conf , int splitSizeMb ) <nl> + { <nl> + conf . setInt ( INPUT _ SPLIT _ SIZE _ IN _ MB _ CONFIG , splitSizeMb ) ; <nl> + } <nl> + <nl> + / * * <nl> + * cassandra . input . split . size will be used if the value is undefined or negative . <nl> + * @ param conf Job configuration you are about to run <nl> + * @ return split size in MB or - 1 if it is undefined . <nl> + * / <nl> + public static int getInputSplitSizeInMb ( Configuration conf ) <nl> + { <nl> + return conf . getInt ( INPUT _ SPLIT _ SIZE _ IN _ MB _ CONFIG , - 1 ) ; <nl> + } <nl> + <nl> + / * * <nl> * Set the predicate that determines what columns will be selected from each row . <nl> * <nl> * @ param conf Job configuration you are about to run <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlInputFormat . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlInputFormat . java <nl> index 36da92d . . c46ceb8 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlInputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlInputFormat . java <nl> @ @ - 40 , 10 + 40 , 12 @ @ import com . datastax . driver . core . Row ; <nl> * ConfigHelper . setInputColumnFamily <nl> * <nl> * You can also configure the number of rows per InputSplit with <nl> - * ConfigHelper . setInputSplitSize . The default split size is 64k rows . <nl> + * 1 : ConfigHelper . setInputSplitSize . The default split size is 64k rows . <nl> + * or <nl> + * 2 : ConfigHelper . setInputSplitSizeInMb . InputSplit size in MB with new , more precise method <nl> + * If no value is provided for InputSplitSizeInMb , InputSplitSize will be used . <nl> * <nl> - * the number of CQL rows per page <nl> - * CQLConfigHelper . setInputCQLPageRowSize . The default page row size is 1000 . You <nl> + * CQLConfigHelper . setInputCQLPageRowSize . The default page row size is 1000 . You <nl> * should set it to " as big as possible , but no bigger . " It set the LIMIT for the CQL <nl> * query , so you need set it big enough to minimize the network overhead , and also <nl> * not too big to avoid out of memory issue .
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index a565ea2 . . 7499f3f 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 4 , 7 + 4 , 7 @ @ dev <nl> * switched to slf4j logging ( CASSANDRA - 625 ) <nl> <nl> <nl> - 0 . 6 . 0 - RC1 <nl> + 0 . 6 . 0 - beta3 <nl> * fix compaction bucketing bug ( CASSANDRA - 814 ) <nl> * update windows batch file ( CASSANDRA - 824 ) <nl> * deprecate KeysCachedFraction configuration directive in favor <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java <nl> index 578a5cb . . 2a64e1a 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java <nl> @ @ - 50 , 11 + 50 , 10 @ @ import org . apache . thrift . transport . TTransportException ; <nl> * You can also configure the number of rows per InputSplit with <nl> * ConfigHelper . setInputSplitSize <nl> * This should be " as big as possible , but no bigger . " Each InputSplit is read from Cassandra <nl> - * with a single get _ slice _ range query , and the per - call overhead of get _ slice _ range is high , <nl> - * so larger split sizes are better - - but if it is too large , you will run out of memory , <nl> - * since no paging is done ( yet ) . <nl> + * with multiple get _ slice _ range queries , and the per - call overhead of get _ slice _ range is high , <nl> + * so larger split sizes are better - - but if it is too large , you will run out of memory . <nl> * <nl> - * The default split size is 4096 rows . <nl> + * The default split size is 64k rows . <nl> * / <nl> public class ColumnFamilyInputFormat extends InputFormat < String , SortedMap < byte [ ] , IColumn > > <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java <nl> index 4ff8809 . . c18b17c 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java <nl> @ @ - 28 , 12 + 28 , 11 @ @ import java . util . List ; <nl> import java . util . SortedMap ; <nl> import java . util . TreeMap ; <nl> <nl> - import org . apache . commons . lang . ArrayUtils ; <nl> - <nl> import com . google . common . collect . AbstractIterator ; <nl> import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . db . * ; <nl> import org . apache . cassandra . db . marshal . AbstractType ; <nl> + import org . apache . cassandra . dht . IPartitioner ; <nl> import org . apache . cassandra . thrift . * ; <nl> import org . apache . cassandra . thrift . Column ; <nl> import org . apache . cassandra . thrift . SuperColumn ; <nl> @ @ - 52 , 7 + 51 , 8 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt <nl> private RowIterator iter ; <nl> private Pair < String , SortedMap < byte [ ] , IColumn > > currentRow ; <nl> private SlicePredicate predicate ; <nl> - private int rowCount ; <nl> + private int totalRowCount ; / / total number of rows to fetch <nl> + private int batchRowCount ; / / fetch this many per batch <nl> private String cfName ; <nl> private String keyspace ; <nl> <nl> @ @ - 70 , 7 + 70 , 8 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt <nl> <nl> public float getProgress ( ) <nl> { <nl> - return ( ( float ) iter . rowsRead ( ) ) / iter . size ( ) ; <nl> + / / the progress is likely to be reported slightly off the actual but close enough <nl> + return ( ( float ) iter . rowsRead ( ) ) / totalRowCount ; <nl> } <nl> <nl> public void initialize ( InputSplit split , TaskAttemptContext context ) throws IOException <nl> @ @ - 78 , 7 + 79 , 8 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt <nl> this . split = ( ColumnFamilySplit ) split ; <nl> Configuration conf = context . getConfiguration ( ) ; <nl> predicate = ConfigHelper . getSlicePredicate ( conf ) ; <nl> - rowCount = ConfigHelper . getInputSplitSize ( conf ) ; <nl> + totalRowCount = ConfigHelper . getInputSplitSize ( conf ) ; <nl> + batchRowCount = ConfigHelper . getRangeBatchSize ( conf ) ; <nl> cfName = ConfigHelper . getColumnFamily ( conf ) ; <nl> keyspace = ConfigHelper . getKeyspace ( conf ) ; <nl> iter = new RowIterator ( ) ; <nl> @ @ - 96 , 11 + 98 , 17 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt <nl> { <nl> <nl> private List < KeySlice > rows ; <nl> + private String startToken ; <nl> + private int totalRead = 0 ; <nl> private int i = 0 ; <nl> private AbstractType comparator = DatabaseDescriptor . getComparator ( keyspace , cfName ) ; <nl> <nl> private void maybeInit ( ) <nl> { <nl> + / / check if we need another batch <nl> + if ( rows ! = null & & i > = rows . size ( ) ) <nl> + rows = null ; <nl> + <nl> if ( rows ! = null ) <nl> return ; <nl> TSocket socket = new TSocket ( getLocation ( ) , <nl> @ @ - 115 , 8 + 123 , 19 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt <nl> { <nl> throw new RuntimeException ( e ) ; <nl> } <nl> - KeyRange keyRange = new KeyRange ( rowCount ) <nl> - . setStart _ token ( split . getStartToken ( ) ) <nl> + <nl> + if ( startToken = = null ) <nl> + { <nl> + startToken = split . getStartToken ( ) ; <nl> + } <nl> + else if ( startToken . equals ( split . getEndToken ( ) ) ) <nl> + { <nl> + rows = null ; <nl> + return ; <nl> + } <nl> + <nl> + KeyRange keyRange = new KeyRange ( batchRowCount ) <nl> + . setStart _ token ( startToken ) <nl> . setEnd _ token ( split . getEndToken ( ) ) ; <nl> try <nl> { <nl> @ @ - 125 , 6 + 144 , 21 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt <nl> predicate , <nl> keyRange , <nl> ConsistencyLevel . ONE ) ; <nl> + <nl> + / / nothing new ? reached the end <nl> + if ( rows . isEmpty ( ) ) <nl> + { <nl> + rows = null ; <nl> + return ; <nl> + } <nl> + <nl> + / / reset to iterate through this new batch <nl> + i = 0 ; <nl> + <nl> + / / prepare for the next slice to be read <nl> + KeySlice lastRow = rows . get ( rows . size ( ) - 1 ) ; <nl> + IPartitioner p = DatabaseDescriptor . getPartitioner ( ) ; <nl> + startToken = p . getTokenFactory ( ) . toString ( p . getToken ( lastRow . getKey ( ) ) ) ; <nl> } <nl> catch ( Exception e ) <nl> { <nl> @ @ - 167 , 23 + 201 , 22 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt <nl> return split . getLocations ( ) [ 0 ] ; <nl> } <nl> <nl> - public int size ( ) <nl> - { <nl> - maybeInit ( ) ; <nl> - return rows . size ( ) ; <nl> - } <nl> - <nl> + / * * <nl> + * @ return total number of rows read by this record reader <nl> + * / <nl> public int rowsRead ( ) <nl> { <nl> - return i ; <nl> + return totalRead ; <nl> } <nl> <nl> @ Override <nl> protected Pair < String , SortedMap < byte [ ] , IColumn > > computeNext ( ) <nl> { <nl> maybeInit ( ) ; <nl> - if ( i = = rows . size ( ) ) <nl> + if ( rows = = null ) <nl> return endOfData ( ) ; <nl> + <nl> + totalRead + + ; <nl> KeySlice ks = rows . get ( i + + ) ; <nl> SortedMap < byte [ ] , IColumn > map = new TreeMap < byte [ ] , IColumn > ( comparator ) ; <nl> for ( ColumnOrSuperColumn cosc : ks . columns ) <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ConfigHelper . java b / src / java / org / apache / cassandra / hadoop / ConfigHelper . java <nl> index 2afe8ae . . b6d40cf 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ConfigHelper . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ConfigHelper . java <nl> @ @ - 4 , 7 + 4 , 6 @ @ import org . apache . cassandra . thrift . InvalidRequestException ; <nl> import org . apache . cassandra . thrift . SlicePredicate ; <nl> import org . apache . cassandra . thrift . ThriftValidation ; <nl> import org . apache . hadoop . conf . Configuration ; <nl> - import org . apache . hadoop . mapreduce . Job ; <nl> import org . apache . thrift . TDeserializer ; <nl> import org . apache . thrift . TException ; <nl> import org . apache . thrift . TSerializer ; <nl> @ @ - 16 , 7 + 15 , 9 @ @ public class ConfigHelper <nl> private static final String COLUMNFAMILY _ CONFIG = " cassandra . input . columnfamily " ; <nl> private static final String PREDICATE _ CONFIG = " cassandra . input . predicate " ; <nl> private static final String INPUT _ SPLIT _ SIZE _ CONFIG = " cassandra . input . split . size " ; <nl> - private static final int DEFAULT _ SPLIT _ SIZE = 4096 ; <nl> + private static final int DEFAULT _ SPLIT _ SIZE = 64 * 1024 ; <nl> + private static final String RANGE _ BATCH _ SIZE _ CONFIG = " cassandra . range . batch . size " ; <nl> + private static final int DEFAULT _ RANGE _ BATCH _ SIZE = 4096 ; <nl> <nl> / * * <nl> * Set the keyspace and column family for this job . <nl> @ @ - 48 , 6 + 49 , 34 @ @ public class ConfigHelper <nl> } <nl> <nl> / * * <nl> + * The number of rows to request with each get range slices request . <nl> + * Too big and you can either get timeouts when it takes Cassandra too <nl> + * long to fetch all the data . Too small and the performance <nl> + * will be eaten up by the overhead of each request . <nl> + * <nl> + * @ param conf Job configuration you are about to run <nl> + * @ param batchsize Number of rows to request each time <nl> + * / <nl> + public static void setRangeBatchSize ( Configuration conf , int batchsize ) <nl> + { <nl> + conf . setInt ( RANGE _ BATCH _ SIZE _ CONFIG , batchsize ) ; <nl> + } <nl> + <nl> + / * * <nl> + * The number of rows to request with each get range slices request . <nl> + * Too big and you can either get timeouts when it takes Cassandra too <nl> + * long to fetch all the data . Too small and the performance <nl> + * will be eaten up by the overhead of each request . <nl> + * <nl> + * @ param conf Job configuration you are about to run <nl> + * @ return Number of rows to request each time <nl> + * / <nl> + public static int getRangeBatchSize ( Configuration conf ) <nl> + { <nl> + return conf . getInt ( RANGE _ BATCH _ SIZE _ CONFIG , DEFAULT _ RANGE _ BATCH _ SIZE ) ; <nl> + } <nl> + <nl> + / * * <nl> * Set the size of the input split . <nl> * This affects the number of maps created , if the number is too small <nl> * the overhead of each map will take up the bulk of the job time .

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index cd6b92e . . 30a76a9 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 2 . 2 . 5 
 + * Fix regression in split size on CqlInputFormat ( CASSANDRA - 10835 ) 
 * Better handling of SSL connection errors inter - node ( CASSANDRA - 10816 ) 
 * Disable reloading of GossipingPropertyFileSnitch ( CASSANDRA - 9474 ) 
 * Verify tables in pseudo - system keyspaces at startup ( CASSANDRA - 10761 ) 
 diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 index 3c088c2 . . d55f205 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 @ @ - 18 , 8 + 18 , 19 @ @ 
 package org . apache . cassandra . hadoop ; 
 
 import java . io . IOException ; 
 - import java . util . * ; 
 - import java . util . concurrent . * ; 
 + import java . util . ArrayList ; 
 + import java . util . Collections ; 
 + import java . util . HashMap ; 
 + import java . util . List ; 
 + import java . util . Map ; 
 + import java . util . Random ; 
 + import java . util . Set ; 
 + import java . util . concurrent . Callable ; 
 + import java . util . concurrent . ExecutorService ; 
 + import java . util . concurrent . Future ; 
 + import java . util . concurrent . LinkedBlockingQueue ; 
 + import java . util . concurrent . ThreadPoolExecutor ; 
 + import java . util . concurrent . TimeUnit ; 
 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 @ @ - 31 , 18 + 42 , 19 @ @ import com . datastax . driver . core . ResultSet ; 
 import com . datastax . driver . core . Row ; 
 import com . datastax . driver . core . Session ; 
 import com . datastax . driver . core . TokenRange ; 
 - 
 import org . apache . cassandra . db . SystemKeyspace ; 
 - import org . apache . cassandra . dht . ByteOrderedPartitioner ; 
 import org . apache . cassandra . dht . IPartitioner ; 
 - import org . apache . cassandra . dht . OrderPreservingPartitioner ; 
 import org . apache . cassandra . dht . Range ; 
 import org . apache . cassandra . dht . Token ; 
 - import org . apache . cassandra . hadoop . cql3 . * ; 
 + import org . apache . cassandra . hadoop . cql3 . CqlConfigHelper ; 
 import org . apache . cassandra . thrift . KeyRange ; 
 import org . apache . hadoop . conf . Configuration ; 
 import org . apache . hadoop . mapred . JobConf ; 
 - import org . apache . hadoop . mapreduce . * ; 
 + import org . apache . hadoop . mapreduce . InputFormat ; 
 + import org . apache . hadoop . mapreduce . InputSplit ; 
 + import org . apache . hadoop . mapreduce . JobContext ; 
 + import org . apache . hadoop . mapreduce . TaskAttemptContext ; 
 + import org . apache . hadoop . mapreduce . TaskAttemptID ; 
 
 public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < K , Y > implements org . apache . hadoop . mapred . InputFormat < K , Y > 
 { 
 @ @ - 230 , 9 + 242 , 10 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < 
 private Map < TokenRange , Long > getSubSplits ( String keyspace , String cfName , TokenRange range , Configuration conf , Session session ) throws IOException 
 { 
 int splitSize = ConfigHelper . getInputSplitSize ( conf ) ; 
 + int splitSizeMb = ConfigHelper . getInputSplitSizeInMb ( conf ) ; 
 try 
 { 
 - return describeSplits ( keyspace , cfName , range , splitSize , session ) ; 
 + return describeSplits ( keyspace , cfName , range , splitSize , splitSizeMb , session ) ; 
 } 
 catch ( Exception e ) 
 { 
 @ @ - 252 , 7 + 265 , 7 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < 
 } 
 } 
 
 - private Map < TokenRange , Long > describeSplits ( String keyspace , String table , TokenRange tokenRange , int splitSize , Session session ) 
 + private Map < TokenRange , Long > describeSplits ( String keyspace , String table , TokenRange tokenRange , int splitSize , int splitSizeMb , Session session ) 
 { 
 String query = String . format ( " SELECT mean _ partition _ size , partitions _ count " + 
 " FROM % s . % s " + 
 @ @ - 275 , 7 + 288 , 10 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < 
 long meanPartitionSize = row . getLong ( " mean _ partition _ size " ) ; 
 long partitionCount = row . getLong ( " partitions _ count " ) ; 
 
 - int splitCount = ( int ) ( ( meanPartitionSize * partitionCount ) / splitSize ) ; 
 + int splitCount = splitSizeMb > 0 
 + ? ( int ) ( meanPartitionSize * partitionCount / splitSizeMb / 1024 / 1024 ) 
 + : ( int ) ( partitionCount / splitSize ) ; 
 + 
 if ( splitCount < = 0 ) splitCount = 1 ; 
 List < TokenRange > splitRanges = tokenRange . splitEvenly ( splitCount ) ; 
 Map < TokenRange , Long > rangesWithLength = new HashMap < > ( ) ; 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ConfigHelper . java b / src / java / org / apache / cassandra / hadoop / ConfigHelper . java 
 index e81860d . . 376c250 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ConfigHelper . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ConfigHelper . java 
 @ @ - 54 , 6 + 54 , 7 @ @ public class ConfigHelper 
 private static final String INPUT _ PREDICATE _ CONFIG = " cassandra . input . predicate " ; 
 private static final String INPUT _ KEYRANGE _ CONFIG = " cassandra . input . keyRange " ; 
 private static final String INPUT _ SPLIT _ SIZE _ CONFIG = " cassandra . input . split . size " ; 
 + private static final String INPUT _ SPLIT _ SIZE _ IN _ MB _ CONFIG = " cassandra . input . split . size _ mb " ; 
 private static final String INPUT _ WIDEROWS _ CONFIG = " cassandra . input . widerows " ; 
 private static final int DEFAULT _ SPLIT _ SIZE = 64 * 1024 ; 
 private static final String RANGE _ BATCH _ SIZE _ CONFIG = " cassandra . range . batch . size " ; 
 @ @ - 176 , 7 + 177 , 7 @ @ public class ConfigHelper 
 * the overhead of each map will take up the bulk of the job time . 
 * 
 * @ param conf Job configuration you are about to run 
 - * @ param splitsize Size of the input split 
 + * @ param splitsize Number of partitions in the input split 
 * / 
 public static void setInputSplitSize ( Configuration conf , int splitsize ) 
 { 
 @ @ - 189 , 6 + 190 , 29 @ @ public class ConfigHelper 
 } 
 
 / * * 
 + * Set the size of the input split . getInputSplitSize value is used if this is not set . 
 + * This affects the number of maps created , if the number is too small 
 + * the overhead of each map will take up the bulk of the job time . 
 + * 
 + * @ param conf Job configuration you are about to run 
 + * @ param splitSizeMb Input split size in MB 
 + * / 
 + public static void setInputSplitSizeInMb ( Configuration conf , int splitSizeMb ) 
 + { 
 + conf . setInt ( INPUT _ SPLIT _ SIZE _ IN _ MB _ CONFIG , splitSizeMb ) ; 
 + } 
 + 
 + / * * 
 + * cassandra . input . split . size will be used if the value is undefined or negative . 
 + * @ param conf Job configuration you are about to run 
 + * @ return split size in MB or - 1 if it is undefined . 
 + * / 
 + public static int getInputSplitSizeInMb ( Configuration conf ) 
 + { 
 + return conf . getInt ( INPUT _ SPLIT _ SIZE _ IN _ MB _ CONFIG , - 1 ) ; 
 + } 
 + 
 + / * * 
 * Set the predicate that determines what columns will be selected from each row . 
 * 
 * @ param conf Job configuration you are about to run 
 diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlInputFormat . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlInputFormat . java 
 index 36da92d . . c46ceb8 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlInputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlInputFormat . java 
 @ @ - 40 , 10 + 40 , 12 @ @ import com . datastax . driver . core . Row ; 
 * ConfigHelper . setInputColumnFamily 
 * 
 * You can also configure the number of rows per InputSplit with 
 - * ConfigHelper . setInputSplitSize . The default split size is 64k rows . 
 + * 1 : ConfigHelper . setInputSplitSize . The default split size is 64k rows . 
 + * or 
 + * 2 : ConfigHelper . setInputSplitSizeInMb . InputSplit size in MB with new , more precise method 
 + * If no value is provided for InputSplitSizeInMb , InputSplitSize will be used . 
 * 
 - * the number of CQL rows per page 
 - * CQLConfigHelper . setInputCQLPageRowSize . The default page row size is 1000 . You 
 + * CQLConfigHelper . setInputCQLPageRowSize . The default page row size is 1000 . You 
 * should set it to " as big as possible , but no bigger . " It set the LIMIT for the CQL 
 * query , so you need set it big enough to minimize the network overhead , and also 
 * not too big to avoid out of memory issue .

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index a565ea2 . . 7499f3f 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 4 , 7 + 4 , 7 @ @ dev 
 * switched to slf4j logging ( CASSANDRA - 625 ) 
 
 
 - 0 . 6 . 0 - RC1 
 + 0 . 6 . 0 - beta3 
 * fix compaction bucketing bug ( CASSANDRA - 814 ) 
 * update windows batch file ( CASSANDRA - 824 ) 
 * deprecate KeysCachedFraction configuration directive in favor 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java 
 index 578a5cb . . 2a64e1a 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java 
 @ @ - 50 , 11 + 50 , 10 @ @ import org . apache . thrift . transport . TTransportException ; 
 * You can also configure the number of rows per InputSplit with 
 * ConfigHelper . setInputSplitSize 
 * This should be " as big as possible , but no bigger . " Each InputSplit is read from Cassandra 
 - * with a single get _ slice _ range query , and the per - call overhead of get _ slice _ range is high , 
 - * so larger split sizes are better - - but if it is too large , you will run out of memory , 
 - * since no paging is done ( yet ) . 
 + * with multiple get _ slice _ range queries , and the per - call overhead of get _ slice _ range is high , 
 + * so larger split sizes are better - - but if it is too large , you will run out of memory . 
 * 
 - * The default split size is 4096 rows . 
 + * The default split size is 64k rows . 
 * / 
 public class ColumnFamilyInputFormat extends InputFormat < String , SortedMap < byte [ ] , IColumn > > 
 { 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java 
 index 4ff8809 . . c18b17c 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java 
 @ @ - 28 , 12 + 28 , 11 @ @ import java . util . List ; 
 import java . util . SortedMap ; 
 import java . util . TreeMap ; 
 
 - import org . apache . commons . lang . ArrayUtils ; 
 - 
 import com . google . common . collect . AbstractIterator ; 
 import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . db . * ; 
 import org . apache . cassandra . db . marshal . AbstractType ; 
 + import org . apache . cassandra . dht . IPartitioner ; 
 import org . apache . cassandra . thrift . * ; 
 import org . apache . cassandra . thrift . Column ; 
 import org . apache . cassandra . thrift . SuperColumn ; 
 @ @ - 52 , 7 + 51 , 8 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt 
 private RowIterator iter ; 
 private Pair < String , SortedMap < byte [ ] , IColumn > > currentRow ; 
 private SlicePredicate predicate ; 
 - private int rowCount ; 
 + private int totalRowCount ; / / total number of rows to fetch 
 + private int batchRowCount ; / / fetch this many per batch 
 private String cfName ; 
 private String keyspace ; 
 
 @ @ - 70 , 7 + 70 , 8 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt 
 
 public float getProgress ( ) 
 { 
 - return ( ( float ) iter . rowsRead ( ) ) / iter . size ( ) ; 
 + / / the progress is likely to be reported slightly off the actual but close enough 
 + return ( ( float ) iter . rowsRead ( ) ) / totalRowCount ; 
 } 
 
 public void initialize ( InputSplit split , TaskAttemptContext context ) throws IOException 
 @ @ - 78 , 7 + 79 , 8 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt 
 this . split = ( ColumnFamilySplit ) split ; 
 Configuration conf = context . getConfiguration ( ) ; 
 predicate = ConfigHelper . getSlicePredicate ( conf ) ; 
 - rowCount = ConfigHelper . getInputSplitSize ( conf ) ; 
 + totalRowCount = ConfigHelper . getInputSplitSize ( conf ) ; 
 + batchRowCount = ConfigHelper . getRangeBatchSize ( conf ) ; 
 cfName = ConfigHelper . getColumnFamily ( conf ) ; 
 keyspace = ConfigHelper . getKeyspace ( conf ) ; 
 iter = new RowIterator ( ) ; 
 @ @ - 96 , 11 + 98 , 17 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt 
 { 
 
 private List < KeySlice > rows ; 
 + private String startToken ; 
 + private int totalRead = 0 ; 
 private int i = 0 ; 
 private AbstractType comparator = DatabaseDescriptor . getComparator ( keyspace , cfName ) ; 
 
 private void maybeInit ( ) 
 { 
 + / / check if we need another batch 
 + if ( rows ! = null & & i > = rows . size ( ) ) 
 + rows = null ; 
 + 
 if ( rows ! = null ) 
 return ; 
 TSocket socket = new TSocket ( getLocation ( ) , 
 @ @ - 115 , 8 + 123 , 19 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt 
 { 
 throw new RuntimeException ( e ) ; 
 } 
 - KeyRange keyRange = new KeyRange ( rowCount ) 
 - . setStart _ token ( split . getStartToken ( ) ) 
 + 
 + if ( startToken = = null ) 
 + { 
 + startToken = split . getStartToken ( ) ; 
 + } 
 + else if ( startToken . equals ( split . getEndToken ( ) ) ) 
 + { 
 + rows = null ; 
 + return ; 
 + } 
 + 
 + KeyRange keyRange = new KeyRange ( batchRowCount ) 
 + . setStart _ token ( startToken ) 
 . setEnd _ token ( split . getEndToken ( ) ) ; 
 try 
 { 
 @ @ - 125 , 6 + 144 , 21 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt 
 predicate , 
 keyRange , 
 ConsistencyLevel . ONE ) ; 
 + 
 + / / nothing new ? reached the end 
 + if ( rows . isEmpty ( ) ) 
 + { 
 + rows = null ; 
 + return ; 
 + } 
 + 
 + / / reset to iterate through this new batch 
 + i = 0 ; 
 + 
 + / / prepare for the next slice to be read 
 + KeySlice lastRow = rows . get ( rows . size ( ) - 1 ) ; 
 + IPartitioner p = DatabaseDescriptor . getPartitioner ( ) ; 
 + startToken = p . getTokenFactory ( ) . toString ( p . getToken ( lastRow . getKey ( ) ) ) ; 
 } 
 catch ( Exception e ) 
 { 
 @ @ - 167 , 23 + 201 , 22 @ @ public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byt 
 return split . getLocations ( ) [ 0 ] ; 
 } 
 
 - public int size ( ) 
 - { 
 - maybeInit ( ) ; 
 - return rows . size ( ) ; 
 - } 
 - 
 + / * * 
 + * @ return total number of rows read by this record reader 
 + * / 
 public int rowsRead ( ) 
 { 
 - return i ; 
 + return totalRead ; 
 } 
 
 @ Override 
 protected Pair < String , SortedMap < byte [ ] , IColumn > > computeNext ( ) 
 { 
 maybeInit ( ) ; 
 - if ( i = = rows . size ( ) ) 
 + if ( rows = = null ) 
 return endOfData ( ) ; 
 + 
 + totalRead + + ; 
 KeySlice ks = rows . get ( i + + ) ; 
 SortedMap < byte [ ] , IColumn > map = new TreeMap < byte [ ] , IColumn > ( comparator ) ; 
 for ( ColumnOrSuperColumn cosc : ks . columns ) 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ConfigHelper . java b / src / java / org / apache / cassandra / hadoop / ConfigHelper . java 
 index 2afe8ae . . b6d40cf 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ConfigHelper . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ConfigHelper . java 
 @ @ - 4 , 7 + 4 , 6 @ @ import org . apache . cassandra . thrift . InvalidRequestException ; 
 import org . apache . cassandra . thrift . SlicePredicate ; 
 import org . apache . cassandra . thrift . ThriftValidation ; 
 import org . apache . hadoop . conf . Configuration ; 
 - import org . apache . hadoop . mapreduce . Job ; 
 import org . apache . thrift . TDeserializer ; 
 import org . apache . thrift . TException ; 
 import org . apache . thrift . TSerializer ; 
 @ @ - 16 , 7 + 15 , 9 @ @ public class ConfigHelper 
 private static final String COLUMNFAMILY _ CONFIG = " cassandra . input . columnfamily " ; 
 private static final String PREDICATE _ CONFIG = " cassandra . input . predicate " ; 
 private static final String INPUT _ SPLIT _ SIZE _ CONFIG = " cassandra . input . split . size " ; 
 - private static final int DEFAULT _ SPLIT _ SIZE = 4096 ; 
 + private static final int DEFAULT _ SPLIT _ SIZE = 64 * 1024 ; 
 + private static final String RANGE _ BATCH _ SIZE _ CONFIG = " cassandra . range . batch . size " ; 
 + private static final int DEFAULT _ RANGE _ BATCH _ SIZE = 4096 ; 
 
 / * * 
 * Set the keyspace and column family for this job . 
 @ @ - 48 , 6 + 49 , 34 @ @ public class ConfigHelper 
 } 
 
 / * * 
 + * The number of rows to request with each get range slices request . 
 + * Too big and you can either get timeouts when it takes Cassandra too 
 + * long to fetch all the data . Too small and the performance 
 + * will be eaten up by the overhead of each request . 
 + * 
 + * @ param conf Job configuration you are about to run 
 + * @ param batchsize Number of rows to request each time 
 + * / 
 + public static void setRangeBatchSize ( Configuration conf , int batchsize ) 
 + { 
 + conf . setInt ( RANGE _ BATCH _ SIZE _ CONFIG , batchsize ) ; 
 + } 
 + 
 + / * * 
 + * The number of rows to request with each get range slices request . 
 + * Too big and you can either get timeouts when it takes Cassandra too 
 + * long to fetch all the data . Too small and the performance 
 + * will be eaten up by the overhead of each request . 
 + * 
 + * @ param conf Job configuration you are about to run 
 + * @ return Number of rows to request each time 
 + * / 
 + public static int getRangeBatchSize ( Configuration conf ) 
 + { 
 + return conf . getInt ( RANGE _ BATCH _ SIZE _ CONFIG , DEFAULT _ RANGE _ BATCH _ SIZE ) ; 
 + } 
 + 
 + / * * 
 * Set the size of the input split . 
 * This affects the number of maps created , if the number is too small 
 * the overhead of each map will take up the bulk of the job time .
