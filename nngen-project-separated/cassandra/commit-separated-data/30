BLEU SCORE: 0.03214954573057458

TEST MSG: Special case page handling for DISTINCT queries
GENERATED MSG: Workaround for netty issue causing corrupted data to come off the wire

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 7b88686 . . 3582d4f 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 2 . 1 . 21 <nl> + * Paged Range Slice queries with DISTINCT can drop rows from results ( CASSANDRA - 14956 ) <nl> * Update release checksum algorithms to SHA - 256 , SHA - 512 ( CASSANDRA - 14970 ) <nl> * Check checksum before decompressing data ( CASSANDRA - 14284 ) <nl> * CVE - 2017 - 5929 Security vulnerability in Logback warning in NEWS . txt ( CASSANDRA - 14183 ) <nl> diff - - git a / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java b / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java <nl> index 8bbf6d6 . . 445a507 100644 <nl> - - - a / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java <nl> + + + b / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java <nl> @ @ - 207 , 7 + 207 , 7 @ @ abstract class AbstractQueryPager implements QueryPager <nl> <nl> protected abstract boolean isReversed ( ) ; <nl> <nl> - private List < Row > discardFirst ( List < Row > rows ) <nl> + protected List < Row > discardFirst ( List < Row > rows ) <nl> { <nl> return discardFirst ( rows , 1 ) ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / service / pager / RangeSliceQueryPager . java b / src / java / org / apache / cassandra / service / pager / RangeSliceQueryPager . java <nl> index 3ac777e . . a37db02 100644 <nl> - - - a / src / java / org / apache / cassandra / service / pager / RangeSliceQueryPager . java <nl> + + + b / src / java / org / apache / cassandra / service / pager / RangeSliceQueryPager . java <nl> @ @ - 17 , 6 + 17 , 7 @ @ <nl> * / <nl> package org . apache . cassandra . service . pager ; <nl> <nl> + import java . util . ArrayList ; <nl> import java . util . List ; <nl> <nl> import org . apache . cassandra . config . CFMetaData ; <nl> @ @ - 114 , 6 + 115 , 28 @ @ public class RangeSliceQueryPager extends AbstractQueryPager <nl> & & firstCell . name ( ) . isSameCQL3RowAs ( metadata . comparator , lastReturnedName ) ; <nl> } <nl> <nl> + protected List < Row > discardFirst ( List < Row > rows ) <nl> + { <nl> + if ( rows . isEmpty ( ) ) <nl> + return rows ; <nl> + <nl> + / / Special case for distinct queries because the superclass ' discardFirst keeps dropping cells <nl> + / / until it has removed the first * live * row . In a distinct query we only fetch the first row <nl> + / / from a given partition , which may be entirely non - live . In the case where such a non - live <nl> + / / row is the last in page N & the first in page N + 1 , we would also end up discarding an <nl> + / / additional live row from page N + 1 . <nl> + / / The simplest solution is to just remove whichever row is first in the page , without bothering <nl> + / / to do liveness checks etc . <nl> + if ( isDistinct ( ) ) <nl> + { <nl> + List < Row > newRows = new ArrayList < > ( Math . max ( 1 , rows . size ( ) - 1 ) ) ; <nl> + newRows . addAll ( rows . subList ( 1 , rows . size ( ) ) ) ; <nl> + return newRows ; <nl> + } <nl> + <nl> + return super . discardFirst ( rows ) ; <nl> + } <nl> + <nl> private boolean isDistinct ( ) <nl> { <nl> / / As this pager is never used for Thrift queries , checking the countCQL3Rows is enough . <nl> diff - - git a / test / unit / org / apache / cassandra / cql3 / PagingTest . java b / test / unit / org / apache / cassandra / cql3 / PagingTest . java <nl> new file mode 100644 <nl> index 0000000 . . 531ddde <nl> - - - / dev / null <nl> + + + b / test / unit / org / apache / cassandra / cql3 / PagingTest . java <nl> @ @ - 0 , 0 + 1 , 160 @ @ <nl> + / * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , software <nl> + * distributed under the License is distributed on an " AS IS " BASIS , <nl> + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> + * See the License for the specific language governing permissions and <nl> + * limitations under the License . <nl> + * / <nl> + package org . apache . cassandra . cql3 ; <nl> + <nl> + import java . net . InetAddress ; <nl> + import java . util . Iterator ; <nl> + import java . util . List ; <nl> + <nl> + import org . junit . AfterClass ; <nl> + import org . junit . BeforeClass ; <nl> + import org . junit . Test ; <nl> + <nl> + import com . datastax . driver . core . Cluster ; <nl> + import com . datastax . driver . core . ResultSet ; <nl> + import com . datastax . driver . core . Row ; <nl> + import com . datastax . driver . core . Session ; <nl> + import com . datastax . driver . core . SimpleStatement ; <nl> + import com . datastax . driver . core . Statement ; <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + <nl> + import org . apache . cassandra . dht . LongToken ; <nl> + import org . apache . cassandra . dht . Murmur3Partitioner ; <nl> + import org . apache . cassandra . locator . AbstractEndpointSnitch ; <nl> + import org . apache . cassandra . locator . IEndpointSnitch ; <nl> + import org . apache . cassandra . service . EmbeddedCassandraService ; <nl> + import org . apache . cassandra . service . StorageService ; <nl> + import org . apache . cassandra . utils . FBUtilities ; <nl> + <nl> + import static junit . framework . Assert . assertFalse ; <nl> + import static org . junit . Assert . assertEquals ; <nl> + <nl> + <nl> + public class PagingTest <nl> + { <nl> + private static Cluster cluster ; <nl> + private static Session session ; <nl> + <nl> + private static final String KEYSPACE = " paging _ test " ; <nl> + private static final String createKsStatement = " CREATE KEYSPACE " + KEYSPACE + <nl> + " WITH REPLICATION = { ' class ' : ' SimpleStrategy ' , ' replication _ factor ' : 2 } ; " ; <nl> + <nl> + private static final String dropKsStatement = " DROP KEYSPACE IF EXISTS " + KEYSPACE ; <nl> + <nl> + @ BeforeClass <nl> + public static void setup ( ) throws Exception <nl> + { <nl> + DatabaseDescriptor . setPartitioner ( new Murmur3Partitioner ( ) ) ; <nl> + EmbeddedCassandraService cassandra = new EmbeddedCassandraService ( ) ; <nl> + cassandra . start ( ) ; <nl> + <nl> + / / Currently the native server start method return before the server is fully binded to the socket , so we need <nl> + / / to wait slightly before trying to connect to it . We should fix this but in the meantime using a sleep . <nl> + Thread . sleep ( 500 ) ; <nl> + <nl> + cluster = Cluster . builder ( ) . addContactPoint ( " 127 . 0 . 0 . 1 " ) <nl> + . withPort ( DatabaseDescriptor . getNativeTransportPort ( ) ) <nl> + . build ( ) ; <nl> + session = cluster . connect ( ) ; <nl> + <nl> + session . execute ( dropKsStatement ) ; <nl> + session . execute ( createKsStatement ) ; <nl> + } <nl> + <nl> + @ AfterClass <nl> + public static void tearDown ( ) <nl> + { <nl> + cluster . close ( ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Makes sure that we don ' t drop any live rows when paging with DISTINCT queries <nl> + * <nl> + * * We need to have more rows than fetch _ size <nl> + * * The node must have a token within the first page ( so that the range gets split up in StorageProxy # getRestrictedRanges ) <nl> + * - This means that the second read in the second range will read back too many rows <nl> + * * The extra rows are dropped ( so that we only return fetch _ size rows to client ) <nl> + * * This means that the last row recorded in AbstractQueryPager # recordLast is a non - live one <nl> + * * For the next page , the first row returned will be the same non - live row as above <nl> + * * The bug in CASSANDRA - 14956 caused us to drop that non - live row + the first live row in the next page <nl> + * / <nl> + @ Test <nl> + public void testPaging ( ) throws InterruptedException <nl> + { <nl> + String table = KEYSPACE + " . paging " ; <nl> + String createTableStatement = " CREATE TABLE IF NOT EXISTS " + table + " ( id int , id2 int , id3 int , val text , PRIMARY KEY ( ( id , id2 ) , id3 ) ) ; " ; <nl> + String dropTableStatement = " DROP TABLE IF EXISTS " + table + ' ; ' ; <nl> + <nl> + / / custom snitch to avoid merging ranges back together after StorageProxy # getRestrictedRanges splits them up <nl> + IEndpointSnitch snitch = new AbstractEndpointSnitch ( ) <nl> + { <nl> + private IEndpointSnitch oldSnitch = DatabaseDescriptor . getEndpointSnitch ( ) ; <nl> + public int compareEndpoints ( InetAddress target , InetAddress a1 , InetAddress a2 ) <nl> + { <nl> + return oldSnitch . compareEndpoints ( target , a1 , a2 ) ; <nl> + } <nl> + <nl> + public String getRack ( InetAddress endpoint ) <nl> + { <nl> + return oldSnitch . getRack ( endpoint ) ; <nl> + } <nl> + <nl> + public String getDatacenter ( InetAddress endpoint ) <nl> + { <nl> + return oldSnitch . getDatacenter ( endpoint ) ; <nl> + } <nl> + <nl> + @ Override <nl> + public boolean isWorthMergingForRangeQuery ( List < InetAddress > merged , List < InetAddress > l1 , List < InetAddress > l2 ) <nl> + { <nl> + return false ; <nl> + } <nl> + } ; <nl> + DatabaseDescriptor . setEndpointSnitch ( snitch ) ; <nl> + StorageService . instance . getTokenMetadata ( ) . clearUnsafe ( ) ; <nl> + StorageService . instance . getTokenMetadata ( ) . updateNormalToken ( new LongToken ( 5097162189738624638L ) , FBUtilities . getBroadcastAddress ( ) ) ; <nl> + session . execute ( createTableStatement ) ; <nl> + <nl> + for ( int i = 0 ; i < 110 ; i + + ) <nl> + { <nl> + / / removing row with idx 10 causes the last row in the first page read to be empty <nl> + String ttlClause = i = = 10 ? " USING TTL 1 " : " " ; <nl> + session . execute ( String . format ( " INSERT INTO % s ( id , id2 , id3 , val ) VALUES ( % d , % d , % d , ' % d ' ) % s " , table , i , i , i , i , ttlClause ) ) ; <nl> + } <nl> + Thread . sleep ( 1500 ) ; <nl> + <nl> + Statement stmt = new SimpleStatement ( String . format ( " SELECT DISTINCT token ( id , id2 ) , id , id2 FROM % s " , table ) ) ; <nl> + stmt . setFetchSize ( 100 ) ; <nl> + ResultSet res = session . execute ( stmt ) ; <nl> + stmt . setFetchSize ( 200 ) ; <nl> + ResultSet res2 = session . execute ( stmt ) ; <nl> + <nl> + Iterator < Row > iter1 = res . iterator ( ) ; <nl> + Iterator < Row > iter2 = res2 . iterator ( ) ; <nl> + <nl> + while ( iter1 . hasNext ( ) & & iter2 . hasNext ( ) ) <nl> + { <nl> + Row row1 = iter1 . next ( ) ; <nl> + Row row2 = iter2 . next ( ) ; <nl> + assertEquals ( row1 . getInt ( " id " ) , row2 . getInt ( " id " ) ) ; <nl> + } <nl> + assertFalse ( iter1 . hasNext ( ) ) ; <nl> + assertFalse ( iter2 . hasNext ( ) ) ; <nl> + session . execute ( dropTableStatement ) ; <nl> + } <nl> + }
NEAREST DIFF (one line): ELIMINATEDSENTENCE

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 7b88686 . . 3582d4f 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 2 . 1 . 21 
 + * Paged Range Slice queries with DISTINCT can drop rows from results ( CASSANDRA - 14956 ) 
 * Update release checksum algorithms to SHA - 256 , SHA - 512 ( CASSANDRA - 14970 ) 
 * Check checksum before decompressing data ( CASSANDRA - 14284 ) 
 * CVE - 2017 - 5929 Security vulnerability in Logback warning in NEWS . txt ( CASSANDRA - 14183 ) 
 diff - - git a / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java b / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java 
 index 8bbf6d6 . . 445a507 100644 
 - - - a / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java 
 + + + b / src / java / org / apache / cassandra / service / pager / AbstractQueryPager . java 
 @ @ - 207 , 7 + 207 , 7 @ @ abstract class AbstractQueryPager implements QueryPager 
 
 protected abstract boolean isReversed ( ) ; 
 
 - private List < Row > discardFirst ( List < Row > rows ) 
 + protected List < Row > discardFirst ( List < Row > rows ) 
 { 
 return discardFirst ( rows , 1 ) ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / service / pager / RangeSliceQueryPager . java b / src / java / org / apache / cassandra / service / pager / RangeSliceQueryPager . java 
 index 3ac777e . . a37db02 100644 
 - - - a / src / java / org / apache / cassandra / service / pager / RangeSliceQueryPager . java 
 + + + b / src / java / org / apache / cassandra / service / pager / RangeSliceQueryPager . java 
 @ @ - 17 , 6 + 17 , 7 @ @ 
 * / 
 package org . apache . cassandra . service . pager ; 
 
 + import java . util . ArrayList ; 
 import java . util . List ; 
 
 import org . apache . cassandra . config . CFMetaData ; 
 @ @ - 114 , 6 + 115 , 28 @ @ public class RangeSliceQueryPager extends AbstractQueryPager 
 & & firstCell . name ( ) . isSameCQL3RowAs ( metadata . comparator , lastReturnedName ) ; 
 } 
 
 + protected List < Row > discardFirst ( List < Row > rows ) 
 + { 
 + if ( rows . isEmpty ( ) ) 
 + return rows ; 
 + 
 + / / Special case for distinct queries because the superclass ' discardFirst keeps dropping cells 
 + / / until it has removed the first * live * row . In a distinct query we only fetch the first row 
 + / / from a given partition , which may be entirely non - live . In the case where such a non - live 
 + / / row is the last in page N & the first in page N + 1 , we would also end up discarding an 
 + / / additional live row from page N + 1 . 
 + / / The simplest solution is to just remove whichever row is first in the page , without bothering 
 + / / to do liveness checks etc . 
 + if ( isDistinct ( ) ) 
 + { 
 + List < Row > newRows = new ArrayList < > ( Math . max ( 1 , rows . size ( ) - 1 ) ) ; 
 + newRows . addAll ( rows . subList ( 1 , rows . size ( ) ) ) ; 
 + return newRows ; 
 + } 
 + 
 + return super . discardFirst ( rows ) ; 
 + } 
 + 
 private boolean isDistinct ( ) 
 { 
 / / As this pager is never used for Thrift queries , checking the countCQL3Rows is enough . 
 diff - - git a / test / unit / org / apache / cassandra / cql3 / PagingTest . java b / test / unit / org / apache / cassandra / cql3 / PagingTest . java 
 new file mode 100644 
 index 0000000 . . 531ddde 
 - - - / dev / null 
 + + + b / test / unit / org / apache / cassandra / cql3 / PagingTest . java 
 @ @ - 0 , 0 + 1 , 160 @ @ 
 + / * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , software 
 + * distributed under the License is distributed on an " AS IS " BASIS , 
 + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 + * See the License for the specific language governing permissions and 
 + * limitations under the License . 
 + * / 
 + package org . apache . cassandra . cql3 ; 
 + 
 + import java . net . InetAddress ; 
 + import java . util . Iterator ; 
 + import java . util . List ; 
 + 
 + import org . junit . AfterClass ; 
 + import org . junit . BeforeClass ; 
 + import org . junit . Test ; 
 + 
 + import com . datastax . driver . core . Cluster ; 
 + import com . datastax . driver . core . ResultSet ; 
 + import com . datastax . driver . core . Row ; 
 + import com . datastax . driver . core . Session ; 
 + import com . datastax . driver . core . SimpleStatement ; 
 + import com . datastax . driver . core . Statement ; 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 + 
 + import org . apache . cassandra . dht . LongToken ; 
 + import org . apache . cassandra . dht . Murmur3Partitioner ; 
 + import org . apache . cassandra . locator . AbstractEndpointSnitch ; 
 + import org . apache . cassandra . locator . IEndpointSnitch ; 
 + import org . apache . cassandra . service . EmbeddedCassandraService ; 
 + import org . apache . cassandra . service . StorageService ; 
 + import org . apache . cassandra . utils . FBUtilities ; 
 + 
 + import static junit . framework . Assert . assertFalse ; 
 + import static org . junit . Assert . assertEquals ; 
 + 
 + 
 + public class PagingTest 
 + { 
 + private static Cluster cluster ; 
 + private static Session session ; 
 + 
 + private static final String KEYSPACE = " paging _ test " ; 
 + private static final String createKsStatement = " CREATE KEYSPACE " + KEYSPACE + 
 + " WITH REPLICATION = { ' class ' : ' SimpleStrategy ' , ' replication _ factor ' : 2 } ; " ; 
 + 
 + private static final String dropKsStatement = " DROP KEYSPACE IF EXISTS " + KEYSPACE ; 
 + 
 + @ BeforeClass 
 + public static void setup ( ) throws Exception 
 + { 
 + DatabaseDescriptor . setPartitioner ( new Murmur3Partitioner ( ) ) ; 
 + EmbeddedCassandraService cassandra = new EmbeddedCassandraService ( ) ; 
 + cassandra . start ( ) ; 
 + 
 + / / Currently the native server start method return before the server is fully binded to the socket , so we need 
 + / / to wait slightly before trying to connect to it . We should fix this but in the meantime using a sleep . 
 + Thread . sleep ( 500 ) ; 
 + 
 + cluster = Cluster . builder ( ) . addContactPoint ( " 127 . 0 . 0 . 1 " ) 
 + . withPort ( DatabaseDescriptor . getNativeTransportPort ( ) ) 
 + . build ( ) ; 
 + session = cluster . connect ( ) ; 
 + 
 + session . execute ( dropKsStatement ) ; 
 + session . execute ( createKsStatement ) ; 
 + } 
 + 
 + @ AfterClass 
 + public static void tearDown ( ) 
 + { 
 + cluster . close ( ) ; 
 + } 
 + 
 + / * * 
 + * Makes sure that we don ' t drop any live rows when paging with DISTINCT queries 
 + * 
 + * * We need to have more rows than fetch _ size 
 + * * The node must have a token within the first page ( so that the range gets split up in StorageProxy # getRestrictedRanges ) 
 + * - This means that the second read in the second range will read back too many rows 
 + * * The extra rows are dropped ( so that we only return fetch _ size rows to client ) 
 + * * This means that the last row recorded in AbstractQueryPager # recordLast is a non - live one 
 + * * For the next page , the first row returned will be the same non - live row as above 
 + * * The bug in CASSANDRA - 14956 caused us to drop that non - live row + the first live row in the next page 
 + * / 
 + @ Test 
 + public void testPaging ( ) throws InterruptedException 
 + { 
 + String table = KEYSPACE + " . paging " ; 
 + String createTableStatement = " CREATE TABLE IF NOT EXISTS " + table + " ( id int , id2 int , id3 int , val text , PRIMARY KEY ( ( id , id2 ) , id3 ) ) ; " ; 
 + String dropTableStatement = " DROP TABLE IF EXISTS " + table + ' ; ' ; 
 + 
 + / / custom snitch to avoid merging ranges back together after StorageProxy # getRestrictedRanges splits them up 
 + IEndpointSnitch snitch = new AbstractEndpointSnitch ( ) 
 + { 
 + private IEndpointSnitch oldSnitch = DatabaseDescriptor . getEndpointSnitch ( ) ; 
 + public int compareEndpoints ( InetAddress target , InetAddress a1 , InetAddress a2 ) 
 + { 
 + return oldSnitch . compareEndpoints ( target , a1 , a2 ) ; 
 + } 
 + 
 + public String getRack ( InetAddress endpoint ) 
 + { 
 + return oldSnitch . getRack ( endpoint ) ; 
 + } 
 + 
 + public String getDatacenter ( InetAddress endpoint ) 
 + { 
 + return oldSnitch . getDatacenter ( endpoint ) ; 
 + } 
 + 
 + @ Override 
 + public boolean isWorthMergingForRangeQuery ( List < InetAddress > merged , List < InetAddress > l1 , List < InetAddress > l2 ) 
 + { 
 + return false ; 
 + } 
 + } ; 
 + DatabaseDescriptor . setEndpointSnitch ( snitch ) ; 
 + StorageService . instance . getTokenMetadata ( ) . clearUnsafe ( ) ; 
 + StorageService . instance . getTokenMetadata ( ) . updateNormalToken ( new LongToken ( 5097162189738624638L ) , FBUtilities . getBroadcastAddress ( ) ) ; 
 + session . execute ( createTableStatement ) ; 
 + 
 + for ( int i = 0 ; i < 110 ; i + + ) 
 + { 
 + / / removing row with idx 10 causes the last row in the first page read to be empty 
 + String ttlClause = i = = 10 ? " USING TTL 1 " : " " ; 
 + session . execute ( String . format ( " INSERT INTO % s ( id , id2 , id3 , val ) VALUES ( % d , % d , % d , ' % d ' ) % s " , table , i , i , i , i , ttlClause ) ) ; 
 + } 
 + Thread . sleep ( 1500 ) ; 
 + 
 + Statement stmt = new SimpleStatement ( String . format ( " SELECT DISTINCT token ( id , id2 ) , id , id2 FROM % s " , table ) ) ; 
 + stmt . setFetchSize ( 100 ) ; 
 + ResultSet res = session . execute ( stmt ) ; 
 + stmt . setFetchSize ( 200 ) ; 
 + ResultSet res2 = session . execute ( stmt ) ; 
 + 
 + Iterator < Row > iter1 = res . iterator ( ) ; 
 + Iterator < Row > iter2 = res2 . iterator ( ) ; 
 + 
 + while ( iter1 . hasNext ( ) & & iter2 . hasNext ( ) ) 
 + { 
 + Row row1 = iter1 . next ( ) ; 
 + Row row2 = iter2 . next ( ) ; 
 + assertEquals ( row1 . getInt ( " id " ) , row2 . getInt ( " id " ) ) ; 
 + } 
 + assertFalse ( iter1 . hasNext ( ) ) ; 
 + assertFalse ( iter2 . hasNext ( ) ) ; 
 + session . execute ( dropTableStatement ) ; 
 + } 
 + }

NEAREST DIFF:
ELIMINATEDSENTENCE
