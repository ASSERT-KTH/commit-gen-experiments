BLEU SCORE: 0.03283637368030199

TEST MSG: Make commitlog archiver thread pool name consistent
GENERATED MSG: CFS . valid defaults to true

TEST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogArchiver . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogArchiver . java <nl> index 1385ea4 . . 6161435 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogArchiver . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogArchiver . java <nl> @ @ - 51 , 7 + 51 , 7 @ @ public class CommitLogArchiver <nl> } <nl> <nl> public final Map < String , Future < ? > > archivePending = new ConcurrentHashMap < String , Future < ? > > ( ) ; <nl> - public final ExecutorService executor = new JMXEnabledThreadPoolExecutor ( " commitlog _ archiver " ) ; <nl> + public final ExecutorService executor = new JMXEnabledThreadPoolExecutor ( " CommitLogArchiver " ) ; <nl> private final String archiveCommand ; <nl> private final String restoreCommand ; <nl> private final String restoreDirectories ;
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index f0cffa5 . . 670a8a2 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 37 , 7 + 37 , 7 @ @ <nl> * CFMetaData . convertToThrift method to set RowCacheProvider ( CASSANDRA - 3405 ) <nl> * acquire compactionlock during truncate ( CASSANDRA - 3399 ) <nl> * fix displaying cfdef entries for super columnfamilies ( CASSANDRA - 3415 ) <nl> - <nl> + * ( Hadoop ) Fix empty row filtering ( CASSANDRA - 3450 ) <nl> <nl> 0 . 8 . 7 <nl> * Kill server on wrapped OOME such as from FileChannel . map ( CASSANDRA - 3201 ) <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java <nl> index 87408c6 . . 1c27638 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java <nl> @ @ - 235 , 50 + 235 , 55 @ @ public class ColumnFamilyRecordReader extends RecordReader < ByteBuffer , SortedMap <nl> { <nl> startToken = split . getStartToken ( ) ; <nl> } <nl> - else if ( startToken . equals ( split . getEndToken ( ) ) ) <nl> - { <nl> - rows = null ; <nl> - return ; <nl> - } <nl> - <nl> - KeyRange keyRange = new KeyRange ( batchRowCount ) <nl> - . setStart _ token ( startToken ) <nl> - . setEnd _ token ( split . getEndToken ( ) ) ; <nl> - try <nl> - { <nl> - rows = client . get _ range _ slices ( new ColumnParent ( cfName ) , <nl> - predicate , <nl> - keyRange , <nl> - consistencyLevel ) ; <nl> - <nl> - / / nothing new ? reached the end <nl> - if ( rows . isEmpty ( ) ) <nl> + <nl> + / / The removal of empty CF rows could result in an empty List < KeySlice > rows . <nl> + / / Keep trying until we return on reaching the end of the range or rows is nonEmpty . <nl> + while ( rows = = null | | rows . isEmpty ( ) ) { <nl> + if ( startToken . equals ( split . getEndToken ( ) ) ) <nl> { <nl> rows = null ; <nl> return ; <nl> } <nl> <nl> - / / Pre - compute the last row key , before removing empty rows <nl> - ByteBuffer lastRowKey = rows . get ( rows . size ( ) - 1 ) . key ; <nl> + KeyRange keyRange = new KeyRange ( batchRowCount ) <nl> + . setStart _ token ( startToken ) <nl> + . setEnd _ token ( split . getEndToken ( ) ) ; <nl> + try <nl> + { <nl> + rows = client . get _ range _ slices ( new ColumnParent ( cfName ) , <nl> + predicate , <nl> + keyRange , <nl> + consistencyLevel ) ; <nl> + <nl> + / / nothing new ? reached the end <nl> + if ( rows . isEmpty ( ) ) <nl> + { <nl> + rows = null ; <nl> + return ; <nl> + } <nl> <nl> - / / only remove empty rows if the slice predicate is empty <nl> - if ( isPredicateEmpty ( predicate ) ) <nl> + / / Pre - compute the last row key , before removing empty rows <nl> + ByteBuffer lastRowKey = rows . get ( rows . size ( ) - 1 ) . key ; <nl> + <nl> + / / only remove empty rows if the slice predicate is empty <nl> + if ( isPredicateEmpty ( predicate ) ) <nl> + { <nl> + Iterator < KeySlice > rowsIterator = rows . iterator ( ) ; <nl> + while ( rowsIterator . hasNext ( ) ) <nl> + if ( rowsIterator . next ( ) . columns . isEmpty ( ) ) <nl> + rowsIterator . remove ( ) ; <nl> + } <nl> + <nl> + / / reset to iterate through the new batch <nl> + i = 0 ; <nl> + <nl> + / / prepare for the next slice to be read <nl> + startToken = partitioner . getTokenFactory ( ) . toString ( partitioner . getToken ( lastRowKey ) ) ; <nl> + } <nl> + catch ( Exception e ) <nl> { <nl> - Iterator < KeySlice > rowsIterator = rows . iterator ( ) ; <nl> - while ( rowsIterator . hasNext ( ) ) <nl> - if ( rowsIterator . next ( ) . columns . isEmpty ( ) ) <nl> - rowsIterator . remove ( ) ; <nl> + throw new RuntimeException ( e ) ; <nl> } <nl> - <nl> - / / reset to iterate through the new batch <nl> - i = 0 ; <nl> - <nl> - / / prepare for the next slice to be read <nl> - startToken = partitioner . getTokenFactory ( ) . toString ( partitioner . getToken ( lastRowKey ) ) ; <nl> - } <nl> - catch ( Exception e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> } <nl> } <nl> <nl> @ @ - 354 , 8 + 359 , 10 @ @ public class ColumnFamilyRecordReader extends RecordReader < ByteBuffer , SortedMap <nl> { <nl> if ( predicate ! = null ) <nl> if ( predicate . isSetSlice _ range ( ) ) <nl> - if ( predicate . getSlice _ range ( ) . getStart ( ) ! = null & & predicate . getSlice _ range ( ) . getFinish ( ) ! = null ) <nl> + { <nl> + if ( predicate . getSlice _ range ( ) . getStart ( ) ! = null | | predicate . getSlice _ range ( ) . getFinish ( ) ! = null ) <nl> return false ; <nl> + } <nl> else if ( predicate . isSetColumn _ names ( ) ) <nl> return false ; <nl>

TEST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogArchiver . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogArchiver . java 
 index 1385ea4 . . 6161435 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogArchiver . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogArchiver . java 
 @ @ - 51 , 7 + 51 , 7 @ @ public class CommitLogArchiver 
 } 
 
 public final Map < String , Future < ? > > archivePending = new ConcurrentHashMap < String , Future < ? > > ( ) ; 
 - public final ExecutorService executor = new JMXEnabledThreadPoolExecutor ( " commitlog _ archiver " ) ; 
 + public final ExecutorService executor = new JMXEnabledThreadPoolExecutor ( " CommitLogArchiver " ) ; 
 private final String archiveCommand ; 
 private final String restoreCommand ; 
 private final String restoreDirectories ;

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index f0cffa5 . . 670a8a2 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 37 , 7 + 37 , 7 @ @ 
 * CFMetaData . convertToThrift method to set RowCacheProvider ( CASSANDRA - 3405 ) 
 * acquire compactionlock during truncate ( CASSANDRA - 3399 ) 
 * fix displaying cfdef entries for super columnfamilies ( CASSANDRA - 3415 ) 
 - 
 + * ( Hadoop ) Fix empty row filtering ( CASSANDRA - 3450 ) 
 
 0 . 8 . 7 
 * Kill server on wrapped OOME such as from FileChannel . map ( CASSANDRA - 3201 ) 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java 
 index 87408c6 . . 1c27638 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java 
 @ @ - 235 , 50 + 235 , 55 @ @ public class ColumnFamilyRecordReader extends RecordReader < ByteBuffer , SortedMap 
 { 
 startToken = split . getStartToken ( ) ; 
 } 
 - else if ( startToken . equals ( split . getEndToken ( ) ) ) 
 - { 
 - rows = null ; 
 - return ; 
 - } 
 - 
 - KeyRange keyRange = new KeyRange ( batchRowCount ) 
 - . setStart _ token ( startToken ) 
 - . setEnd _ token ( split . getEndToken ( ) ) ; 
 - try 
 - { 
 - rows = client . get _ range _ slices ( new ColumnParent ( cfName ) , 
 - predicate , 
 - keyRange , 
 - consistencyLevel ) ; 
 - 
 - / / nothing new ? reached the end 
 - if ( rows . isEmpty ( ) ) 
 + 
 + / / The removal of empty CF rows could result in an empty List < KeySlice > rows . 
 + / / Keep trying until we return on reaching the end of the range or rows is nonEmpty . 
 + while ( rows = = null | | rows . isEmpty ( ) ) { 
 + if ( startToken . equals ( split . getEndToken ( ) ) ) 
 { 
 rows = null ; 
 return ; 
 } 
 
 - / / Pre - compute the last row key , before removing empty rows 
 - ByteBuffer lastRowKey = rows . get ( rows . size ( ) - 1 ) . key ; 
 + KeyRange keyRange = new KeyRange ( batchRowCount ) 
 + . setStart _ token ( startToken ) 
 + . setEnd _ token ( split . getEndToken ( ) ) ; 
 + try 
 + { 
 + rows = client . get _ range _ slices ( new ColumnParent ( cfName ) , 
 + predicate , 
 + keyRange , 
 + consistencyLevel ) ; 
 + 
 + / / nothing new ? reached the end 
 + if ( rows . isEmpty ( ) ) 
 + { 
 + rows = null ; 
 + return ; 
 + } 
 
 - / / only remove empty rows if the slice predicate is empty 
 - if ( isPredicateEmpty ( predicate ) ) 
 + / / Pre - compute the last row key , before removing empty rows 
 + ByteBuffer lastRowKey = rows . get ( rows . size ( ) - 1 ) . key ; 
 + 
 + / / only remove empty rows if the slice predicate is empty 
 + if ( isPredicateEmpty ( predicate ) ) 
 + { 
 + Iterator < KeySlice > rowsIterator = rows . iterator ( ) ; 
 + while ( rowsIterator . hasNext ( ) ) 
 + if ( rowsIterator . next ( ) . columns . isEmpty ( ) ) 
 + rowsIterator . remove ( ) ; 
 + } 
 + 
 + / / reset to iterate through the new batch 
 + i = 0 ; 
 + 
 + / / prepare for the next slice to be read 
 + startToken = partitioner . getTokenFactory ( ) . toString ( partitioner . getToken ( lastRowKey ) ) ; 
 + } 
 + catch ( Exception e ) 
 { 
 - Iterator < KeySlice > rowsIterator = rows . iterator ( ) ; 
 - while ( rowsIterator . hasNext ( ) ) 
 - if ( rowsIterator . next ( ) . columns . isEmpty ( ) ) 
 - rowsIterator . remove ( ) ; 
 + throw new RuntimeException ( e ) ; 
 } 
 - 
 - / / reset to iterate through the new batch 
 - i = 0 ; 
 - 
 - / / prepare for the next slice to be read 
 - startToken = partitioner . getTokenFactory ( ) . toString ( partitioner . getToken ( lastRowKey ) ) ; 
 - } 
 - catch ( Exception e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 } 
 } 
 
 @ @ - 354 , 8 + 359 , 10 @ @ public class ColumnFamilyRecordReader extends RecordReader < ByteBuffer , SortedMap 
 { 
 if ( predicate ! = null ) 
 if ( predicate . isSetSlice _ range ( ) ) 
 - if ( predicate . getSlice _ range ( ) . getStart ( ) ! = null & & predicate . getSlice _ range ( ) . getFinish ( ) ! = null ) 
 + { 
 + if ( predicate . getSlice _ range ( ) . getStart ( ) ! = null | | predicate . getSlice _ range ( ) . getFinish ( ) ! = null ) 
 return false ; 
 + } 
 else if ( predicate . isSetColumn _ names ( ) ) 
 return false ; 

