BLEU SCORE: 0.016932492841722675

TEST MSG: COPY FROM on large datasets : fixed problem on single core machines
GENERATED MSG: Twisted driver for CQL

TEST DIFF (one line): diff - - git a / pylib / cqlshlib / copyutil . py b / pylib / cqlshlib / copyutil . py <nl> index cd03765 . . ba2a47b 100644 <nl> - - - a / pylib / cqlshlib / copyutil . py <nl> + + + b / pylib / cqlshlib / copyutil . py <nl> @ @ - 29 , 16 + 29 , 17 @ @ import re <nl> import struct <nl> import sys <nl> import time <nl> + import threading <nl> import traceback <nl> <nl> from bisect import bisect _ right <nl> from calendar import timegm <nl> from collections import defaultdict , namedtuple <nl> from decimal import Decimal <nl> + from Queue import Queue <nl> from random import randrange <nl> from StringIO import StringIO <nl> from select import select <nl> - from threading import Lock <nl> from uuid import UUID <nl> from util import profile _ on , profile _ off <nl> <nl> @ @ - 161 , 11 + 162 , 11 @ @ class CopyTask ( object ) : <nl> self . options = self . parse _ options ( opts , direction ) <nl> <nl> self . num _ processes = self . options . copy [ ' numprocesses ' ] <nl> - if direction = = ' in ' : <nl> - self . num _ processes + = 1 # add the feeder process <nl> - <nl> self . printmsg ( ' Using % d child processes ' % ( self . num _ processes , ) ) <nl> <nl> + if direction = = ' from ' : <nl> + self . num _ processes + = 1 # add the feeder process <nl> + <nl> self . processes = [ ] <nl> self . inmsg = OneWayChannels ( self . num _ processes ) <nl> self . outmsg = OneWayChannels ( self . num _ processes ) <nl> @ @ - 295 , 17 + 296 , 20 @ @ class CopyTask ( object ) : <nl> def get _ num _ processes ( cap ) : <nl> " " " <nl> Pick a reasonable number of child processes . We need to leave at <nl> - least one core for the parent process . <nl> + least one core for the parent or feeder process . <nl> " " " <nl> return max ( 1 , min ( cap , CopyTask . get _ num _ cores ( ) - 1 ) ) <nl> <nl> @ staticmethod <nl> def get _ num _ cores ( ) : <nl> " " " <nl> - Return the number of cores if available . <nl> + Return the number of cores if available . If the test environment variable <nl> + is set , then return the number carried by this variable . This is to test single - core <nl> + machine more easily . <nl> " " " <nl> try : <nl> - return mp . cpu _ count ( ) <nl> + num _ cores _ for _ testing = os . environ . get ( ' CQLSH _ COPY _ TEST _ NUM _ CORES ' , ' ' ) <nl> + return int ( num _ cores _ for _ testing ) if num _ cores _ for _ testing else mp . cpu _ count ( ) <nl> except NotImplementedError : <nl> return 1 <nl> <nl> @ @ - 690 , 22 + 694 , 20 @ @ class ExportTask ( CopyTask ) : <nl> if token _ range is None and result is None : # a request has finished <nl> succeeded + = 1 <nl> elif isinstance ( result , Exception ) : # an error occurred <nl> - if token _ range is None : # the entire process failed <nl> - shell . printerr ( ' Error from worker process : % s ' % ( result ) ) <nl> - else : # only this token _ range failed , retry up to max _ attempts if no rows received yet , <nl> - # If rows were already received we ' d risk duplicating data . <nl> - # Note that there is still a slight risk of duplicating data , even if we have <nl> - # an error with no rows received yet , it ' s just less likely . To avoid retrying on <nl> - # all timeouts would however mean we could risk not exporting some rows . <nl> - if ranges [ token _ range ] [ ' attempts ' ] < max _ attempts and ranges [ token _ range ] [ ' rows ' ] = = 0 : <nl> - shell . printerr ( ' Error for % s : % s ( will try again later attempt % d of % d ) ' <nl> - % ( token _ range , result , ranges [ token _ range ] [ ' attempts ' ] , max _ attempts ) ) <nl> - self . send _ work ( ranges , [ token _ range ] ) <nl> - else : <nl> - shell . printerr ( ' Error for % s : % s ( permanently given up after % d rows and % d attempts ) ' <nl> - % ( token _ range , result , ranges [ token _ range ] [ ' rows ' ] , <nl> - ranges [ token _ range ] [ ' attempts ' ] ) ) <nl> - failed + = 1 <nl> + # This token _ range failed , retry up to max _ attempts if no rows received yet , <nl> + # If rows were already received we ' d risk duplicating data . <nl> + # Note that there is still a slight risk of duplicating data , even if we have <nl> + # an error with no rows received yet , it ' s just less likely . To avoid retrying on <nl> + # all timeouts would however mean we could risk not exporting some rows . <nl> + if ranges [ token _ range ] [ ' attempts ' ] < max _ attempts and ranges [ token _ range ] [ ' rows ' ] = = 0 : <nl> + shell . printerr ( ' Error for % s : % s ( will try again later attempt % d of % d ) ' <nl> + % ( token _ range , result , ranges [ token _ range ] [ ' attempts ' ] , max _ attempts ) ) <nl> + self . send _ work ( ranges , [ token _ range ] ) <nl> + else : <nl> + shell . printerr ( ' Error for % s : % s ( permanently given up after % d rows and % d attempts ) ' <nl> + % ( token _ range , result , ranges [ token _ range ] [ ' rows ' ] , <nl> + ranges [ token _ range ] [ ' attempts ' ] ) ) <nl> + failed + = 1 <nl> else : # partial result received <nl> data , num = result <nl> self . writer . write ( data , num ) <nl> @ @ - 1313 , 7 + 1315 , 7 @ @ class ExportSession ( object ) : <nl> self . cluster = cluster <nl> self . session = session <nl> self . requests = 1 <nl> - self . lock = Lock ( ) <nl> + self . lock = threading . Lock ( ) <nl> self . consistency _ level = export _ process . consistency _ level <nl> <nl> def add _ request ( self ) : <nl> @ @ - 1351 , 6 + 1353 , 7 @ @ class ExportProcess ( ChildProcess ) : <nl> self . hosts _ to _ sessions = dict ( ) <nl> self . formatters = dict ( ) <nl> self . options = options <nl> + self . responses = None <nl> <nl> def run ( self ) : <nl> try : <nl> @ @ - 1368 , 6 + 1371 , 8 @ @ class ExportProcess ( ChildProcess ) : <nl> we can signal a global error by sending ( None , error ) . <nl> We terminate when the inbound queue is closed . <nl> " " " <nl> + self . init _ feeder _ thread ( ) <nl> + <nl> while True : <nl> if self . num _ requests ( ) > self . max _ requests : <nl> time . sleep ( 0 . 001 ) # 1 millisecond <nl> @ @ - 1376 , 6 + 1381 , 37 @ @ class ExportProcess ( ChildProcess ) : <nl> token _ range , info = self . inmsg . recv ( ) <nl> self . start _ request ( token _ range , info ) <nl> <nl> + def init _ feeder _ thread ( self ) : <nl> + " " " <nl> + Start a thread to feed response messages to the parent process . <nl> + <nl> + It is not safe to write on the pipe from the main thread if the parent process is still sending work and <nl> + not receiving yet . This will in fact block the main thread on the send , which in turn won ' t be able to call <nl> + recv ( ) , and will therefore block the parent process on its send ( ) . <nl> + <nl> + It is also not safe to write on the pipe from the driver receiving thread whilst the parent process is <nl> + sending work , because if the receiving thread stops making progress , then the main thread may no longer <nl> + call recv ( ) due to the check on the maximum number of requests in inner _ run ( ) . <nl> + <nl> + These deadlocks are easiest to reproduce with a single worker process , but may well affect multiple worker <nl> + processes too . <nl> + <nl> + It is important that the order of the responses in the queue is respected , or else the parent process may <nl> + kill off worker processes before it has received all the pages of the last token range . <nl> + " " " <nl> + def feed _ errors ( ) : <nl> + while True : <nl> + try : <nl> + self . outmsg . send ( self . responses . get ( ) ) <nl> + except Exception , e : <nl> + self . printdebugmsg ( e . message ) <nl> + <nl> + self . responses = Queue ( ) <nl> + <nl> + thread = threading . Thread ( target = feed _ errors ) <nl> + thread . setDaemon ( True ) <nl> + thread . start ( ) <nl> + <nl> @ staticmethod <nl> def get _ error _ message ( err , print _ traceback = False ) : <nl> if isinstance ( err , str ) : <nl> @ @ - 1388 , 10 + 1424 , 13 @ @ class ExportProcess ( ChildProcess ) : <nl> msg = str ( err ) <nl> return msg <nl> <nl> - def report _ error ( self , err , token _ range = None ) : <nl> + def report _ error ( self , err , token _ range ) : <nl> msg = self . get _ error _ message ( err , print _ traceback = self . debug ) <nl> self . printdebugmsg ( msg ) <nl> - self . outmsg . send ( ( token _ range , Exception ( msg ) ) ) <nl> + self . send ( ( token _ range , Exception ( msg ) ) ) <nl> + <nl> + def send ( self , response ) : <nl> + self . responses . put ( response ) <nl> <nl> def start _ request ( self , token _ range , info ) : <nl> " " " <nl> @ @ - 1434 , 7 + 1473 , 8 @ @ class ExportProcess ( ChildProcess ) : <nl> self . printdebugmsg ( " Warning : failed to connect to some replicas : % s " % ( errors , ) ) <nl> return ret <nl> <nl> - self . report _ error ( " Failed to connect to all replicas % s for % s , errors : % s " % ( hosts , token _ range , errors ) ) <nl> + self . report _ error ( " Failed to connect to all replicas % s for % s , errors : % s " % ( hosts , token _ range , errors ) , <nl> + token _ range ) <nl> return None <nl> <nl> def connect ( self , host ) : <nl> @ @ - 1467 , 7 + 1507 , 7 @ @ class ExportProcess ( ChildProcess ) : <nl> self . write _ rows _ to _ csv ( token _ range , rows ) <nl> else : <nl> self . write _ rows _ to _ csv ( token _ range , rows ) <nl> - self . outmsg . send ( ( None , None ) ) <nl> + self . send ( ( None , None ) ) <nl> session . complete _ request ( ) <nl> <nl> def err _ callback ( err ) : <nl> @ @ - 1488 , 7 + 1528 , 7 @ @ class ExportProcess ( ChildProcess ) : <nl> writer . writerow ( map ( self . format _ value , row ) ) <nl> <nl> data = ( output . getvalue ( ) , len ( rows ) ) <nl> - self . outmsg . send ( ( token _ range , data ) ) <nl> + self . send ( ( token _ range , data ) ) <nl> output . close ( ) <nl> <nl> except Exception , e :
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index fda6719 . . 6ce4af9 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 3 @ @ <nl> - < < < < < < < . working <nl> 0 . 8 - dev <nl> * avoid double RowMutation serialization on write path ( CASSANDRA - 1800 ) <nl> * adds support for columns that act as incr / decr counters <nl> @ @ - 12 , 7 + 11 , 6 @ @ <nl> * Fix for Cli to support updating replicate _ on _ write ( CASSANDRA - 2236 ) <nl> <nl> <nl> - = = = = = = = <nl> 0 . 7 . 5 <nl> * Avoid seeking when sstable2json exports the entire file ( CASSANDRA - 2318 ) <nl> * fix tombstone handling in repair and sstable2json ( CASSANDRA - 2279 ) <nl> @ @ - 24 , 7 + 22 , 6 @ @ <nl> * shut down server for OOM on a Thrift thread ( CASSANDRA - 2269 ) <nl> <nl> <nl> - > > > > > > > . merge - right . r1081840 <nl> 0 . 7 . 4 <nl> * add nodetool join command ( CASSANDRA - 2160 ) <nl> * fix secondary indexes on pre - existing or streamed data ( CASSANDRA - 2244 ) <nl> diff - - git a / src / java / org / apache / cassandra / db / marshal / AbstractCommutativeType . java b / src / java / org / apache / cassandra / db / marshal / AbstractCommutativeType . java <nl> index 2fa1b16 . . 77b9573 100644 <nl> - - - a / src / java / org / apache / cassandra / db / marshal / AbstractCommutativeType . java <nl> + + + b / src / java / org / apache / cassandra / db / marshal / AbstractCommutativeType . java <nl> @ @ - 20 , 12 + 20 , 10 @ @ <nl> * / <nl> package org . apache . cassandra . db . marshal ; <nl> <nl> - import java . net . InetAddress ; <nl> import java . nio . ByteBuffer ; <nl> <nl> import org . apache . cassandra . db . Column ; <nl> - import org . apache . cassandra . db . IColumnContainer ; <nl> - import org . apache . cassandra . utils . ByteBufferUtil ; <nl> + import org . apache . cassandra . db . context . CounterContext ; <nl> <nl> public abstract class AbstractCommutativeType extends AbstractType < Long > <nl> { <nl> @ @ - 36 , 7 + 34 , 7 @ @ public abstract class AbstractCommutativeType extends AbstractType < Long > <nl> <nl> public Long compose ( ByteBuffer bytes ) <nl> { <nl> - return ByteBufferUtil . toLong ( bytes ) ; <nl> + return CounterContext . instance ( ) . total ( bytes ) ; <nl> } <nl> <nl> / * *

TEST DIFF:
diff - - git a / pylib / cqlshlib / copyutil . py b / pylib / cqlshlib / copyutil . py 
 index cd03765 . . ba2a47b 100644 
 - - - a / pylib / cqlshlib / copyutil . py 
 + + + b / pylib / cqlshlib / copyutil . py 
 @ @ - 29 , 16 + 29 , 17 @ @ import re 
 import struct 
 import sys 
 import time 
 + import threading 
 import traceback 
 
 from bisect import bisect _ right 
 from calendar import timegm 
 from collections import defaultdict , namedtuple 
 from decimal import Decimal 
 + from Queue import Queue 
 from random import randrange 
 from StringIO import StringIO 
 from select import select 
 - from threading import Lock 
 from uuid import UUID 
 from util import profile _ on , profile _ off 
 
 @ @ - 161 , 11 + 162 , 11 @ @ class CopyTask ( object ) : 
 self . options = self . parse _ options ( opts , direction ) 
 
 self . num _ processes = self . options . copy [ ' numprocesses ' ] 
 - if direction = = ' in ' : 
 - self . num _ processes + = 1 # add the feeder process 
 - 
 self . printmsg ( ' Using % d child processes ' % ( self . num _ processes , ) ) 
 
 + if direction = = ' from ' : 
 + self . num _ processes + = 1 # add the feeder process 
 + 
 self . processes = [ ] 
 self . inmsg = OneWayChannels ( self . num _ processes ) 
 self . outmsg = OneWayChannels ( self . num _ processes ) 
 @ @ - 295 , 17 + 296 , 20 @ @ class CopyTask ( object ) : 
 def get _ num _ processes ( cap ) : 
 " " " 
 Pick a reasonable number of child processes . We need to leave at 
 - least one core for the parent process . 
 + least one core for the parent or feeder process . 
 " " " 
 return max ( 1 , min ( cap , CopyTask . get _ num _ cores ( ) - 1 ) ) 
 
 @ staticmethod 
 def get _ num _ cores ( ) : 
 " " " 
 - Return the number of cores if available . 
 + Return the number of cores if available . If the test environment variable 
 + is set , then return the number carried by this variable . This is to test single - core 
 + machine more easily . 
 " " " 
 try : 
 - return mp . cpu _ count ( ) 
 + num _ cores _ for _ testing = os . environ . get ( ' CQLSH _ COPY _ TEST _ NUM _ CORES ' , ' ' ) 
 + return int ( num _ cores _ for _ testing ) if num _ cores _ for _ testing else mp . cpu _ count ( ) 
 except NotImplementedError : 
 return 1 
 
 @ @ - 690 , 22 + 694 , 20 @ @ class ExportTask ( CopyTask ) : 
 if token _ range is None and result is None : # a request has finished 
 succeeded + = 1 
 elif isinstance ( result , Exception ) : # an error occurred 
 - if token _ range is None : # the entire process failed 
 - shell . printerr ( ' Error from worker process : % s ' % ( result ) ) 
 - else : # only this token _ range failed , retry up to max _ attempts if no rows received yet , 
 - # If rows were already received we ' d risk duplicating data . 
 - # Note that there is still a slight risk of duplicating data , even if we have 
 - # an error with no rows received yet , it ' s just less likely . To avoid retrying on 
 - # all timeouts would however mean we could risk not exporting some rows . 
 - if ranges [ token _ range ] [ ' attempts ' ] < max _ attempts and ranges [ token _ range ] [ ' rows ' ] = = 0 : 
 - shell . printerr ( ' Error for % s : % s ( will try again later attempt % d of % d ) ' 
 - % ( token _ range , result , ranges [ token _ range ] [ ' attempts ' ] , max _ attempts ) ) 
 - self . send _ work ( ranges , [ token _ range ] ) 
 - else : 
 - shell . printerr ( ' Error for % s : % s ( permanently given up after % d rows and % d attempts ) ' 
 - % ( token _ range , result , ranges [ token _ range ] [ ' rows ' ] , 
 - ranges [ token _ range ] [ ' attempts ' ] ) ) 
 - failed + = 1 
 + # This token _ range failed , retry up to max _ attempts if no rows received yet , 
 + # If rows were already received we ' d risk duplicating data . 
 + # Note that there is still a slight risk of duplicating data , even if we have 
 + # an error with no rows received yet , it ' s just less likely . To avoid retrying on 
 + # all timeouts would however mean we could risk not exporting some rows . 
 + if ranges [ token _ range ] [ ' attempts ' ] < max _ attempts and ranges [ token _ range ] [ ' rows ' ] = = 0 : 
 + shell . printerr ( ' Error for % s : % s ( will try again later attempt % d of % d ) ' 
 + % ( token _ range , result , ranges [ token _ range ] [ ' attempts ' ] , max _ attempts ) ) 
 + self . send _ work ( ranges , [ token _ range ] ) 
 + else : 
 + shell . printerr ( ' Error for % s : % s ( permanently given up after % d rows and % d attempts ) ' 
 + % ( token _ range , result , ranges [ token _ range ] [ ' rows ' ] , 
 + ranges [ token _ range ] [ ' attempts ' ] ) ) 
 + failed + = 1 
 else : # partial result received 
 data , num = result 
 self . writer . write ( data , num ) 
 @ @ - 1313 , 7 + 1315 , 7 @ @ class ExportSession ( object ) : 
 self . cluster = cluster 
 self . session = session 
 self . requests = 1 
 - self . lock = Lock ( ) 
 + self . lock = threading . Lock ( ) 
 self . consistency _ level = export _ process . consistency _ level 
 
 def add _ request ( self ) : 
 @ @ - 1351 , 6 + 1353 , 7 @ @ class ExportProcess ( ChildProcess ) : 
 self . hosts _ to _ sessions = dict ( ) 
 self . formatters = dict ( ) 
 self . options = options 
 + self . responses = None 
 
 def run ( self ) : 
 try : 
 @ @ - 1368 , 6 + 1371 , 8 @ @ class ExportProcess ( ChildProcess ) : 
 we can signal a global error by sending ( None , error ) . 
 We terminate when the inbound queue is closed . 
 " " " 
 + self . init _ feeder _ thread ( ) 
 + 
 while True : 
 if self . num _ requests ( ) > self . max _ requests : 
 time . sleep ( 0 . 001 ) # 1 millisecond 
 @ @ - 1376 , 6 + 1381 , 37 @ @ class ExportProcess ( ChildProcess ) : 
 token _ range , info = self . inmsg . recv ( ) 
 self . start _ request ( token _ range , info ) 
 
 + def init _ feeder _ thread ( self ) : 
 + " " " 
 + Start a thread to feed response messages to the parent process . 
 + 
 + It is not safe to write on the pipe from the main thread if the parent process is still sending work and 
 + not receiving yet . This will in fact block the main thread on the send , which in turn won ' t be able to call 
 + recv ( ) , and will therefore block the parent process on its send ( ) . 
 + 
 + It is also not safe to write on the pipe from the driver receiving thread whilst the parent process is 
 + sending work , because if the receiving thread stops making progress , then the main thread may no longer 
 + call recv ( ) due to the check on the maximum number of requests in inner _ run ( ) . 
 + 
 + These deadlocks are easiest to reproduce with a single worker process , but may well affect multiple worker 
 + processes too . 
 + 
 + It is important that the order of the responses in the queue is respected , or else the parent process may 
 + kill off worker processes before it has received all the pages of the last token range . 
 + " " " 
 + def feed _ errors ( ) : 
 + while True : 
 + try : 
 + self . outmsg . send ( self . responses . get ( ) ) 
 + except Exception , e : 
 + self . printdebugmsg ( e . message ) 
 + 
 + self . responses = Queue ( ) 
 + 
 + thread = threading . Thread ( target = feed _ errors ) 
 + thread . setDaemon ( True ) 
 + thread . start ( ) 
 + 
 @ staticmethod 
 def get _ error _ message ( err , print _ traceback = False ) : 
 if isinstance ( err , str ) : 
 @ @ - 1388 , 10 + 1424 , 13 @ @ class ExportProcess ( ChildProcess ) : 
 msg = str ( err ) 
 return msg 
 
 - def report _ error ( self , err , token _ range = None ) : 
 + def report _ error ( self , err , token _ range ) : 
 msg = self . get _ error _ message ( err , print _ traceback = self . debug ) 
 self . printdebugmsg ( msg ) 
 - self . outmsg . send ( ( token _ range , Exception ( msg ) ) ) 
 + self . send ( ( token _ range , Exception ( msg ) ) ) 
 + 
 + def send ( self , response ) : 
 + self . responses . put ( response ) 
 
 def start _ request ( self , token _ range , info ) : 
 " " " 
 @ @ - 1434 , 7 + 1473 , 8 @ @ class ExportProcess ( ChildProcess ) : 
 self . printdebugmsg ( " Warning : failed to connect to some replicas : % s " % ( errors , ) ) 
 return ret 
 
 - self . report _ error ( " Failed to connect to all replicas % s for % s , errors : % s " % ( hosts , token _ range , errors ) ) 
 + self . report _ error ( " Failed to connect to all replicas % s for % s , errors : % s " % ( hosts , token _ range , errors ) , 
 + token _ range ) 
 return None 
 
 def connect ( self , host ) : 
 @ @ - 1467 , 7 + 1507 , 7 @ @ class ExportProcess ( ChildProcess ) : 
 self . write _ rows _ to _ csv ( token _ range , rows ) 
 else : 
 self . write _ rows _ to _ csv ( token _ range , rows ) 
 - self . outmsg . send ( ( None , None ) ) 
 + self . send ( ( None , None ) ) 
 session . complete _ request ( ) 
 
 def err _ callback ( err ) : 
 @ @ - 1488 , 7 + 1528 , 7 @ @ class ExportProcess ( ChildProcess ) : 
 writer . writerow ( map ( self . format _ value , row ) ) 
 
 data = ( output . getvalue ( ) , len ( rows ) ) 
 - self . outmsg . send ( ( token _ range , data ) ) 
 + self . send ( ( token _ range , data ) ) 
 output . close ( ) 
 
 except Exception , e :

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index fda6719 . . 6ce4af9 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 3 @ @ 
 - < < < < < < < . working 
 0 . 8 - dev 
 * avoid double RowMutation serialization on write path ( CASSANDRA - 1800 ) 
 * adds support for columns that act as incr / decr counters 
 @ @ - 12 , 7 + 11 , 6 @ @ 
 * Fix for Cli to support updating replicate _ on _ write ( CASSANDRA - 2236 ) 
 
 
 - = = = = = = = 
 0 . 7 . 5 
 * Avoid seeking when sstable2json exports the entire file ( CASSANDRA - 2318 ) 
 * fix tombstone handling in repair and sstable2json ( CASSANDRA - 2279 ) 
 @ @ - 24 , 7 + 22 , 6 @ @ 
 * shut down server for OOM on a Thrift thread ( CASSANDRA - 2269 ) 
 
 
 - > > > > > > > . merge - right . r1081840 
 0 . 7 . 4 
 * add nodetool join command ( CASSANDRA - 2160 ) 
 * fix secondary indexes on pre - existing or streamed data ( CASSANDRA - 2244 ) 
 diff - - git a / src / java / org / apache / cassandra / db / marshal / AbstractCommutativeType . java b / src / java / org / apache / cassandra / db / marshal / AbstractCommutativeType . java 
 index 2fa1b16 . . 77b9573 100644 
 - - - a / src / java / org / apache / cassandra / db / marshal / AbstractCommutativeType . java 
 + + + b / src / java / org / apache / cassandra / db / marshal / AbstractCommutativeType . java 
 @ @ - 20 , 12 + 20 , 10 @ @ 
 * / 
 package org . apache . cassandra . db . marshal ; 
 
 - import java . net . InetAddress ; 
 import java . nio . ByteBuffer ; 
 
 import org . apache . cassandra . db . Column ; 
 - import org . apache . cassandra . db . IColumnContainer ; 
 - import org . apache . cassandra . utils . ByteBufferUtil ; 
 + import org . apache . cassandra . db . context . CounterContext ; 
 
 public abstract class AbstractCommutativeType extends AbstractType < Long > 
 { 
 @ @ - 36 , 7 + 34 , 7 @ @ public abstract class AbstractCommutativeType extends AbstractType < Long > 
 
 public Long compose ( ByteBuffer bytes ) 
 { 
 - return ByteBufferUtil . toLong ( bytes ) ; 
 + return CounterContext . instance ( ) . total ( bytes ) ; 
 } 
 
 / * *
