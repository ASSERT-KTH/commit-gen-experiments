BLEU SCORE: 0.027611988917697356

TEST MSG: Use canonical path for directory in SSTable descriptor
GENERATED MSG: support sstable2json against snapshot sstables

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 78ea961 . . c8a4f21 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 2 . 2 . 6 <nl> + * Use canonical path for directory in SSTable descriptor ( CASSANDRA - 10587 ) <nl> * Add cassandra - stress keystore option ( CASSANDRA - 9325 ) <nl> * Fix out - of - space error treatment in memtable flushing ( CASSANDRA - 11448 ) . <nl> * Dont mark sstables as repairing with sub range repairs ( CASSANDRA - 11451 ) <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / Descriptor . java b / src / java / org / apache / cassandra / io / sstable / Descriptor . java <nl> index 9f259fe . . ed81616 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / Descriptor . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / Descriptor . java <nl> @ @ - 18 , 6 + 18 , 8 @ @ <nl> package org . apache . cassandra . io . sstable ; <nl> <nl> import java . io . File ; <nl> + import java . io . IOError ; <nl> + import java . io . IOException ; <nl> import java . util . ArrayDeque ; <nl> import java . util . Deque ; <nl> import java . util . StringTokenizer ; <nl> @ @ - 58 , 7 + 60 , 7 @ @ public class Descriptor <nl> } <nl> } <nl> <nl> - <nl> + / * * canonicalized path to the directory where SSTable resides * / <nl> public final File directory ; <nl> / * * version has the following format : < code > [ a - z ] + < / code > * / <nl> public final Version version ; <nl> @ @ - 91 , 14 + 93 , 21 @ @ public class Descriptor <nl> { <nl> assert version ! = null & & directory ! = null & & ksname ! = null & & cfname ! = null & & formatType . info . getLatestVersion ( ) . getClass ( ) . equals ( version . getClass ( ) ) ; <nl> this . version = version ; <nl> - this . directory = directory ; <nl> + try <nl> + { <nl> + this . directory = directory . getCanonicalFile ( ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + throw new IOError ( e ) ; <nl> + } <nl> this . ksname = ksname ; <nl> this . cfname = cfname ; <nl> this . generation = generation ; <nl> this . type = temp ; <nl> this . formatType = formatType ; <nl> <nl> - hashCode = Objects . hashCode ( version , directory , generation , ksname , cfname , temp , formatType ) ; <nl> + hashCode = Objects . hashCode ( version , this . directory , generation , ksname , cfname , temp , formatType ) ; <nl> } <nl> <nl> public Descriptor withGeneration ( int newGeneration ) <nl> @ @ - 168 , 8 + 177 , 7 @ @ public class Descriptor <nl> * / <nl> public static Descriptor fromFilename ( String filename ) <nl> { <nl> - File file = new File ( filename ) ; <nl> - return fromFilename ( file . getParentFile ( ) , file . getName ( ) , false ) . left ; <nl> + return fromFilename ( filename , false ) ; <nl> } <nl> <nl> public static Descriptor fromFilename ( String filename , SSTableFormat . Type formatType ) <nl> @ @ - 179 , 7 + 187 , 7 @ @ public class Descriptor <nl> <nl> public static Descriptor fromFilename ( String filename , boolean skipComponent ) <nl> { <nl> - File file = new File ( filename ) ; <nl> + File file = new File ( filename ) . getAbsoluteFile ( ) ; <nl> return fromFilename ( file . getParentFile ( ) , file . getName ( ) , skipComponent ) . left ; <nl> } <nl> <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java b / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java <nl> index 70ab8ba . . 6354fc2 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java <nl> @ @ - 84 , 16 + 84 , 22 @ @ public class DescriptorTest <nl> / / secondary index <nl> String idxName = " myidx " ; <nl> File idxDir = new File ( dir . getAbsolutePath ( ) + File . separator + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName ) ; <nl> - checkFromFilename ( new Descriptor ( idxDir , ksname , cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , 4 , Descriptor . Type . FINAL ) , false ) ; <nl> + checkFromFilename ( new Descriptor ( idxDir , ksname , cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , <nl> + 4 , Descriptor . Type . FINAL ) , false ) ; <nl> / / secondary index tmp <nl> - checkFromFilename ( new Descriptor ( idxDir , ksname , cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , 5 , Descriptor . Type . TEMP ) , false ) ; <nl> + checkFromFilename ( new Descriptor ( idxDir , ksname , cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , <nl> + 5 , Descriptor . Type . TEMP ) , false ) ; <nl> <nl> / / legacy version <nl> - checkFromFilename ( new Descriptor ( " ja " , dir , ksname , cfname , 1 , Descriptor . Type . FINAL , SSTableFormat . Type . LEGACY ) , false ) ; <nl> + checkFromFilename ( new Descriptor ( " ja " , dir , ksname , cfname , 1 , Descriptor . Type . FINAL , <nl> + SSTableFormat . Type . LEGACY ) , false ) ; <nl> / / legacy tmp <nl> - checkFromFilename ( new Descriptor ( " ja " , dir , ksname , cfname , 2 , Descriptor . Type . TEMP , SSTableFormat . Type . LEGACY ) , false ) ; <nl> + checkFromFilename ( new Descriptor ( " ja " , dir , ksname , cfname , 2 , Descriptor . Type . TEMP , SSTableFormat . Type . LEGACY ) , <nl> + false ) ; <nl> / / legacy secondary index <nl> - checkFromFilename ( new Descriptor ( " ja " , dir , ksname , cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , 3 , Descriptor . Type . FINAL , SSTableFormat . Type . LEGACY ) , false ) ; <nl> + checkFromFilename ( new Descriptor ( " ja " , dir , ksname , <nl> + cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , 3 , <nl> + Descriptor . Type . FINAL , SSTableFormat . Type . LEGACY ) , false ) ; <nl> } <nl> <nl> private void checkFromFilename ( Descriptor original , boolean skipComponent ) <nl> @ @ - 121 , 23 + 127 , 38 @ @ public class DescriptorTest <nl> } <nl> <nl> @ Test <nl> + public void testEquality ( ) <nl> + { <nl> + / / Descriptor should be equal when parent directory points to the same directory <nl> + File dir = new File ( " . " ) ; <nl> + Descriptor desc1 = new Descriptor ( dir , " ks " , " cf " , 1 , Descriptor . Type . FINAL ) ; <nl> + Descriptor desc2 = new Descriptor ( dir . getAbsoluteFile ( ) , " ks " , " cf " , 1 , Descriptor . Type . FINAL ) ; <nl> + assertEquals ( desc1 , desc2 ) ; <nl> + assertEquals ( desc1 . hashCode ( ) , desc2 . hashCode ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> public void validateNames ( ) <nl> { <nl> <nl> - String names [ ] = { <nl> - / * " system - schema _ keyspaces - ka - 1 - CompressionInfo . db " , " system - schema _ keyspaces - ka - 1 - Summary . db " , <nl> - " system - schema _ keyspaces - ka - 1 - Data . db " , " system - schema _ keyspaces - ka - 1 - TOC . txt " , <nl> - " system - schema _ keyspaces - ka - 1 - Digest . sha1 " , " system - schema _ keyspaces - ka - 2 - CompressionInfo . db " , <nl> - " system - schema _ keyspaces - ka - 1 - Filter . db " , " system - schema _ keyspaces - ka - 2 - Data . db " , <nl> - " system - schema _ keyspaces - ka - 1 - Index . db " , " system - schema _ keyspaces - ka - 2 - Digest . sha1 " , <nl> - " system - schema _ keyspaces - ka - 1 - Statistics . db " , <nl> - " system - schema _ keyspacest - tmp - ka - 1 - Data . db " , * / <nl> - " system - schema _ keyspace - ka - 1 - " + SSTableFormat . Type . BIG . name + " - Data . db " <nl> + String [ ] names = { <nl> + / / old formats <nl> + " system - schema _ keyspaces - jb - 1 - Data . db " , <nl> + " system - schema _ keyspaces - tmp - jb - 1 - Data . db " , <nl> + " system - schema _ keyspaces - ka - 1 - big - Data . db " , <nl> + " system - schema _ keyspaces - tmp - ka - 1 - big - Data . db " , <nl> + / / 2ndary index <nl> + " keyspace1 - standard1 . idx1 - ka - 1 - big - Data . db " , <nl> + / / new formats <nl> + " la - 1 - big - Data . db " , <nl> + " tmp - la - 1 - big - Data . db " , <nl> + / / 2ndary index <nl> + " . idx1 " + File . separator + " la - 1 - big - Data . db " , <nl> } ; <nl> <nl> for ( String name : names ) <nl> { <nl> - Descriptor d = Descriptor . fromFilename ( name ) ; <nl> + assertNotNull ( Descriptor . fromFilename ( name ) ) ; <nl> } <nl> } <nl>
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 844a9f8 . . 1ca1a7a 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 8 , 6 + 8 , 7 @ @ <nl> - improve JDBC spec compliance ( CASSANDRA - 2720 ) <nl> - ALTER COLUMNFAMILY ( CASSANDRA - 1709 ) <nl> - DROP INDEX ( CASSANDRA - 2617 ) <nl> + - add SCHEMA / TABLE as aliases for KS / CF ( CASSANDRA - 2743 ) <nl> * add support for comparator parameters and a generic ReverseType <nl> ( CASSANDRA - 2355 ) <nl> * add CompositeType and DynamicCompositeType ( CASSANDRA - 2231 ) <nl> @ @ - 45 , 6 + 46 , 9 @ @ <nl> by nio sockets ( CASSANDRA - 2654 ) <nl> * restrict repair streaming to specific columnfamilies ( CASSANDRA - 2280 ) <nl> * fix nodetool ring use with Ec2Snitch ( CASSANDRA - 2733 ) <nl> + * fix removing columns and subcolumns that are supressed by a row or <nl> + supercolumn tombstone during replica resolution ( CASSANDRA - 2590 ) <nl> + * support sstable2json against snapshot sstables ( CASSANDRA - 2386 ) <nl> <nl> <nl> 0 . 8 . 0 - final <nl> diff - - git a / examples / bmt / CassandraBulkLoader . java b / examples / bmt / CassandraBulkLoader . java <nl> deleted file mode 100644 <nl> index 6fb1a91 . . 0000000 <nl> - - - a / examples / bmt / CassandraBulkLoader . java <nl> + + + / dev / null <nl> @ @ - 1 , 293 + 0 , 0 @ @ <nl> - / * <nl> - * Licensed to the Apache Software Foundation ( ASF ) under one <nl> - * or more contributor license agreements . See the NOTICE file <nl> - * distributed with this work for additional information <nl> - * regarding copyright ownership . The ASF licenses this file <nl> - * to you under the Apache License , Version 2 . 0 ( the <nl> - * " License " ) ; you may not use this file except in compliance <nl> - * with the License . You may obtain a copy of the License at <nl> - * <nl> - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> - * <nl> - * Unless required by applicable law or agreed to in writing , software <nl> - * distributed under the License is distributed on an " AS IS " BASIS , <nl> - * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> - * See the License for the specific language governing permissions and <nl> - * limitations under the License . <nl> - * / <nl> - <nl> - / * * <nl> - * Cassandra has a back door called the Binary Memtable . The purpose of this backdoor is to <nl> - * mass import large amounts of data , without using the Thrift interface . <nl> - * <nl> - * Inserting data through the binary memtable , allows you to skip the commit log overhead , and an ack <nl> - * from Thrift on every insert . The example below utilizes Hadoop to generate all the data necessary <nl> - * to send to Cassandra , and sends it using the Binary Memtable interface . What Hadoop ends up doing is <nl> - * creating the actual data that gets put into an SSTable as if you were using Thrift . With enough Hadoop nodes <nl> - * inserting the data , the bottleneck at this point should become the network . <nl> - * <nl> - * We recommend adjusting the compaction threshold to 0 , while the import is running . After the import , you need <nl> - * to run ` nodeprobe - host < IP > flush _ binary < Keyspace > ` on every node , as this will flush the remaining data still left <nl> - * in memory to disk . Then it ' s recommended to adjust the compaction threshold to it ' s original value . <nl> - * <nl> - * The example below is a sample Hadoop job , and it inserts SuperColumns . It can be tweaked to work with normal Columns . <nl> - * <nl> - * You should construct your data you want to import as rows delimited by a new line . You end up grouping by < Key > <nl> - * in the mapper , so that the end result generates the data set into a column oriented subset . Once you get to the <nl> - * reduce aspect , you can generate the ColumnFamilies you want inserted , and send it to your nodes . <nl> - * <nl> - * For Cassandra 0 . 6 . 4 , we modified this example to wait for acks from all Cassandra nodes for each row <nl> - * before proceeding to the next . This means to keep Cassandra similarly busy you can either <nl> - * 1 ) add more reducer tasks , <nl> - * 2 ) remove the " wait for acks " block of code , <nl> - * 3 ) parallelize the writing of rows to Cassandra , e . g . with an Executor . <nl> - * <nl> - * THIS CANNOT RUN ON THE SAME IP ADDRESS AS A CASSANDRA INSTANCE . <nl> - * / <nl> - <nl> - package org . apache . cassandra . bulkloader ; <nl> - <nl> - import java . io . IOException ; <nl> - import java . net . InetAddress ; <nl> - import java . net . URI ; <nl> - import java . net . URISyntaxException ; <nl> - import java . nio . ByteBuffer ; <nl> - import java . util . ArrayList ; <nl> - import java . util . Iterator ; <nl> - import java . util . LinkedList ; <nl> - import java . util . List ; <nl> - import java . util . concurrent . TimeUnit ; <nl> - import java . util . concurrent . TimeoutException ; <nl> - <nl> - import com . google . common . base . Charsets ; <nl> - <nl> - import org . apache . cassandra . config . CFMetaData ; <nl> - import org . apache . cassandra . config . ConfigurationException ; <nl> - import org . apache . cassandra . config . DatabaseDescriptor ; <nl> - import org . apache . cassandra . db . Column ; <nl> - import org . apache . cassandra . db . ColumnFamily ; <nl> - import org . apache . cassandra . db . ColumnFamilyType ; <nl> - import org . apache . cassandra . db . RowMutation ; <nl> - import org . apache . cassandra . db . filter . QueryPath ; <nl> - import org . apache . cassandra . io . util . DataOutputBuffer ; <nl> - import org . apache . cassandra . net . IAsyncResult ; <nl> - import org . apache . cassandra . net . Message ; <nl> - import org . apache . cassandra . net . MessagingService ; <nl> - import org . apache . cassandra . service . StorageService ; <nl> - import org . apache . cassandra . utils . FBUtilities ; <nl> - import org . apache . cassandra . utils . ByteBufferUtil ; <nl> - import org . apache . hadoop . filecache . DistributedCache ; <nl> - import org . apache . hadoop . fs . Path ; <nl> - import org . apache . hadoop . io . Text ; <nl> - import org . apache . hadoop . mapred . * ; <nl> - <nl> - public class CassandraBulkLoader { <nl> - public static class Map extends MapReduceBase implements Mapper < Text , Text , Text , Text > { <nl> - public void map ( Text key , Text value , OutputCollector < Text , Text > output , Reporter reporter ) throws IOException { <nl> - / / This is a simple key / value mapper . <nl> - output . collect ( key , value ) ; <nl> - } <nl> - } <nl> - <nl> - public static class Reduce extends MapReduceBase implements Reducer < Text , Text , Text , Text > { <nl> - private Path [ ] localFiles ; <nl> - private JobConf jobconf ; <nl> - <nl> - public void configure ( JobConf job ) { <nl> - this . jobconf = job ; <nl> - String cassConfig ; <nl> - <nl> - / / Get the cached files <nl> - try <nl> - { <nl> - localFiles = DistributedCache . getLocalCacheFiles ( job ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - cassConfig = localFiles [ 0 ] . getParent ( ) . toString ( ) ; <nl> - <nl> - System . setProperty ( " storage - config " , cassConfig ) ; <nl> - <nl> - try <nl> - { <nl> - StorageService . instance . initClient ( ) ; <nl> - } <nl> - catch ( Exception e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - try <nl> - { <nl> - Thread . sleep ( 10 * 1000 ) ; <nl> - } <nl> - catch ( InterruptedException e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - } <nl> - <nl> - public void close ( ) <nl> - { <nl> - try <nl> - { <nl> - / / release the cache <nl> - DistributedCache . releaseCache ( new URI ( " / cassandra / storage - conf . xml # storage - conf . xml " ) , this . jobconf ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - catch ( URISyntaxException e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - try <nl> - { <nl> - / / Sleep just in case the number of keys we send over is small <nl> - Thread . sleep ( 3 * 1000 ) ; <nl> - } <nl> - catch ( InterruptedException e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - StorageService . instance . stopClient ( ) ; <nl> - } <nl> - <nl> - public void reduce ( Text key , Iterator < Text > values , OutputCollector < Text , Text > output , Reporter reporter ) throws IOException <nl> - { <nl> - ColumnFamily columnFamily ; <nl> - String keyspace = " Keyspace1 " ; <nl> - String cfName = " Super1 " ; <nl> - Message message ; <nl> - List < ColumnFamily > columnFamilies ; <nl> - columnFamilies = new LinkedList < ColumnFamily > ( ) ; <nl> - String line ; <nl> - <nl> - / * Create a column family * / <nl> - columnFamily = ColumnFamily . create ( keyspace , cfName ) ; <nl> - while ( values . hasNext ( ) ) { <nl> - / / Split the value ( line based on your own delimiter ) <nl> - line = values . next ( ) . toString ( ) ; <nl> - String [ ] fields = line . split ( " \ 1 " ) ; <nl> - String SuperColumnName = fields [ 1 ] ; <nl> - String ColumnName = fields [ 2 ] ; <nl> - String ColumnValue = fields [ 3 ] ; <nl> - int timestamp = 0 ; <nl> - columnFamily . addColumn ( new QueryPath ( cfName , <nl> - ByteBufferUtil . bytes ( SuperColumnName ) , <nl> - ByteBufferUtil . bytes ( ColumnName ) ) , <nl> - ByteBufferUtil . bytes ( ColumnValue ) , <nl> - timestamp ) ; <nl> - } <nl> - <nl> - columnFamilies . add ( columnFamily ) ; <nl> - <nl> - / * Get serialized message to send to cluster * / <nl> - message = createMessage ( keyspace , key . getBytes ( ) , cfName , columnFamilies ) ; <nl> - List < IAsyncResult > results = new ArrayList < IAsyncResult > ( ) ; <nl> - for ( InetAddress endpoint : StorageService . instance . getNaturalEndpoints ( keyspace , ByteBufferUtil . bytes ( key ) ) ) <nl> - { <nl> - / * Send message to end point * / <nl> - results . add ( MessagingService . instance ( ) . sendRR ( message , endpoint ) ) ; <nl> - } <nl> - / * wait for acks * / <nl> - for ( IAsyncResult result : results ) <nl> - { <nl> - try <nl> - { <nl> - result . get ( DatabaseDescriptor . getRpcTimeout ( ) , TimeUnit . MILLISECONDS ) ; <nl> - } <nl> - catch ( TimeoutException e ) <nl> - { <nl> - / / you should probably add retry logic here <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - } <nl> - <nl> - output . collect ( key , new Text ( " inserted into Cassandra node ( s ) " ) ) ; <nl> - } <nl> - } <nl> - <nl> - public static void runJob ( String [ ] args ) <nl> - { <nl> - JobConf conf = new JobConf ( CassandraBulkLoader . class ) ; <nl> - <nl> - if ( args . length > = 4 ) <nl> - { <nl> - conf . setNumReduceTasks ( new Integer ( args [ 3 ] ) ) ; <nl> - } <nl> - <nl> - try <nl> - { <nl> - / / We store the cassandra storage - conf . xml on the HDFS cluster <nl> - DistributedCache . addCacheFile ( new URI ( " / cassandra / storage - conf . xml # storage - conf . xml " ) , conf ) ; <nl> - } <nl> - catch ( URISyntaxException e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - conf . setInputFormat ( KeyValueTextInputFormat . class ) ; <nl> - conf . setJobName ( " CassandraBulkLoader _ v2 " ) ; <nl> - conf . setMapperClass ( Map . class ) ; <nl> - conf . setReducerClass ( Reduce . class ) ; <nl> - <nl> - conf . setOutputKeyClass ( Text . class ) ; <nl> - conf . setOutputValueClass ( Text . class ) ; <nl> - <nl> - FileInputFormat . setInputPaths ( conf , new Path ( args [ 1 ] ) ) ; <nl> - FileOutputFormat . setOutputPath ( conf , new Path ( args [ 2 ] ) ) ; <nl> - try <nl> - { <nl> - JobClient . runJob ( conf ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - } <nl> - <nl> - public static Message createMessage ( String keyspace , byte [ ] key , String columnFamily , List < ColumnFamily > columnFamilies ) <nl> - { <nl> - ColumnFamily baseColumnFamily ; <nl> - DataOutputBuffer bufOut = new DataOutputBuffer ( ) ; <nl> - RowMutation rm ; <nl> - Message message ; <nl> - Column column ; <nl> - <nl> - / * Get the first column family from list , this is just to get past validation * / <nl> - baseColumnFamily = new ColumnFamily ( ColumnFamilyType . Standard , <nl> - DatabaseDescriptor . getComparator ( keyspace , columnFamily ) , <nl> - DatabaseDescriptor . getSubComparator ( keyspace , columnFamily ) , <nl> - CFMetaData . getId ( keyspace , columnFamily ) ) ; <nl> - <nl> - for ( ColumnFamily cf : columnFamilies ) { <nl> - bufOut . reset ( ) ; <nl> - ColumnFamily . serializer ( ) . serializeWithIndexes ( cf , bufOut ) ; <nl> - byte [ ] data = new byte [ bufOut . getLength ( ) ] ; <nl> - System . arraycopy ( bufOut . getData ( ) , 0 , data , 0 , bufOut . getLength ( ) ) ; <nl> - <nl> - column = new Column ( FBUtilities . toByteBuffer ( cf . id ( ) ) , ByteBuffer . wrap ( data ) , 0 ) ; <nl> - baseColumnFamily . addColumn ( column ) ; <nl> - } <nl> - rm = new RowMutation ( keyspace , ByteBuffer . wrap ( key ) ) ; <nl> - rm . add ( baseColumnFamily ) ; <nl> - <nl> - try <nl> - { <nl> - / * Make message * / <nl> - message = rm . makeRowMutationMessage ( StorageService . Verb . BINARY , MessagingService . version _ ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - <nl> - return message ; <nl> - } <nl> - public static void main ( String [ ] args ) throws Exception <nl> - { <nl> - runJob ( args ) ; <nl> - } <nl> - } <nl> diff - - git a / examples / bmt / README . txt b / examples / bmt / README . txt <nl> deleted file mode 100644 <nl> index 78ba11c . . 0000000 <nl> - - - a / examples / bmt / README . txt <nl> + + + / dev / null <nl> @ @ - 1 , 34 + 0 , 0 @ @ <nl> - This is an example for the deprecated BinaryMemtable bulk - load interface . <nl> - <nl> - Inserting data through the binary memtable , allows you to skip the <nl> - commit log overhead , and an ack from Thrift on every insert . The <nl> - example below utilizes Hadoop to generate all the data necessary to <nl> - send to Cassandra , and sends it using the Binary Memtable <nl> - interface . What Hadoop ends up doing is creating the actual data that <nl> - gets put into an SSTable as if you were using Thrift . With enough <nl> - Hadoop nodes inserting the data , the bottleneck at this point should <nl> - become the network . <nl> - <nl> - We recommend adjusting the compaction threshold to 0 while the import <nl> - is running . After the import , you need to run ` nodeprobe - host < IP > <nl> - flush _ binary < Keyspace > ` on every node , as this will flush the <nl> - remaining data still left in memory to disk . Then it ' s recommended to <nl> - adjust the compaction threshold to it ' s original value . <nl> - <nl> - The example in CassandraBulkLoader . java is a sample Hadoop job that <nl> - inserts SuperColumns . It can be tweaked to work with normal Columns . <nl> - <nl> - You should construct your data you want to import as rows delimited by <nl> - a new line . You end up grouping by < Key > in the mapper , so that <nl> - the end result generates the data set into a column oriented <nl> - subset . Once you get to the reduce aspect , you can generate the <nl> - ColumnFamilies you want inserted , and send it to your nodes . <nl> - <nl> - For Cassandra 0 . 6 . 4 , we modified this example to wait for acks from <nl> - all Cassandra nodes for each row before proceeding to the next . This <nl> - means to keep Cassandra similarly busy you can either <nl> - 1 ) add more reducer tasks , <nl> - 2 ) remove the " wait for acks " block of code , <nl> - 3 ) parallelize the writing of rows to Cassandra , e . g . with an Executor . <nl> - <nl> - THIS CANNOT RUN ON THE SAME IP ADDRESS AS A CASSANDRA INSTANCE . <nl> diff - - git a / src / java / org / apache / cassandra / cql / Cql . g b / src / java / org / apache / cassandra / cql / Cql . g <nl> index 9e0a70f . . 9261e62 100644 <nl> - - - a / src / java / org / apache / cassandra / cql / Cql . g <nl> + + + b / src / java / org / apache / cassandra / cql / Cql . g <nl> @ @ - 498 , 8 + 498 , 10 @ @ K _ TRUNCATE : T R U N C A T E ; <nl> K _ DELETE : D E L E T E ; <nl> K _ IN : I N ; <nl> K _ CREATE : C R E A T E ; <nl> - K _ KEYSPACE : K E Y S P A C E ; <nl> - K _ COLUMNFAMILY : C O L U M N F A M I L Y ; <nl> + K _ KEYSPACE : ( K E Y S P A C E <nl> + | S C H E M A ) ; <nl> + K _ COLUMNFAMILY : ( C O L U M N F A M I L Y <nl> + | T A B L E ) ; <nl> K _ INDEX : I N D E X ; <nl> K _ ON : O N ; <nl> K _ DROP : D R O P ; <nl> diff - - git a / src / java / org / apache / cassandra / db / Table . java b / src / java / org / apache / cassandra / db / Table . java <nl> index 3ea4000 . . 80c9f36 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Table . java <nl> + + + b / src / java / org / apache / cassandra / db / Table . java <nl> @ @ - 55 , 8 + 55 , 9 @ @ public class Table <nl> { <nl> public static final String SYSTEM _ TABLE = " system " ; <nl> <nl> + public static final String SNAPSHOT _ SUBDIR _ NAME = " snapshots " ; <nl> + <nl> private static final Logger logger = LoggerFactory . getLogger ( Table . class ) ; <nl> - private static final String SNAPSHOT _ SUBDIR _ NAME = " snapshots " ; <nl> <nl> / * * <nl> * accesses to CFS . memtable should acquire this for thread safety . <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / Descriptor . java b / src / java / org / apache / cassandra / io / sstable / Descriptor . java <nl> index 5e945db . . 041b166 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / Descriptor . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / Descriptor . java <nl> @ @ - 26 , 6 + 26 , 7 @ @ import java . util . StringTokenizer ; <nl> <nl> import com . google . common . base . Objects ; <nl> <nl> + import org . apache . cassandra . db . Table ; <nl> import org . apache . cassandra . utils . Pair ; <nl> <nl> / * * <nl> @ @ - 131 , 7 + 132 , 7 @ @ public class Descriptor <nl> public static Pair < Descriptor , String > fromFilename ( File directory , String name ) <nl> { <nl> / / name of parent directory is keyspace name <nl> - String ksname = directory . getName ( ) ; <nl> + String ksname = extractKeyspaceName ( directory ) ; <nl> <nl> / / tokenize the filename <nl> StringTokenizer st = new StringTokenizer ( name , " - " ) ; <nl> @ @ - 165 , 6 + 166 , 43 @ @ public class Descriptor <nl> } <nl> <nl> / * * <nl> + * Extracts the keyspace name out of the directory name . Snapshot directories have a slightly different <nl> + * path structure and need to be treated differently . <nl> + * <nl> + * Regular path : " < ksname > / < cfname > - [ tmp - ] [ < version > - ] < gen > - < component > " <nl> + * Snapshot path : " < ksname > / snapshots / < snapshot - name > / < cfname > - [ tmp - ] [ < version > - ] < gen > - < component > " <nl> + * <nl> + * @ param directory a directory containing SSTables <nl> + * @ return the keyspace name <nl> + * / <nl> + public static String extractKeyspaceName ( File directory ) { <nl> + <nl> + if ( isSnapshotInPath ( directory ) ) <nl> + { <nl> + / / We need to move backwards . If this is a snapshot , first parent takes us to : <nl> + / / < ksname > / snapshots / and second call to parent takes us to < ksname > . <nl> + return directory . getParentFile ( ) . getParentFile ( ) . getName ( ) ; <nl> + } <nl> + return directory . getName ( ) ; <nl> + } <nl> + <nl> + / * * <nl> + * @ return < code > TRUE < / code > if this directory represents a snapshot directory . < code > FALSE < / code > otherwise . <nl> + * / <nl> + private static boolean isSnapshotInPath ( File directory ) { <nl> + File curDirectory = directory ; <nl> + while ( curDirectory ! = null ) <nl> + { <nl> + if ( curDirectory . getName ( ) . equals ( Table . SNAPSHOT _ SUBDIR _ NAME ) ) <nl> + return true ; <nl> + curDirectory = curDirectory . getParentFile ( ) ; <nl> + } <nl> + <nl> + / / The directory does not represent a snapshot directory . <nl> + return false ; <nl> + } <nl> + <nl> + / * * <nl> * @ return A clone of this descriptor with the given ' temporary ' status . <nl> * / <nl> public Descriptor asTemporary ( boolean temporary ) <nl> diff - - git a / src / java / org / apache / cassandra / service / RowRepairResolver . java b / src / java / org / apache / cassandra / service / RowRepairResolver . java <nl> index 6355187 . . 727d44b 100644 <nl> - - - a / src / java / org / apache / cassandra / service / RowRepairResolver . java <nl> + + + b / src / java / org / apache / cassandra / service / RowRepairResolver . java <nl> @ @ - 26 , 7 + 26 , 12 @ @ import java . util . ArrayList ; <nl> import java . util . List ; <nl> import java . util . Map ; <nl> <nl> + import org . apache . commons . collections . iterators . CollatingIterator ; <nl> + <nl> import org . apache . cassandra . db . * ; <nl> + import org . apache . cassandra . db . columniterator . IdentityQueryFilter ; <nl> + import org . apache . cassandra . db . filter . QueryFilter ; <nl> + import org . apache . cassandra . db . filter . QueryPath ; <nl> import org . apache . cassandra . gms . Gossiper ; <nl> import org . apache . cassandra . net . Message ; <nl> import org . apache . cassandra . net . MessagingService ; <nl> @ @ - 122 , 19 + 127 , 30 @ @ public class RowRepairResolver extends AbstractRowResolver <nl> ColumnFamily resolved = null ; <nl> for ( ColumnFamily cf : versions ) <nl> { <nl> - if ( cf ! = null ) <nl> - { <nl> - resolved = cf . cloneMe ( ) ; <nl> - break ; <nl> - } <nl> + if ( cf = = null ) <nl> + continue ; <nl> + <nl> + if ( resolved = = null ) <nl> + resolved = cf . cloneMeShallow ( ) ; <nl> + else <nl> + resolved . delete ( cf ) ; <nl> } <nl> if ( resolved = = null ) <nl> return null ; <nl> <nl> - for ( ColumnFamily cf : versions ) <nl> - resolved . resolve ( cf ) ; <nl> - <nl> - return resolved ; <nl> + / / mimic the collectCollatedColumn + removeDeleted path that getColumnFamily takes . <nl> + / / this will handle removing columns and subcolumns that are supressed by a row or <nl> + / / supercolumn tombstone . <nl> + QueryFilter filter = new QueryFilter ( null , new QueryPath ( resolved . metadata ( ) . cfName ) , new IdentityQueryFilter ( ) ) ; <nl> + CollatingIterator iter = new CollatingIterator ( resolved . metadata ( ) . comparator . columnComparator ) ; <nl> + for ( ColumnFamily version : versions ) <nl> + { <nl> + if ( version = = null ) <nl> + continue ; <nl> + iter . addIterator ( version . getColumnsMap ( ) . values ( ) . iterator ( ) ) ; <nl> + } <nl> + filter . collectCollatedColumns ( resolved , iter , Integer . MIN _ VALUE ) ; <nl> + return ColumnFamilyStore . removeDeleted ( resolved , Integer . MIN _ VALUE ) ; <nl> } <nl> <nl> public Row getData ( ) throws IOException <nl> diff - - git a / test / system / test _ cql . py b / test / system / test _ cql . py <nl> index f036cba . . 1a37827 100644 <nl> - - - a / test / system / test _ cql . py <nl> + + + b / test / system / test _ cql . py <nl> @ @ - 44 , 7 + 44 , 7 @ @ def load _ sample ( dbconn ) : <nl> WITH comparator = ascii AND default _ validation = ascii ; <nl> " " " ) <nl> dbconn . execute ( " " " <nl> - CREATE COLUMNFAMILY StandardString2 ( KEY text PRIMARY KEY ) <nl> + CREATE TABLE StandardString2 ( KEY text PRIMARY KEY ) <nl> WITH comparator = ascii AND default _ validation = ascii ; <nl> " " " ) <nl> dbconn . execute ( " " " <nl> @ @ - 56 , 7 + 56 , 7 @ @ def load _ sample ( dbconn ) : <nl> WITH comparator = bigint AND default _ validation = ascii ; <nl> " " " ) <nl> dbconn . execute ( " " " <nl> - CREATE COLUMNFAMILY StandardIntegerA ( KEY text PRIMARY KEY ) <nl> + CREATE TABLE StandardIntegerA ( KEY text PRIMARY KEY ) <nl> WITH comparator = varint AND default _ validation = ascii ; <nl> " " " ) <nl> dbconn . execute ( " " " <nl> @ @ - 76 , 7 + 76 , 7 @ @ def load _ sample ( dbconn ) : <nl> WITH comparator = ascii AND default _ validation = ascii ; <nl> " " " ) <nl> dbconn . execute ( " " " <nl> - CREATE COLUMNFAMILY CounterCF ( KEY text PRIMARY KEY , count _ me counter ) <nl> + CREATE TABLE CounterCF ( KEY text PRIMARY KEY , count _ me counter ) <nl> WITH comparator = ascii AND default _ validation = counter ; <nl> " " " ) <nl> dbconn . execute ( " CREATE INDEX ON IndexedA ( birthdate ) " ) <nl> @ @ - 414 , 7 + 414 , 7 @ @ class TestCql ( ThriftTester ) : <nl> " create a new keyspace " <nl> cursor = init ( ) <nl> cursor . execute ( " " " <nl> - CREATE KEYSPACE TestKeyspace42 WITH strategy _ options : DC1 = ' 1 ' <nl> + CREATE SCHEMA TestKeyspace42 WITH strategy _ options : DC1 = ' 1 ' <nl> AND strategy _ class = ' NetworkTopologyStrategy ' <nl> " " " ) <nl> <nl> @ @ - 436 , 7 + 436 , 7 @ @ class TestCql ( ThriftTester ) : <nl> # TODO : temporary ( until this can be done with CQL ) . <nl> thrift _ client . describe _ keyspace ( " Keyspace4Drop " ) <nl> <nl> - cursor . execute ( ' DROP KEYSPACE Keyspace4Drop ; ' ) <nl> + cursor . execute ( ' DROP SCHEMA Keyspace4Drop ; ' ) <nl> <nl> # Technically this should throw a ttypes . NotFound ( ) , but this is <nl> # temporary and so not worth requiring it on PYTHONPATH . <nl> @ @ - 448 , 7 + 448 , 7 @ @ class TestCql ( ThriftTester ) : <nl> " create a new column family " <nl> cursor = init ( ) <nl> cursor . execute ( " " " <nl> - CREATE KEYSPACE CreateCFKeyspace WITH strategy _ options : replication _ factor = ' 1 ' <nl> + CREATE SCHEMA CreateCFKeyspace WITH strategy _ options : replication _ factor = ' 1 ' <nl> AND strategy _ class = ' SimpleStrategy ' ; <nl> " " " ) <nl> cursor . execute ( " USE CreateCFKeyspace ; " ) <nl> diff - - git a / test / unit / org / apache / cassandra / Util . java b / test / unit / org / apache / cassandra / Util . java <nl> index 9096698 . . ec3cdb0 100644 <nl> - - - a / test / unit / org / apache / cassandra / Util . java <nl> + + + b / test / unit / org / apache / cassandra / Util . java <nl> @ @ - 58 , 6 + 58 , 14 @ @ public class Util <nl> return new Column ( ByteBufferUtil . bytes ( name ) , ByteBufferUtil . bytes ( value ) , timestamp ) ; <nl> } <nl> <nl> + public static SuperColumn superColumn ( ColumnFamily cf , String name , Column . . . columns ) <nl> + { <nl> + SuperColumn sc = new SuperColumn ( ByteBufferUtil . bytes ( name ) , cf . metadata ( ) . comparator ) ; <nl> + for ( Column c : columns ) <nl> + sc . addColumn ( c ) ; <nl> + return sc ; <nl> + } <nl> + <nl> public static Token token ( String key ) <nl> { <nl> return StorageService . getPartitioner ( ) . getToken ( ByteBufferUtil . bytes ( key ) ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / db / TableTest . java b / test / unit / org / apache / cassandra / db / TableTest . java <nl> index af32a0f . . 5c5ae04 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / TableTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / TableTest . java <nl> @ @ - 511 , 7 + 511 , 18 @ @ public class TableTest extends CleanupHelper <nl> <nl> public static void assertColumns ( ColumnFamily cf , String . . . columnNames ) <nl> { <nl> - Collection < IColumn > columns = cf = = null ? new TreeSet < IColumn > ( ) : cf . getSortedColumns ( ) ; <nl> + assertColumns ( ( IColumnContainer ) cf , columnNames ) ; <nl> + } <nl> + <nl> + public static void assertSubColumns ( ColumnFamily cf , String scName , String . . . columnNames ) <nl> + { <nl> + IColumnContainer sc = cf = = null ? null : ( ( IColumnContainer ) cf . getColumn ( ByteBufferUtil . bytes ( scName ) ) ) ; <nl> + assertColumns ( sc , columnNames ) ; <nl> + } <nl> + <nl> + public static void assertColumns ( IColumnContainer container , String . . . columnNames ) <nl> + { <nl> + Collection < IColumn > columns = container = = null ? new TreeSet < IColumn > ( ) : container . getSortedColumns ( ) ; <nl> List < String > L = new ArrayList < String > ( ) ; <nl> for ( IColumn column : columns ) <nl> { <nl> @ @ - 540 , 9 + 551 , 28 @ @ public class TableTest extends CleanupHelper <nl> <nl> assert Arrays . equals ( la , columnNames1 ) <nl> : String . format ( " Columns [ % s ( as string : % s ) ] ) ] is not expected [ % s ] " , <nl> - ( ( cf = = null ) ? " " : cf . getComparator ( ) . getColumnsString ( columns ) ) , <nl> + ( ( container = = null ) ? " " : container . getComparator ( ) . getColumnsString ( columns ) ) , <nl> lasb . toString ( ) , <nl> StringUtils . join ( columnNames1 , " , " ) ) ; <nl> } <nl> <nl> + public static void assertColumn ( ColumnFamily cf , String name , String value , long timestamp ) <nl> + { <nl> + assertColumn ( cf . getColumn ( ByteBufferUtil . bytes ( name ) ) , value , timestamp ) ; <nl> + } <nl> + <nl> + public static void assertSubColumn ( ColumnFamily cf , String scName , String name , String value , long timestamp ) <nl> + { <nl> + SuperColumn sc = ( SuperColumn ) cf . getColumn ( ByteBufferUtil . bytes ( scName ) ) ; <nl> + assertColumn ( sc . getSubColumn ( ByteBufferUtil . bytes ( name ) ) , value , timestamp ) ; <nl> + } <nl> + <nl> + public static void assertColumn ( IColumn column , String value , long timestamp ) <nl> + { <nl> + assertNotNull ( column ) ; <nl> + assertEquals ( 0 , ByteBufferUtil . compareUnsigned ( column . value ( ) , ByteBufferUtil . bytes ( value ) ) ) ; <nl> + assertEquals ( timestamp , column . timestamp ( ) ) ; <nl> + } <nl> + <nl> + <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java b / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java <nl> index 3855ca9 . . 1721218 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java <nl> @ @ - 25 , 6 + 25 , 8 @ @ import java . io . File ; <nl> <nl> import org . junit . Test ; <nl> <nl> + import org . apache . cassandra . db . Table ; <nl> + <nl> public class DescriptorTest <nl> { <nl> @ Test <nl> @ @ - 34 , 4 + 36 , 28 @ @ public class DescriptorTest <nl> assert descriptor . version . equals ( Descriptor . LEGACY _ VERSION ) ; <nl> assert descriptor . usesOldBloomFilter ; <nl> } <nl> + <nl> + @ Test <nl> + public void testExtractKeyspace ( ) <nl> + { <nl> + / / Test a path representing a SNAPSHOT directory <nl> + String dirPath = " Keyspace10 " + File . separator + Table . SNAPSHOT _ SUBDIR _ NAME + File . separator + System . currentTimeMillis ( ) ; <nl> + assertKeyspace ( " Keyspace10 " , dirPath ) ; <nl> + <nl> + / / Test a path representing a regular SSTables directory <nl> + dirPath = " Keyspace11 " ; <nl> + assertKeyspace ( " Keyspace11 " , dirPath ) ; <nl> + } <nl> + <nl> + private void assertKeyspace ( String expectedKsName , String dirPath ) { <nl> + File dir = new File ( dirPath ) ; <nl> + dir . deleteOnExit ( ) ; <nl> + <nl> + / / Create and check . <nl> + if ( ! dir . mkdirs ( ) ) <nl> + throw new RuntimeException ( " Unable to create directories : " + dirPath ) ; <nl> + <nl> + String currentKsName = Descriptor . extractKeyspaceName ( dir ) ; <nl> + assert expectedKsName . equals ( currentKsName ) ; <nl> + } <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / service / RowResolverTest . java b / test / unit / org / apache / cassandra / service / RowResolverTest . java <nl> index 4c847cc . . 9d944ae 100644 <nl> - - - a / test / unit / org / apache / cassandra / service / RowResolverTest . java <nl> + + + b / test / unit / org / apache / cassandra / service / RowResolverTest . java <nl> @ @ - 23 , 14 + 23 , 16 @ @ package org . apache . cassandra . service ; <nl> <nl> import java . util . Arrays ; <nl> <nl> - import org . apache . cassandra . SchemaLoader ; <nl> import org . junit . Test ; <nl> <nl> + import org . apache . cassandra . SchemaLoader ; <nl> import org . apache . cassandra . db . ColumnFamily ; <nl> + import org . apache . cassandra . db . SuperColumn ; <nl> <nl> - import static org . apache . cassandra . db . TableTest . assertColumns ; <nl> + import static junit . framework . Assert . * ; <nl> import static org . apache . cassandra . Util . column ; <nl> - import static junit . framework . Assert . assertNull ; <nl> + import static org . apache . cassandra . Util . superColumn ; <nl> + import static org . apache . cassandra . db . TableTest . * ; <nl> <nl> public class RowResolverTest extends SchemaLoader <nl> { <nl> @ @ - 93 , 4 + 95 , 110 @ @ public class RowResolverTest extends SchemaLoader <nl> { <nl> assertNull ( RowRepairResolver . resolveSuperset ( Arrays . < ColumnFamily > asList ( null , null ) ) ) ; <nl> } <nl> + <nl> + @ Test <nl> + public void testResolveDeleted ( ) <nl> + { <nl> + / / one CF with columns timestamped before a delete in another cf <nl> + ColumnFamily cf1 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; <nl> + cf1 . addColumn ( column ( " one " , " A " , 0 ) ) ; <nl> + <nl> + ColumnFamily cf2 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; <nl> + cf2 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 1 ) ; <nl> + <nl> + ColumnFamily resolved = RowRepairResolver . resolveSuperset ( Arrays . asList ( cf1 , cf2 ) ) ; <nl> + / / no columns in the cf <nl> + assertColumns ( resolved ) ; <nl> + assertTrue ( resolved . isMarkedForDelete ( ) ) ; <nl> + assertEquals ( 1 , resolved . getMarkedForDeleteAt ( ) ) ; <nl> + <nl> + ColumnFamily scf1 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; <nl> + scf1 . addColumn ( superColumn ( scf1 , " super - foo " , column ( " one " , " A " , 0 ) ) ) ; <nl> + <nl> + ColumnFamily scf2 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; <nl> + scf2 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 1 ) ; <nl> + <nl> + ColumnFamily superResolved = RowRepairResolver . resolveSuperset ( Arrays . asList ( scf1 , scf2 ) ) ; <nl> + / / no columns in the cf <nl> + assertColumns ( superResolved ) ; <nl> + assertTrue ( superResolved . isMarkedForDelete ( ) ) ; <nl> + assertEquals ( 1 , superResolved . getMarkedForDeleteAt ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testResolveDeletedSuper ( ) <nl> + { <nl> + / / subcolumn is newer than a tombstone on its parent , but not newer than the row deletion <nl> + ColumnFamily scf1 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; <nl> + SuperColumn sc = superColumn ( scf1 , " super - foo " , column ( " one " , " A " , 1 ) ) ; <nl> + sc . markForDeleteAt ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 0 ) ; <nl> + scf1 . addColumn ( sc ) ; <nl> + <nl> + ColumnFamily scf2 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; <nl> + scf2 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 2 ) ; <nl> + <nl> + ColumnFamily superResolved = RowRepairResolver . resolveSuperset ( Arrays . asList ( scf1 , scf2 ) ) ; <nl> + / / no columns in the cf <nl> + assertColumns ( superResolved ) ; <nl> + assertTrue ( superResolved . isMarkedForDelete ( ) ) ; <nl> + assertEquals ( 2 , superResolved . getMarkedForDeleteAt ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testResolveMultipleDeleted ( ) <nl> + { <nl> + / / deletes and columns with interleaved timestamp , with out of order return sequence <nl> + <nl> + ColumnFamily cf1 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; <nl> + cf1 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 0 ) ; <nl> + <nl> + / / these columns created after the previous deletion <nl> + ColumnFamily cf2 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; <nl> + cf2 . addColumn ( column ( " one " , " A " , 1 ) ) ; <nl> + cf2 . addColumn ( column ( " two " , " A " , 1 ) ) ; <nl> + <nl> + / / this column created after the next delete <nl> + ColumnFamily cf3 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; <nl> + cf3 . addColumn ( column ( " two " , " B " , 3 ) ) ; <nl> + <nl> + ColumnFamily cf4 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; <nl> + cf4 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 2 ) ; <nl> + <nl> + ColumnFamily resolved = RowRepairResolver . resolveSuperset ( Arrays . asList ( cf1 , cf2 , cf3 , cf4 ) ) ; <nl> + / / will have deleted marker and one column <nl> + assertColumns ( resolved , " two " ) ; <nl> + assertColumn ( resolved , " two " , " B " , 3 ) ; <nl> + assertTrue ( resolved . isMarkedForDelete ( ) ) ; <nl> + assertEquals ( 2 , resolved . getMarkedForDeleteAt ( ) ) ; <nl> + <nl> + <nl> + ColumnFamily scf1 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; <nl> + scf1 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 0 ) ; <nl> + <nl> + / / these columns created after the previous deletion <nl> + ColumnFamily scf2 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; <nl> + scf2 . addColumn ( superColumn ( scf2 , " super1 " , column ( " one " , " A " , 1 ) , column ( " two " , " A " , 1 ) ) ) ; <nl> + <nl> + / / these columns created after the next delete <nl> + ColumnFamily scf3 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; <nl> + scf3 . addColumn ( superColumn ( scf3 , " super1 " , column ( " two " , " B " , 3 ) ) ) ; <nl> + scf3 . addColumn ( superColumn ( scf3 , " super2 " , column ( " three " , " A " , 3 ) , column ( " four " , " A " , 3 ) ) ) ; <nl> + <nl> + ColumnFamily scf4 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; <nl> + scf4 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 2 ) ; <nl> + <nl> + ColumnFamily superResolved = RowRepairResolver . resolveSuperset ( Arrays . asList ( scf1 , scf2 , scf3 , scf4 ) ) ; <nl> + / / will have deleted marker and two super cols <nl> + assertColumns ( superResolved , " super1 " , " super2 " ) ; <nl> + <nl> + assertSubColumns ( superResolved , " super1 " , " two " ) ; <nl> + assertSubColumn ( superResolved , " super1 " , " two " , " B " , 3 ) ; <nl> + <nl> + assertSubColumns ( superResolved , " super2 " , " four " , " three " ) ; <nl> + assertSubColumn ( superResolved , " super2 " , " three " , " A " , 3 ) ; <nl> + assertSubColumn ( superResolved , " super2 " , " four " , " A " , 3 ) ; <nl> + <nl> + assertTrue ( superResolved . isMarkedForDelete ( ) ) ; <nl> + assertEquals ( 2 , superResolved . getMarkedForDeleteAt ( ) ) ; <nl> + } <nl> } <nl> diff - - git a / tools / stress / build . xml b / tools / stress / build . xml <nl> index f8995d2 . . a186ecf 100644 <nl> - - - a / tools / stress / build . xml <nl> + + + b / tools / stress / build . xml <nl> @ @ - 17 , 7 + 17 , 7 @ @ <nl> ~ specific language governing permissions and limitations <nl> ~ under the License . <nl> - - > <nl> - < project basedir = " . " default = " build " name = " stress " > <nl> + < project basedir = " . " default = " jar " name = " stress " > <nl> < property name = " cassandra . dir " value = " . . / . . " / > <nl> < property name = " cassandra . lib " value = " $ { cassandra . dir } / lib " / > <nl> < property name = " build . src " value = " $ { basedir } / src " / > <nl> @ @ - 49 , 9 + 49 , 19 @ @ <nl> < / target > <nl> <nl> < target name = " jar " depends = " build " > <nl> + < manifest file = " MANIFEST . MF " > <nl> + < attribute name = " Built - By " value = " Pavel Yaskevich " / > <nl> + < attribute name = " Main - Class " value = " org . apache . cassandra . stress . Stress " / > <nl> + < / manifest > <nl> + <nl> < mkdir dir = " $ { build . classes } / META - INF " / > <nl> - < jar jarfile = " $ { build . out } / $ { final . name } . jar " <nl> - basedir = " $ { build . classes } " / > <nl> + <nl> + < jar destfile = " $ { build . out } / $ { final . name } . jar " manifest = " MANIFEST . MF " > <nl> + < fileset dir = " $ { build . classes } " / > <nl> + < fileset dir = " $ { cassandra . dir } / build / classes / main " / > <nl> + < fileset dir = " $ { cassandra . dir } / build / classes / thrift " / > <nl> + < zipgroupfileset dir = " $ { cassandra . lib } " includes = " * . jar " / > <nl> + < / jar > <nl> < / target > <nl> <nl> < target name = " clean " >

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 78ea961 . . c8a4f21 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 2 . 2 . 6 
 + * Use canonical path for directory in SSTable descriptor ( CASSANDRA - 10587 ) 
 * Add cassandra - stress keystore option ( CASSANDRA - 9325 ) 
 * Fix out - of - space error treatment in memtable flushing ( CASSANDRA - 11448 ) . 
 * Dont mark sstables as repairing with sub range repairs ( CASSANDRA - 11451 ) 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / Descriptor . java b / src / java / org / apache / cassandra / io / sstable / Descriptor . java 
 index 9f259fe . . ed81616 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / Descriptor . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / Descriptor . java 
 @ @ - 18 , 6 + 18 , 8 @ @ 
 package org . apache . cassandra . io . sstable ; 
 
 import java . io . File ; 
 + import java . io . IOError ; 
 + import java . io . IOException ; 
 import java . util . ArrayDeque ; 
 import java . util . Deque ; 
 import java . util . StringTokenizer ; 
 @ @ - 58 , 7 + 60 , 7 @ @ public class Descriptor 
 } 
 } 
 
 - 
 + / * * canonicalized path to the directory where SSTable resides * / 
 public final File directory ; 
 / * * version has the following format : < code > [ a - z ] + < / code > * / 
 public final Version version ; 
 @ @ - 91 , 14 + 93 , 21 @ @ public class Descriptor 
 { 
 assert version ! = null & & directory ! = null & & ksname ! = null & & cfname ! = null & & formatType . info . getLatestVersion ( ) . getClass ( ) . equals ( version . getClass ( ) ) ; 
 this . version = version ; 
 - this . directory = directory ; 
 + try 
 + { 
 + this . directory = directory . getCanonicalFile ( ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + throw new IOError ( e ) ; 
 + } 
 this . ksname = ksname ; 
 this . cfname = cfname ; 
 this . generation = generation ; 
 this . type = temp ; 
 this . formatType = formatType ; 
 
 - hashCode = Objects . hashCode ( version , directory , generation , ksname , cfname , temp , formatType ) ; 
 + hashCode = Objects . hashCode ( version , this . directory , generation , ksname , cfname , temp , formatType ) ; 
 } 
 
 public Descriptor withGeneration ( int newGeneration ) 
 @ @ - 168 , 8 + 177 , 7 @ @ public class Descriptor 
 * / 
 public static Descriptor fromFilename ( String filename ) 
 { 
 - File file = new File ( filename ) ; 
 - return fromFilename ( file . getParentFile ( ) , file . getName ( ) , false ) . left ; 
 + return fromFilename ( filename , false ) ; 
 } 
 
 public static Descriptor fromFilename ( String filename , SSTableFormat . Type formatType ) 
 @ @ - 179 , 7 + 187 , 7 @ @ public class Descriptor 
 
 public static Descriptor fromFilename ( String filename , boolean skipComponent ) 
 { 
 - File file = new File ( filename ) ; 
 + File file = new File ( filename ) . getAbsoluteFile ( ) ; 
 return fromFilename ( file . getParentFile ( ) , file . getName ( ) , skipComponent ) . left ; 
 } 
 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java b / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java 
 index 70ab8ba . . 6354fc2 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java 
 @ @ - 84 , 16 + 84 , 22 @ @ public class DescriptorTest 
 / / secondary index 
 String idxName = " myidx " ; 
 File idxDir = new File ( dir . getAbsolutePath ( ) + File . separator + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName ) ; 
 - checkFromFilename ( new Descriptor ( idxDir , ksname , cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , 4 , Descriptor . Type . FINAL ) , false ) ; 
 + checkFromFilename ( new Descriptor ( idxDir , ksname , cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , 
 + 4 , Descriptor . Type . FINAL ) , false ) ; 
 / / secondary index tmp 
 - checkFromFilename ( new Descriptor ( idxDir , ksname , cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , 5 , Descriptor . Type . TEMP ) , false ) ; 
 + checkFromFilename ( new Descriptor ( idxDir , ksname , cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , 
 + 5 , Descriptor . Type . TEMP ) , false ) ; 
 
 / / legacy version 
 - checkFromFilename ( new Descriptor ( " ja " , dir , ksname , cfname , 1 , Descriptor . Type . FINAL , SSTableFormat . Type . LEGACY ) , false ) ; 
 + checkFromFilename ( new Descriptor ( " ja " , dir , ksname , cfname , 1 , Descriptor . Type . FINAL , 
 + SSTableFormat . Type . LEGACY ) , false ) ; 
 / / legacy tmp 
 - checkFromFilename ( new Descriptor ( " ja " , dir , ksname , cfname , 2 , Descriptor . Type . TEMP , SSTableFormat . Type . LEGACY ) , false ) ; 
 + checkFromFilename ( new Descriptor ( " ja " , dir , ksname , cfname , 2 , Descriptor . Type . TEMP , SSTableFormat . Type . LEGACY ) , 
 + false ) ; 
 / / legacy secondary index 
 - checkFromFilename ( new Descriptor ( " ja " , dir , ksname , cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , 3 , Descriptor . Type . FINAL , SSTableFormat . Type . LEGACY ) , false ) ; 
 + checkFromFilename ( new Descriptor ( " ja " , dir , ksname , 
 + cfname + Directories . SECONDARY _ INDEX _ NAME _ SEPARATOR + idxName , 3 , 
 + Descriptor . Type . FINAL , SSTableFormat . Type . LEGACY ) , false ) ; 
 } 
 
 private void checkFromFilename ( Descriptor original , boolean skipComponent ) 
 @ @ - 121 , 23 + 127 , 38 @ @ public class DescriptorTest 
 } 
 
 @ Test 
 + public void testEquality ( ) 
 + { 
 + / / Descriptor should be equal when parent directory points to the same directory 
 + File dir = new File ( " . " ) ; 
 + Descriptor desc1 = new Descriptor ( dir , " ks " , " cf " , 1 , Descriptor . Type . FINAL ) ; 
 + Descriptor desc2 = new Descriptor ( dir . getAbsoluteFile ( ) , " ks " , " cf " , 1 , Descriptor . Type . FINAL ) ; 
 + assertEquals ( desc1 , desc2 ) ; 
 + assertEquals ( desc1 . hashCode ( ) , desc2 . hashCode ( ) ) ; 
 + } 
 + 
 + @ Test 
 public void validateNames ( ) 
 { 
 
 - String names [ ] = { 
 - / * " system - schema _ keyspaces - ka - 1 - CompressionInfo . db " , " system - schema _ keyspaces - ka - 1 - Summary . db " , 
 - " system - schema _ keyspaces - ka - 1 - Data . db " , " system - schema _ keyspaces - ka - 1 - TOC . txt " , 
 - " system - schema _ keyspaces - ka - 1 - Digest . sha1 " , " system - schema _ keyspaces - ka - 2 - CompressionInfo . db " , 
 - " system - schema _ keyspaces - ka - 1 - Filter . db " , " system - schema _ keyspaces - ka - 2 - Data . db " , 
 - " system - schema _ keyspaces - ka - 1 - Index . db " , " system - schema _ keyspaces - ka - 2 - Digest . sha1 " , 
 - " system - schema _ keyspaces - ka - 1 - Statistics . db " , 
 - " system - schema _ keyspacest - tmp - ka - 1 - Data . db " , * / 
 - " system - schema _ keyspace - ka - 1 - " + SSTableFormat . Type . BIG . name + " - Data . db " 
 + String [ ] names = { 
 + / / old formats 
 + " system - schema _ keyspaces - jb - 1 - Data . db " , 
 + " system - schema _ keyspaces - tmp - jb - 1 - Data . db " , 
 + " system - schema _ keyspaces - ka - 1 - big - Data . db " , 
 + " system - schema _ keyspaces - tmp - ka - 1 - big - Data . db " , 
 + / / 2ndary index 
 + " keyspace1 - standard1 . idx1 - ka - 1 - big - Data . db " , 
 + / / new formats 
 + " la - 1 - big - Data . db " , 
 + " tmp - la - 1 - big - Data . db " , 
 + / / 2ndary index 
 + " . idx1 " + File . separator + " la - 1 - big - Data . db " , 
 } ; 
 
 for ( String name : names ) 
 { 
 - Descriptor d = Descriptor . fromFilename ( name ) ; 
 + assertNotNull ( Descriptor . fromFilename ( name ) ) ; 
 } 
 } 


NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 844a9f8 . . 1ca1a7a 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 8 , 6 + 8 , 7 @ @ 
 - improve JDBC spec compliance ( CASSANDRA - 2720 ) 
 - ALTER COLUMNFAMILY ( CASSANDRA - 1709 ) 
 - DROP INDEX ( CASSANDRA - 2617 ) 
 + - add SCHEMA / TABLE as aliases for KS / CF ( CASSANDRA - 2743 ) 
 * add support for comparator parameters and a generic ReverseType 
 ( CASSANDRA - 2355 ) 
 * add CompositeType and DynamicCompositeType ( CASSANDRA - 2231 ) 
 @ @ - 45 , 6 + 46 , 9 @ @ 
 by nio sockets ( CASSANDRA - 2654 ) 
 * restrict repair streaming to specific columnfamilies ( CASSANDRA - 2280 ) 
 * fix nodetool ring use with Ec2Snitch ( CASSANDRA - 2733 ) 
 + * fix removing columns and subcolumns that are supressed by a row or 
 + supercolumn tombstone during replica resolution ( CASSANDRA - 2590 ) 
 + * support sstable2json against snapshot sstables ( CASSANDRA - 2386 ) 
 
 
 0 . 8 . 0 - final 
 diff - - git a / examples / bmt / CassandraBulkLoader . java b / examples / bmt / CassandraBulkLoader . java 
 deleted file mode 100644 
 index 6fb1a91 . . 0000000 
 - - - a / examples / bmt / CassandraBulkLoader . java 
 + + + / dev / null 
 @ @ - 1 , 293 + 0 , 0 @ @ 
 - / * 
 - * Licensed to the Apache Software Foundation ( ASF ) under one 
 - * or more contributor license agreements . See the NOTICE file 
 - * distributed with this work for additional information 
 - * regarding copyright ownership . The ASF licenses this file 
 - * to you under the Apache License , Version 2 . 0 ( the 
 - * " License " ) ; you may not use this file except in compliance 
 - * with the License . You may obtain a copy of the License at 
 - * 
 - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 - * 
 - * Unless required by applicable law or agreed to in writing , software 
 - * distributed under the License is distributed on an " AS IS " BASIS , 
 - * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 - * See the License for the specific language governing permissions and 
 - * limitations under the License . 
 - * / 
 - 
 - / * * 
 - * Cassandra has a back door called the Binary Memtable . The purpose of this backdoor is to 
 - * mass import large amounts of data , without using the Thrift interface . 
 - * 
 - * Inserting data through the binary memtable , allows you to skip the commit log overhead , and an ack 
 - * from Thrift on every insert . The example below utilizes Hadoop to generate all the data necessary 
 - * to send to Cassandra , and sends it using the Binary Memtable interface . What Hadoop ends up doing is 
 - * creating the actual data that gets put into an SSTable as if you were using Thrift . With enough Hadoop nodes 
 - * inserting the data , the bottleneck at this point should become the network . 
 - * 
 - * We recommend adjusting the compaction threshold to 0 , while the import is running . After the import , you need 
 - * to run ` nodeprobe - host < IP > flush _ binary < Keyspace > ` on every node , as this will flush the remaining data still left 
 - * in memory to disk . Then it ' s recommended to adjust the compaction threshold to it ' s original value . 
 - * 
 - * The example below is a sample Hadoop job , and it inserts SuperColumns . It can be tweaked to work with normal Columns . 
 - * 
 - * You should construct your data you want to import as rows delimited by a new line . You end up grouping by < Key > 
 - * in the mapper , so that the end result generates the data set into a column oriented subset . Once you get to the 
 - * reduce aspect , you can generate the ColumnFamilies you want inserted , and send it to your nodes . 
 - * 
 - * For Cassandra 0 . 6 . 4 , we modified this example to wait for acks from all Cassandra nodes for each row 
 - * before proceeding to the next . This means to keep Cassandra similarly busy you can either 
 - * 1 ) add more reducer tasks , 
 - * 2 ) remove the " wait for acks " block of code , 
 - * 3 ) parallelize the writing of rows to Cassandra , e . g . with an Executor . 
 - * 
 - * THIS CANNOT RUN ON THE SAME IP ADDRESS AS A CASSANDRA INSTANCE . 
 - * / 
 - 
 - package org . apache . cassandra . bulkloader ; 
 - 
 - import java . io . IOException ; 
 - import java . net . InetAddress ; 
 - import java . net . URI ; 
 - import java . net . URISyntaxException ; 
 - import java . nio . ByteBuffer ; 
 - import java . util . ArrayList ; 
 - import java . util . Iterator ; 
 - import java . util . LinkedList ; 
 - import java . util . List ; 
 - import java . util . concurrent . TimeUnit ; 
 - import java . util . concurrent . TimeoutException ; 
 - 
 - import com . google . common . base . Charsets ; 
 - 
 - import org . apache . cassandra . config . CFMetaData ; 
 - import org . apache . cassandra . config . ConfigurationException ; 
 - import org . apache . cassandra . config . DatabaseDescriptor ; 
 - import org . apache . cassandra . db . Column ; 
 - import org . apache . cassandra . db . ColumnFamily ; 
 - import org . apache . cassandra . db . ColumnFamilyType ; 
 - import org . apache . cassandra . db . RowMutation ; 
 - import org . apache . cassandra . db . filter . QueryPath ; 
 - import org . apache . cassandra . io . util . DataOutputBuffer ; 
 - import org . apache . cassandra . net . IAsyncResult ; 
 - import org . apache . cassandra . net . Message ; 
 - import org . apache . cassandra . net . MessagingService ; 
 - import org . apache . cassandra . service . StorageService ; 
 - import org . apache . cassandra . utils . FBUtilities ; 
 - import org . apache . cassandra . utils . ByteBufferUtil ; 
 - import org . apache . hadoop . filecache . DistributedCache ; 
 - import org . apache . hadoop . fs . Path ; 
 - import org . apache . hadoop . io . Text ; 
 - import org . apache . hadoop . mapred . * ; 
 - 
 - public class CassandraBulkLoader { 
 - public static class Map extends MapReduceBase implements Mapper < Text , Text , Text , Text > { 
 - public void map ( Text key , Text value , OutputCollector < Text , Text > output , Reporter reporter ) throws IOException { 
 - / / This is a simple key / value mapper . 
 - output . collect ( key , value ) ; 
 - } 
 - } 
 - 
 - public static class Reduce extends MapReduceBase implements Reducer < Text , Text , Text , Text > { 
 - private Path [ ] localFiles ; 
 - private JobConf jobconf ; 
 - 
 - public void configure ( JobConf job ) { 
 - this . jobconf = job ; 
 - String cassConfig ; 
 - 
 - / / Get the cached files 
 - try 
 - { 
 - localFiles = DistributedCache . getLocalCacheFiles ( job ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - cassConfig = localFiles [ 0 ] . getParent ( ) . toString ( ) ; 
 - 
 - System . setProperty ( " storage - config " , cassConfig ) ; 
 - 
 - try 
 - { 
 - StorageService . instance . initClient ( ) ; 
 - } 
 - catch ( Exception e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - try 
 - { 
 - Thread . sleep ( 10 * 1000 ) ; 
 - } 
 - catch ( InterruptedException e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - } 
 - 
 - public void close ( ) 
 - { 
 - try 
 - { 
 - / / release the cache 
 - DistributedCache . releaseCache ( new URI ( " / cassandra / storage - conf . xml # storage - conf . xml " ) , this . jobconf ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - catch ( URISyntaxException e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - try 
 - { 
 - / / Sleep just in case the number of keys we send over is small 
 - Thread . sleep ( 3 * 1000 ) ; 
 - } 
 - catch ( InterruptedException e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - StorageService . instance . stopClient ( ) ; 
 - } 
 - 
 - public void reduce ( Text key , Iterator < Text > values , OutputCollector < Text , Text > output , Reporter reporter ) throws IOException 
 - { 
 - ColumnFamily columnFamily ; 
 - String keyspace = " Keyspace1 " ; 
 - String cfName = " Super1 " ; 
 - Message message ; 
 - List < ColumnFamily > columnFamilies ; 
 - columnFamilies = new LinkedList < ColumnFamily > ( ) ; 
 - String line ; 
 - 
 - / * Create a column family * / 
 - columnFamily = ColumnFamily . create ( keyspace , cfName ) ; 
 - while ( values . hasNext ( ) ) { 
 - / / Split the value ( line based on your own delimiter ) 
 - line = values . next ( ) . toString ( ) ; 
 - String [ ] fields = line . split ( " \ 1 " ) ; 
 - String SuperColumnName = fields [ 1 ] ; 
 - String ColumnName = fields [ 2 ] ; 
 - String ColumnValue = fields [ 3 ] ; 
 - int timestamp = 0 ; 
 - columnFamily . addColumn ( new QueryPath ( cfName , 
 - ByteBufferUtil . bytes ( SuperColumnName ) , 
 - ByteBufferUtil . bytes ( ColumnName ) ) , 
 - ByteBufferUtil . bytes ( ColumnValue ) , 
 - timestamp ) ; 
 - } 
 - 
 - columnFamilies . add ( columnFamily ) ; 
 - 
 - / * Get serialized message to send to cluster * / 
 - message = createMessage ( keyspace , key . getBytes ( ) , cfName , columnFamilies ) ; 
 - List < IAsyncResult > results = new ArrayList < IAsyncResult > ( ) ; 
 - for ( InetAddress endpoint : StorageService . instance . getNaturalEndpoints ( keyspace , ByteBufferUtil . bytes ( key ) ) ) 
 - { 
 - / * Send message to end point * / 
 - results . add ( MessagingService . instance ( ) . sendRR ( message , endpoint ) ) ; 
 - } 
 - / * wait for acks * / 
 - for ( IAsyncResult result : results ) 
 - { 
 - try 
 - { 
 - result . get ( DatabaseDescriptor . getRpcTimeout ( ) , TimeUnit . MILLISECONDS ) ; 
 - } 
 - catch ( TimeoutException e ) 
 - { 
 - / / you should probably add retry logic here 
 - throw new RuntimeException ( e ) ; 
 - } 
 - } 
 - 
 - output . collect ( key , new Text ( " inserted into Cassandra node ( s ) " ) ) ; 
 - } 
 - } 
 - 
 - public static void runJob ( String [ ] args ) 
 - { 
 - JobConf conf = new JobConf ( CassandraBulkLoader . class ) ; 
 - 
 - if ( args . length > = 4 ) 
 - { 
 - conf . setNumReduceTasks ( new Integer ( args [ 3 ] ) ) ; 
 - } 
 - 
 - try 
 - { 
 - / / We store the cassandra storage - conf . xml on the HDFS cluster 
 - DistributedCache . addCacheFile ( new URI ( " / cassandra / storage - conf . xml # storage - conf . xml " ) , conf ) ; 
 - } 
 - catch ( URISyntaxException e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - conf . setInputFormat ( KeyValueTextInputFormat . class ) ; 
 - conf . setJobName ( " CassandraBulkLoader _ v2 " ) ; 
 - conf . setMapperClass ( Map . class ) ; 
 - conf . setReducerClass ( Reduce . class ) ; 
 - 
 - conf . setOutputKeyClass ( Text . class ) ; 
 - conf . setOutputValueClass ( Text . class ) ; 
 - 
 - FileInputFormat . setInputPaths ( conf , new Path ( args [ 1 ] ) ) ; 
 - FileOutputFormat . setOutputPath ( conf , new Path ( args [ 2 ] ) ) ; 
 - try 
 - { 
 - JobClient . runJob ( conf ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - } 
 - 
 - public static Message createMessage ( String keyspace , byte [ ] key , String columnFamily , List < ColumnFamily > columnFamilies ) 
 - { 
 - ColumnFamily baseColumnFamily ; 
 - DataOutputBuffer bufOut = new DataOutputBuffer ( ) ; 
 - RowMutation rm ; 
 - Message message ; 
 - Column column ; 
 - 
 - / * Get the first column family from list , this is just to get past validation * / 
 - baseColumnFamily = new ColumnFamily ( ColumnFamilyType . Standard , 
 - DatabaseDescriptor . getComparator ( keyspace , columnFamily ) , 
 - DatabaseDescriptor . getSubComparator ( keyspace , columnFamily ) , 
 - CFMetaData . getId ( keyspace , columnFamily ) ) ; 
 - 
 - for ( ColumnFamily cf : columnFamilies ) { 
 - bufOut . reset ( ) ; 
 - ColumnFamily . serializer ( ) . serializeWithIndexes ( cf , bufOut ) ; 
 - byte [ ] data = new byte [ bufOut . getLength ( ) ] ; 
 - System . arraycopy ( bufOut . getData ( ) , 0 , data , 0 , bufOut . getLength ( ) ) ; 
 - 
 - column = new Column ( FBUtilities . toByteBuffer ( cf . id ( ) ) , ByteBuffer . wrap ( data ) , 0 ) ; 
 - baseColumnFamily . addColumn ( column ) ; 
 - } 
 - rm = new RowMutation ( keyspace , ByteBuffer . wrap ( key ) ) ; 
 - rm . add ( baseColumnFamily ) ; 
 - 
 - try 
 - { 
 - / * Make message * / 
 - message = rm . makeRowMutationMessage ( StorageService . Verb . BINARY , MessagingService . version _ ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - 
 - return message ; 
 - } 
 - public static void main ( String [ ] args ) throws Exception 
 - { 
 - runJob ( args ) ; 
 - } 
 - } 
 diff - - git a / examples / bmt / README . txt b / examples / bmt / README . txt 
 deleted file mode 100644 
 index 78ba11c . . 0000000 
 - - - a / examples / bmt / README . txt 
 + + + / dev / null 
 @ @ - 1 , 34 + 0 , 0 @ @ 
 - This is an example for the deprecated BinaryMemtable bulk - load interface . 
 - 
 - Inserting data through the binary memtable , allows you to skip the 
 - commit log overhead , and an ack from Thrift on every insert . The 
 - example below utilizes Hadoop to generate all the data necessary to 
 - send to Cassandra , and sends it using the Binary Memtable 
 - interface . What Hadoop ends up doing is creating the actual data that 
 - gets put into an SSTable as if you were using Thrift . With enough 
 - Hadoop nodes inserting the data , the bottleneck at this point should 
 - become the network . 
 - 
 - We recommend adjusting the compaction threshold to 0 while the import 
 - is running . After the import , you need to run ` nodeprobe - host < IP > 
 - flush _ binary < Keyspace > ` on every node , as this will flush the 
 - remaining data still left in memory to disk . Then it ' s recommended to 
 - adjust the compaction threshold to it ' s original value . 
 - 
 - The example in CassandraBulkLoader . java is a sample Hadoop job that 
 - inserts SuperColumns . It can be tweaked to work with normal Columns . 
 - 
 - You should construct your data you want to import as rows delimited by 
 - a new line . You end up grouping by < Key > in the mapper , so that 
 - the end result generates the data set into a column oriented 
 - subset . Once you get to the reduce aspect , you can generate the 
 - ColumnFamilies you want inserted , and send it to your nodes . 
 - 
 - For Cassandra 0 . 6 . 4 , we modified this example to wait for acks from 
 - all Cassandra nodes for each row before proceeding to the next . This 
 - means to keep Cassandra similarly busy you can either 
 - 1 ) add more reducer tasks , 
 - 2 ) remove the " wait for acks " block of code , 
 - 3 ) parallelize the writing of rows to Cassandra , e . g . with an Executor . 
 - 
 - THIS CANNOT RUN ON THE SAME IP ADDRESS AS A CASSANDRA INSTANCE . 
 diff - - git a / src / java / org / apache / cassandra / cql / Cql . g b / src / java / org / apache / cassandra / cql / Cql . g 
 index 9e0a70f . . 9261e62 100644 
 - - - a / src / java / org / apache / cassandra / cql / Cql . g 
 + + + b / src / java / org / apache / cassandra / cql / Cql . g 
 @ @ - 498 , 8 + 498 , 10 @ @ K _ TRUNCATE : T R U N C A T E ; 
 K _ DELETE : D E L E T E ; 
 K _ IN : I N ; 
 K _ CREATE : C R E A T E ; 
 - K _ KEYSPACE : K E Y S P A C E ; 
 - K _ COLUMNFAMILY : C O L U M N F A M I L Y ; 
 + K _ KEYSPACE : ( K E Y S P A C E 
 + | S C H E M A ) ; 
 + K _ COLUMNFAMILY : ( C O L U M N F A M I L Y 
 + | T A B L E ) ; 
 K _ INDEX : I N D E X ; 
 K _ ON : O N ; 
 K _ DROP : D R O P ; 
 diff - - git a / src / java / org / apache / cassandra / db / Table . java b / src / java / org / apache / cassandra / db / Table . java 
 index 3ea4000 . . 80c9f36 100644 
 - - - a / src / java / org / apache / cassandra / db / Table . java 
 + + + b / src / java / org / apache / cassandra / db / Table . java 
 @ @ - 55 , 8 + 55 , 9 @ @ public class Table 
 { 
 public static final String SYSTEM _ TABLE = " system " ; 
 
 + public static final String SNAPSHOT _ SUBDIR _ NAME = " snapshots " ; 
 + 
 private static final Logger logger = LoggerFactory . getLogger ( Table . class ) ; 
 - private static final String SNAPSHOT _ SUBDIR _ NAME = " snapshots " ; 
 
 / * * 
 * accesses to CFS . memtable should acquire this for thread safety . 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / Descriptor . java b / src / java / org / apache / cassandra / io / sstable / Descriptor . java 
 index 5e945db . . 041b166 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / Descriptor . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / Descriptor . java 
 @ @ - 26 , 6 + 26 , 7 @ @ import java . util . StringTokenizer ; 
 
 import com . google . common . base . Objects ; 
 
 + import org . apache . cassandra . db . Table ; 
 import org . apache . cassandra . utils . Pair ; 
 
 / * * 
 @ @ - 131 , 7 + 132 , 7 @ @ public class Descriptor 
 public static Pair < Descriptor , String > fromFilename ( File directory , String name ) 
 { 
 / / name of parent directory is keyspace name 
 - String ksname = directory . getName ( ) ; 
 + String ksname = extractKeyspaceName ( directory ) ; 
 
 / / tokenize the filename 
 StringTokenizer st = new StringTokenizer ( name , " - " ) ; 
 @ @ - 165 , 6 + 166 , 43 @ @ public class Descriptor 
 } 
 
 / * * 
 + * Extracts the keyspace name out of the directory name . Snapshot directories have a slightly different 
 + * path structure and need to be treated differently . 
 + * 
 + * Regular path : " < ksname > / < cfname > - [ tmp - ] [ < version > - ] < gen > - < component > " 
 + * Snapshot path : " < ksname > / snapshots / < snapshot - name > / < cfname > - [ tmp - ] [ < version > - ] < gen > - < component > " 
 + * 
 + * @ param directory a directory containing SSTables 
 + * @ return the keyspace name 
 + * / 
 + public static String extractKeyspaceName ( File directory ) { 
 + 
 + if ( isSnapshotInPath ( directory ) ) 
 + { 
 + / / We need to move backwards . If this is a snapshot , first parent takes us to : 
 + / / < ksname > / snapshots / and second call to parent takes us to < ksname > . 
 + return directory . getParentFile ( ) . getParentFile ( ) . getName ( ) ; 
 + } 
 + return directory . getName ( ) ; 
 + } 
 + 
 + / * * 
 + * @ return < code > TRUE < / code > if this directory represents a snapshot directory . < code > FALSE < / code > otherwise . 
 + * / 
 + private static boolean isSnapshotInPath ( File directory ) { 
 + File curDirectory = directory ; 
 + while ( curDirectory ! = null ) 
 + { 
 + if ( curDirectory . getName ( ) . equals ( Table . SNAPSHOT _ SUBDIR _ NAME ) ) 
 + return true ; 
 + curDirectory = curDirectory . getParentFile ( ) ; 
 + } 
 + 
 + / / The directory does not represent a snapshot directory . 
 + return false ; 
 + } 
 + 
 + / * * 
 * @ return A clone of this descriptor with the given ' temporary ' status . 
 * / 
 public Descriptor asTemporary ( boolean temporary ) 
 diff - - git a / src / java / org / apache / cassandra / service / RowRepairResolver . java b / src / java / org / apache / cassandra / service / RowRepairResolver . java 
 index 6355187 . . 727d44b 100644 
 - - - a / src / java / org / apache / cassandra / service / RowRepairResolver . java 
 + + + b / src / java / org / apache / cassandra / service / RowRepairResolver . java 
 @ @ - 26 , 7 + 26 , 12 @ @ import java . util . ArrayList ; 
 import java . util . List ; 
 import java . util . Map ; 
 
 + import org . apache . commons . collections . iterators . CollatingIterator ; 
 + 
 import org . apache . cassandra . db . * ; 
 + import org . apache . cassandra . db . columniterator . IdentityQueryFilter ; 
 + import org . apache . cassandra . db . filter . QueryFilter ; 
 + import org . apache . cassandra . db . filter . QueryPath ; 
 import org . apache . cassandra . gms . Gossiper ; 
 import org . apache . cassandra . net . Message ; 
 import org . apache . cassandra . net . MessagingService ; 
 @ @ - 122 , 19 + 127 , 30 @ @ public class RowRepairResolver extends AbstractRowResolver 
 ColumnFamily resolved = null ; 
 for ( ColumnFamily cf : versions ) 
 { 
 - if ( cf ! = null ) 
 - { 
 - resolved = cf . cloneMe ( ) ; 
 - break ; 
 - } 
 + if ( cf = = null ) 
 + continue ; 
 + 
 + if ( resolved = = null ) 
 + resolved = cf . cloneMeShallow ( ) ; 
 + else 
 + resolved . delete ( cf ) ; 
 } 
 if ( resolved = = null ) 
 return null ; 
 
 - for ( ColumnFamily cf : versions ) 
 - resolved . resolve ( cf ) ; 
 - 
 - return resolved ; 
 + / / mimic the collectCollatedColumn + removeDeleted path that getColumnFamily takes . 
 + / / this will handle removing columns and subcolumns that are supressed by a row or 
 + / / supercolumn tombstone . 
 + QueryFilter filter = new QueryFilter ( null , new QueryPath ( resolved . metadata ( ) . cfName ) , new IdentityQueryFilter ( ) ) ; 
 + CollatingIterator iter = new CollatingIterator ( resolved . metadata ( ) . comparator . columnComparator ) ; 
 + for ( ColumnFamily version : versions ) 
 + { 
 + if ( version = = null ) 
 + continue ; 
 + iter . addIterator ( version . getColumnsMap ( ) . values ( ) . iterator ( ) ) ; 
 + } 
 + filter . collectCollatedColumns ( resolved , iter , Integer . MIN _ VALUE ) ; 
 + return ColumnFamilyStore . removeDeleted ( resolved , Integer . MIN _ VALUE ) ; 
 } 
 
 public Row getData ( ) throws IOException 
 diff - - git a / test / system / test _ cql . py b / test / system / test _ cql . py 
 index f036cba . . 1a37827 100644 
 - - - a / test / system / test _ cql . py 
 + + + b / test / system / test _ cql . py 
 @ @ - 44 , 7 + 44 , 7 @ @ def load _ sample ( dbconn ) : 
 WITH comparator = ascii AND default _ validation = ascii ; 
 " " " ) 
 dbconn . execute ( " " " 
 - CREATE COLUMNFAMILY StandardString2 ( KEY text PRIMARY KEY ) 
 + CREATE TABLE StandardString2 ( KEY text PRIMARY KEY ) 
 WITH comparator = ascii AND default _ validation = ascii ; 
 " " " ) 
 dbconn . execute ( " " " 
 @ @ - 56 , 7 + 56 , 7 @ @ def load _ sample ( dbconn ) : 
 WITH comparator = bigint AND default _ validation = ascii ; 
 " " " ) 
 dbconn . execute ( " " " 
 - CREATE COLUMNFAMILY StandardIntegerA ( KEY text PRIMARY KEY ) 
 + CREATE TABLE StandardIntegerA ( KEY text PRIMARY KEY ) 
 WITH comparator = varint AND default _ validation = ascii ; 
 " " " ) 
 dbconn . execute ( " " " 
 @ @ - 76 , 7 + 76 , 7 @ @ def load _ sample ( dbconn ) : 
 WITH comparator = ascii AND default _ validation = ascii ; 
 " " " ) 
 dbconn . execute ( " " " 
 - CREATE COLUMNFAMILY CounterCF ( KEY text PRIMARY KEY , count _ me counter ) 
 + CREATE TABLE CounterCF ( KEY text PRIMARY KEY , count _ me counter ) 
 WITH comparator = ascii AND default _ validation = counter ; 
 " " " ) 
 dbconn . execute ( " CREATE INDEX ON IndexedA ( birthdate ) " ) 
 @ @ - 414 , 7 + 414 , 7 @ @ class TestCql ( ThriftTester ) : 
 " create a new keyspace " 
 cursor = init ( ) 
 cursor . execute ( " " " 
 - CREATE KEYSPACE TestKeyspace42 WITH strategy _ options : DC1 = ' 1 ' 
 + CREATE SCHEMA TestKeyspace42 WITH strategy _ options : DC1 = ' 1 ' 
 AND strategy _ class = ' NetworkTopologyStrategy ' 
 " " " ) 
 
 @ @ - 436 , 7 + 436 , 7 @ @ class TestCql ( ThriftTester ) : 
 # TODO : temporary ( until this can be done with CQL ) . 
 thrift _ client . describe _ keyspace ( " Keyspace4Drop " ) 
 
 - cursor . execute ( ' DROP KEYSPACE Keyspace4Drop ; ' ) 
 + cursor . execute ( ' DROP SCHEMA Keyspace4Drop ; ' ) 
 
 # Technically this should throw a ttypes . NotFound ( ) , but this is 
 # temporary and so not worth requiring it on PYTHONPATH . 
 @ @ - 448 , 7 + 448 , 7 @ @ class TestCql ( ThriftTester ) : 
 " create a new column family " 
 cursor = init ( ) 
 cursor . execute ( " " " 
 - CREATE KEYSPACE CreateCFKeyspace WITH strategy _ options : replication _ factor = ' 1 ' 
 + CREATE SCHEMA CreateCFKeyspace WITH strategy _ options : replication _ factor = ' 1 ' 
 AND strategy _ class = ' SimpleStrategy ' ; 
 " " " ) 
 cursor . execute ( " USE CreateCFKeyspace ; " ) 
 diff - - git a / test / unit / org / apache / cassandra / Util . java b / test / unit / org / apache / cassandra / Util . java 
 index 9096698 . . ec3cdb0 100644 
 - - - a / test / unit / org / apache / cassandra / Util . java 
 + + + b / test / unit / org / apache / cassandra / Util . java 
 @ @ - 58 , 6 + 58 , 14 @ @ public class Util 
 return new Column ( ByteBufferUtil . bytes ( name ) , ByteBufferUtil . bytes ( value ) , timestamp ) ; 
 } 
 
 + public static SuperColumn superColumn ( ColumnFamily cf , String name , Column . . . columns ) 
 + { 
 + SuperColumn sc = new SuperColumn ( ByteBufferUtil . bytes ( name ) , cf . metadata ( ) . comparator ) ; 
 + for ( Column c : columns ) 
 + sc . addColumn ( c ) ; 
 + return sc ; 
 + } 
 + 
 public static Token token ( String key ) 
 { 
 return StorageService . getPartitioner ( ) . getToken ( ByteBufferUtil . bytes ( key ) ) ; 
 diff - - git a / test / unit / org / apache / cassandra / db / TableTest . java b / test / unit / org / apache / cassandra / db / TableTest . java 
 index af32a0f . . 5c5ae04 100644 
 - - - a / test / unit / org / apache / cassandra / db / TableTest . java 
 + + + b / test / unit / org / apache / cassandra / db / TableTest . java 
 @ @ - 511 , 7 + 511 , 18 @ @ public class TableTest extends CleanupHelper 
 
 public static void assertColumns ( ColumnFamily cf , String . . . columnNames ) 
 { 
 - Collection < IColumn > columns = cf = = null ? new TreeSet < IColumn > ( ) : cf . getSortedColumns ( ) ; 
 + assertColumns ( ( IColumnContainer ) cf , columnNames ) ; 
 + } 
 + 
 + public static void assertSubColumns ( ColumnFamily cf , String scName , String . . . columnNames ) 
 + { 
 + IColumnContainer sc = cf = = null ? null : ( ( IColumnContainer ) cf . getColumn ( ByteBufferUtil . bytes ( scName ) ) ) ; 
 + assertColumns ( sc , columnNames ) ; 
 + } 
 + 
 + public static void assertColumns ( IColumnContainer container , String . . . columnNames ) 
 + { 
 + Collection < IColumn > columns = container = = null ? new TreeSet < IColumn > ( ) : container . getSortedColumns ( ) ; 
 List < String > L = new ArrayList < String > ( ) ; 
 for ( IColumn column : columns ) 
 { 
 @ @ - 540 , 9 + 551 , 28 @ @ public class TableTest extends CleanupHelper 
 
 assert Arrays . equals ( la , columnNames1 ) 
 : String . format ( " Columns [ % s ( as string : % s ) ] ) ] is not expected [ % s ] " , 
 - ( ( cf = = null ) ? " " : cf . getComparator ( ) . getColumnsString ( columns ) ) , 
 + ( ( container = = null ) ? " " : container . getComparator ( ) . getColumnsString ( columns ) ) , 
 lasb . toString ( ) , 
 StringUtils . join ( columnNames1 , " , " ) ) ; 
 } 
 
 + public static void assertColumn ( ColumnFamily cf , String name , String value , long timestamp ) 
 + { 
 + assertColumn ( cf . getColumn ( ByteBufferUtil . bytes ( name ) ) , value , timestamp ) ; 
 + } 
 + 
 + public static void assertSubColumn ( ColumnFamily cf , String scName , String name , String value , long timestamp ) 
 + { 
 + SuperColumn sc = ( SuperColumn ) cf . getColumn ( ByteBufferUtil . bytes ( scName ) ) ; 
 + assertColumn ( sc . getSubColumn ( ByteBufferUtil . bytes ( name ) ) , value , timestamp ) ; 
 + } 
 + 
 + public static void assertColumn ( IColumn column , String value , long timestamp ) 
 + { 
 + assertNotNull ( column ) ; 
 + assertEquals ( 0 , ByteBufferUtil . compareUnsigned ( column . value ( ) , ByteBufferUtil . bytes ( value ) ) ) ; 
 + assertEquals ( timestamp , column . timestamp ( ) ) ; 
 + } 
 + 
 + 
 } 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java b / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java 
 index 3855ca9 . . 1721218 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / DescriptorTest . java 
 @ @ - 25 , 6 + 25 , 8 @ @ import java . io . File ; 
 
 import org . junit . Test ; 
 
 + import org . apache . cassandra . db . Table ; 
 + 
 public class DescriptorTest 
 { 
 @ Test 
 @ @ - 34 , 4 + 36 , 28 @ @ public class DescriptorTest 
 assert descriptor . version . equals ( Descriptor . LEGACY _ VERSION ) ; 
 assert descriptor . usesOldBloomFilter ; 
 } 
 + 
 + @ Test 
 + public void testExtractKeyspace ( ) 
 + { 
 + / / Test a path representing a SNAPSHOT directory 
 + String dirPath = " Keyspace10 " + File . separator + Table . SNAPSHOT _ SUBDIR _ NAME + File . separator + System . currentTimeMillis ( ) ; 
 + assertKeyspace ( " Keyspace10 " , dirPath ) ; 
 + 
 + / / Test a path representing a regular SSTables directory 
 + dirPath = " Keyspace11 " ; 
 + assertKeyspace ( " Keyspace11 " , dirPath ) ; 
 + } 
 + 
 + private void assertKeyspace ( String expectedKsName , String dirPath ) { 
 + File dir = new File ( dirPath ) ; 
 + dir . deleteOnExit ( ) ; 
 + 
 + / / Create and check . 
 + if ( ! dir . mkdirs ( ) ) 
 + throw new RuntimeException ( " Unable to create directories : " + dirPath ) ; 
 + 
 + String currentKsName = Descriptor . extractKeyspaceName ( dir ) ; 
 + assert expectedKsName . equals ( currentKsName ) ; 
 + } 
 } 
 diff - - git a / test / unit / org / apache / cassandra / service / RowResolverTest . java b / test / unit / org / apache / cassandra / service / RowResolverTest . java 
 index 4c847cc . . 9d944ae 100644 
 - - - a / test / unit / org / apache / cassandra / service / RowResolverTest . java 
 + + + b / test / unit / org / apache / cassandra / service / RowResolverTest . java 
 @ @ - 23 , 14 + 23 , 16 @ @ package org . apache . cassandra . service ; 
 
 import java . util . Arrays ; 
 
 - import org . apache . cassandra . SchemaLoader ; 
 import org . junit . Test ; 
 
 + import org . apache . cassandra . SchemaLoader ; 
 import org . apache . cassandra . db . ColumnFamily ; 
 + import org . apache . cassandra . db . SuperColumn ; 
 
 - import static org . apache . cassandra . db . TableTest . assertColumns ; 
 + import static junit . framework . Assert . * ; 
 import static org . apache . cassandra . Util . column ; 
 - import static junit . framework . Assert . assertNull ; 
 + import static org . apache . cassandra . Util . superColumn ; 
 + import static org . apache . cassandra . db . TableTest . * ; 
 
 public class RowResolverTest extends SchemaLoader 
 { 
 @ @ - 93 , 4 + 95 , 110 @ @ public class RowResolverTest extends SchemaLoader 
 { 
 assertNull ( RowRepairResolver . resolveSuperset ( Arrays . < ColumnFamily > asList ( null , null ) ) ) ; 
 } 
 + 
 + @ Test 
 + public void testResolveDeleted ( ) 
 + { 
 + / / one CF with columns timestamped before a delete in another cf 
 + ColumnFamily cf1 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; 
 + cf1 . addColumn ( column ( " one " , " A " , 0 ) ) ; 
 + 
 + ColumnFamily cf2 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; 
 + cf2 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 1 ) ; 
 + 
 + ColumnFamily resolved = RowRepairResolver . resolveSuperset ( Arrays . asList ( cf1 , cf2 ) ) ; 
 + / / no columns in the cf 
 + assertColumns ( resolved ) ; 
 + assertTrue ( resolved . isMarkedForDelete ( ) ) ; 
 + assertEquals ( 1 , resolved . getMarkedForDeleteAt ( ) ) ; 
 + 
 + ColumnFamily scf1 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; 
 + scf1 . addColumn ( superColumn ( scf1 , " super - foo " , column ( " one " , " A " , 0 ) ) ) ; 
 + 
 + ColumnFamily scf2 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; 
 + scf2 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 1 ) ; 
 + 
 + ColumnFamily superResolved = RowRepairResolver . resolveSuperset ( Arrays . asList ( scf1 , scf2 ) ) ; 
 + / / no columns in the cf 
 + assertColumns ( superResolved ) ; 
 + assertTrue ( superResolved . isMarkedForDelete ( ) ) ; 
 + assertEquals ( 1 , superResolved . getMarkedForDeleteAt ( ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testResolveDeletedSuper ( ) 
 + { 
 + / / subcolumn is newer than a tombstone on its parent , but not newer than the row deletion 
 + ColumnFamily scf1 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; 
 + SuperColumn sc = superColumn ( scf1 , " super - foo " , column ( " one " , " A " , 1 ) ) ; 
 + sc . markForDeleteAt ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 0 ) ; 
 + scf1 . addColumn ( sc ) ; 
 + 
 + ColumnFamily scf2 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; 
 + scf2 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 2 ) ; 
 + 
 + ColumnFamily superResolved = RowRepairResolver . resolveSuperset ( Arrays . asList ( scf1 , scf2 ) ) ; 
 + / / no columns in the cf 
 + assertColumns ( superResolved ) ; 
 + assertTrue ( superResolved . isMarkedForDelete ( ) ) ; 
 + assertEquals ( 2 , superResolved . getMarkedForDeleteAt ( ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testResolveMultipleDeleted ( ) 
 + { 
 + / / deletes and columns with interleaved timestamp , with out of order return sequence 
 + 
 + ColumnFamily cf1 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; 
 + cf1 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 0 ) ; 
 + 
 + / / these columns created after the previous deletion 
 + ColumnFamily cf2 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; 
 + cf2 . addColumn ( column ( " one " , " A " , 1 ) ) ; 
 + cf2 . addColumn ( column ( " two " , " A " , 1 ) ) ; 
 + 
 + / / this column created after the next delete 
 + ColumnFamily cf3 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; 
 + cf3 . addColumn ( column ( " two " , " B " , 3 ) ) ; 
 + 
 + ColumnFamily cf4 = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; 
 + cf4 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 2 ) ; 
 + 
 + ColumnFamily resolved = RowRepairResolver . resolveSuperset ( Arrays . asList ( cf1 , cf2 , cf3 , cf4 ) ) ; 
 + / / will have deleted marker and one column 
 + assertColumns ( resolved , " two " ) ; 
 + assertColumn ( resolved , " two " , " B " , 3 ) ; 
 + assertTrue ( resolved . isMarkedForDelete ( ) ) ; 
 + assertEquals ( 2 , resolved . getMarkedForDeleteAt ( ) ) ; 
 + 
 + 
 + ColumnFamily scf1 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; 
 + scf1 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 0 ) ; 
 + 
 + / / these columns created after the previous deletion 
 + ColumnFamily scf2 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; 
 + scf2 . addColumn ( superColumn ( scf2 , " super1 " , column ( " one " , " A " , 1 ) , column ( " two " , " A " , 1 ) ) ) ; 
 + 
 + / / these columns created after the next delete 
 + ColumnFamily scf3 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; 
 + scf3 . addColumn ( superColumn ( scf3 , " super1 " , column ( " two " , " B " , 3 ) ) ) ; 
 + scf3 . addColumn ( superColumn ( scf3 , " super2 " , column ( " three " , " A " , 3 ) , column ( " four " , " A " , 3 ) ) ) ; 
 + 
 + ColumnFamily scf4 = ColumnFamily . create ( " Keyspace1 " , " Super1 " ) ; 
 + scf4 . delete ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , 2 ) ; 
 + 
 + ColumnFamily superResolved = RowRepairResolver . resolveSuperset ( Arrays . asList ( scf1 , scf2 , scf3 , scf4 ) ) ; 
 + / / will have deleted marker and two super cols 
 + assertColumns ( superResolved , " super1 " , " super2 " ) ; 
 + 
 + assertSubColumns ( superResolved , " super1 " , " two " ) ; 
 + assertSubColumn ( superResolved , " super1 " , " two " , " B " , 3 ) ; 
 + 
 + assertSubColumns ( superResolved , " super2 " , " four " , " three " ) ; 
 + assertSubColumn ( superResolved , " super2 " , " three " , " A " , 3 ) ; 
 + assertSubColumn ( superResolved , " super2 " , " four " , " A " , 3 ) ; 
 + 
 + assertTrue ( superResolved . isMarkedForDelete ( ) ) ; 
 + assertEquals ( 2 , superResolved . getMarkedForDeleteAt ( ) ) ; 
 + } 
 } 
 diff - - git a / tools / stress / build . xml b / tools / stress / build . xml 
 index f8995d2 . . a186ecf 100644 
 - - - a / tools / stress / build . xml 
 + + + b / tools / stress / build . xml 
 @ @ - 17 , 7 + 17 , 7 @ @ 
 ~ specific language governing permissions and limitations 
 ~ under the License . 
 - - > 
 - < project basedir = " . " default = " build " name = " stress " > 
 + < project basedir = " . " default = " jar " name = " stress " > 
 < property name = " cassandra . dir " value = " . . / . . " / > 
 < property name = " cassandra . lib " value = " $ { cassandra . dir } / lib " / > 
 < property name = " build . src " value = " $ { basedir } / src " / > 
 @ @ - 49 , 9 + 49 , 19 @ @ 
 < / target > 
 
 < target name = " jar " depends = " build " > 
 + < manifest file = " MANIFEST . MF " > 
 + < attribute name = " Built - By " value = " Pavel Yaskevich " / > 
 + < attribute name = " Main - Class " value = " org . apache . cassandra . stress . Stress " / > 
 + < / manifest > 
 + 
 < mkdir dir = " $ { build . classes } / META - INF " / > 
 - < jar jarfile = " $ { build . out } / $ { final . name } . jar " 
 - basedir = " $ { build . classes } " / > 
 + 
 + < jar destfile = " $ { build . out } / $ { final . name } . jar " manifest = " MANIFEST . MF " > 
 + < fileset dir = " $ { build . classes } " / > 
 + < fileset dir = " $ { cassandra . dir } / build / classes / main " / > 
 + < fileset dir = " $ { cassandra . dir } / build / classes / thrift " / > 
 + < zipgroupfileset dir = " $ { cassandra . lib } " includes = " * . jar " / > 
 + < / jar > 
 < / target > 
 
 < target name = " clean " >
