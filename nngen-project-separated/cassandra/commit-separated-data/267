BLEU SCORE: 0.24446151121745052

TEST MSG: Implement short read protection on partition boundaries
GENERATED MSG: Fix short read protection performance

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 1c53aa5 . . c5e54d4 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 3 . 0 . 15 <nl> + * Implement short read protection on partition boundaries ( CASSANDRA - 13595 ) <nl> * Fix ISE thrown by UPI . Serializer . hasNext ( ) for some SELECT queries ( CASSANDRA - 13911 ) <nl> * Filter header only commit logs before recovery ( CASSANDRA - 13918 ) <nl> * AssertionError prepending to a list ( CASSANDRA - 13149 ) <nl> diff - - git a / src / java / org / apache / cassandra / db / PartitionRangeReadCommand . java b / src / java / org / apache / cassandra / db / PartitionRangeReadCommand . java <nl> index 9e557e0 . . 84e3c7d 100644 <nl> - - - a / src / java / org / apache / cassandra / db / PartitionRangeReadCommand . java <nl> + + + b / src / java / org / apache / cassandra / db / PartitionRangeReadCommand . java <nl> @ @ - 32 , 6 + 32 , 7 @ @ import org . apache . cassandra . db . partitions . * ; <nl> import org . apache . cassandra . db . rows . BaseRowIterator ; <nl> import org . apache . cassandra . db . transform . Transformation ; <nl> import org . apache . cassandra . dht . AbstractBounds ; <nl> + import org . apache . cassandra . dht . Bounds ; <nl> import org . apache . cassandra . exceptions . RequestExecutionException ; <nl> import org . apache . cassandra . index . Index ; <nl> import org . apache . cassandra . io . sstable . format . SSTableReader ; <nl> @ @ - 400 , 6 + 401 , 19 @ @ public class PartitionRangeReadCommand extends ReadCommand <nl> return DataRange . serializer . serializedSize ( dataRange ( ) , version , metadata ( ) ) ; <nl> } <nl> <nl> + / * <nl> + * We are currently using PartitionRangeReadCommand for most index queries , even if they are explicitly restricted <nl> + * to a single partition key . Return true if that is the case . <nl> + * <nl> + * See CASSANDRA - 11617 and CASSANDRA - 11872 for details . <nl> + * / <nl> + public boolean isLimitedToOnePartition ( ) <nl> + { <nl> + return dataRange . keyRange instanceof Bounds <nl> + & & dataRange . startKey ( ) . kind ( ) = = PartitionPosition . Kind . ROW _ KEY <nl> + & & dataRange . startKey ( ) . equals ( dataRange . stopKey ( ) ) ; <nl> + } <nl> + <nl> private static class Deserializer extends SelectionDeserializer <nl> { <nl> public ReadCommand deserialize ( DataInputPlus in , <nl> diff - - git a / src / java / org / apache / cassandra / db / ReadCommand . java b / src / java / org / apache / cassandra / db / ReadCommand . java <nl> index 160b104 . . 2d399d8 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ReadCommand . java <nl> + + + b / src / java / org / apache / cassandra / db / ReadCommand . java <nl> @ @ - 170 , 6 + 170 , 8 @ @ public abstract class ReadCommand implements ReadQuery <nl> protected abstract void serializeSelection ( DataOutputPlus out , int version ) throws IOException ; <nl> protected abstract long selectionSerializedSize ( int version ) ; <nl> <nl> + public abstract boolean isLimitedToOnePartition ( ) ; <nl> + <nl> / * * <nl> * The metadata for the table queried . <nl> * <nl> diff - - git a / src / java / org / apache / cassandra / db / SinglePartitionReadCommand . java b / src / java / org / apache / cassandra / db / SinglePartitionReadCommand . java <nl> index 7a66eca . . 4b10530 100644 <nl> - - - a / src / java / org / apache / cassandra / db / SinglePartitionReadCommand . java <nl> + + + b / src / java / org / apache / cassandra / db / SinglePartitionReadCommand . java <nl> @ @ - 1096 , 6 + 1096 , 11 @ @ public class SinglePartitionReadCommand extends ReadCommand <nl> + ClusteringIndexFilter . serializer . serializedSize ( clusteringIndexFilter ( ) , version ) ; <nl> } <nl> <nl> + public boolean isLimitedToOnePartition ( ) <nl> + { <nl> + return true ; <nl> + } <nl> + <nl> / * * <nl> * Groups multiple single partition read commands . <nl> * / <nl> diff - - git a / src / java / org / apache / cassandra / db / filter / DataLimits . java b / src / java / org / apache / cassandra / db / filter / DataLimits . java <nl> index 6b74293 . . 4c57a76 100644 <nl> - - - a / src / java / org / apache / cassandra / db / filter / DataLimits . java <nl> + + + b / src / java / org / apache / cassandra / db / filter / DataLimits . java <nl> @ @ - 312 , 10 + 312 , 7 @ @ public abstract class DataLimits <nl> <nl> public DataLimits forShortReadRetry ( int toFetch ) <nl> { <nl> - / / When we do a short read retry , we ' re only ever querying the single partition on which we have a short read . So <nl> - / / we use toFetch as the row limit and use no perPartitionLimit ( it would be equivalent in practice to use toFetch <nl> - / / for both argument or just for perPartitionLimit with no limit on rowLimit ) . <nl> - return new CQLLimits ( toFetch , NO _ LIMIT , isDistinct ) ; <nl> + return new CQLLimits ( toFetch , perPartitionLimit , isDistinct ) ; <nl> } <nl> <nl> public boolean hasEnoughLiveData ( CachedPartition cached , int nowInSec , boolean countPartitionsWithOnlyStaticData , boolean enforceStrictLiveness ) <nl> diff - - git a / src / java / org / apache / cassandra / service / DataResolver . java b / src / java / org / apache / cassandra / service / DataResolver . java <nl> index 7d8ffc5 . . 5fb34c6 100644 <nl> - - - a / src / java / org / apache / cassandra / service / DataResolver . java <nl> + + + b / src / java / org / apache / cassandra / service / DataResolver . java <nl> @ @ - 34 , 6 + 34 , 9 @ @ import org . apache . cassandra . db . filter . DataLimits ; <nl> import org . apache . cassandra . db . partitions . * ; <nl> import org . apache . cassandra . db . rows . * ; <nl> import org . apache . cassandra . db . transform . * ; <nl> + import org . apache . cassandra . dht . AbstractBounds ; <nl> + import org . apache . cassandra . dht . ExcludingBounds ; <nl> + import org . apache . cassandra . dht . Range ; <nl> import org . apache . cassandra . exceptions . ReadTimeoutException ; <nl> import org . apache . cassandra . net . * ; <nl> import org . apache . cassandra . tracing . Tracing ; <nl> @ @ - 91 , 7 + 94 , 7 @ @ public class DataResolver extends ResponseResolver <nl> * have more rows than the client requested . To make sure that we still conform to the original limit , <nl> * we apply a top - level post - reconciliation counter to the merged partition iterator . <nl> * <nl> - * Short read protection logic ( ShortReadRowProtection . moreContents ( ) ) relies on this counter to be applied <nl> + * Short read protection logic ( ShortReadRowsProtection . moreContents ( ) ) relies on this counter to be applied <nl> * to the current partition to work . For this reason we have to apply the counter transformation before <nl> * empty partition discard logic kicks in - for it will eagerly consume the iterator . <nl> * <nl> @ @ - 121 , 26 + 124 , 13 @ @ public class DataResolver extends ResponseResolver <nl> if ( results . size ( ) = = 1 ) <nl> return results . get ( 0 ) ; <nl> <nl> - / / So - called " short reads " stems from nodes returning only a subset of the results they have for a partition due to the limit , <nl> - / / but that subset not being enough post - reconciliation . So if we don ' t have limit , don ' t bother . <nl> + / * <nl> + * So - called short reads stems from nodes returning only a subset of the results they have due to the limit , <nl> + * but that subset not being enough post - reconciliation . So if we don ' t have a limit , don ' t bother . <nl> + * / <nl> if ( ! command . limits ( ) . isUnlimited ( ) ) <nl> - { <nl> for ( int i = 0 ; i < results . size ( ) ; i + + ) <nl> - { <nl> - DataLimits . Counter singleResultCounter = <nl> - command . limits ( ) . newCounter ( command . nowInSec ( ) , false , command . selectsFullPartition ( ) , enforceStrictLiveness ) . onlyCount ( ) ; <nl> - <nl> - ShortReadResponseProtection protection = <nl> - new ShortReadResponseProtection ( sources [ i ] , singleResultCounter , mergedResultCounter ) ; <nl> - <nl> - / * <nl> - * The order of transformations is important here . See ShortReadResponseProtection . applyToPartition ( ) <nl> - * comments for details . We want singleResultCounter . applyToPartition ( ) to be called after SRRP applies <nl> - * its transformations , so that this order is preserved when calling applyToRows ( ) too . <nl> - * / <nl> - results . set ( i , Transformation . apply ( Transformation . apply ( results . get ( i ) , protection ) , singleResultCounter ) ) ; <nl> - } <nl> - } <nl> + results . set ( i , extendWithShortReadProtection ( results . get ( i ) , sources [ i ] , mergedResultCounter ) ) ; <nl> <nl> return UnfilteredPartitionIterators . merge ( results , command . nowInSec ( ) , new RepairMergeListener ( sources ) ) ; <nl> } <nl> @ @ - 476 , 14 + 466 , 60 @ @ public class DataResolver extends ResponseResolver <nl> } <nl> } <nl> <nl> - private class ShortReadResponseProtection extends Transformation < UnfilteredRowIterator > <nl> + private UnfilteredPartitionIterator extendWithShortReadProtection ( UnfilteredPartitionIterator partitions , <nl> + InetAddress source , <nl> + DataLimits . Counter mergedResultCounter ) <nl> + { <nl> + DataLimits . Counter singleResultCounter = <nl> + command . limits ( ) . newCounter ( command . nowInSec ( ) , false , command . selectsFullPartition ( ) , enforceStrictLiveness ) . onlyCount ( ) ; <nl> + <nl> + ShortReadPartitionsProtection protection = <nl> + new ShortReadPartitionsProtection ( source , singleResultCounter , mergedResultCounter ) ; <nl> + <nl> + / * <nl> + * The order of extention and transformations is important here . Extending with more partitions has to happen <nl> + * first due to the way BaseIterator . hasMoreContents ( ) works : only transformations applied after extension will <nl> + * be called on the first partition of the extended iterator . <nl> + * <nl> + * Additionally , we want singleResultCounter to be applied after SRPP , so that its applyToPartition ( ) method will <nl> + * be called last , after the extension done by SRRP . applyToPartition ( ) call . That way we preserve the same order <nl> + * when it comes to calling SRRP . moreContents ( ) and applyToRow ( ) callbacks . <nl> + * <nl> + * See ShortReadPartitionsProtection . applyToPartition ( ) for more details . <nl> + * / <nl> + <nl> + / / extend with moreContents ( ) only if it ' s a range read command with no partition key specified <nl> + if ( ! command . isLimitedToOnePartition ( ) ) <nl> + partitions = MorePartitions . extend ( partitions , protection ) ; / / register SRPP . moreContents ( ) <nl> + <nl> + partitions = Transformation . apply ( partitions , protection ) ; / / register SRPP . applyToPartition ( ) <nl> + partitions = Transformation . apply ( partitions , singleResultCounter ) ; / / register the per - source counter <nl> + <nl> + return partitions ; <nl> + } <nl> + <nl> + / * <nl> + * We have a potential short read if the result from a given node contains the requested number of rows <nl> + * ( i . e . it has stopped returning results due to the limit ) , but some of them haven ' t <nl> + * made it into the final post - reconciliation result due to other nodes ' row , range , and / or partition tombstones . <nl> + * <nl> + * If that is the case , then that node may have more rows that we should fetch , as otherwise we could <nl> + * ultimately return fewer rows than required . Also , those additional rows may contain tombstones which <nl> + * which we also need to fetch as they may shadow rows or partitions from other replicas ' results , which we would <nl> + * otherwise return incorrectly . <nl> + * / <nl> + private class ShortReadPartitionsProtection extends Transformation < UnfilteredRowIterator > implements MorePartitions < UnfilteredPartitionIterator > <nl> { <nl> private final InetAddress source ; <nl> <nl> private final DataLimits . Counter singleResultCounter ; / / unmerged per - source counter <nl> private final DataLimits . Counter mergedResultCounter ; / / merged end - result counter <nl> <nl> - private ShortReadResponseProtection ( InetAddress source , DataLimits . Counter singleResultCounter , DataLimits . Counter mergedResultCounter ) <nl> + private DecoratedKey lastPartitionKey ; / / key of the last observed partition <nl> + <nl> + private boolean partitionsFetched ; / / whether we ' ve seen any new partitions since iteration start or last moreContents ( ) call <nl> + <nl> + private ShortReadPartitionsProtection ( InetAddress source , DataLimits . Counter singleResultCounter , DataLimits . Counter mergedResultCounter ) <nl> { <nl> this . source = source ; <nl> this . singleResultCounter = singleResultCounter ; <nl> @ @ - 493 , 29 + 529 , 100 @ @ public class DataResolver extends ResponseResolver <nl> @ Override <nl> public UnfilteredRowIterator applyToPartition ( UnfilteredRowIterator partition ) <nl> { <nl> - ShortReadRowProtection protection = new ShortReadRowProtection ( partition . metadata ( ) , partition . partitionKey ( ) ) ; <nl> + partitionsFetched = true ; <nl> + <nl> + lastPartitionKey = partition . partitionKey ( ) ; <nl> <nl> / * <nl> - * Extend for moreContents ( ) then apply protection to track lastClustering . <nl> + * Extend for moreContents ( ) then apply protection to track lastClustering by applyToRow ( ) . <nl> * <nl> * If we don ' t apply the transformation * after * extending the partition with MoreRows , <nl> * applyToRow ( ) method of protection will not be called on the first row of the new extension iterator . <nl> * / <nl> + ShortReadRowsProtection protection = new ShortReadRowsProtection ( partition . metadata ( ) , partition . partitionKey ( ) ) ; <nl> return Transformation . apply ( MoreRows . extend ( partition , protection ) , protection ) ; <nl> } <nl> <nl> - private class ShortReadRowProtection extends Transformation implements MoreRows < UnfilteredRowIterator > <nl> + / * <nl> + * We only get here once all the rows and partitions in this iterator have been iterated over , and so <nl> + * if the node had returned the requested number of rows but we still get here , then some results were <nl> + * skipped during reconciliation . <nl> + * / <nl> + public UnfilteredPartitionIterator moreContents ( ) <nl> + { <nl> + / / never try to request additional partitions from replicas if our reconciled partitions are already filled to the limit <nl> + assert ! mergedResultCounter . isDone ( ) ; <nl> + <nl> + / / we do not apply short read protection when we have no limits at all <nl> + assert ! command . limits ( ) . isUnlimited ( ) ; <nl> + <nl> + / * <nl> + * If this is a single partition read command or an ( indexed ) partition range read command with <nl> + * a partition key specified , then we can ' t and shouldn ' t try fetch more partitions . <nl> + * / <nl> + assert ! command . isLimitedToOnePartition ( ) ; <nl> + <nl> + / * <nl> + * If the returned result doesn ' t have enough rows / partitions to satisfy even the original limit , don ' t ask for more . <nl> + * <nl> + * Can only take the short cut if there is no per partition limit set . Otherwise it ' s possible to hit false <nl> + * positives due to some rows being uncounted for in certain scenarios ( see CASSANDRA - 13911 ) . <nl> + * / <nl> + if ( ! singleResultCounter . isDone ( ) & & command . limits ( ) . perPartitionCount ( ) = = DataLimits . NO _ LIMIT ) <nl> + return null ; <nl> + <nl> + / * <nl> + * Either we had an empty iterator as the initial response , or our moreContents ( ) call got us an empty iterator . <nl> + * There is no point to ask the replica for more rows - it has no more in the requested range . <nl> + * / <nl> + if ( ! partitionsFetched ) <nl> + return null ; <nl> + partitionsFetched = false ; <nl> + <nl> + / * <nl> + * We are going to fetch one partition at a time for thrift and potentially more for CQL . <nl> + * The row limit will either be set to the per partition limit - if the command has no total row limit set , or <nl> + * the total # of rows remaining - if it has some . If we don ' t grab enough rows in some of the partitions , <nl> + * then future ShortReadRowsProtection . moreContents ( ) calls will fetch the missing ones . <nl> + * / <nl> + int toQuery = command . limits ( ) . count ( ) ! = DataLimits . NO _ LIMIT <nl> + ? command . limits ( ) . count ( ) - mergedResultCounter . counted ( ) <nl> + : command . limits ( ) . perPartitionCount ( ) ; <nl> + <nl> + ColumnFamilyStore . metricsFor ( command . metadata ( ) . cfId ) . shortReadProtectionRequests . mark ( ) ; <nl> + Tracing . trace ( " Requesting { } extra rows from { } for short read protection " , toQuery , source ) ; <nl> + <nl> + PartitionRangeReadCommand cmd = makeFetchAdditionalPartitionReadCommand ( toQuery ) ; <nl> + return executeReadCommand ( cmd ) ; <nl> + } <nl> + <nl> + private PartitionRangeReadCommand makeFetchAdditionalPartitionReadCommand ( int toQuery ) <nl> + { <nl> + PartitionRangeReadCommand cmd = ( PartitionRangeReadCommand ) command ; <nl> + <nl> + DataLimits newLimits = cmd . limits ( ) . forShortReadRetry ( toQuery ) ; <nl> + <nl> + AbstractBounds < PartitionPosition > bounds = cmd . dataRange ( ) . keyRange ( ) ; <nl> + AbstractBounds < PartitionPosition > newBounds = bounds . inclusiveRight ( ) <nl> + ? new Range < > ( lastPartitionKey , bounds . right ) <nl> + : new ExcludingBounds < > ( lastPartitionKey , bounds . right ) ; <nl> + DataRange newDataRange = cmd . dataRange ( ) . forSubRange ( newBounds ) ; <nl> + <nl> + return cmd . withUpdatedLimitsAndDataRange ( newLimits , newDataRange ) ; <nl> + } <nl> + <nl> + private class ShortReadRowsProtection extends Transformation implements MoreRows < UnfilteredRowIterator > <nl> { <nl> private final CFMetaData metadata ; <nl> private final DecoratedKey partitionKey ; <nl> <nl> - private Clustering lastClustering ; <nl> + private Clustering lastClustering ; / / clustering of the last observed row <nl> <nl> private int lastCounted = 0 ; / / last seen recorded # before attempting to fetch more rows <nl> private int lastFetched = 0 ; / / # rows returned by last attempt to get more ( or by the original read command ) <nl> private int lastQueried = 0 ; / / # extra rows requested from the replica last time <nl> <nl> - private ShortReadRowProtection ( CFMetaData metadata , DecoratedKey partitionKey ) <nl> + private ShortReadRowsProtection ( CFMetaData metadata , DecoratedKey partitionKey ) <nl> { <nl> this . metadata = metadata ; <nl> this . partitionKey = partitionKey ; <nl> @ @ - 529 , 18 + 636 , 9 @ @ public class DataResolver extends ResponseResolver <nl> } <nl> <nl> / * <nl> - * We have a potential short read if the result from a given node contains the requested number of rows <nl> - * for that partition ( i . e . it has stopped returning results due to the limit ) , but some of them haven ' t <nl> - * made it into the final post - reconciliation result due to other nodes ' tombstones . <nl> - * <nl> - * If that is the case , then that node may have more rows that we should fetch , as otherwise we could <nl> - * ultimately return fewer rows than required . Also , those additional rows may contain tombstones which <nl> - * which we also need to fetch as they may shadow rows from other replicas ' results , which we would <nl> - * otherwise return incorrectly . <nl> - * <nl> - * Also note that we only get here once all the rows for this partition have been iterated over , and so <nl> - * if the node had returned the requested number of rows but we still get here , then some results were <nl> - * skipped during reconciliation . <nl> + * We only get here once all the rows in this iterator have been iterated over , and so if the node <nl> + * had returned the requested number of rows but we still get here , then some results were skipped <nl> + * during reconciliation . <nl> * / <nl> public UnfilteredRowIterator moreContents ( ) <nl> { <nl> @ @ - 622 , 18 + 720 , 17 @ @ public class DataResolver extends ResponseResolver <nl> * Note : it ' s ok to retrieve more rows that necessary since singleResultCounter is not stopping and only <nl> * counts . <nl> * <nl> - * With that in mind , we ' ll just request the minimum of ( count ( ) , perPartitionCount ( ) ) limits , <nl> - * but no fewer than 8 rows ( an arbitrary round lower bound ) , to ensure that we won ' t fetch row by row <nl> - * for SELECT DISTINCT queries ( that set per partition limit to 1 ) . <nl> + * With that in mind , we ' ll just request the minimum of ( count ( ) , perPartitionCount ( ) ) limits . <nl> * <nl> * See CASSANDRA - 13794 for more details . <nl> * / <nl> - lastQueried = Math . max ( Math . min ( command . limits ( ) . count ( ) , command . limits ( ) . perPartitionCount ( ) ) , 8 ) ; <nl> + lastQueried = Math . min ( command . limits ( ) . count ( ) , command . limits ( ) . perPartitionCount ( ) ) ; <nl> <nl> ColumnFamilyStore . metricsFor ( metadata . cfId ) . shortReadProtectionRequests . mark ( ) ; <nl> Tracing . trace ( " Requesting { } extra rows from { } for short read protection " , lastQueried , source ) ; <nl> <nl> - return executeReadCommand ( makeFetchAdditionalRowsReadCommand ( lastQueried ) ) ; <nl> + SinglePartitionReadCommand cmd = makeFetchAdditionalRowsReadCommand ( lastQueried ) ; <nl> + return UnfilteredPartitionIterators . getOnlyElement ( executeReadCommand ( cmd ) , cmd ) ; <nl> } <nl> <nl> private SinglePartitionReadCommand makeFetchAdditionalRowsReadCommand ( int toQuery ) <nl> @ @ - 649 , 24 + 746 , 25 @ @ public class DataResolver extends ResponseResolver <nl> command . rowFilter ( ) , <nl> command . limits ( ) . forShortReadRetry ( toQuery ) , <nl> partitionKey , <nl> - filter ) ; <nl> + filter , <nl> + command . indexMetadata ( ) ) ; <nl> } <nl> + } <nl> <nl> - private UnfilteredRowIterator executeReadCommand ( SinglePartitionReadCommand cmd ) <nl> - { <nl> - DataResolver resolver = new DataResolver ( keyspace , cmd , ConsistencyLevel . ONE , 1 ) ; <nl> - ReadCallback handler = new ReadCallback ( resolver , ConsistencyLevel . ONE , cmd , Collections . singletonList ( source ) ) ; <nl> - <nl> - if ( StorageProxy . canDoLocalRequest ( source ) ) <nl> - StageManager . getStage ( Stage . READ ) . maybeExecuteImmediately ( new StorageProxy . LocalReadRunnable ( cmd , handler ) ) ; <nl> - else <nl> - MessagingService . instance ( ) . sendRRWithFailure ( cmd . createMessage ( MessagingService . current _ version ) , source , handler ) ; <nl> - <nl> - / / We don ' t call handler . get ( ) because we want to preserve tombstones since we ' re still in the middle of merging node results . <nl> - handler . awaitResults ( ) ; <nl> - assert resolver . responses . size ( ) = = 1 ; <nl> - return UnfilteredPartitionIterators . getOnlyElement ( resolver . responses . get ( 0 ) . payload . makeIterator ( command ) , cmd ) ; <nl> - } <nl> + private UnfilteredPartitionIterator executeReadCommand ( ReadCommand cmd ) <nl> + { <nl> + DataResolver resolver = new DataResolver ( keyspace , cmd , ConsistencyLevel . ONE , 1 ) ; <nl> + ReadCallback handler = new ReadCallback ( resolver , ConsistencyLevel . ONE , cmd , Collections . singletonList ( source ) ) ; <nl> + <nl> + if ( StorageProxy . canDoLocalRequest ( source ) ) <nl> + StageManager . getStage ( Stage . READ ) . maybeExecuteImmediately ( new StorageProxy . LocalReadRunnable ( cmd , handler ) ) ; <nl> + else <nl> + MessagingService . instance ( ) . sendRRWithFailure ( cmd . createMessage ( MessagingService . current _ version ) , source , handler ) ; <nl> + <nl> + / / We don ' t call handler . get ( ) because we want to preserve tombstones since we ' re still in the middle of merging node results . <nl> + handler . awaitResults ( ) ; <nl> + assert resolver . responses . size ( ) = = 1 ; <nl> + return resolver . responses . get ( 0 ) . payload . makeIterator ( command ) ; <nl> } <nl> } <nl> }
NEAREST DIFF (one line): ELIMINATEDSENTENCE

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 1c53aa5 . . c5e54d4 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 3 . 0 . 15 
 + * Implement short read protection on partition boundaries ( CASSANDRA - 13595 ) 
 * Fix ISE thrown by UPI . Serializer . hasNext ( ) for some SELECT queries ( CASSANDRA - 13911 ) 
 * Filter header only commit logs before recovery ( CASSANDRA - 13918 ) 
 * AssertionError prepending to a list ( CASSANDRA - 13149 ) 
 diff - - git a / src / java / org / apache / cassandra / db / PartitionRangeReadCommand . java b / src / java / org / apache / cassandra / db / PartitionRangeReadCommand . java 
 index 9e557e0 . . 84e3c7d 100644 
 - - - a / src / java / org / apache / cassandra / db / PartitionRangeReadCommand . java 
 + + + b / src / java / org / apache / cassandra / db / PartitionRangeReadCommand . java 
 @ @ - 32 , 6 + 32 , 7 @ @ import org . apache . cassandra . db . partitions . * ; 
 import org . apache . cassandra . db . rows . BaseRowIterator ; 
 import org . apache . cassandra . db . transform . Transformation ; 
 import org . apache . cassandra . dht . AbstractBounds ; 
 + import org . apache . cassandra . dht . Bounds ; 
 import org . apache . cassandra . exceptions . RequestExecutionException ; 
 import org . apache . cassandra . index . Index ; 
 import org . apache . cassandra . io . sstable . format . SSTableReader ; 
 @ @ - 400 , 6 + 401 , 19 @ @ public class PartitionRangeReadCommand extends ReadCommand 
 return DataRange . serializer . serializedSize ( dataRange ( ) , version , metadata ( ) ) ; 
 } 
 
 + / * 
 + * We are currently using PartitionRangeReadCommand for most index queries , even if they are explicitly restricted 
 + * to a single partition key . Return true if that is the case . 
 + * 
 + * See CASSANDRA - 11617 and CASSANDRA - 11872 for details . 
 + * / 
 + public boolean isLimitedToOnePartition ( ) 
 + { 
 + return dataRange . keyRange instanceof Bounds 
 + & & dataRange . startKey ( ) . kind ( ) = = PartitionPosition . Kind . ROW _ KEY 
 + & & dataRange . startKey ( ) . equals ( dataRange . stopKey ( ) ) ; 
 + } 
 + 
 private static class Deserializer extends SelectionDeserializer 
 { 
 public ReadCommand deserialize ( DataInputPlus in , 
 diff - - git a / src / java / org / apache / cassandra / db / ReadCommand . java b / src / java / org / apache / cassandra / db / ReadCommand . java 
 index 160b104 . . 2d399d8 100644 
 - - - a / src / java / org / apache / cassandra / db / ReadCommand . java 
 + + + b / src / java / org / apache / cassandra / db / ReadCommand . java 
 @ @ - 170 , 6 + 170 , 8 @ @ public abstract class ReadCommand implements ReadQuery 
 protected abstract void serializeSelection ( DataOutputPlus out , int version ) throws IOException ; 
 protected abstract long selectionSerializedSize ( int version ) ; 
 
 + public abstract boolean isLimitedToOnePartition ( ) ; 
 + 
 / * * 
 * The metadata for the table queried . 
 * 
 diff - - git a / src / java / org / apache / cassandra / db / SinglePartitionReadCommand . java b / src / java / org / apache / cassandra / db / SinglePartitionReadCommand . java 
 index 7a66eca . . 4b10530 100644 
 - - - a / src / java / org / apache / cassandra / db / SinglePartitionReadCommand . java 
 + + + b / src / java / org / apache / cassandra / db / SinglePartitionReadCommand . java 
 @ @ - 1096 , 6 + 1096 , 11 @ @ public class SinglePartitionReadCommand extends ReadCommand 
 + ClusteringIndexFilter . serializer . serializedSize ( clusteringIndexFilter ( ) , version ) ; 
 } 
 
 + public boolean isLimitedToOnePartition ( ) 
 + { 
 + return true ; 
 + } 
 + 
 / * * 
 * Groups multiple single partition read commands . 
 * / 
 diff - - git a / src / java / org / apache / cassandra / db / filter / DataLimits . java b / src / java / org / apache / cassandra / db / filter / DataLimits . java 
 index 6b74293 . . 4c57a76 100644 
 - - - a / src / java / org / apache / cassandra / db / filter / DataLimits . java 
 + + + b / src / java / org / apache / cassandra / db / filter / DataLimits . java 
 @ @ - 312 , 10 + 312 , 7 @ @ public abstract class DataLimits 
 
 public DataLimits forShortReadRetry ( int toFetch ) 
 { 
 - / / When we do a short read retry , we ' re only ever querying the single partition on which we have a short read . So 
 - / / we use toFetch as the row limit and use no perPartitionLimit ( it would be equivalent in practice to use toFetch 
 - / / for both argument or just for perPartitionLimit with no limit on rowLimit ) . 
 - return new CQLLimits ( toFetch , NO _ LIMIT , isDistinct ) ; 
 + return new CQLLimits ( toFetch , perPartitionLimit , isDistinct ) ; 
 } 
 
 public boolean hasEnoughLiveData ( CachedPartition cached , int nowInSec , boolean countPartitionsWithOnlyStaticData , boolean enforceStrictLiveness ) 
 diff - - git a / src / java / org / apache / cassandra / service / DataResolver . java b / src / java / org / apache / cassandra / service / DataResolver . java 
 index 7d8ffc5 . . 5fb34c6 100644 
 - - - a / src / java / org / apache / cassandra / service / DataResolver . java 
 + + + b / src / java / org / apache / cassandra / service / DataResolver . java 
 @ @ - 34 , 6 + 34 , 9 @ @ import org . apache . cassandra . db . filter . DataLimits ; 
 import org . apache . cassandra . db . partitions . * ; 
 import org . apache . cassandra . db . rows . * ; 
 import org . apache . cassandra . db . transform . * ; 
 + import org . apache . cassandra . dht . AbstractBounds ; 
 + import org . apache . cassandra . dht . ExcludingBounds ; 
 + import org . apache . cassandra . dht . Range ; 
 import org . apache . cassandra . exceptions . ReadTimeoutException ; 
 import org . apache . cassandra . net . * ; 
 import org . apache . cassandra . tracing . Tracing ; 
 @ @ - 91 , 7 + 94 , 7 @ @ public class DataResolver extends ResponseResolver 
 * have more rows than the client requested . To make sure that we still conform to the original limit , 
 * we apply a top - level post - reconciliation counter to the merged partition iterator . 
 * 
 - * Short read protection logic ( ShortReadRowProtection . moreContents ( ) ) relies on this counter to be applied 
 + * Short read protection logic ( ShortReadRowsProtection . moreContents ( ) ) relies on this counter to be applied 
 * to the current partition to work . For this reason we have to apply the counter transformation before 
 * empty partition discard logic kicks in - for it will eagerly consume the iterator . 
 * 
 @ @ - 121 , 26 + 124 , 13 @ @ public class DataResolver extends ResponseResolver 
 if ( results . size ( ) = = 1 ) 
 return results . get ( 0 ) ; 
 
 - / / So - called " short reads " stems from nodes returning only a subset of the results they have for a partition due to the limit , 
 - / / but that subset not being enough post - reconciliation . So if we don ' t have limit , don ' t bother . 
 + / * 
 + * So - called short reads stems from nodes returning only a subset of the results they have due to the limit , 
 + * but that subset not being enough post - reconciliation . So if we don ' t have a limit , don ' t bother . 
 + * / 
 if ( ! command . limits ( ) . isUnlimited ( ) ) 
 - { 
 for ( int i = 0 ; i < results . size ( ) ; i + + ) 
 - { 
 - DataLimits . Counter singleResultCounter = 
 - command . limits ( ) . newCounter ( command . nowInSec ( ) , false , command . selectsFullPartition ( ) , enforceStrictLiveness ) . onlyCount ( ) ; 
 - 
 - ShortReadResponseProtection protection = 
 - new ShortReadResponseProtection ( sources [ i ] , singleResultCounter , mergedResultCounter ) ; 
 - 
 - / * 
 - * The order of transformations is important here . See ShortReadResponseProtection . applyToPartition ( ) 
 - * comments for details . We want singleResultCounter . applyToPartition ( ) to be called after SRRP applies 
 - * its transformations , so that this order is preserved when calling applyToRows ( ) too . 
 - * / 
 - results . set ( i , Transformation . apply ( Transformation . apply ( results . get ( i ) , protection ) , singleResultCounter ) ) ; 
 - } 
 - } 
 + results . set ( i , extendWithShortReadProtection ( results . get ( i ) , sources [ i ] , mergedResultCounter ) ) ; 
 
 return UnfilteredPartitionIterators . merge ( results , command . nowInSec ( ) , new RepairMergeListener ( sources ) ) ; 
 } 
 @ @ - 476 , 14 + 466 , 60 @ @ public class DataResolver extends ResponseResolver 
 } 
 } 
 
 - private class ShortReadResponseProtection extends Transformation < UnfilteredRowIterator > 
 + private UnfilteredPartitionIterator extendWithShortReadProtection ( UnfilteredPartitionIterator partitions , 
 + InetAddress source , 
 + DataLimits . Counter mergedResultCounter ) 
 + { 
 + DataLimits . Counter singleResultCounter = 
 + command . limits ( ) . newCounter ( command . nowInSec ( ) , false , command . selectsFullPartition ( ) , enforceStrictLiveness ) . onlyCount ( ) ; 
 + 
 + ShortReadPartitionsProtection protection = 
 + new ShortReadPartitionsProtection ( source , singleResultCounter , mergedResultCounter ) ; 
 + 
 + / * 
 + * The order of extention and transformations is important here . Extending with more partitions has to happen 
 + * first due to the way BaseIterator . hasMoreContents ( ) works : only transformations applied after extension will 
 + * be called on the first partition of the extended iterator . 
 + * 
 + * Additionally , we want singleResultCounter to be applied after SRPP , so that its applyToPartition ( ) method will 
 + * be called last , after the extension done by SRRP . applyToPartition ( ) call . That way we preserve the same order 
 + * when it comes to calling SRRP . moreContents ( ) and applyToRow ( ) callbacks . 
 + * 
 + * See ShortReadPartitionsProtection . applyToPartition ( ) for more details . 
 + * / 
 + 
 + / / extend with moreContents ( ) only if it ' s a range read command with no partition key specified 
 + if ( ! command . isLimitedToOnePartition ( ) ) 
 + partitions = MorePartitions . extend ( partitions , protection ) ; / / register SRPP . moreContents ( ) 
 + 
 + partitions = Transformation . apply ( partitions , protection ) ; / / register SRPP . applyToPartition ( ) 
 + partitions = Transformation . apply ( partitions , singleResultCounter ) ; / / register the per - source counter 
 + 
 + return partitions ; 
 + } 
 + 
 + / * 
 + * We have a potential short read if the result from a given node contains the requested number of rows 
 + * ( i . e . it has stopped returning results due to the limit ) , but some of them haven ' t 
 + * made it into the final post - reconciliation result due to other nodes ' row , range , and / or partition tombstones . 
 + * 
 + * If that is the case , then that node may have more rows that we should fetch , as otherwise we could 
 + * ultimately return fewer rows than required . Also , those additional rows may contain tombstones which 
 + * which we also need to fetch as they may shadow rows or partitions from other replicas ' results , which we would 
 + * otherwise return incorrectly . 
 + * / 
 + private class ShortReadPartitionsProtection extends Transformation < UnfilteredRowIterator > implements MorePartitions < UnfilteredPartitionIterator > 
 { 
 private final InetAddress source ; 
 
 private final DataLimits . Counter singleResultCounter ; / / unmerged per - source counter 
 private final DataLimits . Counter mergedResultCounter ; / / merged end - result counter 
 
 - private ShortReadResponseProtection ( InetAddress source , DataLimits . Counter singleResultCounter , DataLimits . Counter mergedResultCounter ) 
 + private DecoratedKey lastPartitionKey ; / / key of the last observed partition 
 + 
 + private boolean partitionsFetched ; / / whether we ' ve seen any new partitions since iteration start or last moreContents ( ) call 
 + 
 + private ShortReadPartitionsProtection ( InetAddress source , DataLimits . Counter singleResultCounter , DataLimits . Counter mergedResultCounter ) 
 { 
 this . source = source ; 
 this . singleResultCounter = singleResultCounter ; 
 @ @ - 493 , 29 + 529 , 100 @ @ public class DataResolver extends ResponseResolver 
 @ Override 
 public UnfilteredRowIterator applyToPartition ( UnfilteredRowIterator partition ) 
 { 
 - ShortReadRowProtection protection = new ShortReadRowProtection ( partition . metadata ( ) , partition . partitionKey ( ) ) ; 
 + partitionsFetched = true ; 
 + 
 + lastPartitionKey = partition . partitionKey ( ) ; 
 
 / * 
 - * Extend for moreContents ( ) then apply protection to track lastClustering . 
 + * Extend for moreContents ( ) then apply protection to track lastClustering by applyToRow ( ) . 
 * 
 * If we don ' t apply the transformation * after * extending the partition with MoreRows , 
 * applyToRow ( ) method of protection will not be called on the first row of the new extension iterator . 
 * / 
 + ShortReadRowsProtection protection = new ShortReadRowsProtection ( partition . metadata ( ) , partition . partitionKey ( ) ) ; 
 return Transformation . apply ( MoreRows . extend ( partition , protection ) , protection ) ; 
 } 
 
 - private class ShortReadRowProtection extends Transformation implements MoreRows < UnfilteredRowIterator > 
 + / * 
 + * We only get here once all the rows and partitions in this iterator have been iterated over , and so 
 + * if the node had returned the requested number of rows but we still get here , then some results were 
 + * skipped during reconciliation . 
 + * / 
 + public UnfilteredPartitionIterator moreContents ( ) 
 + { 
 + / / never try to request additional partitions from replicas if our reconciled partitions are already filled to the limit 
 + assert ! mergedResultCounter . isDone ( ) ; 
 + 
 + / / we do not apply short read protection when we have no limits at all 
 + assert ! command . limits ( ) . isUnlimited ( ) ; 
 + 
 + / * 
 + * If this is a single partition read command or an ( indexed ) partition range read command with 
 + * a partition key specified , then we can ' t and shouldn ' t try fetch more partitions . 
 + * / 
 + assert ! command . isLimitedToOnePartition ( ) ; 
 + 
 + / * 
 + * If the returned result doesn ' t have enough rows / partitions to satisfy even the original limit , don ' t ask for more . 
 + * 
 + * Can only take the short cut if there is no per partition limit set . Otherwise it ' s possible to hit false 
 + * positives due to some rows being uncounted for in certain scenarios ( see CASSANDRA - 13911 ) . 
 + * / 
 + if ( ! singleResultCounter . isDone ( ) & & command . limits ( ) . perPartitionCount ( ) = = DataLimits . NO _ LIMIT ) 
 + return null ; 
 + 
 + / * 
 + * Either we had an empty iterator as the initial response , or our moreContents ( ) call got us an empty iterator . 
 + * There is no point to ask the replica for more rows - it has no more in the requested range . 
 + * / 
 + if ( ! partitionsFetched ) 
 + return null ; 
 + partitionsFetched = false ; 
 + 
 + / * 
 + * We are going to fetch one partition at a time for thrift and potentially more for CQL . 
 + * The row limit will either be set to the per partition limit - if the command has no total row limit set , or 
 + * the total # of rows remaining - if it has some . If we don ' t grab enough rows in some of the partitions , 
 + * then future ShortReadRowsProtection . moreContents ( ) calls will fetch the missing ones . 
 + * / 
 + int toQuery = command . limits ( ) . count ( ) ! = DataLimits . NO _ LIMIT 
 + ? command . limits ( ) . count ( ) - mergedResultCounter . counted ( ) 
 + : command . limits ( ) . perPartitionCount ( ) ; 
 + 
 + ColumnFamilyStore . metricsFor ( command . metadata ( ) . cfId ) . shortReadProtectionRequests . mark ( ) ; 
 + Tracing . trace ( " Requesting { } extra rows from { } for short read protection " , toQuery , source ) ; 
 + 
 + PartitionRangeReadCommand cmd = makeFetchAdditionalPartitionReadCommand ( toQuery ) ; 
 + return executeReadCommand ( cmd ) ; 
 + } 
 + 
 + private PartitionRangeReadCommand makeFetchAdditionalPartitionReadCommand ( int toQuery ) 
 + { 
 + PartitionRangeReadCommand cmd = ( PartitionRangeReadCommand ) command ; 
 + 
 + DataLimits newLimits = cmd . limits ( ) . forShortReadRetry ( toQuery ) ; 
 + 
 + AbstractBounds < PartitionPosition > bounds = cmd . dataRange ( ) . keyRange ( ) ; 
 + AbstractBounds < PartitionPosition > newBounds = bounds . inclusiveRight ( ) 
 + ? new Range < > ( lastPartitionKey , bounds . right ) 
 + : new ExcludingBounds < > ( lastPartitionKey , bounds . right ) ; 
 + DataRange newDataRange = cmd . dataRange ( ) . forSubRange ( newBounds ) ; 
 + 
 + return cmd . withUpdatedLimitsAndDataRange ( newLimits , newDataRange ) ; 
 + } 
 + 
 + private class ShortReadRowsProtection extends Transformation implements MoreRows < UnfilteredRowIterator > 
 { 
 private final CFMetaData metadata ; 
 private final DecoratedKey partitionKey ; 
 
 - private Clustering lastClustering ; 
 + private Clustering lastClustering ; / / clustering of the last observed row 
 
 private int lastCounted = 0 ; / / last seen recorded # before attempting to fetch more rows 
 private int lastFetched = 0 ; / / # rows returned by last attempt to get more ( or by the original read command ) 
 private int lastQueried = 0 ; / / # extra rows requested from the replica last time 
 
 - private ShortReadRowProtection ( CFMetaData metadata , DecoratedKey partitionKey ) 
 + private ShortReadRowsProtection ( CFMetaData metadata , DecoratedKey partitionKey ) 
 { 
 this . metadata = metadata ; 
 this . partitionKey = partitionKey ; 
 @ @ - 529 , 18 + 636 , 9 @ @ public class DataResolver extends ResponseResolver 
 } 
 
 / * 
 - * We have a potential short read if the result from a given node contains the requested number of rows 
 - * for that partition ( i . e . it has stopped returning results due to the limit ) , but some of them haven ' t 
 - * made it into the final post - reconciliation result due to other nodes ' tombstones . 
 - * 
 - * If that is the case , then that node may have more rows that we should fetch , as otherwise we could 
 - * ultimately return fewer rows than required . Also , those additional rows may contain tombstones which 
 - * which we also need to fetch as they may shadow rows from other replicas ' results , which we would 
 - * otherwise return incorrectly . 
 - * 
 - * Also note that we only get here once all the rows for this partition have been iterated over , and so 
 - * if the node had returned the requested number of rows but we still get here , then some results were 
 - * skipped during reconciliation . 
 + * We only get here once all the rows in this iterator have been iterated over , and so if the node 
 + * had returned the requested number of rows but we still get here , then some results were skipped 
 + * during reconciliation . 
 * / 
 public UnfilteredRowIterator moreContents ( ) 
 { 
 @ @ - 622 , 18 + 720 , 17 @ @ public class DataResolver extends ResponseResolver 
 * Note : it ' s ok to retrieve more rows that necessary since singleResultCounter is not stopping and only 
 * counts . 
 * 
 - * With that in mind , we ' ll just request the minimum of ( count ( ) , perPartitionCount ( ) ) limits , 
 - * but no fewer than 8 rows ( an arbitrary round lower bound ) , to ensure that we won ' t fetch row by row 
 - * for SELECT DISTINCT queries ( that set per partition limit to 1 ) . 
 + * With that in mind , we ' ll just request the minimum of ( count ( ) , perPartitionCount ( ) ) limits . 
 * 
 * See CASSANDRA - 13794 for more details . 
 * / 
 - lastQueried = Math . max ( Math . min ( command . limits ( ) . count ( ) , command . limits ( ) . perPartitionCount ( ) ) , 8 ) ; 
 + lastQueried = Math . min ( command . limits ( ) . count ( ) , command . limits ( ) . perPartitionCount ( ) ) ; 
 
 ColumnFamilyStore . metricsFor ( metadata . cfId ) . shortReadProtectionRequests . mark ( ) ; 
 Tracing . trace ( " Requesting { } extra rows from { } for short read protection " , lastQueried , source ) ; 
 
 - return executeReadCommand ( makeFetchAdditionalRowsReadCommand ( lastQueried ) ) ; 
 + SinglePartitionReadCommand cmd = makeFetchAdditionalRowsReadCommand ( lastQueried ) ; 
 + return UnfilteredPartitionIterators . getOnlyElement ( executeReadCommand ( cmd ) , cmd ) ; 
 } 
 
 private SinglePartitionReadCommand makeFetchAdditionalRowsReadCommand ( int toQuery ) 
 @ @ - 649 , 24 + 746 , 25 @ @ public class DataResolver extends ResponseResolver 
 command . rowFilter ( ) , 
 command . limits ( ) . forShortReadRetry ( toQuery ) , 
 partitionKey , 
 - filter ) ; 
 + filter , 
 + command . indexMetadata ( ) ) ; 
 } 
 + } 
 
 - private UnfilteredRowIterator executeReadCommand ( SinglePartitionReadCommand cmd ) 
 - { 
 - DataResolver resolver = new DataResolver ( keyspace , cmd , ConsistencyLevel . ONE , 1 ) ; 
 - ReadCallback handler = new ReadCallback ( resolver , ConsistencyLevel . ONE , cmd , Collections . singletonList ( source ) ) ; 
 - 
 - if ( StorageProxy . canDoLocalRequest ( source ) ) 
 - StageManager . getStage ( Stage . READ ) . maybeExecuteImmediately ( new StorageProxy . LocalReadRunnable ( cmd , handler ) ) ; 
 - else 
 - MessagingService . instance ( ) . sendRRWithFailure ( cmd . createMessage ( MessagingService . current _ version ) , source , handler ) ; 
 - 
 - / / We don ' t call handler . get ( ) because we want to preserve tombstones since we ' re still in the middle of merging node results . 
 - handler . awaitResults ( ) ; 
 - assert resolver . responses . size ( ) = = 1 ; 
 - return UnfilteredPartitionIterators . getOnlyElement ( resolver . responses . get ( 0 ) . payload . makeIterator ( command ) , cmd ) ; 
 - } 
 + private UnfilteredPartitionIterator executeReadCommand ( ReadCommand cmd ) 
 + { 
 + DataResolver resolver = new DataResolver ( keyspace , cmd , ConsistencyLevel . ONE , 1 ) ; 
 + ReadCallback handler = new ReadCallback ( resolver , ConsistencyLevel . ONE , cmd , Collections . singletonList ( source ) ) ; 
 + 
 + if ( StorageProxy . canDoLocalRequest ( source ) ) 
 + StageManager . getStage ( Stage . READ ) . maybeExecuteImmediately ( new StorageProxy . LocalReadRunnable ( cmd , handler ) ) ; 
 + else 
 + MessagingService . instance ( ) . sendRRWithFailure ( cmd . createMessage ( MessagingService . current _ version ) , source , handler ) ; 
 + 
 + / / We don ' t call handler . get ( ) because we want to preserve tombstones since we ' re still in the middle of merging node results . 
 + handler . awaitResults ( ) ; 
 + assert resolver . responses . size ( ) = = 1 ; 
 + return resolver . responses . get ( 0 ) . payload . makeIterator ( command ) ; 
 } 
 } 
 }

NEAREST DIFF:
ELIMINATEDSENTENCE
