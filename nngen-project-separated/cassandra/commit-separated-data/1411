BLEU SCORE: 0.004514856899053214

TEST MSG: Replace all usages of Adler32 with CRC32 which has a fast instrinsic now
GENERATED MSG: split 2ary index build out from bloom / row index build , and move into stream session post - processing . bloom / row index construction moved into SSTableWriter . Builder and is now run on CompactionManager executor

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index c7d466a . . f1ac423 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 3 . 0 . 0 - beta1 <nl> + * Replace all usages of Adler32 with CRC32 <nl> * Fix row deletion bug for Materialized Views ( CASSANDRA - 10014 ) <nl> * Support mixed - version clusters with Cassandra 2 . 1 and 2 . 2 ( CASSANDRA - 9704 ) <nl> * Fix multiple slices on RowSearchers ( CASSANDRA - 10002 ) <nl> @ @ - 9 , 6 + 10 , 7 @ @ <nl> * Add transparent data encryption core classes ( CASSANDRA - 9945 ) <nl> * Bytecode inspection for Java - UDFs ( CASSANDRA - 9890 ) <nl> * Use byte to serialize MT hash length ( CASSANDRA - 9792 ) <nl> + * Replace usage of Adler32 with CRC32 ( CASSANDRA - 8684 ) <nl> Merged from 2 . 2 : <nl> * Add checksum to saved cache files ( CASSANDRA - 9265 ) <nl> * Log warning when using an aggregate without partition key ( CASSANDRA - 9737 ) <nl> diff - - git a / src / java / org / apache / cassandra / cache / AutoSavingCache . java b / src / java / org / apache / cassandra / cache / AutoSavingCache . java <nl> index 3c5b6a5 . . 2a838ab 100644 <nl> - - - a / src / java / org / apache / cassandra / cache / AutoSavingCache . java <nl> + + + b / src / java / org / apache / cassandra / cache / AutoSavingCache . java <nl> @ @ - 62 , 7 + 62 , 7 @ @ public class AutoSavingCache < K extends CacheKey , V > extends InstrumentingCache < K <nl> protected final CacheService . CacheType cacheType ; <nl> <nl> private final CacheSerializer < K , V > cacheLoader ; <nl> - private static final String CURRENT _ VERSION = " c " ; <nl> + private static final String CURRENT _ VERSION = " d " ; <nl> <nl> private static volatile IStreamFactory streamFactory = new IStreamFactory ( ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java <nl> index 01b4655 . . c38f4d2 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java <nl> @ @ - 24 , 7 + 24 , 7 @ @ import java . util . Map ; <nl> import java . util . TreeMap ; <nl> import java . util . concurrent . ThreadLocalRandom ; <nl> import java . util . zip . Adler32 ; <nl> - <nl> + import java . util . zip . Checksum ; <nl> <nl> import com . google . common . primitives . Ints ; <nl> <nl> @ @ - 58 , 7 + 58 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> private ByteBuffer compressed ; <nl> <nl> / / re - use single crc object <nl> - private final Adler32 checksum ; <nl> + private final Checksum checksum ; <nl> <nl> / / raw checksum bytes <nl> private ByteBuffer checksumBytes ; <nl> @ @ - 67 , 7 + 67 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> { <nl> super ( channel , metadata . chunkLength ( ) , metadata . compressedFileLength , metadata . compressor ( ) . preferredBufferType ( ) ) ; <nl> this . metadata = metadata ; <nl> - checksum = new Adler32 ( ) ; <nl> + checksum = metadata . checksumType . newInstance ( ) ; <nl> <nl> chunkSegments = file = = null ? null : file . chunkSegments ( ) ; <nl> if ( chunkSegments = = null ) <nl> @ @ - 131 , 7 + 131 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> if ( metadata . parameters . getCrcCheckChance ( ) > ThreadLocalRandom . current ( ) . nextDouble ( ) ) <nl> { <nl> compressed . rewind ( ) ; <nl> - checksum . update ( compressed ) ; <nl> + metadata . checksumType . update ( checksum , ( compressed ) ) ; <nl> <nl> if ( checksum ( chunk ) ! = ( int ) checksum . getValue ( ) ) <nl> throw new CorruptBlockException ( getPath ( ) , chunk ) ; <nl> @ @ - 193 , 7 + 193 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> { <nl> compressedChunk . position ( chunkOffset ) . limit ( chunkOffset + chunk . length ) ; <nl> <nl> - checksum . update ( compressedChunk ) ; <nl> + metadata . checksumType . update ( checksum , compressedChunk ) ; <nl> <nl> compressedChunk . limit ( compressedChunk . capacity ( ) ) ; <nl> if ( compressedChunk . getInt ( ) ! = ( int ) checksum . getValue ( ) ) <nl> diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> index bc1e6f6 . . a4afa3f 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> @ @ - 23 , 7 + 23 , 7 @ @ import java . io . File ; <nl> import java . io . IOException ; <nl> import java . nio . ByteBuffer ; <nl> import java . nio . channels . Channels ; <nl> - import java . util . zip . Adler32 ; <nl> + import java . util . zip . CRC32 ; <nl> <nl> import org . apache . cassandra . io . FSReadError ; <nl> import org . apache . cassandra . io . FSWriteError ; <nl> @ @ - 204 , 7 + 204 , 7 @ @ public class CompressedSequentialWriter extends SequentialWriter <nl> throw new CorruptBlockException ( getPath ( ) , chunkOffset , chunkSize ) ; <nl> } <nl> <nl> - Adler32 checksum = new Adler32 ( ) ; <nl> + CRC32 checksum = new CRC32 ( ) ; <nl> compressed . rewind ( ) ; <nl> checksum . update ( compressed ) ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / io / compress / CompressionMetadata . java b / src / java / org / apache / cassandra / io / compress / CompressionMetadata . java <nl> index bd6da2c . . f5d8f7e 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / CompressionMetadata . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / CompressionMetadata . java <nl> @ @ - 53 , 6 + 53 , 7 @ @ import org . apache . cassandra . io . util . FileUtils ; <nl> import org . apache . cassandra . io . util . Memory ; <nl> import org . apache . cassandra . io . util . SafeMemory ; <nl> import org . apache . cassandra . schema . CompressionParams ; <nl> + import org . apache . cassandra . utils . ChecksumType ; <nl> import org . apache . cassandra . utils . Pair ; <nl> import org . apache . cassandra . utils . concurrent . Transactional ; <nl> import org . apache . cassandra . utils . concurrent . Ref ; <nl> @ @ - 71 , 6 + 72 , 7 @ @ public class CompressionMetadata <nl> private final long chunkOffsetsSize ; <nl> public final String indexFilePath ; <nl> public final CompressionParams parameters ; <nl> + public final ChecksumType checksumType ; <nl> <nl> / * * <nl> * Create metadata about given compressed file including uncompressed data length , chunk size <nl> @ @ - 86 , 13 + 88 , 14 @ @ public class CompressionMetadata <nl> public static CompressionMetadata create ( String dataFilePath ) <nl> { <nl> Descriptor desc = Descriptor . fromFilename ( dataFilePath ) ; <nl> - return new CompressionMetadata ( desc . filenameFor ( Component . COMPRESSION _ INFO ) , new File ( dataFilePath ) . length ( ) ) ; <nl> + return new CompressionMetadata ( desc . filenameFor ( Component . COMPRESSION _ INFO ) , new File ( dataFilePath ) . length ( ) , desc . version . compressedChecksumType ( ) ) ; <nl> } <nl> <nl> @ VisibleForTesting <nl> - CompressionMetadata ( String indexFilePath , long compressedLength ) <nl> + CompressionMetadata ( String indexFilePath , long compressedLength , ChecksumType checksumType ) <nl> { <nl> this . indexFilePath = indexFilePath ; <nl> + this . checksumType = checksumType ; <nl> <nl> try ( DataInputStream stream = new DataInputStream ( new FileInputStream ( indexFilePath ) ) ) <nl> { <nl> @ @ - 131 , 7 + 134 , 7 @ @ public class CompressionMetadata <nl> this . chunkOffsetsSize = chunkOffsets . size ( ) ; <nl> } <nl> <nl> - private CompressionMetadata ( String filePath , CompressionParams parameters , SafeMemory offsets , long offsetsSize , long dataLength , long compressedLength ) <nl> + private CompressionMetadata ( String filePath , CompressionParams parameters , SafeMemory offsets , long offsetsSize , long dataLength , long compressedLength , ChecksumType checksumType ) <nl> { <nl> this . indexFilePath = filePath ; <nl> this . parameters = parameters ; <nl> @ @ - 139 , 6 + 142 , 7 @ @ public class CompressionMetadata <nl> this . compressedFileLength = compressedLength ; <nl> this . chunkOffsets = offsets ; <nl> this . chunkOffsetsSize = offsetsSize ; <nl> + this . checksumType = checksumType ; <nl> } <nl> <nl> public ICompressor compressor ( ) <nl> @ @ - 380 , 7 + 384 , 7 @ @ public class CompressionMetadata <nl> if ( count < this . count ) <nl> compressedLength = offsets . getLong ( count * 8L ) ; <nl> <nl> - return new CompressionMetadata ( filePath , parameters , offsets , count * 8L , dataLength , compressedLength ) ; <nl> + return new CompressionMetadata ( filePath , parameters , offsets , count * 8L , dataLength , compressedLength , ChecksumType . CRC32 ) ; <nl> } <nl> <nl> / * * <nl> @ @ - 398 , 7 + 402 , 7 @ @ public class CompressionMetadata <nl> / * * <nl> * Reset the writer so that the next chunk offset written will be the <nl> * one of { @ code chunkIndex } . <nl> - * <nl> + * <nl> * @ param chunkIndex the next index to write <nl> * / <nl> public void resetAndTruncate ( int chunkIndex ) <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / Component . java b / src / java / org / apache / cassandra / io / sstable / Component . java <nl> index a431f29 . . 54dd35b 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / Component . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / Component . java <nl> @ @ - 48 , 7 + 48 , 7 @ @ public class Component <nl> / / statistical metadata about the content of the sstable <nl> STATS ( " Statistics . db " ) , <nl> / / holds adler32 checksum of the data file <nl> - DIGEST ( " Digest . adler32 " ) , <nl> + DIGEST ( new String [ ] { " Digest . crc32 " , " Digest . adler32 " } ) , <nl> / / holds the CRC32 for chunks in an a uncompressed file . <nl> CRC ( " CRC . db " ) , <nl> / / holds SSTable Index Summary ( sampling of Index component ) <nl> @ @ - 56 , 19 + 56 , 25 @ @ public class Component <nl> / / table of contents , stores the list of all components for the sstable <nl> TOC ( " TOC . txt " ) , <nl> / / custom component , used by e . g . custom compaction strategy <nl> - CUSTOM ( null ) ; <nl> + CUSTOM ( new String [ ] { null } ) ; <nl> <nl> - final String repr ; <nl> + final String [ ] repr ; <nl> Type ( String repr ) <nl> { <nl> + this ( new String [ ] { repr } ) ; <nl> + } <nl> + <nl> + Type ( String [ ] repr ) <nl> + { <nl> this . repr = repr ; <nl> } <nl> <nl> static Type fromRepresentation ( String repr ) <nl> { <nl> for ( Type type : TYPES ) <nl> - if ( repr . equals ( type . repr ) ) <nl> - return type ; <nl> + for ( String representation : type . repr ) <nl> + if ( repr . equals ( representation ) ) <nl> + return type ; <nl> return CUSTOM ; <nl> } <nl> } <nl> @ @ - 90 , 7 + 96 , 7 @ @ public class Component <nl> <nl> public Component ( Type type ) <nl> { <nl> - this ( type , type . repr ) ; <nl> + this ( type , type . repr [ 0 ] ) ; <nl> assert type ! = Type . CUSTOM ; <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / format / Version . java b / src / java / org / apache / cassandra / io / sstable / format / Version . java <nl> index 10ceb24 . . 9ef0b43 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / format / Version . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / format / Version . java <nl> @ @ - 19 , 6 + 19 , 8 @ @ package org . apache . cassandra . io . sstable . format ; <nl> <nl> import java . util . regex . Pattern ; <nl> <nl> + import org . apache . cassandra . utils . ChecksumType ; <nl> + <nl> / * * <nl> * A set of feature flags associated with a SSTable format <nl> * <nl> @ @ - 48 , 7 + 50 , 9 @ @ public abstract class Version <nl> <nl> public abstract boolean hasNewStatsFile ( ) ; <nl> <nl> - public abstract boolean hasAllAdlerChecksums ( ) ; <nl> + public abstract ChecksumType compressedChecksumType ( ) ; <nl> + <nl> + public abstract ChecksumType uncompressedChecksumType ( ) ; <nl> <nl> public abstract boolean hasRepairedAt ( ) ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / format / big / BigFormat . java b / src / java / org / apache / cassandra / io / sstable / format / big / BigFormat . java <nl> index 860cd9f . . 6df4b1e 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / format / big / BigFormat . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / format / big / BigFormat . java <nl> @ @ - 32 , 6 + 32 , 7 @ @ import org . apache . cassandra . io . sstable . format . Version ; <nl> import org . apache . cassandra . io . sstable . metadata . MetadataCollector ; <nl> import org . apache . cassandra . io . sstable . metadata . StatsMetadata ; <nl> import org . apache . cassandra . net . MessagingService ; <nl> + import org . apache . cassandra . utils . ChecksumType ; <nl> <nl> / * * <nl> * Legacy bigtable format <nl> @ @ - 81 , 11 + 82 , 11 @ @ public class BigFormat implements SSTableFormat <nl> static class WriterFactory extends SSTableWriter . Factory <nl> { <nl> @ Override <nl> - public SSTableWriter open ( Descriptor descriptor , <nl> - long keyCount , <nl> - long repairedAt , <nl> - CFMetaData metadata , <nl> - MetadataCollector metadataCollector , <nl> + public SSTableWriter open ( Descriptor descriptor , <nl> + long keyCount , <nl> + long repairedAt , <nl> + CFMetaData metadata , <nl> + MetadataCollector metadataCollector , <nl> SerializationHeader header , <nl> LifecycleTransaction txn ) <nl> { <nl> @ @ - 126 , 7 + 127 , 8 @ @ public class BigFormat implements SSTableFormat <nl> private final boolean isLatestVersion ; <nl> private final boolean hasSamplingLevel ; <nl> private final boolean newStatsFile ; <nl> - private final boolean hasAllAdlerChecksums ; <nl> + private final ChecksumType compressedChecksumType ; <nl> + private final ChecksumType uncompressedChecksumType ; <nl> private final boolean hasRepairedAt ; <nl> private final boolean tracksLegacyCounterShards ; <nl> private final boolean newFileName ; <nl> @ @ - 145 , 7 + 147 , 19 @ @ public class BigFormat implements SSTableFormat <nl> isLatestVersion = version . compareTo ( current _ version ) = = 0 ; <nl> hasSamplingLevel = version . compareTo ( " ka " ) > = 0 ; <nl> newStatsFile = version . compareTo ( " ka " ) > = 0 ; <nl> - hasAllAdlerChecksums = version . compareTo ( " ka " ) > = 0 ; <nl> + <nl> + / / For a while Adler32 was in use , now the CRC32 instrinsic is very good especially after Haswell <nl> + / / PureJavaCRC32 was always faster than Adler32 . See CASSANDRA - 8684 <nl> + ChecksumType checksumType = ChecksumType . CRC32 ; <nl> + if ( version . compareTo ( " ka " ) > = 0 & & version . compareTo ( " ma " ) < 0 ) <nl> + checksumType = ChecksumType . Adler32 ; <nl> + this . uncompressedChecksumType = checksumType ; <nl> + <nl> + checksumType = ChecksumType . CRC32 ; <nl> + if ( version . compareTo ( " jb " ) > = 0 & & version . compareTo ( " ma " ) < 0 ) <nl> + checksumType = ChecksumType . Adler32 ; <nl> + this . compressedChecksumType = checksumType ; <nl> + <nl> hasRepairedAt = version . compareTo ( " ka " ) > = 0 ; <nl> tracksLegacyCounterShards = version . compareTo ( " ka " ) > = 0 ; <nl> <nl> @ @ - 177 , 9 + 191 , 15 @ @ public class BigFormat implements SSTableFormat <nl> } <nl> <nl> @ Override <nl> - public boolean hasAllAdlerChecksums ( ) <nl> + public ChecksumType compressedChecksumType ( ) <nl> + { <nl> + return compressedChecksumType ; <nl> + } <nl> + <nl> + @ Override <nl> + public ChecksumType uncompressedChecksumType ( ) <nl> { <nl> - return hasAllAdlerChecksums ; <nl> + return uncompressedChecksumType ; <nl> } <nl> <nl> @ Override <nl> diff - - git a / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java b / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java <nl> index 976ff23 . . 3fc247b 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java <nl> + + + b / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java <nl> @ @ - 20 , 7 + 20 , 7 @ @ package org . apache . cassandra . io . util ; <nl> <nl> import java . io . File ; <nl> import java . io . IOException ; <nl> - import java . util . zip . Adler32 ; <nl> + import java . util . zip . CRC32 ; <nl> <nl> import org . apache . cassandra . io . compress . BufferType ; <nl> import org . apache . cassandra . utils . ByteBufferUtil ; <nl> @ @ - 52 , 7 + 52 , 7 @ @ public class ChecksummedRandomAccessReader extends RandomAccessReader <nl> { <nl> ChannelProxy channel = new ChannelProxy ( file ) ; <nl> RandomAccessReader crcReader = RandomAccessReader . open ( crcFile ) ; <nl> - DataIntegrityMetadata . ChecksumValidator validator = new DataIntegrityMetadata . ChecksumValidator ( new Adler32 ( ) , <nl> + DataIntegrityMetadata . ChecksumValidator validator = new DataIntegrityMetadata . ChecksumValidator ( new CRC32 ( ) , <nl> crcReader , <nl> file . getPath ( ) ) ; <nl> return new ChecksummedRandomAccessReader ( file , channel , validator ) ; <nl> diff - - git a / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java b / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java <nl> index ac2ab47 . . 70cd860 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java <nl> + + + b / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java <nl> @ @ - 25 , 7 + 25 , 6 @ @ import java . io . IOError ; <nl> import java . io . IOException ; <nl> import java . nio . ByteBuffer ; <nl> import java . nio . file . Files ; <nl> - import java . util . zip . Adler32 ; <nl> import java . util . zip . CRC32 ; <nl> import java . util . zip . CheckedInputStream ; <nl> import java . util . zip . Checksum ; <nl> @ @ - 35 , 7 + 34 , 6 @ @ import com . google . common . base . Charsets ; <nl> import org . apache . cassandra . io . FSWriteError ; <nl> import org . apache . cassandra . io . sstable . Component ; <nl> import org . apache . cassandra . io . sstable . Descriptor ; <nl> - import org . apache . cassandra . utils . FBUtilities ; <nl> <nl> public class DataIntegrityMetadata <nl> { <nl> @ @ - 53 , 7 + 51 , 7 @ @ public class DataIntegrityMetadata <nl> <nl> public ChecksumValidator ( Descriptor descriptor ) throws IOException <nl> { <nl> - this ( descriptor . version . hasAllAdlerChecksums ( ) ? new Adler32 ( ) : new CRC32 ( ) , <nl> + this ( descriptor . version . uncompressedChecksumType ( ) . newInstance ( ) , <nl> RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . CRC ) ) ) , <nl> descriptor . filenameFor ( Component . DATA ) ) ; <nl> } <nl> @ @ - 110 , 7 + 108 , 7 @ @ public class DataIntegrityMetadata <nl> public FileDigestValidator ( Descriptor descriptor ) throws IOException <nl> { <nl> this . descriptor = descriptor ; <nl> - checksum = descriptor . version . hasAllAdlerChecksums ( ) ? new Adler32 ( ) : new CRC32 ( ) ; <nl> + checksum = descriptor . version . uncompressedChecksumType ( ) . newInstance ( ) ; <nl> digestReader = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . DIGEST ) ) ) ; <nl> dataReader = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . DATA ) ) ) ; <nl> try <nl> @ @ - 154 , 9 + 152 , 9 @ @ public class DataIntegrityMetadata <nl> <nl> public static class ChecksumWriter <nl> { <nl> - private final Adler32 incrementalChecksum = new Adler32 ( ) ; <nl> + private final CRC32 incrementalChecksum = new CRC32 ( ) ; <nl> private final DataOutput incrementalOut ; <nl> - private final Adler32 fullChecksum = new Adler32 ( ) ; <nl> + private final CRC32 fullChecksum = new CRC32 ( ) ; <nl> <nl> public ChecksumWriter ( DataOutput incrementalOut ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / streaming / compress / CompressedInputStream . java b / src / java / org / apache / cassandra / streaming / compress / CompressedInputStream . java <nl> index 099fd14 . . 0a118b2 100644 <nl> - - - a / src / java / org / apache / cassandra / streaming / compress / CompressedInputStream . java <nl> + + + b / src / java / org / apache / cassandra / streaming / compress / CompressedInputStream . java <nl> @ @ - 24 , 13 + 24 , 13 @ @ import java . util . Iterator ; <nl> import java . util . concurrent . ArrayBlockingQueue ; <nl> import java . util . concurrent . BlockingQueue ; <nl> import java . util . concurrent . ThreadLocalRandom ; <nl> - import java . util . zip . Adler32 ; <nl> import java . util . zip . Checksum ; <nl> <nl> import com . google . common . collect . Iterators ; <nl> import com . google . common . primitives . Ints ; <nl> <nl> import org . apache . cassandra . io . compress . CompressionMetadata ; <nl> + import org . apache . cassandra . utils . ChecksumType ; <nl> import org . apache . cassandra . utils . WrappedRunnable ; <nl> <nl> / * * <nl> @ @ - 65 , 10 + 65 , 10 @ @ public class CompressedInputStream extends InputStream <nl> * @ param source Input source to read compressed data from <nl> * @ param info Compression info <nl> * / <nl> - public CompressedInputStream ( InputStream source , CompressionInfo info ) <nl> + public CompressedInputStream ( InputStream source , CompressionInfo info , ChecksumType checksumType ) <nl> { <nl> this . info = info ; <nl> - this . checksum = new Adler32 ( ) ; <nl> + this . checksum = checksumType . newInstance ( ) ; <nl> this . buffer = new byte [ info . parameters . chunkLength ( ) ] ; <nl> / / buffer is limited to store up to 1024 chunks <nl> this . dataBuffer = new ArrayBlockingQueue < byte [ ] > ( Math . min ( info . chunks . length , 1024 ) ) ; <nl> diff - - git a / src / java / org / apache / cassandra / streaming / compress / CompressedStreamReader . java b / src / java / org / apache / cassandra / streaming / compress / CompressedStreamReader . java <nl> index 47832f0 . . 205291b 100644 <nl> - - - a / src / java / org / apache / cassandra / streaming / compress / CompressedStreamReader . java <nl> + + + b / src / java / org / apache / cassandra / streaming / compress / CompressedStreamReader . java <nl> @ @ - 77 , 7 + 77 , 7 @ @ public class CompressedStreamReader extends StreamReader <nl> <nl> SSTableWriter writer = createWriter ( cfs , totalSize , repairedAt , format ) ; <nl> <nl> - CompressedInputStream cis = new CompressedInputStream ( Channels . newInputStream ( channel ) , compressionInfo ) ; <nl> + CompressedInputStream cis = new CompressedInputStream ( Channels . newInputStream ( channel ) , compressionInfo , inputVersion . compressedChecksumType ( ) ) ; <nl> BytesReadTracker in = new BytesReadTracker ( new DataInputStream ( cis ) ) ; <nl> StreamDeserializer deserializer = new StreamDeserializer ( cfs . metadata , in , inputVersion , header . toHeader ( cfs . metadata ) ) ; <nl> try <nl> diff - - git a / src / java / org / apache / cassandra / utils / ChecksumType . java b / src / java / org / apache / cassandra / utils / ChecksumType . java <nl> new file mode 100644 <nl> index 0000000 . . c9a1eb8 <nl> - - - / dev / null <nl> + + + b / src / java / org / apache / cassandra / utils / ChecksumType . java <nl> @ @ - 0 , 0 + 1 , 63 @ @ <nl> + / * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , software <nl> + * distributed under the License is distributed on an " AS IS " BASIS , <nl> + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> + * See the License for the specific language governing permissions and <nl> + * limitations under the License . <nl> + * / <nl> + package org . apache . cassandra . utils ; <nl> + <nl> + import java . nio . ByteBuffer ; <nl> + import java . util . zip . Checksum ; <nl> + import java . util . zip . CRC32 ; <nl> + import java . util . zip . Adler32 ; <nl> + <nl> + public enum ChecksumType <nl> + { <nl> + Adler32 ( ) <nl> + { <nl> + <nl> + @ Override <nl> + public Checksum newInstance ( ) <nl> + { <nl> + return new Adler32 ( ) ; <nl> + } <nl> + <nl> + @ Override <nl> + public void update ( Checksum checksum , ByteBuffer buf ) <nl> + { <nl> + ( ( Adler32 ) checksum ) . update ( buf ) ; <nl> + } <nl> + <nl> + } , <nl> + CRC32 ( ) <nl> + { <nl> + <nl> + @ Override <nl> + public Checksum newInstance ( ) <nl> + { <nl> + return new CRC32 ( ) ; <nl> + } <nl> + <nl> + @ Override <nl> + public void update ( Checksum checksum , ByteBuffer buf ) <nl> + { <nl> + ( ( CRC32 ) checksum ) . update ( buf ) ; <nl> + } <nl> + <nl> + } ; <nl> + <nl> + public abstract Checksum newInstance ( ) ; <nl> + <nl> + public abstract void update ( Checksum checksum , ByteBuffer buf ) ; <nl> + } <nl> diff - - git a / test / unit / org / apache / cassandra / db / VerifyTest . java b / test / unit / org / apache / cassandra / db / VerifyTest . java <nl> index 3bd4a47 . . 13ce0c1 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / VerifyTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / VerifyTest . java <nl> @ @ - 19 , 6 + 19 , 7 @ @ <nl> package org . apache . cassandra . db ; <nl> <nl> import com . google . common . base . Charsets ; <nl> + <nl> import org . apache . cassandra . OrderedJUnit4ClassRunner ; <nl> import org . apache . cassandra . SchemaLoader ; <nl> import org . apache . cassandra . Util ; <nl> @ @ - 43 , 7 + 44 , 7 @ @ import org . junit . runner . RunWith ; <nl> <nl> import java . io . * ; <nl> import java . nio . file . Files ; <nl> - import java . util . zip . Adler32 ; <nl> + import java . util . zip . CRC32 ; <nl> import java . util . zip . CheckedInputStream ; <nl> <nl> import static org . junit . Assert . fail ; <nl> @ @ - 371 , 8 + 372 , 8 @ @ public class VerifyTest <nl> protected long simpleFullChecksum ( String filename ) throws IOException <nl> { <nl> FileInputStream inputStream = new FileInputStream ( filename ) ; <nl> - Adler32 adlerChecksum = new Adler32 ( ) ; <nl> - CheckedInputStream cinStream = new CheckedInputStream ( inputStream , adlerChecksum ) ; <nl> + CRC32 checksum = new CRC32 ( ) ; <nl> + CheckedInputStream cinStream = new CheckedInputStream ( inputStream , checksum ) ; <nl> byte [ ] b = new byte [ 128 ] ; <nl> while ( cinStream . read ( b ) > = 0 ) { <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> index cc76a9e . . 8f94cf2 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> @ @ - 24 , 7 + 24 , 6 @ @ import java . io . RandomAccessFile ; <nl> import java . util . Random ; <nl> <nl> import org . junit . Test ; <nl> - <nl> import org . apache . cassandra . db . ClusteringComparator ; <nl> import org . apache . cassandra . db . marshal . BytesType ; <nl> import org . apache . cassandra . exceptions . ConfigurationException ; <nl> @ @ - 35 , 6 + 34 , 7 @ @ import org . apache . cassandra . io . util . FileMark ; <nl> import org . apache . cassandra . io . util . RandomAccessReader ; <nl> import org . apache . cassandra . io . util . SequentialWriter ; <nl> import org . apache . cassandra . schema . CompressionParams ; <nl> + import org . apache . cassandra . utils . ChecksumType ; <nl> import org . apache . cassandra . utils . SyncUtil ; <nl> <nl> import static org . junit . Assert . assertEquals ; <nl> @ @ - 84 , 7 + 84 , 7 @ @ public class CompressedRandomAccessReaderTest <nl> writer . write ( " x " . getBytes ( ) ) ; <nl> writer . finish ( ) ; <nl> <nl> - CompressedRandomAccessReader reader = CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) ) ; <nl> + CompressedRandomAccessReader reader = CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) , ChecksumType . CRC32 ) ) ; <nl> String res = reader . readLine ( ) ; <nl> assertEquals ( res , " xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx " ) ; <nl> assertEquals ( 40 , res . length ( ) ) ; <nl> @ @ - 129 , 7 + 129 , 7 @ @ public class CompressedRandomAccessReaderTest <nl> <nl> assert f . exists ( ) ; <nl> RandomAccessReader reader = compressed <nl> - ? CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) ) <nl> + ? CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) , ChecksumType . CRC32 ) ) <nl> : RandomAccessReader . open ( f ) ; <nl> String expected = " The quick brown fox jumps over the lazy dog " ; <nl> assertEquals ( expected . length ( ) , reader . length ( ) ) ; <nl> @ @ - 171 , 7 + 171 , 7 @ @ public class CompressedRandomAccessReaderTest <nl> ChannelProxy channel = new ChannelProxy ( file ) ; <nl> <nl> / / open compression metadata and get chunk information <nl> - CompressionMetadata meta = new CompressionMetadata ( metadata . getPath ( ) , file . length ( ) ) ; <nl> + CompressionMetadata meta = new CompressionMetadata ( metadata . getPath ( ) , file . length ( ) , ChecksumType . CRC32 ) ; <nl> CompressionMetadata . Chunk chunk = meta . chunkFor ( 0 ) ; <nl> <nl> RandomAccessReader reader = CompressedRandomAccessReader . open ( channel , meta ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java <nl> index db99317 . . 28af0ae 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java <nl> @ @ - 31 , 6 + 31 , 7 @ @ import org . junit . After ; <nl> import org . junit . Test ; <nl> <nl> import junit . framework . Assert ; <nl> + <nl> import org . apache . cassandra . db . ClusteringComparator ; <nl> import org . apache . cassandra . db . marshal . AbstractType ; <nl> import org . apache . cassandra . db . marshal . BytesType ; <nl> @ @ - 41 , 6 + 42 , 7 @ @ import org . apache . cassandra . io . util . FileMark ; <nl> import org . apache . cassandra . io . util . RandomAccessReader ; <nl> import org . apache . cassandra . io . util . SequentialWriterTest ; <nl> import org . apache . cassandra . schema . CompressionParams ; <nl> + import org . apache . cassandra . utils . ChecksumType ; <nl> <nl> public class CompressedSequentialWriterTest extends SequentialWriterTest <nl> { <nl> @ @ - 115 , 7 + 117 , 7 @ @ public class CompressedSequentialWriterTest extends SequentialWriterTest <nl> } <nl> <nl> assert f . exists ( ) ; <nl> - RandomAccessReader reader = CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) ) ; <nl> + RandomAccessReader reader = CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) , ChecksumType . CRC32 ) ) ; <nl> assertEquals ( dataPre . length + rawPost . length , reader . length ( ) ) ; <nl> byte [ ] result = new byte [ ( int ) reader . length ( ) ] ; <nl> <nl> diff - - git a / test / unit / org / apache / cassandra / streaming / compression / CompressedInputStreamTest . java b / test / unit / org / apache / cassandra / streaming / compression / CompressedInputStreamTest . java <nl> index 37aea91 . . e3014c3 100644 <nl> - - - a / test / unit / org / apache / cassandra / streaming / compression / CompressedInputStreamTest . java <nl> + + + b / test / unit / org / apache / cassandra / streaming / compression / CompressedInputStreamTest . java <nl> @ @ - 21 , 7 + 21 , 6 @ @ import java . io . * ; <nl> import java . util . * ; <nl> <nl> import org . junit . Test ; <nl> - <nl> import org . apache . cassandra . db . ClusteringComparator ; <nl> import org . apache . cassandra . db . marshal . BytesType ; <nl> import org . apache . cassandra . io . compress . CompressedSequentialWriter ; <nl> @ @ - 32 , 6 + 31 , 7 @ @ import org . apache . cassandra . io . sstable . Descriptor ; <nl> import org . apache . cassandra . io . sstable . metadata . MetadataCollector ; <nl> import org . apache . cassandra . streaming . compress . CompressedInputStream ; <nl> import org . apache . cassandra . streaming . compress . CompressionInfo ; <nl> + import org . apache . cassandra . utils . ChecksumType ; <nl> import org . apache . cassandra . utils . Pair ; <nl> <nl> / * * <nl> @ @ - 108 , 7 + 108 , 7 @ @ public class CompressedInputStreamTest <nl> <nl> / / read buffer using CompressedInputStream <nl> CompressionInfo info = new CompressionInfo ( chunks , param ) ; <nl> - CompressedInputStream input = new CompressedInputStream ( new ByteArrayInputStream ( toRead ) , info ) ; <nl> + CompressedInputStream input = new CompressedInputStream ( new ByteArrayInputStream ( toRead ) , info , ChecksumType . CRC32 ) ; <nl> DataInputStream in = new DataInputStream ( input ) ; <nl> <nl> for ( int i = 0 ; i < sections . size ( ) ; i + + )
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> index b9635e8 . . d2fd951 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> @ @ - 33 , 6 + 33 , 7 @ @ import javax . management . ObjectName ; <nl> import com . google . common . collect . Iterables ; <nl> import org . apache . commons . collections . IteratorUtils ; <nl> import org . apache . commons . lang . ArrayUtils ; <nl> + import org . apache . commons . lang . StringUtils ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> @ @ - 205 , 12 + 206 , 9 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> logger . info ( " Creating index { } . { } " , table , indexedCfMetadata . cfName ) ; <nl> Runnable runnable = new WrappedRunnable ( ) <nl> { <nl> - public void runMayThrow ( ) throws IOException , ExecutionException , InterruptedException <nl> + public void runMayThrow ( ) throws IOException <nl> { <nl> - logger . debug ( " Submitting index build to compactionmanager " ) ; <nl> - ReducingKeyIterator iter = new ReducingKeyIterator ( getSSTables ( ) ) ; <nl> - Future future = CompactionManager . instance . submitIndexBuild ( ColumnFamilyStore . this , FBUtilities . getSingleColumnSet ( info . name ) , iter ) ; <nl> - future . get ( ) ; <nl> + buildSecondaryIndexes ( getSSTables ( ) , FBUtilities . getSingleColumnSet ( info . name ) ) ; <nl> logger . info ( " Index { } complete " , indexedCfMetadata . cfName ) ; <nl> SystemTable . setIndexBuilt ( table , indexedCfMetadata . cfName ) ; <nl> } <nl> @ @ - 220 , 6 + 218 , 26 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> indexedColumns . put ( info . name , indexedCfs ) ; <nl> } <nl> <nl> + public void buildSecondaryIndexes ( Collection < SSTableReader > sstables , SortedSet < byte [ ] > columns ) <nl> + { <nl> + logger . debug ( " Submitting index build to compactionmanager " ) ; <nl> + Future future = CompactionManager . instance . submitIndexBuild ( this , columns , new ReducingKeyIterator ( sstables ) ) ; <nl> + try <nl> + { <nl> + future . get ( ) ; <nl> + for ( byte [ ] column : columns ) <nl> + getIndexedColumnFamilyStore ( column ) . forceBlockingFlush ( ) ; <nl> + } <nl> + catch ( InterruptedException e ) <nl> + { <nl> + throw new AssertionError ( e ) ; <nl> + } <nl> + catch ( ExecutionException e ) <nl> + { <nl> + throw new RuntimeException ( e ) ; <nl> + } <nl> + } <nl> + <nl> / / called when dropping or renaming a CF . Performs mbean housekeeping . <nl> void unregisterMBean ( ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / db / CompactionManager . java b / src / java / org / apache / cassandra / db / CompactionManager . java <nl> index 64289db . . d5d7444 100644 <nl> - - - a / src / java / org / apache / cassandra / db / CompactionManager . java <nl> + + + b / src / java / org / apache / cassandra / db / CompactionManager . java <nl> @ @ - 18 , 38 + 18 , 38 @ @ <nl> <nl> package org . apache . cassandra . db ; <nl> <nl> - import java . io . IOException ; <nl> import java . io . File ; <nl> + import java . io . IOException ; <nl> import java . lang . management . ManagementFactory ; <nl> + import java . net . InetAddress ; <nl> import java . util . * ; <nl> import java . util . Map . Entry ; <nl> import java . util . concurrent . Callable ; <nl> import java . util . concurrent . Future ; <nl> - import javax . management . * ; <nl> + import javax . management . MBeanServer ; <nl> + import javax . management . ObjectName ; <nl> <nl> + import org . apache . commons . collections . PredicateUtils ; <nl> + import org . apache . commons . collections . iterators . CollatingIterator ; <nl> + import org . apache . commons . collections . iterators . FilterIterator ; <nl> + import org . apache . commons . lang . StringUtils ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> - <nl> import org . apache . cassandra . concurrent . DebuggableThreadPoolExecutor ; <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . dht . Range ; <nl> - import org . apache . cassandra . io . * ; <nl> + import org . apache . cassandra . io . AbstractCompactedRow ; <nl> + import org . apache . cassandra . io . CompactionIterator ; <nl> + import org . apache . cassandra . io . ICompactionInfo ; <nl> import org . apache . cassandra . io . sstable . * ; <nl> - import org . apache . cassandra . config . DatabaseDescriptor ; <nl> - import org . apache . cassandra . service . StorageService ; <nl> - import org . apache . cassandra . service . AntiEntropyService ; <nl> import org . apache . cassandra . io . util . FileUtils ; <nl> + import org . apache . cassandra . service . AntiEntropyService ; <nl> + import org . apache . cassandra . service . StorageService ; <nl> import org . apache . cassandra . utils . FBUtilities ; <nl> import org . apache . cassandra . utils . Pair ; <nl> import org . cliffc . high _ scale _ lib . NonBlockingHashMap ; <nl> <nl> - import java . net . InetAddress ; <nl> - <nl> - import org . apache . commons . lang . StringUtils ; <nl> - import org . apache . commons . collections . iterators . FilterIterator ; <nl> - import org . apache . commons . collections . iterators . CollatingIterator ; <nl> - import org . apache . commons . collections . PredicateUtils ; <nl> - <nl> public class CompactionManager implements CompactionManagerMBean <nl> { <nl> public static final String MBEAN _ OBJECT _ NAME = " org . apache . cassandra . db : type = CompactionManager " ; <nl> @ @ - 508 , 6 + 508 , 20 @ @ public class CompactionManager implements CompactionManagerMBean <nl> return executor . submit ( runnable ) ; <nl> } <nl> <nl> + public Future < SSTableReader > submitSSTableBuild ( Descriptor desc ) <nl> + { <nl> + final SSTableWriter . Builder builder = SSTableWriter . createBuilder ( desc ) ; <nl> + Callable < SSTableReader > callable = new Callable < SSTableReader > ( ) <nl> + { <nl> + public SSTableReader call ( ) throws IOException <nl> + { <nl> + executor . beginCompaction ( builder . cfs , builder ) ; <nl> + return builder . build ( ) ; <nl> + } <nl> + } ; <nl> + return executor . submit ( callable ) ; <nl> + } <nl> + <nl> private static class AntiCompactionIterator extends CompactionIterator <nl> { <nl> private Set < SSTableScanner > scanners ; <nl> @ @ - 550 , 6 + 564 , 11 @ @ public class CompactionManager implements CompactionManagerMBean <nl> } <nl> return scanners ; <nl> } <nl> + <nl> + public String getTaskType ( ) <nl> + { <nl> + return " Anticompaction " ; <nl> + } <nl> } <nl> <nl> public void checkAllColumnFamilies ( ) throws IOException <nl> diff - - git a / src / java / org / apache / cassandra / db / Table . java b / src / java / org / apache / cassandra / db / Table . java <nl> index d96608b . . 82f41c2 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Table . java <nl> + + + b / src / java / org / apache / cassandra / db / Table . java <nl> @ @ - 456 , 7 + 456 , 8 @ @ public class Table <nl> synchronized ( indexLockFor ( key . key ) ) <nl> { <nl> ColumnFamily cf = readCurrentIndexedColumns ( key , cfs , columns ) ; <nl> - applyIndexUpdates ( key . key , memtablesToFlush , cf , cfs , cf . getColumnNames ( ) , null ) ; <nl> + if ( cf ! = null ) <nl> + applyIndexUpdates ( key . key , memtablesToFlush , cf , cfs , cf . getColumnNames ( ) , null ) ; <nl> } <nl> } <nl> finally <nl> diff - - git a / src / java / org / apache / cassandra / io / CompactionIterator . java b / src / java / org / apache / cassandra / io / CompactionIterator . java <nl> index 304e3dd . . a617a26 100644 <nl> - - - a / src / java / org / apache / cassandra / io / CompactionIterator . java <nl> + + + b / src / java / org / apache / cassandra / io / CompactionIterator . java <nl> @ @ - 161 , 4 + 161 , 8 @ @ implements Closeable , ICompactionInfo <nl> return bytesRead ; <nl> } <nl> <nl> + public String getTaskType ( ) <nl> + { <nl> + return " Compaction " ; <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / ICompactionInfo . java b / src / java / org / apache / cassandra / io / ICompactionInfo . java <nl> index a00c5bb . . 7815c10 100644 <nl> - - - a / src / java / org / apache / cassandra / io / ICompactionInfo . java <nl> + + + b / src / java / org / apache / cassandra / io / ICompactionInfo . java <nl> @ @ - 5 , 4 + 5 , 6 @ @ public interface ICompactionInfo <nl> public long getTotalBytes ( ) ; <nl> <nl> public long getBytesRead ( ) ; <nl> + <nl> + public String getTaskType ( ) ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> index b74c675 . . 07e9b23 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> @ @ - 4 , 16 + 4 , 16 @ @ import java . io . Closeable ; <nl> import java . io . File ; <nl> import java . io . IOError ; <nl> import java . io . IOException ; <nl> + import java . util . Iterator ; <nl> <nl> import com . google . common . collect . AbstractIterator ; <nl> <nl> import org . apache . cassandra . db . DecoratedKey ; <nl> - import org . apache . cassandra . io . ICompactionInfo ; <nl> import org . apache . cassandra . io . util . BufferedRandomAccessFile ; <nl> import org . apache . cassandra . service . StorageService ; <nl> import org . apache . cassandra . utils . FBUtilities ; <nl> <nl> - public class KeyIterator extends AbstractIterator < DecoratedKey > implements IKeyIterator <nl> + public class KeyIterator extends AbstractIterator < DecoratedKey > implements Iterator < DecoratedKey > , Closeable <nl> { <nl> private final BufferedRandomAccessFile in ; <nl> private final Descriptor desc ; <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / ReducingKeyIterator . java b / src / java / org / apache / cassandra / io / sstable / ReducingKeyIterator . java <nl> index e3ebaed . . ed07f5b 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / ReducingKeyIterator . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / ReducingKeyIterator . java <nl> @ @ - 66 , 6 + 66 , 11 @ @ public class ReducingKeyIterator implements IKeyIterator <nl> return m ; <nl> } <nl> <nl> + public String getTaskType ( ) <nl> + { <nl> + return " Secondary index build " ; <nl> + } <nl> + <nl> public boolean hasNext ( ) <nl> { <nl> return iter . hasNext ( ) ; <nl> @ @ - 73 , 7 + 78 , 7 @ @ public class ReducingKeyIterator implements IKeyIterator <nl> <nl> public DecoratedKey next ( ) <nl> { <nl> - return ( DecoratedKey ) iter . next ( ) ; <nl> + return iter . next ( ) ; <nl> } <nl> <nl> public void remove ( ) <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> index 6118339 . . d4bc6b4 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> @ @ - 32 , 6 + 32 , 7 @ @ import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . db . * ; <nl> import org . apache . cassandra . dht . IPartitioner ; <nl> import org . apache . cassandra . io . AbstractCompactedRow ; <nl> + import org . apache . cassandra . io . ICompactionInfo ; <nl> import org . apache . cassandra . io . util . BufferedRandomAccessFile ; <nl> import org . apache . cassandra . io . util . SegmentedFile ; <nl> import org . apache . cassandra . service . StorageService ; <nl> @ @ - 211 , 62 + 212 , 35 @ @ public class SSTableWriter extends SSTable <nl> return dfile . length ( ) / ( dataPosition / keys ) ; <nl> } <nl> <nl> + public static Builder createBuilder ( Descriptor desc ) <nl> + { <nl> + if ( ! desc . isLatestVersion ) <nl> + / / TODO : streaming between different versions will fail : need support for <nl> + / / recovering other versions to provide a stable streaming api <nl> + throw new RuntimeException ( String . format ( " Cannot recover SSTable with version % s ( current version % s ) . " , <nl> + desc . version , Descriptor . CURRENT _ VERSION ) ) ; <nl> + <nl> + return new Builder ( desc ) ; <nl> + } <nl> + <nl> / * * <nl> - * If either of the index or filter files are missing , rebuilds both . <nl> - * TODO : Builds most of the in - memory state of the sstable , but doesn ' t actually open it . <nl> + * Removes the given SSTable from temporary status and opens it , rebuilding the <nl> + * bloom filter and row index from the data file . <nl> * / <nl> - private static void maybeRecover ( Descriptor desc ) throws IOException <nl> + public static class Builder implements ICompactionInfo <nl> { <nl> - logger . debug ( " In maybeRecover with Descriptor { } " , desc ) ; <nl> - File ifile = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; <nl> - File ffile = new File ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; <nl> - if ( ifile . exists ( ) & & ffile . exists ( ) ) <nl> - / / nothing to do <nl> - return ; <nl> - <nl> - ColumnFamilyStore cfs = Table . open ( desc . ksname ) . getColumnFamilyStore ( desc . cfname ) ; <nl> - <nl> - / / remove existing files <nl> - ifile . delete ( ) ; <nl> - ffile . delete ( ) ; <nl> - <nl> - / / open the data file for input , and an IndexWriter for output <nl> - BufferedRandomAccessFile dfile = new BufferedRandomAccessFile ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) , " r " , 8 * 1024 * 1024 ) ; <nl> - IndexWriter iwriter ; <nl> - long estimatedRows ; <nl> - try <nl> - { <nl> - estimatedRows = estimateRows ( desc , dfile ) ; <nl> - iwriter = new IndexWriter ( desc , StorageService . getPartitioner ( ) , estimatedRows ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - dfile . close ( ) ; <nl> - throw e ; <nl> - } <nl> + private final Descriptor desc ; <nl> + public final ColumnFamilyStore cfs ; <nl> + private BufferedRandomAccessFile dfile ; <nl> <nl> - / / build the index and filter <nl> - long rows = 0 ; <nl> - try <nl> - { <nl> - DecoratedKey key ; <nl> - long dataPosition = 0 ; <nl> - while ( dataPosition < dfile . length ( ) ) <nl> - { <nl> - key = SSTableReader . decodeKey ( StorageService . getPartitioner ( ) , desc , FBUtilities . readShortByteArray ( dfile ) ) ; <nl> - long dataSize = SSTableReader . readRowSize ( dfile , desc ) ; <nl> - iwriter . afterAppend ( key , dataPosition ) ; <nl> - dataPosition = dfile . getFilePointer ( ) + dataSize ; <nl> - dfile . seek ( dataPosition ) ; <nl> - rows + + ; <nl> - } <nl> - } <nl> - finally <nl> + public Builder ( Descriptor desc ) <nl> { <nl> + <nl> + this . desc = desc ; <nl> + cfs = Table . open ( desc . ksname ) . getColumnFamilyStore ( desc . cfname ) ; <nl> try <nl> { <nl> - dfile . close ( ) ; <nl> - iwriter . close ( ) ; <nl> + dfile = new BufferedRandomAccessFile ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) , " r " , 8 * 1024 * 1024 ) ; <nl> } <nl> catch ( IOException e ) <nl> { <nl> @ @ - 274 , 44 + 248 , 80 @ @ public class SSTableWriter extends SSTable <nl> } <nl> } <nl> <nl> - if ( ! cfs . getIndexedColumns ( ) . isEmpty ( ) ) <nl> + public SSTableReader build ( ) throws IOException <nl> { <nl> - Future future = CompactionManager . instance . submitIndexBuild ( cfs , cfs . getIndexedColumns ( ) , new KeyIterator ( desc ) ) ; <nl> + File ifile = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; <nl> + File ffile = new File ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; <nl> + assert ! ifile . exists ( ) ; <nl> + assert ! ffile . exists ( ) ; <nl> + <nl> + IndexWriter iwriter ; <nl> + long estimatedRows ; <nl> try <nl> { <nl> - future . get ( ) ; <nl> - for ( byte [ ] column : cfs . getIndexedColumns ( ) ) <nl> - cfs . getIndexedColumnFamilyStore ( column ) . forceBlockingFlush ( ) ; <nl> + estimatedRows = estimateRows ( desc , dfile ) ; <nl> + iwriter = new IndexWriter ( desc , StorageService . getPartitioner ( ) , estimatedRows ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + dfile . close ( ) ; <nl> + throw e ; <nl> } <nl> - catch ( InterruptedException e ) <nl> + <nl> + / / build the index and filter <nl> + long rows = 0 ; <nl> + try <nl> { <nl> - throw new AssertionError ( e ) ; <nl> + DecoratedKey key ; <nl> + long dataPosition = 0 ; <nl> + while ( dataPosition < dfile . length ( ) ) <nl> + { <nl> + key = SSTableReader . decodeKey ( StorageService . getPartitioner ( ) , desc , FBUtilities . readShortByteArray ( dfile ) ) ; <nl> + long dataSize = SSTableReader . readRowSize ( dfile , desc ) ; <nl> + iwriter . afterAppend ( key , dataPosition ) ; <nl> + dataPosition = dfile . getFilePointer ( ) + dataSize ; <nl> + dfile . seek ( dataPosition ) ; <nl> + rows + + ; <nl> + } <nl> } <nl> - catch ( ExecutionException e ) <nl> + finally <nl> { <nl> - throw new RuntimeException ( e ) ; <nl> + try <nl> + { <nl> + dfile . close ( ) ; <nl> + iwriter . close ( ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + throw new IOError ( e ) ; <nl> + } <nl> } <nl> + <nl> + logger . debug ( " estimated row count was % s of real count " , ( ( double ) estimatedRows ) / rows ) ; <nl> + return SSTableReader . open ( rename ( desc , SSTable . componentsFor ( desc ) ) ) ; <nl> } <nl> <nl> - logger . debug ( " estimated row count was % s of real count " , ( ( double ) estimatedRows ) / rows ) ; <nl> - } <nl> + public long getTotalBytes ( ) <nl> + { <nl> + try <nl> + { <nl> + return dfile . length ( ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + throw new IOError ( e ) ; <nl> + } <nl> + } <nl> <nl> - / * * <nl> - * Removes the given SSTable from temporary status and opens it , rebuilding the non - essential portions of the <nl> - * file if necessary . <nl> - * / <nl> - public static SSTableReader recoverAndOpen ( Descriptor desc ) throws IOException <nl> - { <nl> - if ( ! desc . isLatestVersion ) <nl> - / / TODO : streaming between different versions will fail : need support for <nl> - / / recovering other versions to provide a stable streaming api <nl> - throw new RuntimeException ( String . format ( " Cannot recover SSTable with version % s ( current version % s ) . " , <nl> - desc . version , Descriptor . CURRENT _ VERSION ) ) ; <nl> + public long getBytesRead ( ) <nl> + { <nl> + return dfile . getFilePointer ( ) ; <nl> + } <nl> <nl> - / / FIXME : once maybeRecover is recovering BMIs , it should return the recovered <nl> - / / components <nl> - maybeRecover ( desc ) ; <nl> - return SSTableReader . open ( rename ( desc , SSTable . componentsFor ( desc ) ) ) ; <nl> + public String getTaskType ( ) <nl> + { <nl> + return " SSTable rebuild " ; <nl> + } <nl> } <nl> <nl> / * * <nl> diff - - git a / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java b / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java <nl> index 3c3c233 . . 0eafae9 100644 <nl> - - - a / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java <nl> + + + b / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java <nl> @ @ - 100 , 24 + 100 , 6 @ @ public class IncomingStreamReader <nl> fc . close ( ) ; <nl> } <nl> <nl> - addSSTable ( localFile ) ; <nl> - session . finished ( remoteFile ) ; <nl> - } <nl> - <nl> - public static void addSSTable ( PendingFile pendingFile ) <nl> - { <nl> - / / file was successfully streamed <nl> - Descriptor desc = pendingFile . desc ; <nl> - try <nl> - { <nl> - SSTableReader sstable = SSTableWriter . recoverAndOpen ( pendingFile . desc ) ; <nl> - Table . open ( desc . ksname ) . getColumnFamilyStore ( desc . cfname ) . addSSTable ( sstable ) ; <nl> - logger . info ( " Streaming added " + sstable ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - logger . error ( " Failed adding { } " , pendingFile , e ) ; <nl> - throw new RuntimeException ( " Not able to add streamed file " + pendingFile . getFilename ( ) , e ) ; <nl> - } <nl> + session . finished ( remoteFile , localFile ) ; <nl> } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / streaming / StreamInSession . java b / src / java / org / apache / cassandra / streaming / StreamInSession . java <nl> index f535146 . . dab9f8c 100644 <nl> - - - a / src / java / org / apache / cassandra / streaming / StreamInSession . java <nl> + + + b / src / java / org / apache / cassandra / streaming / StreamInSession . java <nl> @ @ - 22 , 7 + 22 , 13 @ @ import java . io . IOException ; <nl> import java . net . InetAddress ; <nl> import java . util . * ; <nl> import java . util . concurrent . ConcurrentMap ; <nl> + import java . util . concurrent . ExecutionException ; <nl> + import java . util . concurrent . Future ; <nl> <nl> + import org . apache . cassandra . db . ColumnFamilyStore ; <nl> + import org . apache . cassandra . db . CompactionManager ; <nl> + import org . apache . cassandra . db . Table ; <nl> + import org . apache . cassandra . io . sstable . * ; <nl> import org . apache . cassandra . net . MessagingService ; <nl> import org . cliffc . high _ scale _ lib . NonBlockingHashMap ; <nl> import org . apache . cassandra . utils . Pair ; <nl> @ @ - 41 , 6 + 47 , 8 @ @ public class StreamInSession <nl> private final Pair < InetAddress , Long > context ; <nl> private final Runnable callback ; <nl> private String table ; <nl> + private final List < Future < SSTableReader > > buildFutures = new ArrayList < Future < SSTableReader > > ( ) ; <nl> + private ColumnFamilyStore cfs ; <nl> <nl> private StreamInSession ( Pair < InetAddress , Long > context , Runnable callback ) <nl> { <nl> @ @ - 84 , 13 + 92 , 19 @ @ public class StreamInSession <nl> if ( logger . isDebugEnabled ( ) ) <nl> logger . debug ( " Adding file { } to Stream Request queue " , file . getFilename ( ) ) ; <nl> this . files . add ( file ) ; <nl> + if ( cfs = = null ) <nl> + cfs = Table . open ( file . desc . ksname ) . getColumnFamilyStore ( file . desc . cfname ) ; <nl> } <nl> } <nl> <nl> - public void finished ( PendingFile remoteFile ) throws IOException <nl> + public void finished ( PendingFile remoteFile , PendingFile localFile ) throws IOException <nl> { <nl> if ( logger . isDebugEnabled ( ) ) <nl> logger . debug ( " Finished { } . Sending ack to { } " , remoteFile , this ) ; <nl> + <nl> + Future future = CompactionManager . instance . submitSSTableBuild ( localFile . desc ) ; <nl> + buildFutures . add ( future ) ; <nl> + <nl> files . remove ( remoteFile ) ; <nl> StreamReply reply = new StreamReply ( remoteFile . getFilename ( ) , getSessionId ( ) , StreamReply . Status . FILE _ FINISHED ) ; <nl> / / send a StreamStatus message telling the source node it can delete this file <nl> @ @ - 108 , 6 + 122 , 31 @ @ public class StreamInSession <nl> { <nl> if ( files . isEmpty ( ) ) <nl> { <nl> + / / wait for bloom filters and row indexes to finish building <nl> + List < SSTableReader > sstables = new ArrayList < SSTableReader > ( buildFutures . size ( ) ) ; <nl> + for ( Future < SSTableReader > future : buildFutures ) <nl> + { <nl> + try <nl> + { <nl> + SSTableReader sstable = future . get ( ) ; <nl> + cfs . addSSTable ( sstable ) ; <nl> + sstables . add ( sstable ) ; <nl> + } <nl> + catch ( InterruptedException e ) <nl> + { <nl> + throw new AssertionError ( e ) ; <nl> + } <nl> + catch ( ExecutionException e ) <nl> + { <nl> + throw new RuntimeException ( e ) ; <nl> + } <nl> + } <nl> + <nl> + / / build secondary indexes <nl> + if ( cfs ! = null & & ! cfs . getIndexedColumns ( ) . isEmpty ( ) ) <nl> + cfs . buildSecondaryIndexes ( sstables , cfs . getIndexedColumns ( ) ) ; <nl> + <nl> + / / send reply to source that we ' re done <nl> StreamReply reply = new StreamReply ( " " , getSessionId ( ) , StreamReply . Status . SESSION _ FINISHED ) ; <nl> logger . info ( " Finished streaming session { } from { } " , getSessionId ( ) , getHost ( ) ) ; <nl> MessagingService . instance . sendOneWay ( reply . createMessage ( ) , getHost ( ) ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableWriterTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableWriterTest . java <nl> index 052199e . . 570c5e1 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableWriterTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableWriterTest . java <nl> @ @ - 29 , 21 + 29 , 17 @ @ import java . util . Arrays ; <nl> import java . util . HashMap ; <nl> import java . util . List ; <nl> import java . util . Map ; <nl> + import java . util . concurrent . ExecutionException ; <nl> <nl> import org . apache . cassandra . CleanupHelper ; <nl> - import org . apache . cassandra . db . Column ; <nl> - import org . apache . cassandra . db . ColumnFamily ; <nl> - import org . apache . cassandra . db . ColumnFamilyStore ; <nl> - import org . apache . cassandra . db . Row ; <nl> - import org . apache . cassandra . db . RowMutation ; <nl> - import org . apache . cassandra . db . Table ; <nl> - import org . apache . cassandra . db . TimestampClock ; <nl> + import org . apache . cassandra . db . * ; <nl> import org . apache . cassandra . db . filter . IFilter ; <nl> import org . apache . cassandra . db . columniterator . IdentityQueryFilter ; <nl> import org . apache . cassandra . db . filter . QueryPath ; <nl> import org . apache . cassandra . dht . IPartitioner ; <nl> import org . apache . cassandra . dht . Range ; <nl> import org . apache . cassandra . io . util . DataOutputBuffer ; <nl> + import org . apache . cassandra . io . util . FileUtils ; <nl> import org . apache . cassandra . service . StorageService ; <nl> import org . apache . cassandra . thrift . IndexClause ; <nl> import org . apache . cassandra . thrift . IndexExpression ; <nl> @ @ - 54 , 7 + 50 , 7 @ @ import org . junit . Test ; <nl> public class SSTableWriterTest extends CleanupHelper { <nl> <nl> @ Test <nl> - public void testRecoverAndOpen ( ) throws IOException <nl> + public void testRecoverAndOpen ( ) throws IOException , ExecutionException , InterruptedException <nl> { <nl> RowMutation rm ; <nl> <nl> @ @ - 80 , 13 + 76 , 13 @ @ public class SSTableWriterTest extends CleanupHelper { <nl> <nl> SSTableReader orig = SSTableUtils . writeRawSSTable ( " Keyspace1 " , " Indexed1 " , entries ) ; <nl> / / whack the index to trigger the recover <nl> - new File ( orig . desc . filenameFor ( Component . PRIMARY _ INDEX ) ) . delete ( ) ; <nl> - new File ( orig . desc . filenameFor ( Component . FILTER ) ) . delete ( ) ; <nl> - <nl> - SSTableReader sstr = SSTableWriter . recoverAndOpen ( orig . desc ) ; <nl> - <nl> + FileUtils . deleteWithConfirm ( orig . desc . filenameFor ( Component . PRIMARY _ INDEX ) ) ; <nl> + FileUtils . deleteWithConfirm ( orig . desc . filenameFor ( Component . FILTER ) ) ; <nl> + <nl> + SSTableReader sstr = CompactionManager . instance . submitSSTableBuild ( orig . desc ) . get ( ) ; <nl> ColumnFamilyStore cfs = Table . open ( " Keyspace1 " ) . getColumnFamilyStore ( " Indexed1 " ) ; <nl> cfs . addSSTable ( sstr ) ; <nl> + cfs . buildSecondaryIndexes ( cfs . getSSTables ( ) , cfs . getIndexedColumns ( ) ) ; <nl> <nl> IndexExpression expr = new IndexExpression ( " birthdate " . getBytes ( " UTF8 " ) , IndexOperator . EQ , FBUtilities . toByteArray ( 1L ) ) ; <nl> IndexClause clause = new IndexClause ( Arrays . asList ( expr ) , " " . getBytes ( ) , 100 ) ; <nl> @ @ - 95 , 7 + 91 , 7 @ @ public class SSTableWriterTest extends CleanupHelper { <nl> Range range = new Range ( p . getMinimumToken ( ) , p . getMinimumToken ( ) ) ; <nl> List < Row > rows = cfs . scan ( clause , range , filter ) ; <nl> <nl> - assertEquals ( " IndexExpression should return two rows on recoverAndOpen " , 2 , rows . size ( ) ) ; <nl> + assertEquals ( " IndexExpression should return two rows on recoverAndOpen " , 2 , rows . size ( ) ) ; <nl> assertTrue ( " First result should be ' k1 ' " , Arrays . equals ( " k1 " . getBytes ( ) , rows . get ( 0 ) . key . key ) ) ; <nl> } <nl> }

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index c7d466a . . f1ac423 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 3 . 0 . 0 - beta1 
 + * Replace all usages of Adler32 with CRC32 
 * Fix row deletion bug for Materialized Views ( CASSANDRA - 10014 ) 
 * Support mixed - version clusters with Cassandra 2 . 1 and 2 . 2 ( CASSANDRA - 9704 ) 
 * Fix multiple slices on RowSearchers ( CASSANDRA - 10002 ) 
 @ @ - 9 , 6 + 10 , 7 @ @ 
 * Add transparent data encryption core classes ( CASSANDRA - 9945 ) 
 * Bytecode inspection for Java - UDFs ( CASSANDRA - 9890 ) 
 * Use byte to serialize MT hash length ( CASSANDRA - 9792 ) 
 + * Replace usage of Adler32 with CRC32 ( CASSANDRA - 8684 ) 
 Merged from 2 . 2 : 
 * Add checksum to saved cache files ( CASSANDRA - 9265 ) 
 * Log warning when using an aggregate without partition key ( CASSANDRA - 9737 ) 
 diff - - git a / src / java / org / apache / cassandra / cache / AutoSavingCache . java b / src / java / org / apache / cassandra / cache / AutoSavingCache . java 
 index 3c5b6a5 . . 2a838ab 100644 
 - - - a / src / java / org / apache / cassandra / cache / AutoSavingCache . java 
 + + + b / src / java / org / apache / cassandra / cache / AutoSavingCache . java 
 @ @ - 62 , 7 + 62 , 7 @ @ public class AutoSavingCache < K extends CacheKey , V > extends InstrumentingCache < K 
 protected final CacheService . CacheType cacheType ; 
 
 private final CacheSerializer < K , V > cacheLoader ; 
 - private static final String CURRENT _ VERSION = " c " ; 
 + private static final String CURRENT _ VERSION = " d " ; 
 
 private static volatile IStreamFactory streamFactory = new IStreamFactory ( ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java 
 index 01b4655 . . c38f4d2 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java 
 + + + b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java 
 @ @ - 24 , 7 + 24 , 7 @ @ import java . util . Map ; 
 import java . util . TreeMap ; 
 import java . util . concurrent . ThreadLocalRandom ; 
 import java . util . zip . Adler32 ; 
 - 
 + import java . util . zip . Checksum ; 
 
 import com . google . common . primitives . Ints ; 
 
 @ @ - 58 , 7 + 58 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 private ByteBuffer compressed ; 
 
 / / re - use single crc object 
 - private final Adler32 checksum ; 
 + private final Checksum checksum ; 
 
 / / raw checksum bytes 
 private ByteBuffer checksumBytes ; 
 @ @ - 67 , 7 + 67 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 { 
 super ( channel , metadata . chunkLength ( ) , metadata . compressedFileLength , metadata . compressor ( ) . preferredBufferType ( ) ) ; 
 this . metadata = metadata ; 
 - checksum = new Adler32 ( ) ; 
 + checksum = metadata . checksumType . newInstance ( ) ; 
 
 chunkSegments = file = = null ? null : file . chunkSegments ( ) ; 
 if ( chunkSegments = = null ) 
 @ @ - 131 , 7 + 131 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 if ( metadata . parameters . getCrcCheckChance ( ) > ThreadLocalRandom . current ( ) . nextDouble ( ) ) 
 { 
 compressed . rewind ( ) ; 
 - checksum . update ( compressed ) ; 
 + metadata . checksumType . update ( checksum , ( compressed ) ) ; 
 
 if ( checksum ( chunk ) ! = ( int ) checksum . getValue ( ) ) 
 throw new CorruptBlockException ( getPath ( ) , chunk ) ; 
 @ @ - 193 , 7 + 193 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 { 
 compressedChunk . position ( chunkOffset ) . limit ( chunkOffset + chunk . length ) ; 
 
 - checksum . update ( compressedChunk ) ; 
 + metadata . checksumType . update ( checksum , compressedChunk ) ; 
 
 compressedChunk . limit ( compressedChunk . capacity ( ) ) ; 
 if ( compressedChunk . getInt ( ) ! = ( int ) checksum . getValue ( ) ) 
 diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 index bc1e6f6 . . a4afa3f 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 + + + b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 @ @ - 23 , 7 + 23 , 7 @ @ import java . io . File ; 
 import java . io . IOException ; 
 import java . nio . ByteBuffer ; 
 import java . nio . channels . Channels ; 
 - import java . util . zip . Adler32 ; 
 + import java . util . zip . CRC32 ; 
 
 import org . apache . cassandra . io . FSReadError ; 
 import org . apache . cassandra . io . FSWriteError ; 
 @ @ - 204 , 7 + 204 , 7 @ @ public class CompressedSequentialWriter extends SequentialWriter 
 throw new CorruptBlockException ( getPath ( ) , chunkOffset , chunkSize ) ; 
 } 
 
 - Adler32 checksum = new Adler32 ( ) ; 
 + CRC32 checksum = new CRC32 ( ) ; 
 compressed . rewind ( ) ; 
 checksum . update ( compressed ) ; 
 
 diff - - git a / src / java / org / apache / cassandra / io / compress / CompressionMetadata . java b / src / java / org / apache / cassandra / io / compress / CompressionMetadata . java 
 index bd6da2c . . f5d8f7e 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / CompressionMetadata . java 
 + + + b / src / java / org / apache / cassandra / io / compress / CompressionMetadata . java 
 @ @ - 53 , 6 + 53 , 7 @ @ import org . apache . cassandra . io . util . FileUtils ; 
 import org . apache . cassandra . io . util . Memory ; 
 import org . apache . cassandra . io . util . SafeMemory ; 
 import org . apache . cassandra . schema . CompressionParams ; 
 + import org . apache . cassandra . utils . ChecksumType ; 
 import org . apache . cassandra . utils . Pair ; 
 import org . apache . cassandra . utils . concurrent . Transactional ; 
 import org . apache . cassandra . utils . concurrent . Ref ; 
 @ @ - 71 , 6 + 72 , 7 @ @ public class CompressionMetadata 
 private final long chunkOffsetsSize ; 
 public final String indexFilePath ; 
 public final CompressionParams parameters ; 
 + public final ChecksumType checksumType ; 
 
 / * * 
 * Create metadata about given compressed file including uncompressed data length , chunk size 
 @ @ - 86 , 13 + 88 , 14 @ @ public class CompressionMetadata 
 public static CompressionMetadata create ( String dataFilePath ) 
 { 
 Descriptor desc = Descriptor . fromFilename ( dataFilePath ) ; 
 - return new CompressionMetadata ( desc . filenameFor ( Component . COMPRESSION _ INFO ) , new File ( dataFilePath ) . length ( ) ) ; 
 + return new CompressionMetadata ( desc . filenameFor ( Component . COMPRESSION _ INFO ) , new File ( dataFilePath ) . length ( ) , desc . version . compressedChecksumType ( ) ) ; 
 } 
 
 @ VisibleForTesting 
 - CompressionMetadata ( String indexFilePath , long compressedLength ) 
 + CompressionMetadata ( String indexFilePath , long compressedLength , ChecksumType checksumType ) 
 { 
 this . indexFilePath = indexFilePath ; 
 + this . checksumType = checksumType ; 
 
 try ( DataInputStream stream = new DataInputStream ( new FileInputStream ( indexFilePath ) ) ) 
 { 
 @ @ - 131 , 7 + 134 , 7 @ @ public class CompressionMetadata 
 this . chunkOffsetsSize = chunkOffsets . size ( ) ; 
 } 
 
 - private CompressionMetadata ( String filePath , CompressionParams parameters , SafeMemory offsets , long offsetsSize , long dataLength , long compressedLength ) 
 + private CompressionMetadata ( String filePath , CompressionParams parameters , SafeMemory offsets , long offsetsSize , long dataLength , long compressedLength , ChecksumType checksumType ) 
 { 
 this . indexFilePath = filePath ; 
 this . parameters = parameters ; 
 @ @ - 139 , 6 + 142 , 7 @ @ public class CompressionMetadata 
 this . compressedFileLength = compressedLength ; 
 this . chunkOffsets = offsets ; 
 this . chunkOffsetsSize = offsetsSize ; 
 + this . checksumType = checksumType ; 
 } 
 
 public ICompressor compressor ( ) 
 @ @ - 380 , 7 + 384 , 7 @ @ public class CompressionMetadata 
 if ( count < this . count ) 
 compressedLength = offsets . getLong ( count * 8L ) ; 
 
 - return new CompressionMetadata ( filePath , parameters , offsets , count * 8L , dataLength , compressedLength ) ; 
 + return new CompressionMetadata ( filePath , parameters , offsets , count * 8L , dataLength , compressedLength , ChecksumType . CRC32 ) ; 
 } 
 
 / * * 
 @ @ - 398 , 7 + 402 , 7 @ @ public class CompressionMetadata 
 / * * 
 * Reset the writer so that the next chunk offset written will be the 
 * one of { @ code chunkIndex } . 
 - * 
 + * 
 * @ param chunkIndex the next index to write 
 * / 
 public void resetAndTruncate ( int chunkIndex ) 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / Component . java b / src / java / org / apache / cassandra / io / sstable / Component . java 
 index a431f29 . . 54dd35b 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / Component . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / Component . java 
 @ @ - 48 , 7 + 48 , 7 @ @ public class Component 
 / / statistical metadata about the content of the sstable 
 STATS ( " Statistics . db " ) , 
 / / holds adler32 checksum of the data file 
 - DIGEST ( " Digest . adler32 " ) , 
 + DIGEST ( new String [ ] { " Digest . crc32 " , " Digest . adler32 " } ) , 
 / / holds the CRC32 for chunks in an a uncompressed file . 
 CRC ( " CRC . db " ) , 
 / / holds SSTable Index Summary ( sampling of Index component ) 
 @ @ - 56 , 19 + 56 , 25 @ @ public class Component 
 / / table of contents , stores the list of all components for the sstable 
 TOC ( " TOC . txt " ) , 
 / / custom component , used by e . g . custom compaction strategy 
 - CUSTOM ( null ) ; 
 + CUSTOM ( new String [ ] { null } ) ; 
 
 - final String repr ; 
 + final String [ ] repr ; 
 Type ( String repr ) 
 { 
 + this ( new String [ ] { repr } ) ; 
 + } 
 + 
 + Type ( String [ ] repr ) 
 + { 
 this . repr = repr ; 
 } 
 
 static Type fromRepresentation ( String repr ) 
 { 
 for ( Type type : TYPES ) 
 - if ( repr . equals ( type . repr ) ) 
 - return type ; 
 + for ( String representation : type . repr ) 
 + if ( repr . equals ( representation ) ) 
 + return type ; 
 return CUSTOM ; 
 } 
 } 
 @ @ - 90 , 7 + 96 , 7 @ @ public class Component 
 
 public Component ( Type type ) 
 { 
 - this ( type , type . repr ) ; 
 + this ( type , type . repr [ 0 ] ) ; 
 assert type ! = Type . CUSTOM ; 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / format / Version . java b / src / java / org / apache / cassandra / io / sstable / format / Version . java 
 index 10ceb24 . . 9ef0b43 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / format / Version . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / format / Version . java 
 @ @ - 19 , 6 + 19 , 8 @ @ package org . apache . cassandra . io . sstable . format ; 
 
 import java . util . regex . Pattern ; 
 
 + import org . apache . cassandra . utils . ChecksumType ; 
 + 
 / * * 
 * A set of feature flags associated with a SSTable format 
 * 
 @ @ - 48 , 7 + 50 , 9 @ @ public abstract class Version 
 
 public abstract boolean hasNewStatsFile ( ) ; 
 
 - public abstract boolean hasAllAdlerChecksums ( ) ; 
 + public abstract ChecksumType compressedChecksumType ( ) ; 
 + 
 + public abstract ChecksumType uncompressedChecksumType ( ) ; 
 
 public abstract boolean hasRepairedAt ( ) ; 
 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / format / big / BigFormat . java b / src / java / org / apache / cassandra / io / sstable / format / big / BigFormat . java 
 index 860cd9f . . 6df4b1e 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / format / big / BigFormat . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / format / big / BigFormat . java 
 @ @ - 32 , 6 + 32 , 7 @ @ import org . apache . cassandra . io . sstable . format . Version ; 
 import org . apache . cassandra . io . sstable . metadata . MetadataCollector ; 
 import org . apache . cassandra . io . sstable . metadata . StatsMetadata ; 
 import org . apache . cassandra . net . MessagingService ; 
 + import org . apache . cassandra . utils . ChecksumType ; 
 
 / * * 
 * Legacy bigtable format 
 @ @ - 81 , 11 + 82 , 11 @ @ public class BigFormat implements SSTableFormat 
 static class WriterFactory extends SSTableWriter . Factory 
 { 
 @ Override 
 - public SSTableWriter open ( Descriptor descriptor , 
 - long keyCount , 
 - long repairedAt , 
 - CFMetaData metadata , 
 - MetadataCollector metadataCollector , 
 + public SSTableWriter open ( Descriptor descriptor , 
 + long keyCount , 
 + long repairedAt , 
 + CFMetaData metadata , 
 + MetadataCollector metadataCollector , 
 SerializationHeader header , 
 LifecycleTransaction txn ) 
 { 
 @ @ - 126 , 7 + 127 , 8 @ @ public class BigFormat implements SSTableFormat 
 private final boolean isLatestVersion ; 
 private final boolean hasSamplingLevel ; 
 private final boolean newStatsFile ; 
 - private final boolean hasAllAdlerChecksums ; 
 + private final ChecksumType compressedChecksumType ; 
 + private final ChecksumType uncompressedChecksumType ; 
 private final boolean hasRepairedAt ; 
 private final boolean tracksLegacyCounterShards ; 
 private final boolean newFileName ; 
 @ @ - 145 , 7 + 147 , 19 @ @ public class BigFormat implements SSTableFormat 
 isLatestVersion = version . compareTo ( current _ version ) = = 0 ; 
 hasSamplingLevel = version . compareTo ( " ka " ) > = 0 ; 
 newStatsFile = version . compareTo ( " ka " ) > = 0 ; 
 - hasAllAdlerChecksums = version . compareTo ( " ka " ) > = 0 ; 
 + 
 + / / For a while Adler32 was in use , now the CRC32 instrinsic is very good especially after Haswell 
 + / / PureJavaCRC32 was always faster than Adler32 . See CASSANDRA - 8684 
 + ChecksumType checksumType = ChecksumType . CRC32 ; 
 + if ( version . compareTo ( " ka " ) > = 0 & & version . compareTo ( " ma " ) < 0 ) 
 + checksumType = ChecksumType . Adler32 ; 
 + this . uncompressedChecksumType = checksumType ; 
 + 
 + checksumType = ChecksumType . CRC32 ; 
 + if ( version . compareTo ( " jb " ) > = 0 & & version . compareTo ( " ma " ) < 0 ) 
 + checksumType = ChecksumType . Adler32 ; 
 + this . compressedChecksumType = checksumType ; 
 + 
 hasRepairedAt = version . compareTo ( " ka " ) > = 0 ; 
 tracksLegacyCounterShards = version . compareTo ( " ka " ) > = 0 ; 
 
 @ @ - 177 , 9 + 191 , 15 @ @ public class BigFormat implements SSTableFormat 
 } 
 
 @ Override 
 - public boolean hasAllAdlerChecksums ( ) 
 + public ChecksumType compressedChecksumType ( ) 
 + { 
 + return compressedChecksumType ; 
 + } 
 + 
 + @ Override 
 + public ChecksumType uncompressedChecksumType ( ) 
 { 
 - return hasAllAdlerChecksums ; 
 + return uncompressedChecksumType ; 
 } 
 
 @ Override 
 diff - - git a / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java b / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java 
 index 976ff23 . . 3fc247b 100644 
 - - - a / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java 
 + + + b / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java 
 @ @ - 20 , 7 + 20 , 7 @ @ package org . apache . cassandra . io . util ; 
 
 import java . io . File ; 
 import java . io . IOException ; 
 - import java . util . zip . Adler32 ; 
 + import java . util . zip . CRC32 ; 
 
 import org . apache . cassandra . io . compress . BufferType ; 
 import org . apache . cassandra . utils . ByteBufferUtil ; 
 @ @ - 52 , 7 + 52 , 7 @ @ public class ChecksummedRandomAccessReader extends RandomAccessReader 
 { 
 ChannelProxy channel = new ChannelProxy ( file ) ; 
 RandomAccessReader crcReader = RandomAccessReader . open ( crcFile ) ; 
 - DataIntegrityMetadata . ChecksumValidator validator = new DataIntegrityMetadata . ChecksumValidator ( new Adler32 ( ) , 
 + DataIntegrityMetadata . ChecksumValidator validator = new DataIntegrityMetadata . ChecksumValidator ( new CRC32 ( ) , 
 crcReader , 
 file . getPath ( ) ) ; 
 return new ChecksummedRandomAccessReader ( file , channel , validator ) ; 
 diff - - git a / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java b / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java 
 index ac2ab47 . . 70cd860 100644 
 - - - a / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java 
 + + + b / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java 
 @ @ - 25 , 7 + 25 , 6 @ @ import java . io . IOError ; 
 import java . io . IOException ; 
 import java . nio . ByteBuffer ; 
 import java . nio . file . Files ; 
 - import java . util . zip . Adler32 ; 
 import java . util . zip . CRC32 ; 
 import java . util . zip . CheckedInputStream ; 
 import java . util . zip . Checksum ; 
 @ @ - 35 , 7 + 34 , 6 @ @ import com . google . common . base . Charsets ; 
 import org . apache . cassandra . io . FSWriteError ; 
 import org . apache . cassandra . io . sstable . Component ; 
 import org . apache . cassandra . io . sstable . Descriptor ; 
 - import org . apache . cassandra . utils . FBUtilities ; 
 
 public class DataIntegrityMetadata 
 { 
 @ @ - 53 , 7 + 51 , 7 @ @ public class DataIntegrityMetadata 
 
 public ChecksumValidator ( Descriptor descriptor ) throws IOException 
 { 
 - this ( descriptor . version . hasAllAdlerChecksums ( ) ? new Adler32 ( ) : new CRC32 ( ) , 
 + this ( descriptor . version . uncompressedChecksumType ( ) . newInstance ( ) , 
 RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . CRC ) ) ) , 
 descriptor . filenameFor ( Component . DATA ) ) ; 
 } 
 @ @ - 110 , 7 + 108 , 7 @ @ public class DataIntegrityMetadata 
 public FileDigestValidator ( Descriptor descriptor ) throws IOException 
 { 
 this . descriptor = descriptor ; 
 - checksum = descriptor . version . hasAllAdlerChecksums ( ) ? new Adler32 ( ) : new CRC32 ( ) ; 
 + checksum = descriptor . version . uncompressedChecksumType ( ) . newInstance ( ) ; 
 digestReader = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . DIGEST ) ) ) ; 
 dataReader = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . DATA ) ) ) ; 
 try 
 @ @ - 154 , 9 + 152 , 9 @ @ public class DataIntegrityMetadata 
 
 public static class ChecksumWriter 
 { 
 - private final Adler32 incrementalChecksum = new Adler32 ( ) ; 
 + private final CRC32 incrementalChecksum = new CRC32 ( ) ; 
 private final DataOutput incrementalOut ; 
 - private final Adler32 fullChecksum = new Adler32 ( ) ; 
 + private final CRC32 fullChecksum = new CRC32 ( ) ; 
 
 public ChecksumWriter ( DataOutput incrementalOut ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / streaming / compress / CompressedInputStream . java b / src / java / org / apache / cassandra / streaming / compress / CompressedInputStream . java 
 index 099fd14 . . 0a118b2 100644 
 - - - a / src / java / org / apache / cassandra / streaming / compress / CompressedInputStream . java 
 + + + b / src / java / org / apache / cassandra / streaming / compress / CompressedInputStream . java 
 @ @ - 24 , 13 + 24 , 13 @ @ import java . util . Iterator ; 
 import java . util . concurrent . ArrayBlockingQueue ; 
 import java . util . concurrent . BlockingQueue ; 
 import java . util . concurrent . ThreadLocalRandom ; 
 - import java . util . zip . Adler32 ; 
 import java . util . zip . Checksum ; 
 
 import com . google . common . collect . Iterators ; 
 import com . google . common . primitives . Ints ; 
 
 import org . apache . cassandra . io . compress . CompressionMetadata ; 
 + import org . apache . cassandra . utils . ChecksumType ; 
 import org . apache . cassandra . utils . WrappedRunnable ; 
 
 / * * 
 @ @ - 65 , 10 + 65 , 10 @ @ public class CompressedInputStream extends InputStream 
 * @ param source Input source to read compressed data from 
 * @ param info Compression info 
 * / 
 - public CompressedInputStream ( InputStream source , CompressionInfo info ) 
 + public CompressedInputStream ( InputStream source , CompressionInfo info , ChecksumType checksumType ) 
 { 
 this . info = info ; 
 - this . checksum = new Adler32 ( ) ; 
 + this . checksum = checksumType . newInstance ( ) ; 
 this . buffer = new byte [ info . parameters . chunkLength ( ) ] ; 
 / / buffer is limited to store up to 1024 chunks 
 this . dataBuffer = new ArrayBlockingQueue < byte [ ] > ( Math . min ( info . chunks . length , 1024 ) ) ; 
 diff - - git a / src / java / org / apache / cassandra / streaming / compress / CompressedStreamReader . java b / src / java / org / apache / cassandra / streaming / compress / CompressedStreamReader . java 
 index 47832f0 . . 205291b 100644 
 - - - a / src / java / org / apache / cassandra / streaming / compress / CompressedStreamReader . java 
 + + + b / src / java / org / apache / cassandra / streaming / compress / CompressedStreamReader . java 
 @ @ - 77 , 7 + 77 , 7 @ @ public class CompressedStreamReader extends StreamReader 
 
 SSTableWriter writer = createWriter ( cfs , totalSize , repairedAt , format ) ; 
 
 - CompressedInputStream cis = new CompressedInputStream ( Channels . newInputStream ( channel ) , compressionInfo ) ; 
 + CompressedInputStream cis = new CompressedInputStream ( Channels . newInputStream ( channel ) , compressionInfo , inputVersion . compressedChecksumType ( ) ) ; 
 BytesReadTracker in = new BytesReadTracker ( new DataInputStream ( cis ) ) ; 
 StreamDeserializer deserializer = new StreamDeserializer ( cfs . metadata , in , inputVersion , header . toHeader ( cfs . metadata ) ) ; 
 try 
 diff - - git a / src / java / org / apache / cassandra / utils / ChecksumType . java b / src / java / org / apache / cassandra / utils / ChecksumType . java 
 new file mode 100644 
 index 0000000 . . c9a1eb8 
 - - - / dev / null 
 + + + b / src / java / org / apache / cassandra / utils / ChecksumType . java 
 @ @ - 0 , 0 + 1 , 63 @ @ 
 + / * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , software 
 + * distributed under the License is distributed on an " AS IS " BASIS , 
 + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 + * See the License for the specific language governing permissions and 
 + * limitations under the License . 
 + * / 
 + package org . apache . cassandra . utils ; 
 + 
 + import java . nio . ByteBuffer ; 
 + import java . util . zip . Checksum ; 
 + import java . util . zip . CRC32 ; 
 + import java . util . zip . Adler32 ; 
 + 
 + public enum ChecksumType 
 + { 
 + Adler32 ( ) 
 + { 
 + 
 + @ Override 
 + public Checksum newInstance ( ) 
 + { 
 + return new Adler32 ( ) ; 
 + } 
 + 
 + @ Override 
 + public void update ( Checksum checksum , ByteBuffer buf ) 
 + { 
 + ( ( Adler32 ) checksum ) . update ( buf ) ; 
 + } 
 + 
 + } , 
 + CRC32 ( ) 
 + { 
 + 
 + @ Override 
 + public Checksum newInstance ( ) 
 + { 
 + return new CRC32 ( ) ; 
 + } 
 + 
 + @ Override 
 + public void update ( Checksum checksum , ByteBuffer buf ) 
 + { 
 + ( ( CRC32 ) checksum ) . update ( buf ) ; 
 + } 
 + 
 + } ; 
 + 
 + public abstract Checksum newInstance ( ) ; 
 + 
 + public abstract void update ( Checksum checksum , ByteBuffer buf ) ; 
 + } 
 diff - - git a / test / unit / org / apache / cassandra / db / VerifyTest . java b / test / unit / org / apache / cassandra / db / VerifyTest . java 
 index 3bd4a47 . . 13ce0c1 100644 
 - - - a / test / unit / org / apache / cassandra / db / VerifyTest . java 
 + + + b / test / unit / org / apache / cassandra / db / VerifyTest . java 
 @ @ - 19 , 6 + 19 , 7 @ @ 
 package org . apache . cassandra . db ; 
 
 import com . google . common . base . Charsets ; 
 + 
 import org . apache . cassandra . OrderedJUnit4ClassRunner ; 
 import org . apache . cassandra . SchemaLoader ; 
 import org . apache . cassandra . Util ; 
 @ @ - 43 , 7 + 44 , 7 @ @ import org . junit . runner . RunWith ; 
 
 import java . io . * ; 
 import java . nio . file . Files ; 
 - import java . util . zip . Adler32 ; 
 + import java . util . zip . CRC32 ; 
 import java . util . zip . CheckedInputStream ; 
 
 import static org . junit . Assert . fail ; 
 @ @ - 371 , 8 + 372 , 8 @ @ public class VerifyTest 
 protected long simpleFullChecksum ( String filename ) throws IOException 
 { 
 FileInputStream inputStream = new FileInputStream ( filename ) ; 
 - Adler32 adlerChecksum = new Adler32 ( ) ; 
 - CheckedInputStream cinStream = new CheckedInputStream ( inputStream , adlerChecksum ) ; 
 + CRC32 checksum = new CRC32 ( ) ; 
 + CheckedInputStream cinStream = new CheckedInputStream ( inputStream , checksum ) ; 
 byte [ ] b = new byte [ 128 ] ; 
 while ( cinStream . read ( b ) > = 0 ) { 
 } 
 diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 index cc76a9e . . 8f94cf2 100644 
 - - - a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 + + + b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 @ @ - 24 , 7 + 24 , 6 @ @ import java . io . RandomAccessFile ; 
 import java . util . Random ; 
 
 import org . junit . Test ; 
 - 
 import org . apache . cassandra . db . ClusteringComparator ; 
 import org . apache . cassandra . db . marshal . BytesType ; 
 import org . apache . cassandra . exceptions . ConfigurationException ; 
 @ @ - 35 , 6 + 34 , 7 @ @ import org . apache . cassandra . io . util . FileMark ; 
 import org . apache . cassandra . io . util . RandomAccessReader ; 
 import org . apache . cassandra . io . util . SequentialWriter ; 
 import org . apache . cassandra . schema . CompressionParams ; 
 + import org . apache . cassandra . utils . ChecksumType ; 
 import org . apache . cassandra . utils . SyncUtil ; 
 
 import static org . junit . Assert . assertEquals ; 
 @ @ - 84 , 7 + 84 , 7 @ @ public class CompressedRandomAccessReaderTest 
 writer . write ( " x " . getBytes ( ) ) ; 
 writer . finish ( ) ; 
 
 - CompressedRandomAccessReader reader = CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) ) ; 
 + CompressedRandomAccessReader reader = CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) , ChecksumType . CRC32 ) ) ; 
 String res = reader . readLine ( ) ; 
 assertEquals ( res , " xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx " ) ; 
 assertEquals ( 40 , res . length ( ) ) ; 
 @ @ - 129 , 7 + 129 , 7 @ @ public class CompressedRandomAccessReaderTest 
 
 assert f . exists ( ) ; 
 RandomAccessReader reader = compressed 
 - ? CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) ) 
 + ? CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) , ChecksumType . CRC32 ) ) 
 : RandomAccessReader . open ( f ) ; 
 String expected = " The quick brown fox jumps over the lazy dog " ; 
 assertEquals ( expected . length ( ) , reader . length ( ) ) ; 
 @ @ - 171 , 7 + 171 , 7 @ @ public class CompressedRandomAccessReaderTest 
 ChannelProxy channel = new ChannelProxy ( file ) ; 
 
 / / open compression metadata and get chunk information 
 - CompressionMetadata meta = new CompressionMetadata ( metadata . getPath ( ) , file . length ( ) ) ; 
 + CompressionMetadata meta = new CompressionMetadata ( metadata . getPath ( ) , file . length ( ) , ChecksumType . CRC32 ) ; 
 CompressionMetadata . Chunk chunk = meta . chunkFor ( 0 ) ; 
 
 RandomAccessReader reader = CompressedRandomAccessReader . open ( channel , meta ) ; 
 diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java 
 index db99317 . . 28af0ae 100644 
 - - - a / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java 
 + + + b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java 
 @ @ - 31 , 6 + 31 , 7 @ @ import org . junit . After ; 
 import org . junit . Test ; 
 
 import junit . framework . Assert ; 
 + 
 import org . apache . cassandra . db . ClusteringComparator ; 
 import org . apache . cassandra . db . marshal . AbstractType ; 
 import org . apache . cassandra . db . marshal . BytesType ; 
 @ @ - 41 , 6 + 42 , 7 @ @ import org . apache . cassandra . io . util . FileMark ; 
 import org . apache . cassandra . io . util . RandomAccessReader ; 
 import org . apache . cassandra . io . util . SequentialWriterTest ; 
 import org . apache . cassandra . schema . CompressionParams ; 
 + import org . apache . cassandra . utils . ChecksumType ; 
 
 public class CompressedSequentialWriterTest extends SequentialWriterTest 
 { 
 @ @ - 115 , 7 + 117 , 7 @ @ public class CompressedSequentialWriterTest extends SequentialWriterTest 
 } 
 
 assert f . exists ( ) ; 
 - RandomAccessReader reader = CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) ) ; 
 + RandomAccessReader reader = CompressedRandomAccessReader . open ( channel , new CompressionMetadata ( filename + " . metadata " , f . length ( ) , ChecksumType . CRC32 ) ) ; 
 assertEquals ( dataPre . length + rawPost . length , reader . length ( ) ) ; 
 byte [ ] result = new byte [ ( int ) reader . length ( ) ] ; 
 
 diff - - git a / test / unit / org / apache / cassandra / streaming / compression / CompressedInputStreamTest . java b / test / unit / org / apache / cassandra / streaming / compression / CompressedInputStreamTest . java 
 index 37aea91 . . e3014c3 100644 
 - - - a / test / unit / org / apache / cassandra / streaming / compression / CompressedInputStreamTest . java 
 + + + b / test / unit / org / apache / cassandra / streaming / compression / CompressedInputStreamTest . java 
 @ @ - 21 , 7 + 21 , 6 @ @ import java . io . * ; 
 import java . util . * ; 
 
 import org . junit . Test ; 
 - 
 import org . apache . cassandra . db . ClusteringComparator ; 
 import org . apache . cassandra . db . marshal . BytesType ; 
 import org . apache . cassandra . io . compress . CompressedSequentialWriter ; 
 @ @ - 32 , 6 + 31 , 7 @ @ import org . apache . cassandra . io . sstable . Descriptor ; 
 import org . apache . cassandra . io . sstable . metadata . MetadataCollector ; 
 import org . apache . cassandra . streaming . compress . CompressedInputStream ; 
 import org . apache . cassandra . streaming . compress . CompressionInfo ; 
 + import org . apache . cassandra . utils . ChecksumType ; 
 import org . apache . cassandra . utils . Pair ; 
 
 / * * 
 @ @ - 108 , 7 + 108 , 7 @ @ public class CompressedInputStreamTest 
 
 / / read buffer using CompressedInputStream 
 CompressionInfo info = new CompressionInfo ( chunks , param ) ; 
 - CompressedInputStream input = new CompressedInputStream ( new ByteArrayInputStream ( toRead ) , info ) ; 
 + CompressedInputStream input = new CompressedInputStream ( new ByteArrayInputStream ( toRead ) , info , ChecksumType . CRC32 ) ; 
 DataInputStream in = new DataInputStream ( input ) ; 
 
 for ( int i = 0 ; i < sections . size ( ) ; i + + )

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 index b9635e8 . . d2fd951 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 @ @ - 33 , 6 + 33 , 7 @ @ import javax . management . ObjectName ; 
 import com . google . common . collect . Iterables ; 
 import org . apache . commons . collections . IteratorUtils ; 
 import org . apache . commons . lang . ArrayUtils ; 
 + import org . apache . commons . lang . StringUtils ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 @ @ - 205 , 12 + 206 , 9 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 logger . info ( " Creating index { } . { } " , table , indexedCfMetadata . cfName ) ; 
 Runnable runnable = new WrappedRunnable ( ) 
 { 
 - public void runMayThrow ( ) throws IOException , ExecutionException , InterruptedException 
 + public void runMayThrow ( ) throws IOException 
 { 
 - logger . debug ( " Submitting index build to compactionmanager " ) ; 
 - ReducingKeyIterator iter = new ReducingKeyIterator ( getSSTables ( ) ) ; 
 - Future future = CompactionManager . instance . submitIndexBuild ( ColumnFamilyStore . this , FBUtilities . getSingleColumnSet ( info . name ) , iter ) ; 
 - future . get ( ) ; 
 + buildSecondaryIndexes ( getSSTables ( ) , FBUtilities . getSingleColumnSet ( info . name ) ) ; 
 logger . info ( " Index { } complete " , indexedCfMetadata . cfName ) ; 
 SystemTable . setIndexBuilt ( table , indexedCfMetadata . cfName ) ; 
 } 
 @ @ - 220 , 6 + 218 , 26 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 indexedColumns . put ( info . name , indexedCfs ) ; 
 } 
 
 + public void buildSecondaryIndexes ( Collection < SSTableReader > sstables , SortedSet < byte [ ] > columns ) 
 + { 
 + logger . debug ( " Submitting index build to compactionmanager " ) ; 
 + Future future = CompactionManager . instance . submitIndexBuild ( this , columns , new ReducingKeyIterator ( sstables ) ) ; 
 + try 
 + { 
 + future . get ( ) ; 
 + for ( byte [ ] column : columns ) 
 + getIndexedColumnFamilyStore ( column ) . forceBlockingFlush ( ) ; 
 + } 
 + catch ( InterruptedException e ) 
 + { 
 + throw new AssertionError ( e ) ; 
 + } 
 + catch ( ExecutionException e ) 
 + { 
 + throw new RuntimeException ( e ) ; 
 + } 
 + } 
 + 
 / / called when dropping or renaming a CF . Performs mbean housekeeping . 
 void unregisterMBean ( ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / db / CompactionManager . java b / src / java / org / apache / cassandra / db / CompactionManager . java 
 index 64289db . . d5d7444 100644 
 - - - a / src / java / org / apache / cassandra / db / CompactionManager . java 
 + + + b / src / java / org / apache / cassandra / db / CompactionManager . java 
 @ @ - 18 , 38 + 18 , 38 @ @ 
 
 package org . apache . cassandra . db ; 
 
 - import java . io . IOException ; 
 import java . io . File ; 
 + import java . io . IOException ; 
 import java . lang . management . ManagementFactory ; 
 + import java . net . InetAddress ; 
 import java . util . * ; 
 import java . util . Map . Entry ; 
 import java . util . concurrent . Callable ; 
 import java . util . concurrent . Future ; 
 - import javax . management . * ; 
 + import javax . management . MBeanServer ; 
 + import javax . management . ObjectName ; 
 
 + import org . apache . commons . collections . PredicateUtils ; 
 + import org . apache . commons . collections . iterators . CollatingIterator ; 
 + import org . apache . commons . collections . iterators . FilterIterator ; 
 + import org . apache . commons . lang . StringUtils ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 - 
 import org . apache . cassandra . concurrent . DebuggableThreadPoolExecutor ; 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . dht . Range ; 
 - import org . apache . cassandra . io . * ; 
 + import org . apache . cassandra . io . AbstractCompactedRow ; 
 + import org . apache . cassandra . io . CompactionIterator ; 
 + import org . apache . cassandra . io . ICompactionInfo ; 
 import org . apache . cassandra . io . sstable . * ; 
 - import org . apache . cassandra . config . DatabaseDescriptor ; 
 - import org . apache . cassandra . service . StorageService ; 
 - import org . apache . cassandra . service . AntiEntropyService ; 
 import org . apache . cassandra . io . util . FileUtils ; 
 + import org . apache . cassandra . service . AntiEntropyService ; 
 + import org . apache . cassandra . service . StorageService ; 
 import org . apache . cassandra . utils . FBUtilities ; 
 import org . apache . cassandra . utils . Pair ; 
 import org . cliffc . high _ scale _ lib . NonBlockingHashMap ; 
 
 - import java . net . InetAddress ; 
 - 
 - import org . apache . commons . lang . StringUtils ; 
 - import org . apache . commons . collections . iterators . FilterIterator ; 
 - import org . apache . commons . collections . iterators . CollatingIterator ; 
 - import org . apache . commons . collections . PredicateUtils ; 
 - 
 public class CompactionManager implements CompactionManagerMBean 
 { 
 public static final String MBEAN _ OBJECT _ NAME = " org . apache . cassandra . db : type = CompactionManager " ; 
 @ @ - 508 , 6 + 508 , 20 @ @ public class CompactionManager implements CompactionManagerMBean 
 return executor . submit ( runnable ) ; 
 } 
 
 + public Future < SSTableReader > submitSSTableBuild ( Descriptor desc ) 
 + { 
 + final SSTableWriter . Builder builder = SSTableWriter . createBuilder ( desc ) ; 
 + Callable < SSTableReader > callable = new Callable < SSTableReader > ( ) 
 + { 
 + public SSTableReader call ( ) throws IOException 
 + { 
 + executor . beginCompaction ( builder . cfs , builder ) ; 
 + return builder . build ( ) ; 
 + } 
 + } ; 
 + return executor . submit ( callable ) ; 
 + } 
 + 
 private static class AntiCompactionIterator extends CompactionIterator 
 { 
 private Set < SSTableScanner > scanners ; 
 @ @ - 550 , 6 + 564 , 11 @ @ public class CompactionManager implements CompactionManagerMBean 
 } 
 return scanners ; 
 } 
 + 
 + public String getTaskType ( ) 
 + { 
 + return " Anticompaction " ; 
 + } 
 } 
 
 public void checkAllColumnFamilies ( ) throws IOException 
 diff - - git a / src / java / org / apache / cassandra / db / Table . java b / src / java / org / apache / cassandra / db / Table . java 
 index d96608b . . 82f41c2 100644 
 - - - a / src / java / org / apache / cassandra / db / Table . java 
 + + + b / src / java / org / apache / cassandra / db / Table . java 
 @ @ - 456 , 7 + 456 , 8 @ @ public class Table 
 synchronized ( indexLockFor ( key . key ) ) 
 { 
 ColumnFamily cf = readCurrentIndexedColumns ( key , cfs , columns ) ; 
 - applyIndexUpdates ( key . key , memtablesToFlush , cf , cfs , cf . getColumnNames ( ) , null ) ; 
 + if ( cf ! = null ) 
 + applyIndexUpdates ( key . key , memtablesToFlush , cf , cfs , cf . getColumnNames ( ) , null ) ; 
 } 
 } 
 finally 
 diff - - git a / src / java / org / apache / cassandra / io / CompactionIterator . java b / src / java / org / apache / cassandra / io / CompactionIterator . java 
 index 304e3dd . . a617a26 100644 
 - - - a / src / java / org / apache / cassandra / io / CompactionIterator . java 
 + + + b / src / java / org / apache / cassandra / io / CompactionIterator . java 
 @ @ - 161 , 4 + 161 , 8 @ @ implements Closeable , ICompactionInfo 
 return bytesRead ; 
 } 
 
 + public String getTaskType ( ) 
 + { 
 + return " Compaction " ; 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / ICompactionInfo . java b / src / java / org / apache / cassandra / io / ICompactionInfo . java 
 index a00c5bb . . 7815c10 100644 
 - - - a / src / java / org / apache / cassandra / io / ICompactionInfo . java 
 + + + b / src / java / org / apache / cassandra / io / ICompactionInfo . java 
 @ @ - 5 , 4 + 5 , 6 @ @ public interface ICompactionInfo 
 public long getTotalBytes ( ) ; 
 
 public long getBytesRead ( ) ; 
 + 
 + public String getTaskType ( ) ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 index b74c675 . . 07e9b23 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 @ @ - 4 , 16 + 4 , 16 @ @ import java . io . Closeable ; 
 import java . io . File ; 
 import java . io . IOError ; 
 import java . io . IOException ; 
 + import java . util . Iterator ; 
 
 import com . google . common . collect . AbstractIterator ; 
 
 import org . apache . cassandra . db . DecoratedKey ; 
 - import org . apache . cassandra . io . ICompactionInfo ; 
 import org . apache . cassandra . io . util . BufferedRandomAccessFile ; 
 import org . apache . cassandra . service . StorageService ; 
 import org . apache . cassandra . utils . FBUtilities ; 
 
 - public class KeyIterator extends AbstractIterator < DecoratedKey > implements IKeyIterator 
 + public class KeyIterator extends AbstractIterator < DecoratedKey > implements Iterator < DecoratedKey > , Closeable 
 { 
 private final BufferedRandomAccessFile in ; 
 private final Descriptor desc ; 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / ReducingKeyIterator . java b / src / java / org / apache / cassandra / io / sstable / ReducingKeyIterator . java 
 index e3ebaed . . ed07f5b 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / ReducingKeyIterator . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / ReducingKeyIterator . java 
 @ @ - 66 , 6 + 66 , 11 @ @ public class ReducingKeyIterator implements IKeyIterator 
 return m ; 
 } 
 
 + public String getTaskType ( ) 
 + { 
 + return " Secondary index build " ; 
 + } 
 + 
 public boolean hasNext ( ) 
 { 
 return iter . hasNext ( ) ; 
 @ @ - 73 , 7 + 78 , 7 @ @ public class ReducingKeyIterator implements IKeyIterator 
 
 public DecoratedKey next ( ) 
 { 
 - return ( DecoratedKey ) iter . next ( ) ; 
 + return iter . next ( ) ; 
 } 
 
 public void remove ( ) 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 index 6118339 . . d4bc6b4 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 @ @ - 32 , 6 + 32 , 7 @ @ import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . db . * ; 
 import org . apache . cassandra . dht . IPartitioner ; 
 import org . apache . cassandra . io . AbstractCompactedRow ; 
 + import org . apache . cassandra . io . ICompactionInfo ; 
 import org . apache . cassandra . io . util . BufferedRandomAccessFile ; 
 import org . apache . cassandra . io . util . SegmentedFile ; 
 import org . apache . cassandra . service . StorageService ; 
 @ @ - 211 , 62 + 212 , 35 @ @ public class SSTableWriter extends SSTable 
 return dfile . length ( ) / ( dataPosition / keys ) ; 
 } 
 
 + public static Builder createBuilder ( Descriptor desc ) 
 + { 
 + if ( ! desc . isLatestVersion ) 
 + / / TODO : streaming between different versions will fail : need support for 
 + / / recovering other versions to provide a stable streaming api 
 + throw new RuntimeException ( String . format ( " Cannot recover SSTable with version % s ( current version % s ) . " , 
 + desc . version , Descriptor . CURRENT _ VERSION ) ) ; 
 + 
 + return new Builder ( desc ) ; 
 + } 
 + 
 / * * 
 - * If either of the index or filter files are missing , rebuilds both . 
 - * TODO : Builds most of the in - memory state of the sstable , but doesn ' t actually open it . 
 + * Removes the given SSTable from temporary status and opens it , rebuilding the 
 + * bloom filter and row index from the data file . 
 * / 
 - private static void maybeRecover ( Descriptor desc ) throws IOException 
 + public static class Builder implements ICompactionInfo 
 { 
 - logger . debug ( " In maybeRecover with Descriptor { } " , desc ) ; 
 - File ifile = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; 
 - File ffile = new File ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; 
 - if ( ifile . exists ( ) & & ffile . exists ( ) ) 
 - / / nothing to do 
 - return ; 
 - 
 - ColumnFamilyStore cfs = Table . open ( desc . ksname ) . getColumnFamilyStore ( desc . cfname ) ; 
 - 
 - / / remove existing files 
 - ifile . delete ( ) ; 
 - ffile . delete ( ) ; 
 - 
 - / / open the data file for input , and an IndexWriter for output 
 - BufferedRandomAccessFile dfile = new BufferedRandomAccessFile ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) , " r " , 8 * 1024 * 1024 ) ; 
 - IndexWriter iwriter ; 
 - long estimatedRows ; 
 - try 
 - { 
 - estimatedRows = estimateRows ( desc , dfile ) ; 
 - iwriter = new IndexWriter ( desc , StorageService . getPartitioner ( ) , estimatedRows ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - dfile . close ( ) ; 
 - throw e ; 
 - } 
 + private final Descriptor desc ; 
 + public final ColumnFamilyStore cfs ; 
 + private BufferedRandomAccessFile dfile ; 
 
 - / / build the index and filter 
 - long rows = 0 ; 
 - try 
 - { 
 - DecoratedKey key ; 
 - long dataPosition = 0 ; 
 - while ( dataPosition < dfile . length ( ) ) 
 - { 
 - key = SSTableReader . decodeKey ( StorageService . getPartitioner ( ) , desc , FBUtilities . readShortByteArray ( dfile ) ) ; 
 - long dataSize = SSTableReader . readRowSize ( dfile , desc ) ; 
 - iwriter . afterAppend ( key , dataPosition ) ; 
 - dataPosition = dfile . getFilePointer ( ) + dataSize ; 
 - dfile . seek ( dataPosition ) ; 
 - rows + + ; 
 - } 
 - } 
 - finally 
 + public Builder ( Descriptor desc ) 
 { 
 + 
 + this . desc = desc ; 
 + cfs = Table . open ( desc . ksname ) . getColumnFamilyStore ( desc . cfname ) ; 
 try 
 { 
 - dfile . close ( ) ; 
 - iwriter . close ( ) ; 
 + dfile = new BufferedRandomAccessFile ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) , " r " , 8 * 1024 * 1024 ) ; 
 } 
 catch ( IOException e ) 
 { 
 @ @ - 274 , 44 + 248 , 80 @ @ public class SSTableWriter extends SSTable 
 } 
 } 
 
 - if ( ! cfs . getIndexedColumns ( ) . isEmpty ( ) ) 
 + public SSTableReader build ( ) throws IOException 
 { 
 - Future future = CompactionManager . instance . submitIndexBuild ( cfs , cfs . getIndexedColumns ( ) , new KeyIterator ( desc ) ) ; 
 + File ifile = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; 
 + File ffile = new File ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; 
 + assert ! ifile . exists ( ) ; 
 + assert ! ffile . exists ( ) ; 
 + 
 + IndexWriter iwriter ; 
 + long estimatedRows ; 
 try 
 { 
 - future . get ( ) ; 
 - for ( byte [ ] column : cfs . getIndexedColumns ( ) ) 
 - cfs . getIndexedColumnFamilyStore ( column ) . forceBlockingFlush ( ) ; 
 + estimatedRows = estimateRows ( desc , dfile ) ; 
 + iwriter = new IndexWriter ( desc , StorageService . getPartitioner ( ) , estimatedRows ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + dfile . close ( ) ; 
 + throw e ; 
 } 
 - catch ( InterruptedException e ) 
 + 
 + / / build the index and filter 
 + long rows = 0 ; 
 + try 
 { 
 - throw new AssertionError ( e ) ; 
 + DecoratedKey key ; 
 + long dataPosition = 0 ; 
 + while ( dataPosition < dfile . length ( ) ) 
 + { 
 + key = SSTableReader . decodeKey ( StorageService . getPartitioner ( ) , desc , FBUtilities . readShortByteArray ( dfile ) ) ; 
 + long dataSize = SSTableReader . readRowSize ( dfile , desc ) ; 
 + iwriter . afterAppend ( key , dataPosition ) ; 
 + dataPosition = dfile . getFilePointer ( ) + dataSize ; 
 + dfile . seek ( dataPosition ) ; 
 + rows + + ; 
 + } 
 } 
 - catch ( ExecutionException e ) 
 + finally 
 { 
 - throw new RuntimeException ( e ) ; 
 + try 
 + { 
 + dfile . close ( ) ; 
 + iwriter . close ( ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + throw new IOError ( e ) ; 
 + } 
 } 
 + 
 + logger . debug ( " estimated row count was % s of real count " , ( ( double ) estimatedRows ) / rows ) ; 
 + return SSTableReader . open ( rename ( desc , SSTable . componentsFor ( desc ) ) ) ; 
 } 
 
 - logger . debug ( " estimated row count was % s of real count " , ( ( double ) estimatedRows ) / rows ) ; 
 - } 
 + public long getTotalBytes ( ) 
 + { 
 + try 
 + { 
 + return dfile . length ( ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + throw new IOError ( e ) ; 
 + } 
 + } 
 
 - / * * 
 - * Removes the given SSTable from temporary status and opens it , rebuilding the non - essential portions of the 
 - * file if necessary . 
 - * / 
 - public static SSTableReader recoverAndOpen ( Descriptor desc ) throws IOException 
 - { 
 - if ( ! desc . isLatestVersion ) 
 - / / TODO : streaming between different versions will fail : need support for 
 - / / recovering other versions to provide a stable streaming api 
 - throw new RuntimeException ( String . format ( " Cannot recover SSTable with version % s ( current version % s ) . " , 
 - desc . version , Descriptor . CURRENT _ VERSION ) ) ; 
 + public long getBytesRead ( ) 
 + { 
 + return dfile . getFilePointer ( ) ; 
 + } 
 
 - / / FIXME : once maybeRecover is recovering BMIs , it should return the recovered 
 - / / components 
 - maybeRecover ( desc ) ; 
 - return SSTableReader . open ( rename ( desc , SSTable . componentsFor ( desc ) ) ) ; 
 + public String getTaskType ( ) 
 + { 
 + return " SSTable rebuild " ; 
 + } 
 } 
 
 / * * 
 diff - - git a / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java b / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java 
 index 3c3c233 . . 0eafae9 100644 
 - - - a / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java 
 + + + b / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java 
 @ @ - 100 , 24 + 100 , 6 @ @ public class IncomingStreamReader 
 fc . close ( ) ; 
 } 
 
 - addSSTable ( localFile ) ; 
 - session . finished ( remoteFile ) ; 
 - } 
 - 
 - public static void addSSTable ( PendingFile pendingFile ) 
 - { 
 - / / file was successfully streamed 
 - Descriptor desc = pendingFile . desc ; 
 - try 
 - { 
 - SSTableReader sstable = SSTableWriter . recoverAndOpen ( pendingFile . desc ) ; 
 - Table . open ( desc . ksname ) . getColumnFamilyStore ( desc . cfname ) . addSSTable ( sstable ) ; 
 - logger . info ( " Streaming added " + sstable ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - logger . error ( " Failed adding { } " , pendingFile , e ) ; 
 - throw new RuntimeException ( " Not able to add streamed file " + pendingFile . getFilename ( ) , e ) ; 
 - } 
 + session . finished ( remoteFile , localFile ) ; 
 } 
 } 
 diff - - git a / src / java / org / apache / cassandra / streaming / StreamInSession . java b / src / java / org / apache / cassandra / streaming / StreamInSession . java 
 index f535146 . . dab9f8c 100644 
 - - - a / src / java / org / apache / cassandra / streaming / StreamInSession . java 
 + + + b / src / java / org / apache / cassandra / streaming / StreamInSession . java 
 @ @ - 22 , 7 + 22 , 13 @ @ import java . io . IOException ; 
 import java . net . InetAddress ; 
 import java . util . * ; 
 import java . util . concurrent . ConcurrentMap ; 
 + import java . util . concurrent . ExecutionException ; 
 + import java . util . concurrent . Future ; 
 
 + import org . apache . cassandra . db . ColumnFamilyStore ; 
 + import org . apache . cassandra . db . CompactionManager ; 
 + import org . apache . cassandra . db . Table ; 
 + import org . apache . cassandra . io . sstable . * ; 
 import org . apache . cassandra . net . MessagingService ; 
 import org . cliffc . high _ scale _ lib . NonBlockingHashMap ; 
 import org . apache . cassandra . utils . Pair ; 
 @ @ - 41 , 6 + 47 , 8 @ @ public class StreamInSession 
 private final Pair < InetAddress , Long > context ; 
 private final Runnable callback ; 
 private String table ; 
 + private final List < Future < SSTableReader > > buildFutures = new ArrayList < Future < SSTableReader > > ( ) ; 
 + private ColumnFamilyStore cfs ; 
 
 private StreamInSession ( Pair < InetAddress , Long > context , Runnable callback ) 
 { 
 @ @ - 84 , 13 + 92 , 19 @ @ public class StreamInSession 
 if ( logger . isDebugEnabled ( ) ) 
 logger . debug ( " Adding file { } to Stream Request queue " , file . getFilename ( ) ) ; 
 this . files . add ( file ) ; 
 + if ( cfs = = null ) 
 + cfs = Table . open ( file . desc . ksname ) . getColumnFamilyStore ( file . desc . cfname ) ; 
 } 
 } 
 
 - public void finished ( PendingFile remoteFile ) throws IOException 
 + public void finished ( PendingFile remoteFile , PendingFile localFile ) throws IOException 
 { 
 if ( logger . isDebugEnabled ( ) ) 
 logger . debug ( " Finished { } . Sending ack to { } " , remoteFile , this ) ; 
 + 
 + Future future = CompactionManager . instance . submitSSTableBuild ( localFile . desc ) ; 
 + buildFutures . add ( future ) ; 
 + 
 files . remove ( remoteFile ) ; 
 StreamReply reply = new StreamReply ( remoteFile . getFilename ( ) , getSessionId ( ) , StreamReply . Status . FILE _ FINISHED ) ; 
 / / send a StreamStatus message telling the source node it can delete this file 
 @ @ - 108 , 6 + 122 , 31 @ @ public class StreamInSession 
 { 
 if ( files . isEmpty ( ) ) 
 { 
 + / / wait for bloom filters and row indexes to finish building 
 + List < SSTableReader > sstables = new ArrayList < SSTableReader > ( buildFutures . size ( ) ) ; 
 + for ( Future < SSTableReader > future : buildFutures ) 
 + { 
 + try 
 + { 
 + SSTableReader sstable = future . get ( ) ; 
 + cfs . addSSTable ( sstable ) ; 
 + sstables . add ( sstable ) ; 
 + } 
 + catch ( InterruptedException e ) 
 + { 
 + throw new AssertionError ( e ) ; 
 + } 
 + catch ( ExecutionException e ) 
 + { 
 + throw new RuntimeException ( e ) ; 
 + } 
 + } 
 + 
 + / / build secondary indexes 
 + if ( cfs ! = null & & ! cfs . getIndexedColumns ( ) . isEmpty ( ) ) 
 + cfs . buildSecondaryIndexes ( sstables , cfs . getIndexedColumns ( ) ) ; 
 + 
 + / / send reply to source that we ' re done 
 StreamReply reply = new StreamReply ( " " , getSessionId ( ) , StreamReply . Status . SESSION _ FINISHED ) ; 
 logger . info ( " Finished streaming session { } from { } " , getSessionId ( ) , getHost ( ) ) ; 
 MessagingService . instance . sendOneWay ( reply . createMessage ( ) , getHost ( ) ) ; 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableWriterTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableWriterTest . java 
 index 052199e . . 570c5e1 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableWriterTest . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableWriterTest . java 
 @ @ - 29 , 21 + 29 , 17 @ @ import java . util . Arrays ; 
 import java . util . HashMap ; 
 import java . util . List ; 
 import java . util . Map ; 
 + import java . util . concurrent . ExecutionException ; 
 
 import org . apache . cassandra . CleanupHelper ; 
 - import org . apache . cassandra . db . Column ; 
 - import org . apache . cassandra . db . ColumnFamily ; 
 - import org . apache . cassandra . db . ColumnFamilyStore ; 
 - import org . apache . cassandra . db . Row ; 
 - import org . apache . cassandra . db . RowMutation ; 
 - import org . apache . cassandra . db . Table ; 
 - import org . apache . cassandra . db . TimestampClock ; 
 + import org . apache . cassandra . db . * ; 
 import org . apache . cassandra . db . filter . IFilter ; 
 import org . apache . cassandra . db . columniterator . IdentityQueryFilter ; 
 import org . apache . cassandra . db . filter . QueryPath ; 
 import org . apache . cassandra . dht . IPartitioner ; 
 import org . apache . cassandra . dht . Range ; 
 import org . apache . cassandra . io . util . DataOutputBuffer ; 
 + import org . apache . cassandra . io . util . FileUtils ; 
 import org . apache . cassandra . service . StorageService ; 
 import org . apache . cassandra . thrift . IndexClause ; 
 import org . apache . cassandra . thrift . IndexExpression ; 
 @ @ - 54 , 7 + 50 , 7 @ @ import org . junit . Test ; 
 public class SSTableWriterTest extends CleanupHelper { 
 
 @ Test 
 - public void testRecoverAndOpen ( ) throws IOException 
 + public void testRecoverAndOpen ( ) throws IOException , ExecutionException , InterruptedException 
 { 
 RowMutation rm ; 
 
 @ @ - 80 , 13 + 76 , 13 @ @ public class SSTableWriterTest extends CleanupHelper { 
 
 SSTableReader orig = SSTableUtils . writeRawSSTable ( " Keyspace1 " , " Indexed1 " , entries ) ; 
 / / whack the index to trigger the recover 
 - new File ( orig . desc . filenameFor ( Component . PRIMARY _ INDEX ) ) . delete ( ) ; 
 - new File ( orig . desc . filenameFor ( Component . FILTER ) ) . delete ( ) ; 
 - 
 - SSTableReader sstr = SSTableWriter . recoverAndOpen ( orig . desc ) ; 
 - 
 + FileUtils . deleteWithConfirm ( orig . desc . filenameFor ( Component . PRIMARY _ INDEX ) ) ; 
 + FileUtils . deleteWithConfirm ( orig . desc . filenameFor ( Component . FILTER ) ) ; 
 + 
 + SSTableReader sstr = CompactionManager . instance . submitSSTableBuild ( orig . desc ) . get ( ) ; 
 ColumnFamilyStore cfs = Table . open ( " Keyspace1 " ) . getColumnFamilyStore ( " Indexed1 " ) ; 
 cfs . addSSTable ( sstr ) ; 
 + cfs . buildSecondaryIndexes ( cfs . getSSTables ( ) , cfs . getIndexedColumns ( ) ) ; 
 
 IndexExpression expr = new IndexExpression ( " birthdate " . getBytes ( " UTF8 " ) , IndexOperator . EQ , FBUtilities . toByteArray ( 1L ) ) ; 
 IndexClause clause = new IndexClause ( Arrays . asList ( expr ) , " " . getBytes ( ) , 100 ) ; 
 @ @ - 95 , 7 + 91 , 7 @ @ public class SSTableWriterTest extends CleanupHelper { 
 Range range = new Range ( p . getMinimumToken ( ) , p . getMinimumToken ( ) ) ; 
 List < Row > rows = cfs . scan ( clause , range , filter ) ; 
 
 - assertEquals ( " IndexExpression should return two rows on recoverAndOpen " , 2 , rows . size ( ) ) ; 
 + assertEquals ( " IndexExpression should return two rows on recoverAndOpen " , 2 , rows . size ( ) ) ; 
 assertTrue ( " First result should be ' k1 ' " , Arrays . equals ( " k1 " . getBytes ( ) , rows . get ( 0 ) . key . key ) ) ; 
 } 
 }
