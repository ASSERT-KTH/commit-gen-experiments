BLEU SCORE: 0.020980574531482755

TEST MSG: move ' local only ' field to a local variable
GENERATED MSG: Remove LoadPushDown methods from pig storage .

TEST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java <nl> index 670bee1 . . 0635459 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java <nl> @ @ - 114 , 7 + 114 , 6 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo <nl> / / wide row hacks <nl> private ByteBuffer lastKey ; <nl> private Map < ByteBuffer , Cell > lastRow ; <nl> - private boolean hasNext = true ; <nl> <nl> public CassandraStorage ( ) <nl> { <nl> @ @ - 135 , 6 + 134 , 7 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo <nl> return limit ; <nl> } <nl> <nl> + @ Override <nl> public void prepareToRead ( RecordReader reader , PigSplit split ) <nl> { <nl> this . reader = reader ; <nl> @ @ - 151 , 7 + 151 , 7 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo <nl> { <nl> while ( true ) <nl> { <nl> - hasNext = reader . nextKeyValue ( ) ; <nl> + boolean hasNext = reader . nextKeyValue ( ) ; <nl> if ( ! hasNext ) <nl> { <nl> if ( tuple = = null )
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> index 6e45df5 . . cc2a707 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> @ @ - 735 , 6 + 735 , 20 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> submitFlush ( binaryMemtable . get ( ) , new CountDownLatch ( 1 ) ) ; <nl> } <nl> <nl> + public void updateRowCache ( DecoratedKey key , ColumnFamily columnFamily ) <nl> + { <nl> + if ( rowCache . isPutCopying ( ) ) <nl> + { <nl> + invalidateCachedRow ( key ) ; <nl> + } <nl> + else <nl> + { <nl> + ColumnFamily cachedRow = getRawCachedRow ( key ) ; <nl> + if ( cachedRow ! = null ) <nl> + cachedRow . addAll ( columnFamily ) ; <nl> + } <nl> + } <nl> + <nl> / * * <nl> * Insert / Update the column family for this key . <nl> * Caller is responsible for acquiring Table . flusherLock ! <nl> @ @ - 749 , 17 + 763 , 8 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> Memtable mt = getMemtableThreadSafe ( ) ; <nl> boolean flushRequested = mt . isThresholdViolated ( ) ; <nl> mt . put ( key , columnFamily ) ; <nl> - if ( rowCache . isPutCopying ( ) ) <nl> - { <nl> - invalidateCachedRow ( key ) ; <nl> - } <nl> - else <nl> - { <nl> - ColumnFamily cachedRow = getRawCachedRow ( key ) ; <nl> - if ( cachedRow ! = null ) <nl> - cachedRow . addAll ( columnFamily ) ; <nl> - writeStats . addNano ( System . nanoTime ( ) - start ) ; <nl> - } <nl> + updateRowCache ( key , columnFamily ) ; <nl> + writeStats . addNano ( System . nanoTime ( ) - start ) ; <nl> <nl> if ( DatabaseDescriptor . estimatesRealMemtableSize ( ) ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / io / PrecompactedRow . java b / src / java / org / apache / cassandra / io / PrecompactedRow . java <nl> index 9b03f4f . . cff8eee 100644 <nl> - - - a / src / java / org / apache / cassandra / io / PrecompactedRow . java <nl> + + + b / src / java / org / apache / cassandra / io / PrecompactedRow . java <nl> @ @ - 131 , 4 + 131 , 15 @ @ public class PrecompactedRow extends AbstractCompactedRow <nl> { <nl> return compactedCf = = null ? 0 : compactedCf . getColumnCount ( ) ; <nl> } <nl> + <nl> + / * * <nl> + * @ return the full column family represented by this compacted row . <nl> + * <nl> + * We do not provide this method for other AbstractCompactedRow , because this fits the whole row into <nl> + * memory and don ' t make sense for those other implementations . <nl> + * / <nl> + public ColumnFamily getFullColumnFamily ( ) throws IOException <nl> + { <nl> + return compactedCf ; <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> index 78f773d . . fdf7c1b 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> @ @ - 285 , 9 + 285 , 9 @ @ public class SSTableWriter extends SSTable <nl> try <nl> { <nl> if ( cfs . metadata . getDefaultValidator ( ) . isCommutative ( ) ) <nl> - indexer = new CommutativeRowIndexer ( desc , cfs . metadata ) ; <nl> + indexer = new CommutativeRowIndexer ( desc , cfs , type ) ; <nl> else <nl> - indexer = new RowIndexer ( desc , cfs . metadata ) ; <nl> + indexer = new RowIndexer ( desc , cfs , type ) ; <nl> } <nl> catch ( IOException e ) <nl> { <nl> @ @ - 320 , 20 + 320 , 22 @ @ public class SSTableWriter extends SSTable <nl> { <nl> protected final Descriptor desc ; <nl> public final BufferedRandomAccessFile dfile ; <nl> + private final OperationType type ; <nl> <nl> protected IndexWriter iwriter ; <nl> - protected CFMetaData metadata ; <nl> + protected ColumnFamilyStore cfs ; <nl> <nl> - RowIndexer ( Descriptor desc , CFMetaData metadata ) throws IOException <nl> + RowIndexer ( Descriptor desc , ColumnFamilyStore cfs , OperationType type ) throws IOException <nl> { <nl> - this ( desc , new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) ) , " r " , 8 * 1024 * 1024 , true ) , metadata ) ; <nl> + this ( desc , new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) ) , " r " , 8 * 1024 * 1024 , true ) , cfs , type ) ; <nl> } <nl> <nl> - protected RowIndexer ( Descriptor desc , BufferedRandomAccessFile dfile , CFMetaData metadata ) throws IOException <nl> + protected RowIndexer ( Descriptor desc , BufferedRandomAccessFile dfile , ColumnFamilyStore cfs , OperationType type ) throws IOException <nl> { <nl> this . desc = desc ; <nl> this . dfile = dfile ; <nl> - this . metadata = metadata ; <nl> + this . type = type ; <nl> + this . cfs = cfs ; <nl> } <nl> <nl> long prepareIndexing ( ) throws IOException <nl> @ @ - 377 , 6 + 379 , 53 @ @ public class SSTableWriter extends SSTable <nl> iwriter . close ( ) ; <nl> } <nl> <nl> + / * <nl> + * If the key is cached , we should : <nl> + * - For AES : run the newly received row by the cache <nl> + * - For other : invalidate the cache ( even if very unlikely , a key could be in cache in theory if a neighbor was boostrapped and <nl> + * then removed quickly afterward ( a key that we had lost but become responsible again could have stayed in cache ) . That key <nl> + * would be obsolete and so we must invalidate the cache ) . <nl> + * / <nl> + protected void updateCache ( DecoratedKey key , long dataSize , AbstractCompactedRow row ) throws IOException <nl> + { <nl> + ColumnFamily cached = cfs . getRawCachedRow ( key ) ; <nl> + if ( cached ! = null ) <nl> + { <nl> + switch ( type ) <nl> + { <nl> + case AES : <nl> + if ( dataSize > DatabaseDescriptor . getInMemoryCompactionLimit ( ) ) <nl> + { <nl> + / / We have a key in cache for a very big row , that is fishy . We don ' t fail here however because that would prevent the sstable <nl> + / / from being build ( and there is no real point anyway ) , so we just invalidate the row for correction and log a warning . <nl> + logger . warn ( " Found a cached row over the in memory compaction limit during post - streaming rebuilt ; it is highly recommended to avoid huge row on column family with row cache enabled . " ) ; <nl> + cfs . invalidateCachedRow ( key ) ; <nl> + } <nl> + else <nl> + { <nl> + ColumnFamily cf ; <nl> + if ( row = = null ) <nl> + { <nl> + / / If not provided , read from disk . <nl> + cf = ColumnFamily . create ( cfs . metadata ) ; <nl> + ColumnFamily . serializer ( ) . deserializeColumns ( dfile , cf , true , true ) ; <nl> + } <nl> + else <nl> + { <nl> + assert row instanceof PrecompactedRow ; <nl> + / / we do not purge so we should not get a null here <nl> + cf = ( ( PrecompactedRow ) row ) . getFullColumnFamily ( ) ; <nl> + } <nl> + cfs . updateRowCache ( key , cf ) ; <nl> + } <nl> + break ; <nl> + default : <nl> + cfs . invalidateCachedRow ( key ) ; <nl> + break ; <nl> + } <nl> + } <nl> + } <nl> + <nl> protected long doIndexing ( ) throws IOException <nl> { <nl> EstimatedHistogram rowSizes = SSTable . defaultRowHistogram ( ) ; <nl> @ @ - 393 , 10 + 442 , 14 @ @ public class SSTableWriter extends SSTable <nl> / / seek to next key <nl> long dataSize = SSTableReader . readRowSize ( dfile , desc ) ; <nl> rowPosition = dfile . getFilePointer ( ) + dataSize ; <nl> - <nl> + <nl> IndexHelper . skipBloomFilter ( dfile ) ; <nl> IndexHelper . skipIndex ( dfile ) ; <nl> - ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( ColumnFamily . create ( metadata ) , dfile ) ; <nl> + ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( ColumnFamily . create ( cfs . metadata ) , dfile ) ; <nl> + <nl> + / / don ' t move that statement around , it expects the dfile to be before the columns <nl> + updateCache ( key , dataSize , null ) ; <nl> + <nl> rowSizes . add ( dataSize ) ; <nl> columnCounts . add ( dfile . readInt ( ) ) ; <nl> <nl> @ @ - 424 , 9 + 477 , 9 @ @ public class SSTableWriter extends SSTable <nl> { <nl> protected BufferedRandomAccessFile writerDfile ; <nl> <nl> - CommutativeRowIndexer ( Descriptor desc , CFMetaData metadata ) throws IOException <nl> + CommutativeRowIndexer ( Descriptor desc , ColumnFamilyStore cfs , OperationType type ) throws IOException <nl> { <nl> - super ( desc , new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) ) , " r " , 8 * 1024 * 1024 , true ) , metadata ) ; <nl> + super ( desc , new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) ) , " r " , 8 * 1024 * 1024 , true ) , cfs , type ) ; <nl> writerDfile = new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) ) , " rw " , 8 * 1024 * 1024 , true ) ; <nl> } <nl> <nl> @ @ - 448 , 7 + 501 , 7 @ @ public class SSTableWriter extends SSTable <nl> <nl> / / skip data size , bloom filter , column index <nl> long dataSize = SSTableReader . readRowSize ( dfile , desc ) ; <nl> - SSTableIdentityIterator iter = new SSTableIdentityIterator ( metadata , dfile , key , dfile . getFilePointer ( ) , dataSize , true ) ; <nl> + SSTableIdentityIterator iter = new SSTableIdentityIterator ( cfs . metadata , dfile , key , dfile . getFilePointer ( ) , dataSize , true ) ; <nl> <nl> AbstractCompactedRow row ; <nl> if ( dataSize > DatabaseDescriptor . getInMemoryCompactionLimit ( ) ) <nl> @ @ - 461 , 6 + 514 , 8 @ @ public class SSTableWriter extends SSTable <nl> row = new PrecompactedRow ( controller , Collections . singletonList ( iter ) ) ; <nl> } <nl> <nl> + updateCache ( key , dataSize , row ) ; <nl> + <nl> rowSizes . add ( dataSize ) ; <nl> columnCounts . add ( row . columnCount ( ) ) ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / streaming / OperationType . java b / src / java / org / apache / cassandra / streaming / OperationType . java <nl> index 9eafcbf . . 86a3953 100644 <nl> - - - a / src / java / org / apache / cassandra / streaming / OperationType . java <nl> + + + b / src / java / org / apache / cassandra / streaming / OperationType . java <nl> @ @ - 23 , 8 + 23 , 6 @ @ package org . apache . cassandra . streaming ; <nl> * / <nl> public enum OperationType <nl> { <nl> - / / TODO : the only types of operation that are currently distinguised are AES and everything else . There is no <nl> - / / sense in having the other types ( yet ) . <nl> AES , <nl> BOOTSTRAP , <nl> UNBOOTSTRAP ,

TEST DIFF:
diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java 
 index 670bee1 . . 0635459 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java 
 + + + b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java 
 @ @ - 114 , 7 + 114 , 6 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo 
 / / wide row hacks 
 private ByteBuffer lastKey ; 
 private Map < ByteBuffer , Cell > lastRow ; 
 - private boolean hasNext = true ; 
 
 public CassandraStorage ( ) 
 { 
 @ @ - 135 , 6 + 134 , 7 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo 
 return limit ; 
 } 
 
 + @ Override 
 public void prepareToRead ( RecordReader reader , PigSplit split ) 
 { 
 this . reader = reader ; 
 @ @ - 151 , 7 + 151 , 7 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo 
 { 
 while ( true ) 
 { 
 - hasNext = reader . nextKeyValue ( ) ; 
 + boolean hasNext = reader . nextKeyValue ( ) ; 
 if ( ! hasNext ) 
 { 
 if ( tuple = = null )

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 index 6e45df5 . . cc2a707 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 @ @ - 735 , 6 + 735 , 20 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 submitFlush ( binaryMemtable . get ( ) , new CountDownLatch ( 1 ) ) ; 
 } 
 
 + public void updateRowCache ( DecoratedKey key , ColumnFamily columnFamily ) 
 + { 
 + if ( rowCache . isPutCopying ( ) ) 
 + { 
 + invalidateCachedRow ( key ) ; 
 + } 
 + else 
 + { 
 + ColumnFamily cachedRow = getRawCachedRow ( key ) ; 
 + if ( cachedRow ! = null ) 
 + cachedRow . addAll ( columnFamily ) ; 
 + } 
 + } 
 + 
 / * * 
 * Insert / Update the column family for this key . 
 * Caller is responsible for acquiring Table . flusherLock ! 
 @ @ - 749 , 17 + 763 , 8 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 Memtable mt = getMemtableThreadSafe ( ) ; 
 boolean flushRequested = mt . isThresholdViolated ( ) ; 
 mt . put ( key , columnFamily ) ; 
 - if ( rowCache . isPutCopying ( ) ) 
 - { 
 - invalidateCachedRow ( key ) ; 
 - } 
 - else 
 - { 
 - ColumnFamily cachedRow = getRawCachedRow ( key ) ; 
 - if ( cachedRow ! = null ) 
 - cachedRow . addAll ( columnFamily ) ; 
 - writeStats . addNano ( System . nanoTime ( ) - start ) ; 
 - } 
 + updateRowCache ( key , columnFamily ) ; 
 + writeStats . addNano ( System . nanoTime ( ) - start ) ; 
 
 if ( DatabaseDescriptor . estimatesRealMemtableSize ( ) ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / io / PrecompactedRow . java b / src / java / org / apache / cassandra / io / PrecompactedRow . java 
 index 9b03f4f . . cff8eee 100644 
 - - - a / src / java / org / apache / cassandra / io / PrecompactedRow . java 
 + + + b / src / java / org / apache / cassandra / io / PrecompactedRow . java 
 @ @ - 131 , 4 + 131 , 15 @ @ public class PrecompactedRow extends AbstractCompactedRow 
 { 
 return compactedCf = = null ? 0 : compactedCf . getColumnCount ( ) ; 
 } 
 + 
 + / * * 
 + * @ return the full column family represented by this compacted row . 
 + * 
 + * We do not provide this method for other AbstractCompactedRow , because this fits the whole row into 
 + * memory and don ' t make sense for those other implementations . 
 + * / 
 + public ColumnFamily getFullColumnFamily ( ) throws IOException 
 + { 
 + return compactedCf ; 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 index 78f773d . . fdf7c1b 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 @ @ - 285 , 9 + 285 , 9 @ @ public class SSTableWriter extends SSTable 
 try 
 { 
 if ( cfs . metadata . getDefaultValidator ( ) . isCommutative ( ) ) 
 - indexer = new CommutativeRowIndexer ( desc , cfs . metadata ) ; 
 + indexer = new CommutativeRowIndexer ( desc , cfs , type ) ; 
 else 
 - indexer = new RowIndexer ( desc , cfs . metadata ) ; 
 + indexer = new RowIndexer ( desc , cfs , type ) ; 
 } 
 catch ( IOException e ) 
 { 
 @ @ - 320 , 20 + 320 , 22 @ @ public class SSTableWriter extends SSTable 
 { 
 protected final Descriptor desc ; 
 public final BufferedRandomAccessFile dfile ; 
 + private final OperationType type ; 
 
 protected IndexWriter iwriter ; 
 - protected CFMetaData metadata ; 
 + protected ColumnFamilyStore cfs ; 
 
 - RowIndexer ( Descriptor desc , CFMetaData metadata ) throws IOException 
 + RowIndexer ( Descriptor desc , ColumnFamilyStore cfs , OperationType type ) throws IOException 
 { 
 - this ( desc , new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) ) , " r " , 8 * 1024 * 1024 , true ) , metadata ) ; 
 + this ( desc , new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) ) , " r " , 8 * 1024 * 1024 , true ) , cfs , type ) ; 
 } 
 
 - protected RowIndexer ( Descriptor desc , BufferedRandomAccessFile dfile , CFMetaData metadata ) throws IOException 
 + protected RowIndexer ( Descriptor desc , BufferedRandomAccessFile dfile , ColumnFamilyStore cfs , OperationType type ) throws IOException 
 { 
 this . desc = desc ; 
 this . dfile = dfile ; 
 - this . metadata = metadata ; 
 + this . type = type ; 
 + this . cfs = cfs ; 
 } 
 
 long prepareIndexing ( ) throws IOException 
 @ @ - 377 , 6 + 379 , 53 @ @ public class SSTableWriter extends SSTable 
 iwriter . close ( ) ; 
 } 
 
 + / * 
 + * If the key is cached , we should : 
 + * - For AES : run the newly received row by the cache 
 + * - For other : invalidate the cache ( even if very unlikely , a key could be in cache in theory if a neighbor was boostrapped and 
 + * then removed quickly afterward ( a key that we had lost but become responsible again could have stayed in cache ) . That key 
 + * would be obsolete and so we must invalidate the cache ) . 
 + * / 
 + protected void updateCache ( DecoratedKey key , long dataSize , AbstractCompactedRow row ) throws IOException 
 + { 
 + ColumnFamily cached = cfs . getRawCachedRow ( key ) ; 
 + if ( cached ! = null ) 
 + { 
 + switch ( type ) 
 + { 
 + case AES : 
 + if ( dataSize > DatabaseDescriptor . getInMemoryCompactionLimit ( ) ) 
 + { 
 + / / We have a key in cache for a very big row , that is fishy . We don ' t fail here however because that would prevent the sstable 
 + / / from being build ( and there is no real point anyway ) , so we just invalidate the row for correction and log a warning . 
 + logger . warn ( " Found a cached row over the in memory compaction limit during post - streaming rebuilt ; it is highly recommended to avoid huge row on column family with row cache enabled . " ) ; 
 + cfs . invalidateCachedRow ( key ) ; 
 + } 
 + else 
 + { 
 + ColumnFamily cf ; 
 + if ( row = = null ) 
 + { 
 + / / If not provided , read from disk . 
 + cf = ColumnFamily . create ( cfs . metadata ) ; 
 + ColumnFamily . serializer ( ) . deserializeColumns ( dfile , cf , true , true ) ; 
 + } 
 + else 
 + { 
 + assert row instanceof PrecompactedRow ; 
 + / / we do not purge so we should not get a null here 
 + cf = ( ( PrecompactedRow ) row ) . getFullColumnFamily ( ) ; 
 + } 
 + cfs . updateRowCache ( key , cf ) ; 
 + } 
 + break ; 
 + default : 
 + cfs . invalidateCachedRow ( key ) ; 
 + break ; 
 + } 
 + } 
 + } 
 + 
 protected long doIndexing ( ) throws IOException 
 { 
 EstimatedHistogram rowSizes = SSTable . defaultRowHistogram ( ) ; 
 @ @ - 393 , 10 + 442 , 14 @ @ public class SSTableWriter extends SSTable 
 / / seek to next key 
 long dataSize = SSTableReader . readRowSize ( dfile , desc ) ; 
 rowPosition = dfile . getFilePointer ( ) + dataSize ; 
 - 
 + 
 IndexHelper . skipBloomFilter ( dfile ) ; 
 IndexHelper . skipIndex ( dfile ) ; 
 - ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( ColumnFamily . create ( metadata ) , dfile ) ; 
 + ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( ColumnFamily . create ( cfs . metadata ) , dfile ) ; 
 + 
 + / / don ' t move that statement around , it expects the dfile to be before the columns 
 + updateCache ( key , dataSize , null ) ; 
 + 
 rowSizes . add ( dataSize ) ; 
 columnCounts . add ( dfile . readInt ( ) ) ; 
 
 @ @ - 424 , 9 + 477 , 9 @ @ public class SSTableWriter extends SSTable 
 { 
 protected BufferedRandomAccessFile writerDfile ; 
 
 - CommutativeRowIndexer ( Descriptor desc , CFMetaData metadata ) throws IOException 
 + CommutativeRowIndexer ( Descriptor desc , ColumnFamilyStore cfs , OperationType type ) throws IOException 
 { 
 - super ( desc , new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) ) , " r " , 8 * 1024 * 1024 , true ) , metadata ) ; 
 + super ( desc , new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) ) , " r " , 8 * 1024 * 1024 , true ) , cfs , type ) ; 
 writerDfile = new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ DATA ) ) , " rw " , 8 * 1024 * 1024 , true ) ; 
 } 
 
 @ @ - 448 , 7 + 501 , 7 @ @ public class SSTableWriter extends SSTable 
 
 / / skip data size , bloom filter , column index 
 long dataSize = SSTableReader . readRowSize ( dfile , desc ) ; 
 - SSTableIdentityIterator iter = new SSTableIdentityIterator ( metadata , dfile , key , dfile . getFilePointer ( ) , dataSize , true ) ; 
 + SSTableIdentityIterator iter = new SSTableIdentityIterator ( cfs . metadata , dfile , key , dfile . getFilePointer ( ) , dataSize , true ) ; 
 
 AbstractCompactedRow row ; 
 if ( dataSize > DatabaseDescriptor . getInMemoryCompactionLimit ( ) ) 
 @ @ - 461 , 6 + 514 , 8 @ @ public class SSTableWriter extends SSTable 
 row = new PrecompactedRow ( controller , Collections . singletonList ( iter ) ) ; 
 } 
 
 + updateCache ( key , dataSize , row ) ; 
 + 
 rowSizes . add ( dataSize ) ; 
 columnCounts . add ( row . columnCount ( ) ) ; 
 
 diff - - git a / src / java / org / apache / cassandra / streaming / OperationType . java b / src / java / org / apache / cassandra / streaming / OperationType . java 
 index 9eafcbf . . 86a3953 100644 
 - - - a / src / java / org / apache / cassandra / streaming / OperationType . java 
 + + + b / src / java / org / apache / cassandra / streaming / OperationType . java 
 @ @ - 23 , 8 + 23 , 6 @ @ package org . apache . cassandra . streaming ; 
 * / 
 public enum OperationType 
 { 
 - / / TODO : the only types of operation that are currently distinguised are AES and everything else . There is no 
 - / / sense in having the other types ( yet ) . 
 AES , 
 BOOTSTRAP , 
 UNBOOTSTRAP ,
