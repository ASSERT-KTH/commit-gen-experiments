BLEU SCORE: 0.037477767366779206

TEST MSG: Make it possible to set the max / min compaction thresholds
GENERATED MSG: Fix error when dropping table during compaction

TEST DIFF (one line): diff - - git a / NEWS . txt b / NEWS . txt <nl> index da80684 . . e1e76de 100644 <nl> - - - a / NEWS . txt <nl> + + + b / NEWS . txt <nl> @ @ - 38 , 6 + 38 , 11 @ @ using the provided ' sstableupgrade ' tool . <nl> <nl> New features <nl> - - - - - - - - - - - - <nl> + - LCS now respects the max _ threshold parameter when compacting - this was hard coded to 32 <nl> + before , but now it is possible to do bigger compactions when compacting from L0 to L1 . <nl> + This also applies to STCS - compactions in L0 - if there are more than 32 sstables in L0 <nl> + we will compact at most max _ threshold sstables in an L0 STCS compaction . See CASSANDRA - 14388 <nl> + for more information . <nl> - There is now an option to automatically upgrade sstables after Cassandra upgrade , enable <nl> either in ` cassandra . yaml : automatic _ sstable _ upgrade ` or via JMX during runtime . See <nl> CASSANDRA - 14197 . <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java <nl> index 7a7b1c1 . . b1091ce 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java <nl> @ @ - 31 , 6 + 31 , 7 @ @ import org . slf4j . LoggerFactory ; <nl> import com . fasterxml . jackson . databind . JsonNode ; <nl> import com . fasterxml . jackson . databind . node . JsonNodeFactory ; <nl> import com . fasterxml . jackson . databind . node . ObjectNode ; <nl> + import org . apache . cassandra . schema . CompactionParams ; <nl> import org . apache . cassandra . schema . TableMetadata ; <nl> import org . apache . cassandra . config . Config ; <nl> import org . apache . cassandra . db . ColumnFamilyStore ; <nl> @ @ - 569 , 6 + 570 , 9 @ @ public class LeveledCompactionStrategy extends AbstractCompactionStrategy <nl> <nl> uncheckedOptions . remove ( LEVEL _ FANOUT _ SIZE _ OPTION ) ; <nl> <nl> + uncheckedOptions . remove ( CompactionParams . Option . MIN _ THRESHOLD . toString ( ) ) ; <nl> + uncheckedOptions . remove ( CompactionParams . Option . MAX _ THRESHOLD . toString ( ) ) ; <nl> + <nl> uncheckedOptions = SizeTieredCompactionStrategyOptions . validateOptions ( options , uncheckedOptions ) ; <nl> <nl> return uncheckedOptions ; <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java b / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java <nl> index 013f0b4 . . 291973f 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java <nl> @ @ - 49 , 9 + 49 , 8 @ @ public class LeveledManifest <nl> private static final Logger logger = LoggerFactory . getLogger ( LeveledManifest . class ) ; <nl> <nl> / * * <nl> - * limit the number of L0 sstables we do at once , because compaction bloom filter creation <nl> - * uses a pessimistic estimate of how many keys overlap ( none ) , so we risk wasting memory <nl> - * or even OOMing when compacting highly overlapping sstables <nl> + * if we have more than MAX _ COMPACTING _ L0 sstables in L0 , we will run a round of STCS with at most <nl> + * cfs . getMaxCompactionThreshold ( ) sstables . <nl> * / <nl> private static final int MAX _ COMPACTING _ L0 = 32 ; <nl> / * * <nl> @ @ - 163 , 6 + 162 , 7 @ @ public class LeveledManifest <nl> / / The add ( . . ) : ed sstable will be sent to level 0 <nl> try <nl> { <nl> + logger . debug ( " Could not add sstable { } in level { } - dropping to 0 " , reader , reader . getSSTableLevel ( ) ) ; <nl> reader . descriptor . getMetadataSerializer ( ) . mutateLevel ( reader . descriptor , 0 ) ; <nl> reader . reloadSSTableMetadata ( ) ; <nl> } <nl> @ @ - 346 , 12 + 346 , 12 @ @ public class LeveledManifest <nl> / / L2 : 12 [ ideal : 100 ] <nl> / / <nl> / / The problem is that L0 has a much higher score ( almost 250 ) than L1 ( 11 ) , so what we ' ll <nl> - / / do is compact a batch of MAX _ COMPACTING _ L0 sstables with all 117 L1 sstables , and put the <nl> - / / result ( say , 120 sstables ) in L1 . Then we ' ll compact the next batch of MAX _ COMPACTING _ L0 , <nl> + / / do is compact a batch of cfs . getMaximumCompactionThreshold ( ) sstables with all 117 L1 sstables , and put the <nl> + / / result ( say , 120 sstables ) in L1 . Then we ' ll compact the next batch of cfs . getMaxCompactionThreshold ( ) , <nl> / / and so forth . So we spend most of our i / o rewriting the L1 data with each batch . <nl> / / <nl> / / If we could just do * all * L0 a single time with L1 , that would be ideal . But we can ' t <nl> - / / - - see the javadoc for MAX _ COMPACTING _ L0 . <nl> + / / since we might run out of memory <nl> / / <nl> / / LevelDB ' s way around this is to simply block writes if L0 compaction falls behind . <nl> / / We don ' t have that luxury . <nl> @ @ - 667 , 7 + 667 , 7 @ @ public class LeveledManifest <nl> / / 1a . add sstables to the candidate set until we have at least maxSSTableSizeInMB <nl> / / 1b . prefer choosing older sstables as candidates , to newer ones <nl> / / 1c . any L0 sstables that overlap a candidate , will also become candidates <nl> - / / 2 . At most MAX _ COMPACTING _ L0 sstables from L0 will be compacted at once <nl> + / / 2 . At most max _ threshold sstables from L0 will be compacted at once <nl> / / 3 . If total candidate size is less than maxSSTableSizeInMB , we won ' t bother compacting with L1 , <nl> / / and the result of the compaction will stay in L0 instead of being promoted ( see promote ( ) ) <nl> / / <nl> @ @ - 693 , 10 + 693 , 10 @ @ public class LeveledManifest <nl> remaining . remove ( newCandidate ) ; <nl> } <nl> <nl> - if ( candidates . size ( ) > MAX _ COMPACTING _ L0 ) <nl> + if ( candidates . size ( ) > cfs . getMaximumCompactionThreshold ( ) ) <nl> { <nl> - / / limit to only the MAX _ COMPACTING _ L0 oldest candidates <nl> - candidates = new HashSet < > ( ageSortedSSTables ( candidates ) . subList ( 0 , MAX _ COMPACTING _ L0 ) ) ; <nl> + / / limit to only the cfs . getMaximumCompactionThreshold ( ) oldest candidates <nl> + candidates = new HashSet < > ( ageSortedSSTables ( candidates ) . subList ( 0 , cfs . getMaximumCompactionThreshold ( ) ) ) ; <nl> break ; <nl> } <nl> } <nl> @ @ - 820 , 7 + 820 , 7 @ @ public class LeveledManifest <nl> <nl> if ( ! DatabaseDescriptor . getDisableSTCSInL0 ( ) & & getLevel ( 0 ) . size ( ) > MAX _ COMPACTING _ L0 ) <nl> { <nl> - int l0compactions = getLevel ( 0 ) . size ( ) / MAX _ COMPACTING _ L0 ; <nl> + int l0compactions = getLevel ( 0 ) . size ( ) / cfs . getMaximumCompactionThreshold ( ) ; <nl> tasks + = l0compactions ; <nl> estimated [ 0 ] + = l0compactions ; <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / db / TableCQLHelperTest . java b / test / unit / org / apache / cassandra / db / TableCQLHelperTest . java <nl> index 1e465b3 . . 701afbb 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / TableCQLHelperTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / TableCQLHelperTest . java <nl> @ @ - 274 , 7 + 274 , 7 @ @ public class TableCQLHelperTest extends CQLTester <nl> " \ tAND speculative _ retry = ' ALWAYS ' \ n " + <nl> " \ tAND comment = ' comment ' \ n " + <nl> " \ tAND caching = { ' keys ' : ' ALL ' , ' rows _ per _ partition ' : ' NONE ' } \ n " + <nl> - " \ tAND compaction = { ' class ' : ' org . apache . cassandra . db . compaction . LeveledCompactionStrategy ' , ' sstable _ size _ in _ mb ' : ' 1 ' } \ n " + <nl> + " \ tAND compaction = { ' max _ threshold ' : ' 32 ' , ' min _ threshold ' : ' 4 ' , ' sstable _ size _ in _ mb ' : ' 1 ' , ' class ' : ' org . apache . cassandra . db . compaction . LeveledCompactionStrategy ' } \ n " + <nl> " \ tAND compression = { ' chunk _ length _ in _ kb ' : ' 64 ' , ' min _ compress _ ratio ' : ' 2 . 0 ' , ' class ' : ' org . apache . cassandra . io . compress . LZ4Compressor ' } \ n " + <nl> " \ tAND cdc = false \ n " + <nl> " \ tAND extensions = { ' ext1 ' : 0x76616c31 } ; " <nl> diff - - git a / test / unit / org / apache / cassandra / db / compaction / CompactionsCQLTest . java b / test / unit / org / apache / cassandra / db / compaction / CompactionsCQLTest . java <nl> index ca420da . . c91d2fe 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / compaction / CompactionsCQLTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / compaction / CompactionsCQLTest . java <nl> @ @ - 17 , 10 + 17 , 12 @ @ <nl> * / <nl> package org . apache . cassandra . db . compaction ; <nl> <nl> + import java . nio . ByteBuffer ; <nl> import java . util . Collection ; <nl> import java . util . HashMap ; <nl> import java . util . List ; <nl> import java . util . Map ; <nl> + import java . util . Random ; <nl> <nl> import org . apache . commons . lang . StringUtils ; <nl> import org . junit . After ; <nl> @ @ - 31 , 11 + 33 , 13 @ @ import org . apache . cassandra . cql3 . CQLTester ; <nl> import org . apache . cassandra . cql3 . UntypedResultSet ; <nl> import org . apache . cassandra . config . Config ; <nl> import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + import org . apache . cassandra . db . ColumnFamilyStore ; <nl> import org . apache . cassandra . db . DeletionTime ; <nl> import org . apache . cassandra . db . Mutation ; <nl> import org . apache . cassandra . db . RangeTombstone ; <nl> import org . apache . cassandra . db . RowUpdateBuilder ; <nl> import org . apache . cassandra . db . Slice ; <nl> + import org . apache . cassandra . db . lifecycle . LifecycleTransaction ; <nl> import org . apache . cassandra . db . partitions . PartitionUpdate ; <nl> import org . apache . cassandra . db . rows . Cell ; <nl> import org . apache . cassandra . db . rows . Row ; <nl> @ @ - 388 , 6 + 392 , 69 @ @ public class CompactionsCQLTest extends CQLTester <nl> DatabaseDescriptor . setColumnIndexSize ( maxSizePre ) ; <nl> } <nl> <nl> + <nl> + @ Test <nl> + public void testLCSThresholdParams ( ) throws Throwable <nl> + { <nl> + createTable ( " create table % s ( id int , id2 int , t blob , primary key ( id , id2 ) ) with compaction = { ' class ' : ' LeveledCompactionStrategy ' , ' sstable _ size _ in _ mb ' : ' 1 ' , ' max _ threshold ' : ' 60 ' } " ) ; <nl> + ColumnFamilyStore cfs = getCurrentColumnFamilyStore ( ) ; <nl> + cfs . disableAutoCompaction ( ) ; <nl> + byte [ ] b = new byte [ 100 * 1024 ] ; <nl> + new Random ( ) . nextBytes ( b ) ; <nl> + ByteBuffer value = ByteBuffer . wrap ( b ) ; <nl> + for ( int i = 0 ; i < 50 ; i + + ) <nl> + { <nl> + for ( int j = 0 ; j < 10 ; j + + ) <nl> + { <nl> + execute ( " insert into % s ( id , id2 , t ) values ( ? , ? , ? ) " , i , j , value ) ; <nl> + } <nl> + cfs . forceBlockingFlush ( ) ; <nl> + } <nl> + assertEquals ( 50 , cfs . getLiveSSTables ( ) . size ( ) ) ; <nl> + LeveledCompactionStrategy lcs = ( LeveledCompactionStrategy ) cfs . getCompactionStrategyManager ( ) . getUnrepaired ( ) . get ( 0 ) ; <nl> + AbstractCompactionTask act = lcs . getNextBackgroundTask ( 0 ) ; <nl> + / / we should be compacting all 50 sstables : <nl> + assertEquals ( 50 , act . transaction . originals ( ) . size ( ) ) ; <nl> + act . execute ( null ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testSTCSinL0 ( ) throws Throwable <nl> + { <nl> + createTable ( " create table % s ( id int , id2 int , t blob , primary key ( id , id2 ) ) with compaction = { ' class ' : ' LeveledCompactionStrategy ' , ' sstable _ size _ in _ mb ' : ' 1 ' , ' max _ threshold ' : ' 60 ' } " ) ; <nl> + ColumnFamilyStore cfs = getCurrentColumnFamilyStore ( ) ; <nl> + cfs . disableAutoCompaction ( ) ; <nl> + execute ( " insert into % s ( id , id2 , t ) values ( ? , ? , ? ) " , 1 , 1 , " L1 " ) ; <nl> + cfs . forceBlockingFlush ( ) ; <nl> + cfs . forceMajorCompaction ( ) ; <nl> + SSTableReader l1sstable = cfs . getLiveSSTables ( ) . iterator ( ) . next ( ) ; <nl> + assertEquals ( 1 , l1sstable . getSSTableLevel ( ) ) ; <nl> + / / now we have a single L1 sstable , create many L0 ones : <nl> + byte [ ] b = new byte [ 100 * 1024 ] ; <nl> + new Random ( ) . nextBytes ( b ) ; <nl> + ByteBuffer value = ByteBuffer . wrap ( b ) ; <nl> + for ( int i = 0 ; i < 50 ; i + + ) <nl> + { <nl> + for ( int j = 0 ; j < 10 ; j + + ) <nl> + { <nl> + execute ( " insert into % s ( id , id2 , t ) values ( ? , ? , ? ) " , i , j , value ) ; <nl> + } <nl> + cfs . forceBlockingFlush ( ) ; <nl> + } <nl> + assertEquals ( 51 , cfs . getLiveSSTables ( ) . size ( ) ) ; <nl> + <nl> + / / mark the L1 sstable as compacting to make sure we trigger STCS in L0 : <nl> + LifecycleTransaction txn = cfs . getTracker ( ) . tryModify ( l1sstable , OperationType . COMPACTION ) ; <nl> + LeveledCompactionStrategy lcs = ( LeveledCompactionStrategy ) cfs . getCompactionStrategyManager ( ) . getUnrepaired ( ) . get ( 0 ) ; <nl> + AbstractCompactionTask act = lcs . getNextBackgroundTask ( 0 ) ; <nl> + / / note that max _ threshold is 60 ( more than the amount of L0 sstables ) , but MAX _ COMPACTING _ L0 is 32 , which means we will trigger STCS with at most max _ threshold sstables <nl> + assertEquals ( 50 , act . transaction . originals ( ) . size ( ) ) ; <nl> + assertEquals ( 0 , ( ( LeveledCompactionTask ) act ) . getLevel ( ) ) ; <nl> + assertTrue ( act . transaction . originals ( ) . stream ( ) . allMatch ( s - > s . getSSTableLevel ( ) = = 0 ) ) ; <nl> + txn . abort ( ) ; / / unmark the l1 sstable compacting <nl> + act . execute ( null ) ; <nl> + } <nl> + <nl> private void prepareWide ( ) throws Throwable <nl> { <nl> createTable ( " CREATE TABLE % s ( id int , id2 int , b text , primary key ( id , id2 ) ) " ) ;
NEAREST DIFF (one line): ELIMINATEDSENTENCE

TEST DIFF:
diff - - git a / NEWS . txt b / NEWS . txt 
 index da80684 . . e1e76de 100644 
 - - - a / NEWS . txt 
 + + + b / NEWS . txt 
 @ @ - 38 , 6 + 38 , 11 @ @ using the provided ' sstableupgrade ' tool . 
 
 New features 
 - - - - - - - - - - - - 
 + - LCS now respects the max _ threshold parameter when compacting - this was hard coded to 32 
 + before , but now it is possible to do bigger compactions when compacting from L0 to L1 . 
 + This also applies to STCS - compactions in L0 - if there are more than 32 sstables in L0 
 + we will compact at most max _ threshold sstables in an L0 STCS compaction . See CASSANDRA - 14388 
 + for more information . 
 - There is now an option to automatically upgrade sstables after Cassandra upgrade , enable 
 either in ` cassandra . yaml : automatic _ sstable _ upgrade ` or via JMX during runtime . See 
 CASSANDRA - 14197 . 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java 
 index 7a7b1c1 . . b1091ce 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java 
 @ @ - 31 , 6 + 31 , 7 @ @ import org . slf4j . LoggerFactory ; 
 import com . fasterxml . jackson . databind . JsonNode ; 
 import com . fasterxml . jackson . databind . node . JsonNodeFactory ; 
 import com . fasterxml . jackson . databind . node . ObjectNode ; 
 + import org . apache . cassandra . schema . CompactionParams ; 
 import org . apache . cassandra . schema . TableMetadata ; 
 import org . apache . cassandra . config . Config ; 
 import org . apache . cassandra . db . ColumnFamilyStore ; 
 @ @ - 569 , 6 + 570 , 9 @ @ public class LeveledCompactionStrategy extends AbstractCompactionStrategy 
 
 uncheckedOptions . remove ( LEVEL _ FANOUT _ SIZE _ OPTION ) ; 
 
 + uncheckedOptions . remove ( CompactionParams . Option . MIN _ THRESHOLD . toString ( ) ) ; 
 + uncheckedOptions . remove ( CompactionParams . Option . MAX _ THRESHOLD . toString ( ) ) ; 
 + 
 uncheckedOptions = SizeTieredCompactionStrategyOptions . validateOptions ( options , uncheckedOptions ) ; 
 
 return uncheckedOptions ; 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java b / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java 
 index 013f0b4 . . 291973f 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java 
 @ @ - 49 , 9 + 49 , 8 @ @ public class LeveledManifest 
 private static final Logger logger = LoggerFactory . getLogger ( LeveledManifest . class ) ; 
 
 / * * 
 - * limit the number of L0 sstables we do at once , because compaction bloom filter creation 
 - * uses a pessimistic estimate of how many keys overlap ( none ) , so we risk wasting memory 
 - * or even OOMing when compacting highly overlapping sstables 
 + * if we have more than MAX _ COMPACTING _ L0 sstables in L0 , we will run a round of STCS with at most 
 + * cfs . getMaxCompactionThreshold ( ) sstables . 
 * / 
 private static final int MAX _ COMPACTING _ L0 = 32 ; 
 / * * 
 @ @ - 163 , 6 + 162 , 7 @ @ public class LeveledManifest 
 / / The add ( . . ) : ed sstable will be sent to level 0 
 try 
 { 
 + logger . debug ( " Could not add sstable { } in level { } - dropping to 0 " , reader , reader . getSSTableLevel ( ) ) ; 
 reader . descriptor . getMetadataSerializer ( ) . mutateLevel ( reader . descriptor , 0 ) ; 
 reader . reloadSSTableMetadata ( ) ; 
 } 
 @ @ - 346 , 12 + 346 , 12 @ @ public class LeveledManifest 
 / / L2 : 12 [ ideal : 100 ] 
 / / 
 / / The problem is that L0 has a much higher score ( almost 250 ) than L1 ( 11 ) , so what we ' ll 
 - / / do is compact a batch of MAX _ COMPACTING _ L0 sstables with all 117 L1 sstables , and put the 
 - / / result ( say , 120 sstables ) in L1 . Then we ' ll compact the next batch of MAX _ COMPACTING _ L0 , 
 + / / do is compact a batch of cfs . getMaximumCompactionThreshold ( ) sstables with all 117 L1 sstables , and put the 
 + / / result ( say , 120 sstables ) in L1 . Then we ' ll compact the next batch of cfs . getMaxCompactionThreshold ( ) , 
 / / and so forth . So we spend most of our i / o rewriting the L1 data with each batch . 
 / / 
 / / If we could just do * all * L0 a single time with L1 , that would be ideal . But we can ' t 
 - / / - - see the javadoc for MAX _ COMPACTING _ L0 . 
 + / / since we might run out of memory 
 / / 
 / / LevelDB ' s way around this is to simply block writes if L0 compaction falls behind . 
 / / We don ' t have that luxury . 
 @ @ - 667 , 7 + 667 , 7 @ @ public class LeveledManifest 
 / / 1a . add sstables to the candidate set until we have at least maxSSTableSizeInMB 
 / / 1b . prefer choosing older sstables as candidates , to newer ones 
 / / 1c . any L0 sstables that overlap a candidate , will also become candidates 
 - / / 2 . At most MAX _ COMPACTING _ L0 sstables from L0 will be compacted at once 
 + / / 2 . At most max _ threshold sstables from L0 will be compacted at once 
 / / 3 . If total candidate size is less than maxSSTableSizeInMB , we won ' t bother compacting with L1 , 
 / / and the result of the compaction will stay in L0 instead of being promoted ( see promote ( ) ) 
 / / 
 @ @ - 693 , 10 + 693 , 10 @ @ public class LeveledManifest 
 remaining . remove ( newCandidate ) ; 
 } 
 
 - if ( candidates . size ( ) > MAX _ COMPACTING _ L0 ) 
 + if ( candidates . size ( ) > cfs . getMaximumCompactionThreshold ( ) ) 
 { 
 - / / limit to only the MAX _ COMPACTING _ L0 oldest candidates 
 - candidates = new HashSet < > ( ageSortedSSTables ( candidates ) . subList ( 0 , MAX _ COMPACTING _ L0 ) ) ; 
 + / / limit to only the cfs . getMaximumCompactionThreshold ( ) oldest candidates 
 + candidates = new HashSet < > ( ageSortedSSTables ( candidates ) . subList ( 0 , cfs . getMaximumCompactionThreshold ( ) ) ) ; 
 break ; 
 } 
 } 
 @ @ - 820 , 7 + 820 , 7 @ @ public class LeveledManifest 
 
 if ( ! DatabaseDescriptor . getDisableSTCSInL0 ( ) & & getLevel ( 0 ) . size ( ) > MAX _ COMPACTING _ L0 ) 
 { 
 - int l0compactions = getLevel ( 0 ) . size ( ) / MAX _ COMPACTING _ L0 ; 
 + int l0compactions = getLevel ( 0 ) . size ( ) / cfs . getMaximumCompactionThreshold ( ) ; 
 tasks + = l0compactions ; 
 estimated [ 0 ] + = l0compactions ; 
 } 
 diff - - git a / test / unit / org / apache / cassandra / db / TableCQLHelperTest . java b / test / unit / org / apache / cassandra / db / TableCQLHelperTest . java 
 index 1e465b3 . . 701afbb 100644 
 - - - a / test / unit / org / apache / cassandra / db / TableCQLHelperTest . java 
 + + + b / test / unit / org / apache / cassandra / db / TableCQLHelperTest . java 
 @ @ - 274 , 7 + 274 , 7 @ @ public class TableCQLHelperTest extends CQLTester 
 " \ tAND speculative _ retry = ' ALWAYS ' \ n " + 
 " \ tAND comment = ' comment ' \ n " + 
 " \ tAND caching = { ' keys ' : ' ALL ' , ' rows _ per _ partition ' : ' NONE ' } \ n " + 
 - " \ tAND compaction = { ' class ' : ' org . apache . cassandra . db . compaction . LeveledCompactionStrategy ' , ' sstable _ size _ in _ mb ' : ' 1 ' } \ n " + 
 + " \ tAND compaction = { ' max _ threshold ' : ' 32 ' , ' min _ threshold ' : ' 4 ' , ' sstable _ size _ in _ mb ' : ' 1 ' , ' class ' : ' org . apache . cassandra . db . compaction . LeveledCompactionStrategy ' } \ n " + 
 " \ tAND compression = { ' chunk _ length _ in _ kb ' : ' 64 ' , ' min _ compress _ ratio ' : ' 2 . 0 ' , ' class ' : ' org . apache . cassandra . io . compress . LZ4Compressor ' } \ n " + 
 " \ tAND cdc = false \ n " + 
 " \ tAND extensions = { ' ext1 ' : 0x76616c31 } ; " 
 diff - - git a / test / unit / org / apache / cassandra / db / compaction / CompactionsCQLTest . java b / test / unit / org / apache / cassandra / db / compaction / CompactionsCQLTest . java 
 index ca420da . . c91d2fe 100644 
 - - - a / test / unit / org / apache / cassandra / db / compaction / CompactionsCQLTest . java 
 + + + b / test / unit / org / apache / cassandra / db / compaction / CompactionsCQLTest . java 
 @ @ - 17 , 10 + 17 , 12 @ @ 
 * / 
 package org . apache . cassandra . db . compaction ; 
 
 + import java . nio . ByteBuffer ; 
 import java . util . Collection ; 
 import java . util . HashMap ; 
 import java . util . List ; 
 import java . util . Map ; 
 + import java . util . Random ; 
 
 import org . apache . commons . lang . StringUtils ; 
 import org . junit . After ; 
 @ @ - 31 , 11 + 33 , 13 @ @ import org . apache . cassandra . cql3 . CQLTester ; 
 import org . apache . cassandra . cql3 . UntypedResultSet ; 
 import org . apache . cassandra . config . Config ; 
 import org . apache . cassandra . config . DatabaseDescriptor ; 
 + import org . apache . cassandra . db . ColumnFamilyStore ; 
 import org . apache . cassandra . db . DeletionTime ; 
 import org . apache . cassandra . db . Mutation ; 
 import org . apache . cassandra . db . RangeTombstone ; 
 import org . apache . cassandra . db . RowUpdateBuilder ; 
 import org . apache . cassandra . db . Slice ; 
 + import org . apache . cassandra . db . lifecycle . LifecycleTransaction ; 
 import org . apache . cassandra . db . partitions . PartitionUpdate ; 
 import org . apache . cassandra . db . rows . Cell ; 
 import org . apache . cassandra . db . rows . Row ; 
 @ @ - 388 , 6 + 392 , 69 @ @ public class CompactionsCQLTest extends CQLTester 
 DatabaseDescriptor . setColumnIndexSize ( maxSizePre ) ; 
 } 
 
 + 
 + @ Test 
 + public void testLCSThresholdParams ( ) throws Throwable 
 + { 
 + createTable ( " create table % s ( id int , id2 int , t blob , primary key ( id , id2 ) ) with compaction = { ' class ' : ' LeveledCompactionStrategy ' , ' sstable _ size _ in _ mb ' : ' 1 ' , ' max _ threshold ' : ' 60 ' } " ) ; 
 + ColumnFamilyStore cfs = getCurrentColumnFamilyStore ( ) ; 
 + cfs . disableAutoCompaction ( ) ; 
 + byte [ ] b = new byte [ 100 * 1024 ] ; 
 + new Random ( ) . nextBytes ( b ) ; 
 + ByteBuffer value = ByteBuffer . wrap ( b ) ; 
 + for ( int i = 0 ; i < 50 ; i + + ) 
 + { 
 + for ( int j = 0 ; j < 10 ; j + + ) 
 + { 
 + execute ( " insert into % s ( id , id2 , t ) values ( ? , ? , ? ) " , i , j , value ) ; 
 + } 
 + cfs . forceBlockingFlush ( ) ; 
 + } 
 + assertEquals ( 50 , cfs . getLiveSSTables ( ) . size ( ) ) ; 
 + LeveledCompactionStrategy lcs = ( LeveledCompactionStrategy ) cfs . getCompactionStrategyManager ( ) . getUnrepaired ( ) . get ( 0 ) ; 
 + AbstractCompactionTask act = lcs . getNextBackgroundTask ( 0 ) ; 
 + / / we should be compacting all 50 sstables : 
 + assertEquals ( 50 , act . transaction . originals ( ) . size ( ) ) ; 
 + act . execute ( null ) ; 
 + } 
 + 
 + @ Test 
 + public void testSTCSinL0 ( ) throws Throwable 
 + { 
 + createTable ( " create table % s ( id int , id2 int , t blob , primary key ( id , id2 ) ) with compaction = { ' class ' : ' LeveledCompactionStrategy ' , ' sstable _ size _ in _ mb ' : ' 1 ' , ' max _ threshold ' : ' 60 ' } " ) ; 
 + ColumnFamilyStore cfs = getCurrentColumnFamilyStore ( ) ; 
 + cfs . disableAutoCompaction ( ) ; 
 + execute ( " insert into % s ( id , id2 , t ) values ( ? , ? , ? ) " , 1 , 1 , " L1 " ) ; 
 + cfs . forceBlockingFlush ( ) ; 
 + cfs . forceMajorCompaction ( ) ; 
 + SSTableReader l1sstable = cfs . getLiveSSTables ( ) . iterator ( ) . next ( ) ; 
 + assertEquals ( 1 , l1sstable . getSSTableLevel ( ) ) ; 
 + / / now we have a single L1 sstable , create many L0 ones : 
 + byte [ ] b = new byte [ 100 * 1024 ] ; 
 + new Random ( ) . nextBytes ( b ) ; 
 + ByteBuffer value = ByteBuffer . wrap ( b ) ; 
 + for ( int i = 0 ; i < 50 ; i + + ) 
 + { 
 + for ( int j = 0 ; j < 10 ; j + + ) 
 + { 
 + execute ( " insert into % s ( id , id2 , t ) values ( ? , ? , ? ) " , i , j , value ) ; 
 + } 
 + cfs . forceBlockingFlush ( ) ; 
 + } 
 + assertEquals ( 51 , cfs . getLiveSSTables ( ) . size ( ) ) ; 
 + 
 + / / mark the L1 sstable as compacting to make sure we trigger STCS in L0 : 
 + LifecycleTransaction txn = cfs . getTracker ( ) . tryModify ( l1sstable , OperationType . COMPACTION ) ; 
 + LeveledCompactionStrategy lcs = ( LeveledCompactionStrategy ) cfs . getCompactionStrategyManager ( ) . getUnrepaired ( ) . get ( 0 ) ; 
 + AbstractCompactionTask act = lcs . getNextBackgroundTask ( 0 ) ; 
 + / / note that max _ threshold is 60 ( more than the amount of L0 sstables ) , but MAX _ COMPACTING _ L0 is 32 , which means we will trigger STCS with at most max _ threshold sstables 
 + assertEquals ( 50 , act . transaction . originals ( ) . size ( ) ) ; 
 + assertEquals ( 0 , ( ( LeveledCompactionTask ) act ) . getLevel ( ) ) ; 
 + assertTrue ( act . transaction . originals ( ) . stream ( ) . allMatch ( s - > s . getSSTableLevel ( ) = = 0 ) ) ; 
 + txn . abort ( ) ; / / unmark the l1 sstable compacting 
 + act . execute ( null ) ; 
 + } 
 + 
 private void prepareWide ( ) throws Throwable 
 { 
 createTable ( " CREATE TABLE % s ( id int , id2 int , b text , primary key ( id , id2 ) ) " ) ;

NEAREST DIFF:
ELIMINATEDSENTENCE
