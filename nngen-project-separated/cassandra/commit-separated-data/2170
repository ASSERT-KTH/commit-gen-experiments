BLEU SCORE: 0.08392229812593097

TEST MSG: Don ' t clear out range tombstones during compaction .
GENERATED MSG: Fix wrong purge of deleted cf during compaction

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 7fb6de9 . . badb45e 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 11 , 6 + 11 , 7 @ @ <nl> are thrown while handling native protocol messages ( CASSANDRA - 7470 ) <nl> * Fix row size miscalculation in LazilyCompactedRow ( CASSANDRA - 7543 ) <nl> * Fix race in background compaction check ( CASSANDRA - 7745 ) <nl> + * Don ' t clear out range tombstones during compaction ( CASSANDRA - 7808 ) <nl> <nl> <nl> 1 . 2 . 18 <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / LazilyCompactedRow . java b / src / java / org / apache / cassandra / db / compaction / LazilyCompactedRow . java <nl> index d9f753c . . 4360b0b 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / LazilyCompactedRow . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / LazilyCompactedRow . java <nl> @ @ - 167 , 8 + 167 , 9 @ @ public class LazilyCompactedRow extends AbstractCompactedRow implements Iterable <nl> <nl> public boolean isEmpty ( ) <nl> { <nl> + / / need to clone emptyColumnFamily to avoid resetting the deletion time . See CASSANDRA - 7808 . <nl> boolean cfIrrelevant = shouldPurge <nl> - ? ColumnFamilyStore . removeDeletedCF ( emptyColumnFamily , controller . gcBefore ) = = null <nl> + ? ColumnFamilyStore . removeDeletedCF ( emptyColumnFamily . cloneMeShallow ( ) , controller . gcBefore ) = = null <nl> : ! emptyColumnFamily . isMarkedForDelete ( ) ; / / tombstones are relevant <nl> return cfIrrelevant & & columnStats . columnCount = = 0 ; <nl> } <nl> @ @ - 285 , 11 + 286 , 12 @ @ public class LazilyCompactedRow extends AbstractCompactedRow implements Iterable <nl> ColumnFamily purged = PrecompactedRow . removeDeletedAndOldShards ( key , shouldPurge , controller , container ) ; <nl> if ( purged = = null | | ! purged . iterator ( ) . hasNext ( ) ) <nl> { <nl> - container . clear ( ) ; <nl> + / / don ' t call clear ( ) because that resets the deletion time . See CASSANDRA - 7808 . <nl> + container = emptyColumnFamily . cloneMeShallow ( ) ; <nl> return null ; <nl> } <nl> IColumn reduced = purged . iterator ( ) . next ( ) ; <nl> - container . clear ( ) ; <nl> + container = emptyColumnFamily . cloneMeShallow ( ) ; <nl> <nl> / / PrecompactedRow . removeDeletedAndOldShards have only checked the top - level CF deletion times , <nl> / / not the range tombstone . For that we use the columnIndexer tombstone tracker . <nl> diff - - git a / test / unit / org / apache / cassandra / db / RangeTombstoneTest . java b / test / unit / org / apache / cassandra / db / RangeTombstoneTest . java <nl> index c2f8b83 . . 59be938 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / RangeTombstoneTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / RangeTombstoneTest . java <nl> @ @ - 18 , 17 + 18 , 22 @ @ <nl> * / <nl> package org . apache . cassandra . db ; <nl> <nl> + import java . io . IOException ; <nl> import java . nio . ByteBuffer ; <nl> import java . util . * ; <nl> + import java . util . concurrent . ExecutionException ; <nl> <nl> import org . junit . Test ; <nl> <nl> import org . apache . cassandra . SchemaLoader ; <nl> + import org . apache . cassandra . Util ; <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . db . compaction . CompactionManager ; <nl> import org . apache . cassandra . db . filter . * ; <nl> import org . apache . cassandra . utils . ByteBufferUtil ; <nl> <nl> import static org . apache . cassandra . Util . dk ; <nl> + import static org . junit . Assert . assertEquals ; <nl> <nl> public class RangeTombstoneTest extends SchemaLoader <nl> { <nl> @ @ - 97 , 6 + 102 , 61 @ @ public class RangeTombstoneTest extends SchemaLoader <nl> } <nl> <nl> @ Test <nl> + public void test7808 _ 1 ( ) throws ExecutionException , InterruptedException <nl> + { <nl> + DatabaseDescriptor . setInMemoryCompactionLimit ( 0 ) ; <nl> + Table table = Table . open ( KSNAME ) ; <nl> + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CFNAME ) ; <nl> + cfs . metadata . gcGraceSeconds ( 2 ) ; <nl> + <nl> + String key = " 7808 _ 1 " ; <nl> + RowMutation rm ; <nl> + rm = new RowMutation ( KSNAME , ByteBufferUtil . bytes ( key ) ) ; <nl> + for ( int i = 0 ; i < 40 ; i + = 2 ) <nl> + add ( rm , i , 0 ) ; <nl> + rm . apply ( ) ; <nl> + cfs . forceBlockingFlush ( ) ; <nl> + rm = new RowMutation ( KSNAME , ByteBufferUtil . bytes ( key ) ) ; <nl> + ColumnFamily cf = rm . addOrGet ( CFNAME ) ; <nl> + cf . delete ( new DeletionInfo ( 1 , 1 ) ) ; <nl> + rm . apply ( ) ; <nl> + cfs . forceBlockingFlush ( ) ; <nl> + Thread . sleep ( 5 ) ; <nl> + cfs . forceMajorCompaction ( ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void test7808 _ 2 ( ) throws ExecutionException , InterruptedException , IOException <nl> + { <nl> + DatabaseDescriptor . setInMemoryCompactionLimit ( 0 ) ; <nl> + Table table = Table . open ( KSNAME ) ; <nl> + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CFNAME ) ; <nl> + cfs . metadata . gcGraceSeconds ( 2 ) ; <nl> + <nl> + String key = " 7808 _ 2 " ; <nl> + RowMutation rm ; <nl> + rm = new RowMutation ( KSNAME , ByteBufferUtil . bytes ( key ) ) ; <nl> + for ( int i = 10 ; i < 20 ; i + + ) <nl> + add ( rm , i , 0 ) ; <nl> + rm . apply ( ) ; <nl> + cfs . forceBlockingFlush ( ) ; <nl> + <nl> + rm = new RowMutation ( KSNAME , ByteBufferUtil . bytes ( key ) ) ; <nl> + ColumnFamily cf = rm . addOrGet ( CFNAME ) ; <nl> + cf . delete ( new DeletionInfo ( 0 , 0 ) ) ; <nl> + rm . apply ( ) ; <nl> + <nl> + rm = new RowMutation ( KSNAME , ByteBufferUtil . bytes ( key ) ) ; <nl> + add ( rm , 5 , 1 ) ; <nl> + rm . apply ( ) ; <nl> + <nl> + cfs . forceBlockingFlush ( ) ; <nl> + Thread . sleep ( 5 ) ; <nl> + cfs . forceMajorCompaction ( ) ; <nl> + assertEquals ( 1 , Util . getColumnFamily ( table , Util . dk ( key ) , CFNAME ) . getColumnCount ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> public void overlappingRangeTest ( ) throws Exception <nl> { <nl> CompactionManager . instance . disableAutoCompaction ( ) ;
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> index 0eb13f1 . . ca0b8aa 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> @ @ - 832 , 7 + 832 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> public Future < SSTableReader > submitSSTableBuild ( final Descriptor desc , OperationType type ) <nl> { <nl> / / invalid descriptions due to missing or dropped CFS are handled by SSTW and StreamInSession . <nl> - final SSTableWriter . Builder builder = SSTableWriter . createBuilder ( desc , type ) ; <nl> + final Rebuilder builder = SSTableWriter . createBuilder ( desc , type ) ; <nl> Callable < SSTableReader > callable = new Callable < SSTableReader > ( ) <nl> { <nl> public SSTableReader call ( ) throws IOException <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / IndexWriter . java b / src / java / org / apache / cassandra / io / sstable / IndexWriter . java <nl> new file mode 100644 <nl> index 0000000 . . 8510241 <nl> - - - / dev / null <nl> + + + b / src / java / org / apache / cassandra / io / sstable / IndexWriter . java <nl> @ @ - 0 , 0 + 1 , 93 @ @ <nl> + package org . apache . cassandra . io . sstable ; <nl> + <nl> + import java . io . DataOutputStream ; <nl> + import java . io . File ; <nl> + import java . io . FileOutputStream ; <nl> + import java . io . IOException ; <nl> + <nl> + import org . slf4j . Logger ; <nl> + import org . slf4j . LoggerFactory ; <nl> + <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + import org . apache . cassandra . db . DecoratedKey ; <nl> + import org . apache . cassandra . dht . IPartitioner ; <nl> + import org . apache . cassandra . io . util . BufferedRandomAccessFile ; <nl> + import org . apache . cassandra . io . util . FileMark ; <nl> + import org . apache . cassandra . io . util . FileUtils ; <nl> + import org . apache . cassandra . io . util . SegmentedFile ; <nl> + import org . apache . cassandra . utils . BloomFilter ; <nl> + import org . apache . cassandra . utils . ByteBufferUtil ; <nl> + <nl> + / * * <nl> + * Encapsulates writing the index and filter for an SSTable . The state of this object is not valid until it has been closed . <nl> + * / <nl> + class IndexWriter <nl> + { <nl> + private static final Logger logger = LoggerFactory . getLogger ( SSTableWriter . class ) ; <nl> + <nl> + private final BufferedRandomAccessFile indexFile ; <nl> + public final Descriptor desc ; <nl> + public final IPartitioner partitioner ; <nl> + public final SegmentedFile . Builder builder ; <nl> + public final IndexSummary summary ; <nl> + public final BloomFilter bf ; <nl> + private FileMark mark ; <nl> + <nl> + IndexWriter ( Descriptor desc , IPartitioner part , long keyCount ) throws IOException <nl> + { <nl> + this . desc = desc ; <nl> + this . partitioner = part ; <nl> + indexFile = new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) , " rw " , 8 * 1024 * 1024 , true ) ; <nl> + builder = SegmentedFile . getBuilder ( DatabaseDescriptor . getIndexAccessMode ( ) ) ; <nl> + summary = new IndexSummary ( keyCount ) ; <nl> + bf = BloomFilter . getFilter ( keyCount , 15 ) ; <nl> + } <nl> + <nl> + public void afterAppend ( DecoratedKey key , long dataPosition ) throws IOException <nl> + { <nl> + bf . add ( key . key ) ; <nl> + long indexPosition = indexFile . getFilePointer ( ) ; <nl> + ByteBufferUtil . writeWithShortLength ( key . key , indexFile ) ; <nl> + indexFile . writeLong ( dataPosition ) ; <nl> + if ( logger . isTraceEnabled ( ) ) <nl> + logger . trace ( " wrote index of " + key + " at " + indexPosition ) ; <nl> + <nl> + summary . maybeAddEntry ( key , indexPosition ) ; <nl> + builder . addPotentialBoundary ( indexPosition ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Closes the index and bloomfilter , making the public state of this writer valid for consumption . <nl> + * / <nl> + public void close ( ) throws IOException <nl> + { <nl> + / / bloom filter <nl> + FileOutputStream fos = new FileOutputStream ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; <nl> + DataOutputStream stream = new DataOutputStream ( fos ) ; <nl> + BloomFilter . serializer ( ) . serialize ( bf , stream ) ; <nl> + stream . flush ( ) ; <nl> + fos . getFD ( ) . sync ( ) ; <nl> + stream . close ( ) ; <nl> + <nl> + / / index <nl> + long position = indexFile . getFilePointer ( ) ; <nl> + indexFile . close ( ) ; / / calls force <nl> + FileUtils . truncate ( indexFile . getPath ( ) , position ) ; <nl> + <nl> + / / finalize in - memory index state <nl> + summary . complete ( ) ; <nl> + } <nl> + <nl> + public void mark ( ) <nl> + { <nl> + mark = indexFile . mark ( ) ; <nl> + } <nl> + <nl> + public void reset ( ) throws IOException <nl> + { <nl> + / / we can ' t un - set the bloom filter addition , but extra keys in there are harmless . <nl> + / / we can ' t reset dbuilder either , but that is the last thing called in afterappend so <nl> + / / we assume that if that worked then we won ' t be trying to reset . <nl> + indexFile . reset ( mark ) ; <nl> + } <nl> + } <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / Rebuilder . java b / src / java / org / apache / cassandra / io / sstable / Rebuilder . java <nl> new file mode 100644 <nl> index 0000000 . . 803eb64 <nl> - - - / dev / null <nl> + + + b / src / java / org / apache / cassandra / io / sstable / Rebuilder . java <nl> @ @ - 0 , 0 + 1 , 92 @ @ <nl> + package org . apache . cassandra . io . sstable ; <nl> + <nl> + import java . io . File ; <nl> + import java . io . IOError ; <nl> + import java . io . IOException ; <nl> + <nl> + import org . slf4j . Logger ; <nl> + import org . slf4j . LoggerFactory ; <nl> + <nl> + import org . apache . cassandra . db . ColumnFamilyStore ; <nl> + import org . apache . cassandra . db . Table ; <nl> + import org . apache . cassandra . db . compaction . CompactionInfo ; <nl> + import org . apache . cassandra . db . compaction . CompactionType ; <nl> + import org . apache . cassandra . streaming . OperationType ; <nl> + <nl> + / * * <nl> + * Removes the given SSTable from temporary status and opens it , rebuilding the <nl> + * bloom filter and row index from the data file . <nl> + * / <nl> + public class Rebuilder implements CompactionInfo . Holder <nl> + { <nl> + private static final Logger logger = LoggerFactory . getLogger ( SSTableWriter . class ) ; <nl> + <nl> + private final Descriptor desc ; <nl> + private final OperationType type ; <nl> + private final ColumnFamilyStore cfs ; <nl> + private SSTableWriter . RowIndexer indexer ; <nl> + <nl> + public Rebuilder ( Descriptor desc , OperationType type ) <nl> + { <nl> + this . desc = desc ; <nl> + this . type = type ; <nl> + cfs = Table . open ( desc . ksname ) . getColumnFamilyStore ( desc . cfname ) ; <nl> + } <nl> + <nl> + public CompactionInfo getCompactionInfo ( ) <nl> + { <nl> + maybeOpenIndexer ( ) ; <nl> + try <nl> + { <nl> + / / both file offsets are still valid post - close <nl> + return new CompactionInfo ( desc . ksname , <nl> + desc . cfname , <nl> + CompactionType . SSTABLE _ BUILD , <nl> + indexer . dfile . getFilePointer ( ) , <nl> + indexer . dfile . length ( ) ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + throw new IOError ( e ) ; <nl> + } <nl> + } <nl> + <nl> + / / lazy - initialize the file to avoid opening it until it ' s actually executing on the CompactionManager , <nl> + / / since the 8MB buffers can use up heap quickly <nl> + private void maybeOpenIndexer ( ) <nl> + { <nl> + if ( indexer ! = null ) <nl> + return ; <nl> + try <nl> + { <nl> + if ( cfs . metadata . getDefaultValidator ( ) . isCommutative ( ) ) <nl> + indexer = new SSTableWriter . CommutativeRowIndexer ( desc , cfs , type ) ; <nl> + else <nl> + indexer = new SSTableWriter . RowIndexer ( desc , cfs , type ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + throw new IOError ( e ) ; <nl> + } <nl> + } <nl> + <nl> + public SSTableReader build ( ) throws IOException <nl> + { <nl> + if ( cfs . isInvalid ( ) ) <nl> + return null ; <nl> + maybeOpenIndexer ( ) ; <nl> + <nl> + File ifile = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; <nl> + File ffile = new File ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; <nl> + assert ! ifile . exists ( ) ; <nl> + assert ! ffile . exists ( ) ; <nl> + <nl> + long estimatedRows = indexer . prepareIndexing ( ) ; <nl> + <nl> + / / build the index and filter <nl> + long rows = indexer . index ( ) ; <nl> + <nl> + logger . debug ( " estimated row count was { } of real count " , ( ( double ) estimatedRows ) / rows ) ; <nl> + return SSTableReader . open ( SSTableWriter . rename ( desc , SSTable . componentsFor ( desc , false ) ) ) ; <nl> + } <nl> + } <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> index da179e9 . . fa902b3 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> @ @ - 19 , 7 + 19 , 9 @ @ <nl> <nl> package org . apache . cassandra . io . sstable ; <nl> <nl> - import java . io . * ; <nl> + import java . io . File ; <nl> + import java . io . IOError ; <nl> + import java . io . IOException ; <nl> import java . nio . ByteBuffer ; <nl> import java . util . Arrays ; <nl> import java . util . Collections ; <nl> @ @ - 27 , 10 + 29 , 6 @ @ import java . util . HashSet ; <nl> import java . util . Set ; <nl> <nl> import com . google . common . collect . Sets ; <nl> - <nl> - import org . apache . cassandra . db . commitlog . ReplayPosition ; <nl> - import org . apache . cassandra . db . compaction . * ; <nl> - import org . apache . cassandra . utils . ByteBufferUtil ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> @ @ - 39 , 7 + 37 , 10 @ @ import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . db . ColumnFamily ; <nl> import org . apache . cassandra . db . ColumnFamilyStore ; <nl> import org . apache . cassandra . db . DecoratedKey ; <nl> - import org . apache . cassandra . db . Table ; <nl> + import org . apache . cassandra . db . commitlog . ReplayPosition ; <nl> + import org . apache . cassandra . db . compaction . AbstractCompactedRow ; <nl> + import org . apache . cassandra . db . compaction . CompactionController ; <nl> + import org . apache . cassandra . db . compaction . PrecompactedRow ; <nl> import org . apache . cassandra . dht . IPartitioner ; <nl> import org . apache . cassandra . io . util . BufferedRandomAccessFile ; <nl> import org . apache . cassandra . io . util . FileMark ; <nl> @ @ - 47 , 7 + 48 , 7 @ @ import org . apache . cassandra . io . util . FileUtils ; <nl> import org . apache . cassandra . io . util . SegmentedFile ; <nl> import org . apache . cassandra . service . StorageService ; <nl> import org . apache . cassandra . streaming . OperationType ; <nl> - import org . apache . cassandra . utils . BloomFilter ; <nl> + import org . apache . cassandra . utils . ByteBufferUtil ; <nl> import org . apache . cassandra . utils . EstimatedHistogram ; <nl> import org . apache . cassandra . utils . FBUtilities ; <nl> <nl> @ @ - 232 , 7 + 233 , 7 @ @ public class SSTableWriter extends SSTable <nl> return dataFile . getFilePointer ( ) ; <nl> } <nl> <nl> - public static Builder createBuilder ( Descriptor desc , OperationType type ) <nl> + public static Rebuilder createBuilder ( Descriptor desc , OperationType type ) <nl> { <nl> if ( ! desc . isLatestVersion ) <nl> / / TODO : streaming between different versions will fail : need support for <nl> @ @ - 240 , 83 + 241 , 7 @ @ public class SSTableWriter extends SSTable <nl> throw new RuntimeException ( String . format ( " Cannot recover SSTable with version % s ( current version % s ) . " , <nl> desc . version , Descriptor . CURRENT _ VERSION ) ) ; <nl> <nl> - return new Builder ( desc , type ) ; <nl> - } <nl> - <nl> - / * * <nl> - * Removes the given SSTable from temporary status and opens it , rebuilding the <nl> - * bloom filter and row index from the data file . <nl> - * / <nl> - public static class Builder implements CompactionInfo . Holder <nl> - { <nl> - private final Descriptor desc ; <nl> - private final OperationType type ; <nl> - private final ColumnFamilyStore cfs ; <nl> - private RowIndexer indexer ; <nl> - <nl> - public Builder ( Descriptor desc , OperationType type ) <nl> - { <nl> - this . desc = desc ; <nl> - this . type = type ; <nl> - cfs = Table . open ( desc . ksname ) . getColumnFamilyStore ( desc . cfname ) ; <nl> - } <nl> - <nl> - public CompactionInfo getCompactionInfo ( ) <nl> - { <nl> - maybeOpenIndexer ( ) ; <nl> - try <nl> - { <nl> - / / both file offsets are still valid post - close <nl> - return new CompactionInfo ( desc . ksname , <nl> - desc . cfname , <nl> - CompactionType . SSTABLE _ BUILD , <nl> - indexer . dfile . getFilePointer ( ) , <nl> - indexer . dfile . length ( ) ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - throw new IOError ( e ) ; <nl> - } <nl> - } <nl> - <nl> - / / lazy - initialize the file to avoid opening it until it ' s actually executing on the CompactionManager , <nl> - / / since the 8MB buffers can use up heap quickly <nl> - private void maybeOpenIndexer ( ) <nl> - { <nl> - if ( indexer ! = null ) <nl> - return ; <nl> - try <nl> - { <nl> - if ( cfs . metadata . getDefaultValidator ( ) . isCommutative ( ) ) <nl> - indexer = new CommutativeRowIndexer ( desc , cfs , type ) ; <nl> - else <nl> - indexer = new RowIndexer ( desc , cfs , type ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - throw new IOError ( e ) ; <nl> - } <nl> - } <nl> - <nl> - public SSTableReader build ( ) throws IOException <nl> - { <nl> - if ( cfs . isInvalid ( ) ) <nl> - return null ; <nl> - maybeOpenIndexer ( ) ; <nl> - <nl> - File ifile = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; <nl> - File ffile = new File ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; <nl> - assert ! ifile . exists ( ) ; <nl> - assert ! ffile . exists ( ) ; <nl> - <nl> - long estimatedRows = indexer . prepareIndexing ( ) ; <nl> - <nl> - / / build the index and filter <nl> - long rows = indexer . index ( ) ; <nl> - <nl> - logger . debug ( " estimated row count was { } of real count " , ( ( double ) estimatedRows ) / rows ) ; <nl> - return SSTableReader . open ( rename ( desc , SSTable . componentsFor ( desc , false ) ) ) ; <nl> - } <nl> + return new Rebuilder ( desc , type ) ; <nl> } <nl> <nl> static class RowIndexer <nl> @ @ - 540 , 75 + 465 , 4 @ @ public class SSTableWriter extends SSTable <nl> } <nl> } <nl> <nl> - / * * <nl> - * Encapsulates writing the index and filter for an SSTable . The state of this object is not valid until it has been closed . <nl> - * / <nl> - static class IndexWriter <nl> - { <nl> - private final BufferedRandomAccessFile indexFile ; <nl> - public final Descriptor desc ; <nl> - public final IPartitioner partitioner ; <nl> - public final SegmentedFile . Builder builder ; <nl> - public final IndexSummary summary ; <nl> - public final BloomFilter bf ; <nl> - private FileMark mark ; <nl> - <nl> - IndexWriter ( Descriptor desc , IPartitioner part , long keyCount ) throws IOException <nl> - { <nl> - this . desc = desc ; <nl> - this . partitioner = part ; <nl> - indexFile = new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) , " rw " , 8 * 1024 * 1024 , true ) ; <nl> - builder = SegmentedFile . getBuilder ( DatabaseDescriptor . getIndexAccessMode ( ) ) ; <nl> - summary = new IndexSummary ( keyCount ) ; <nl> - bf = BloomFilter . getFilter ( keyCount , 15 ) ; <nl> - } <nl> - <nl> - public void afterAppend ( DecoratedKey key , long dataPosition ) throws IOException <nl> - { <nl> - bf . add ( key . key ) ; <nl> - long indexPosition = indexFile . getFilePointer ( ) ; <nl> - ByteBufferUtil . writeWithShortLength ( key . key , indexFile ) ; <nl> - indexFile . writeLong ( dataPosition ) ; <nl> - if ( logger . isTraceEnabled ( ) ) <nl> - logger . trace ( " wrote index of " + key + " at " + indexPosition ) ; <nl> - <nl> - summary . maybeAddEntry ( key , indexPosition ) ; <nl> - builder . addPotentialBoundary ( indexPosition ) ; <nl> - } <nl> - <nl> - / * * <nl> - * Closes the index and bloomfilter , making the public state of this writer valid for consumption . <nl> - * / <nl> - public void close ( ) throws IOException <nl> - { <nl> - / / bloom filter <nl> - FileOutputStream fos = new FileOutputStream ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; <nl> - DataOutputStream stream = new DataOutputStream ( fos ) ; <nl> - BloomFilter . serializer ( ) . serialize ( bf , stream ) ; <nl> - stream . flush ( ) ; <nl> - fos . getFD ( ) . sync ( ) ; <nl> - stream . close ( ) ; <nl> - <nl> - / / index <nl> - long position = indexFile . getFilePointer ( ) ; <nl> - indexFile . close ( ) ; / / calls force <nl> - FileUtils . truncate ( indexFile . getPath ( ) , position ) ; <nl> - <nl> - / / finalize in - memory index state <nl> - summary . complete ( ) ; <nl> - } <nl> - <nl> - public void mark ( ) <nl> - { <nl> - mark = indexFile . mark ( ) ; <nl> - } <nl> - <nl> - public void reset ( ) throws IOException <nl> - { <nl> - / / we can ' t un - set the bloom filter addition , but extra keys in there are harmless . <nl> - / / we can ' t reset dbuilder either , but that is the last thing called in afterappend so <nl> - / / we assume that if that worked then we won ' t be trying to reset . <nl> - indexFile . reset ( mark ) ; <nl> - } <nl> - } <nl> }

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 7fb6de9 . . badb45e 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 11 , 6 + 11 , 7 @ @ 
 are thrown while handling native protocol messages ( CASSANDRA - 7470 ) 
 * Fix row size miscalculation in LazilyCompactedRow ( CASSANDRA - 7543 ) 
 * Fix race in background compaction check ( CASSANDRA - 7745 ) 
 + * Don ' t clear out range tombstones during compaction ( CASSANDRA - 7808 ) 
 
 
 1 . 2 . 18 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / LazilyCompactedRow . java b / src / java / org / apache / cassandra / db / compaction / LazilyCompactedRow . java 
 index d9f753c . . 4360b0b 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / LazilyCompactedRow . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / LazilyCompactedRow . java 
 @ @ - 167 , 8 + 167 , 9 @ @ public class LazilyCompactedRow extends AbstractCompactedRow implements Iterable 
 
 public boolean isEmpty ( ) 
 { 
 + / / need to clone emptyColumnFamily to avoid resetting the deletion time . See CASSANDRA - 7808 . 
 boolean cfIrrelevant = shouldPurge 
 - ? ColumnFamilyStore . removeDeletedCF ( emptyColumnFamily , controller . gcBefore ) = = null 
 + ? ColumnFamilyStore . removeDeletedCF ( emptyColumnFamily . cloneMeShallow ( ) , controller . gcBefore ) = = null 
 : ! emptyColumnFamily . isMarkedForDelete ( ) ; / / tombstones are relevant 
 return cfIrrelevant & & columnStats . columnCount = = 0 ; 
 } 
 @ @ - 285 , 11 + 286 , 12 @ @ public class LazilyCompactedRow extends AbstractCompactedRow implements Iterable 
 ColumnFamily purged = PrecompactedRow . removeDeletedAndOldShards ( key , shouldPurge , controller , container ) ; 
 if ( purged = = null | | ! purged . iterator ( ) . hasNext ( ) ) 
 { 
 - container . clear ( ) ; 
 + / / don ' t call clear ( ) because that resets the deletion time . See CASSANDRA - 7808 . 
 + container = emptyColumnFamily . cloneMeShallow ( ) ; 
 return null ; 
 } 
 IColumn reduced = purged . iterator ( ) . next ( ) ; 
 - container . clear ( ) ; 
 + container = emptyColumnFamily . cloneMeShallow ( ) ; 
 
 / / PrecompactedRow . removeDeletedAndOldShards have only checked the top - level CF deletion times , 
 / / not the range tombstone . For that we use the columnIndexer tombstone tracker . 
 diff - - git a / test / unit / org / apache / cassandra / db / RangeTombstoneTest . java b / test / unit / org / apache / cassandra / db / RangeTombstoneTest . java 
 index c2f8b83 . . 59be938 100644 
 - - - a / test / unit / org / apache / cassandra / db / RangeTombstoneTest . java 
 + + + b / test / unit / org / apache / cassandra / db / RangeTombstoneTest . java 
 @ @ - 18 , 17 + 18 , 22 @ @ 
 * / 
 package org . apache . cassandra . db ; 
 
 + import java . io . IOException ; 
 import java . nio . ByteBuffer ; 
 import java . util . * ; 
 + import java . util . concurrent . ExecutionException ; 
 
 import org . junit . Test ; 
 
 import org . apache . cassandra . SchemaLoader ; 
 + import org . apache . cassandra . Util ; 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . db . compaction . CompactionManager ; 
 import org . apache . cassandra . db . filter . * ; 
 import org . apache . cassandra . utils . ByteBufferUtil ; 
 
 import static org . apache . cassandra . Util . dk ; 
 + import static org . junit . Assert . assertEquals ; 
 
 public class RangeTombstoneTest extends SchemaLoader 
 { 
 @ @ - 97 , 6 + 102 , 61 @ @ public class RangeTombstoneTest extends SchemaLoader 
 } 
 
 @ Test 
 + public void test7808 _ 1 ( ) throws ExecutionException , InterruptedException 
 + { 
 + DatabaseDescriptor . setInMemoryCompactionLimit ( 0 ) ; 
 + Table table = Table . open ( KSNAME ) ; 
 + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CFNAME ) ; 
 + cfs . metadata . gcGraceSeconds ( 2 ) ; 
 + 
 + String key = " 7808 _ 1 " ; 
 + RowMutation rm ; 
 + rm = new RowMutation ( KSNAME , ByteBufferUtil . bytes ( key ) ) ; 
 + for ( int i = 0 ; i < 40 ; i + = 2 ) 
 + add ( rm , i , 0 ) ; 
 + rm . apply ( ) ; 
 + cfs . forceBlockingFlush ( ) ; 
 + rm = new RowMutation ( KSNAME , ByteBufferUtil . bytes ( key ) ) ; 
 + ColumnFamily cf = rm . addOrGet ( CFNAME ) ; 
 + cf . delete ( new DeletionInfo ( 1 , 1 ) ) ; 
 + rm . apply ( ) ; 
 + cfs . forceBlockingFlush ( ) ; 
 + Thread . sleep ( 5 ) ; 
 + cfs . forceMajorCompaction ( ) ; 
 + } 
 + 
 + @ Test 
 + public void test7808 _ 2 ( ) throws ExecutionException , InterruptedException , IOException 
 + { 
 + DatabaseDescriptor . setInMemoryCompactionLimit ( 0 ) ; 
 + Table table = Table . open ( KSNAME ) ; 
 + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CFNAME ) ; 
 + cfs . metadata . gcGraceSeconds ( 2 ) ; 
 + 
 + String key = " 7808 _ 2 " ; 
 + RowMutation rm ; 
 + rm = new RowMutation ( KSNAME , ByteBufferUtil . bytes ( key ) ) ; 
 + for ( int i = 10 ; i < 20 ; i + + ) 
 + add ( rm , i , 0 ) ; 
 + rm . apply ( ) ; 
 + cfs . forceBlockingFlush ( ) ; 
 + 
 + rm = new RowMutation ( KSNAME , ByteBufferUtil . bytes ( key ) ) ; 
 + ColumnFamily cf = rm . addOrGet ( CFNAME ) ; 
 + cf . delete ( new DeletionInfo ( 0 , 0 ) ) ; 
 + rm . apply ( ) ; 
 + 
 + rm = new RowMutation ( KSNAME , ByteBufferUtil . bytes ( key ) ) ; 
 + add ( rm , 5 , 1 ) ; 
 + rm . apply ( ) ; 
 + 
 + cfs . forceBlockingFlush ( ) ; 
 + Thread . sleep ( 5 ) ; 
 + cfs . forceMajorCompaction ( ) ; 
 + assertEquals ( 1 , Util . getColumnFamily ( table , Util . dk ( key ) , CFNAME ) . getColumnCount ( ) ) ; 
 + } 
 + 
 + @ Test 
 public void overlappingRangeTest ( ) throws Exception 
 { 
 CompactionManager . instance . disableAutoCompaction ( ) ;

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 index 0eb13f1 . . ca0b8aa 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 @ @ - 832 , 7 + 832 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 public Future < SSTableReader > submitSSTableBuild ( final Descriptor desc , OperationType type ) 
 { 
 / / invalid descriptions due to missing or dropped CFS are handled by SSTW and StreamInSession . 
 - final SSTableWriter . Builder builder = SSTableWriter . createBuilder ( desc , type ) ; 
 + final Rebuilder builder = SSTableWriter . createBuilder ( desc , type ) ; 
 Callable < SSTableReader > callable = new Callable < SSTableReader > ( ) 
 { 
 public SSTableReader call ( ) throws IOException 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / IndexWriter . java b / src / java / org / apache / cassandra / io / sstable / IndexWriter . java 
 new file mode 100644 
 index 0000000 . . 8510241 
 - - - / dev / null 
 + + + b / src / java / org / apache / cassandra / io / sstable / IndexWriter . java 
 @ @ - 0 , 0 + 1 , 93 @ @ 
 + package org . apache . cassandra . io . sstable ; 
 + 
 + import java . io . DataOutputStream ; 
 + import java . io . File ; 
 + import java . io . FileOutputStream ; 
 + import java . io . IOException ; 
 + 
 + import org . slf4j . Logger ; 
 + import org . slf4j . LoggerFactory ; 
 + 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 + import org . apache . cassandra . db . DecoratedKey ; 
 + import org . apache . cassandra . dht . IPartitioner ; 
 + import org . apache . cassandra . io . util . BufferedRandomAccessFile ; 
 + import org . apache . cassandra . io . util . FileMark ; 
 + import org . apache . cassandra . io . util . FileUtils ; 
 + import org . apache . cassandra . io . util . SegmentedFile ; 
 + import org . apache . cassandra . utils . BloomFilter ; 
 + import org . apache . cassandra . utils . ByteBufferUtil ; 
 + 
 + / * * 
 + * Encapsulates writing the index and filter for an SSTable . The state of this object is not valid until it has been closed . 
 + * / 
 + class IndexWriter 
 + { 
 + private static final Logger logger = LoggerFactory . getLogger ( SSTableWriter . class ) ; 
 + 
 + private final BufferedRandomAccessFile indexFile ; 
 + public final Descriptor desc ; 
 + public final IPartitioner partitioner ; 
 + public final SegmentedFile . Builder builder ; 
 + public final IndexSummary summary ; 
 + public final BloomFilter bf ; 
 + private FileMark mark ; 
 + 
 + IndexWriter ( Descriptor desc , IPartitioner part , long keyCount ) throws IOException 
 + { 
 + this . desc = desc ; 
 + this . partitioner = part ; 
 + indexFile = new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) , " rw " , 8 * 1024 * 1024 , true ) ; 
 + builder = SegmentedFile . getBuilder ( DatabaseDescriptor . getIndexAccessMode ( ) ) ; 
 + summary = new IndexSummary ( keyCount ) ; 
 + bf = BloomFilter . getFilter ( keyCount , 15 ) ; 
 + } 
 + 
 + public void afterAppend ( DecoratedKey key , long dataPosition ) throws IOException 
 + { 
 + bf . add ( key . key ) ; 
 + long indexPosition = indexFile . getFilePointer ( ) ; 
 + ByteBufferUtil . writeWithShortLength ( key . key , indexFile ) ; 
 + indexFile . writeLong ( dataPosition ) ; 
 + if ( logger . isTraceEnabled ( ) ) 
 + logger . trace ( " wrote index of " + key + " at " + indexPosition ) ; 
 + 
 + summary . maybeAddEntry ( key , indexPosition ) ; 
 + builder . addPotentialBoundary ( indexPosition ) ; 
 + } 
 + 
 + / * * 
 + * Closes the index and bloomfilter , making the public state of this writer valid for consumption . 
 + * / 
 + public void close ( ) throws IOException 
 + { 
 + / / bloom filter 
 + FileOutputStream fos = new FileOutputStream ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; 
 + DataOutputStream stream = new DataOutputStream ( fos ) ; 
 + BloomFilter . serializer ( ) . serialize ( bf , stream ) ; 
 + stream . flush ( ) ; 
 + fos . getFD ( ) . sync ( ) ; 
 + stream . close ( ) ; 
 + 
 + / / index 
 + long position = indexFile . getFilePointer ( ) ; 
 + indexFile . close ( ) ; / / calls force 
 + FileUtils . truncate ( indexFile . getPath ( ) , position ) ; 
 + 
 + / / finalize in - memory index state 
 + summary . complete ( ) ; 
 + } 
 + 
 + public void mark ( ) 
 + { 
 + mark = indexFile . mark ( ) ; 
 + } 
 + 
 + public void reset ( ) throws IOException 
 + { 
 + / / we can ' t un - set the bloom filter addition , but extra keys in there are harmless . 
 + / / we can ' t reset dbuilder either , but that is the last thing called in afterappend so 
 + / / we assume that if that worked then we won ' t be trying to reset . 
 + indexFile . reset ( mark ) ; 
 + } 
 + } 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / Rebuilder . java b / src / java / org / apache / cassandra / io / sstable / Rebuilder . java 
 new file mode 100644 
 index 0000000 . . 803eb64 
 - - - / dev / null 
 + + + b / src / java / org / apache / cassandra / io / sstable / Rebuilder . java 
 @ @ - 0 , 0 + 1 , 92 @ @ 
 + package org . apache . cassandra . io . sstable ; 
 + 
 + import java . io . File ; 
 + import java . io . IOError ; 
 + import java . io . IOException ; 
 + 
 + import org . slf4j . Logger ; 
 + import org . slf4j . LoggerFactory ; 
 + 
 + import org . apache . cassandra . db . ColumnFamilyStore ; 
 + import org . apache . cassandra . db . Table ; 
 + import org . apache . cassandra . db . compaction . CompactionInfo ; 
 + import org . apache . cassandra . db . compaction . CompactionType ; 
 + import org . apache . cassandra . streaming . OperationType ; 
 + 
 + / * * 
 + * Removes the given SSTable from temporary status and opens it , rebuilding the 
 + * bloom filter and row index from the data file . 
 + * / 
 + public class Rebuilder implements CompactionInfo . Holder 
 + { 
 + private static final Logger logger = LoggerFactory . getLogger ( SSTableWriter . class ) ; 
 + 
 + private final Descriptor desc ; 
 + private final OperationType type ; 
 + private final ColumnFamilyStore cfs ; 
 + private SSTableWriter . RowIndexer indexer ; 
 + 
 + public Rebuilder ( Descriptor desc , OperationType type ) 
 + { 
 + this . desc = desc ; 
 + this . type = type ; 
 + cfs = Table . open ( desc . ksname ) . getColumnFamilyStore ( desc . cfname ) ; 
 + } 
 + 
 + public CompactionInfo getCompactionInfo ( ) 
 + { 
 + maybeOpenIndexer ( ) ; 
 + try 
 + { 
 + / / both file offsets are still valid post - close 
 + return new CompactionInfo ( desc . ksname , 
 + desc . cfname , 
 + CompactionType . SSTABLE _ BUILD , 
 + indexer . dfile . getFilePointer ( ) , 
 + indexer . dfile . length ( ) ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + throw new IOError ( e ) ; 
 + } 
 + } 
 + 
 + / / lazy - initialize the file to avoid opening it until it ' s actually executing on the CompactionManager , 
 + / / since the 8MB buffers can use up heap quickly 
 + private void maybeOpenIndexer ( ) 
 + { 
 + if ( indexer ! = null ) 
 + return ; 
 + try 
 + { 
 + if ( cfs . metadata . getDefaultValidator ( ) . isCommutative ( ) ) 
 + indexer = new SSTableWriter . CommutativeRowIndexer ( desc , cfs , type ) ; 
 + else 
 + indexer = new SSTableWriter . RowIndexer ( desc , cfs , type ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + throw new IOError ( e ) ; 
 + } 
 + } 
 + 
 + public SSTableReader build ( ) throws IOException 
 + { 
 + if ( cfs . isInvalid ( ) ) 
 + return null ; 
 + maybeOpenIndexer ( ) ; 
 + 
 + File ifile = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; 
 + File ffile = new File ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; 
 + assert ! ifile . exists ( ) ; 
 + assert ! ffile . exists ( ) ; 
 + 
 + long estimatedRows = indexer . prepareIndexing ( ) ; 
 + 
 + / / build the index and filter 
 + long rows = indexer . index ( ) ; 
 + 
 + logger . debug ( " estimated row count was { } of real count " , ( ( double ) estimatedRows ) / rows ) ; 
 + return SSTableReader . open ( SSTableWriter . rename ( desc , SSTable . componentsFor ( desc , false ) ) ) ; 
 + } 
 + } 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 index da179e9 . . fa902b3 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 @ @ - 19 , 7 + 19 , 9 @ @ 
 
 package org . apache . cassandra . io . sstable ; 
 
 - import java . io . * ; 
 + import java . io . File ; 
 + import java . io . IOError ; 
 + import java . io . IOException ; 
 import java . nio . ByteBuffer ; 
 import java . util . Arrays ; 
 import java . util . Collections ; 
 @ @ - 27 , 10 + 29 , 6 @ @ import java . util . HashSet ; 
 import java . util . Set ; 
 
 import com . google . common . collect . Sets ; 
 - 
 - import org . apache . cassandra . db . commitlog . ReplayPosition ; 
 - import org . apache . cassandra . db . compaction . * ; 
 - import org . apache . cassandra . utils . ByteBufferUtil ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 @ @ - 39 , 7 + 37 , 10 @ @ import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . db . ColumnFamily ; 
 import org . apache . cassandra . db . ColumnFamilyStore ; 
 import org . apache . cassandra . db . DecoratedKey ; 
 - import org . apache . cassandra . db . Table ; 
 + import org . apache . cassandra . db . commitlog . ReplayPosition ; 
 + import org . apache . cassandra . db . compaction . AbstractCompactedRow ; 
 + import org . apache . cassandra . db . compaction . CompactionController ; 
 + import org . apache . cassandra . db . compaction . PrecompactedRow ; 
 import org . apache . cassandra . dht . IPartitioner ; 
 import org . apache . cassandra . io . util . BufferedRandomAccessFile ; 
 import org . apache . cassandra . io . util . FileMark ; 
 @ @ - 47 , 7 + 48 , 7 @ @ import org . apache . cassandra . io . util . FileUtils ; 
 import org . apache . cassandra . io . util . SegmentedFile ; 
 import org . apache . cassandra . service . StorageService ; 
 import org . apache . cassandra . streaming . OperationType ; 
 - import org . apache . cassandra . utils . BloomFilter ; 
 + import org . apache . cassandra . utils . ByteBufferUtil ; 
 import org . apache . cassandra . utils . EstimatedHistogram ; 
 import org . apache . cassandra . utils . FBUtilities ; 
 
 @ @ - 232 , 7 + 233 , 7 @ @ public class SSTableWriter extends SSTable 
 return dataFile . getFilePointer ( ) ; 
 } 
 
 - public static Builder createBuilder ( Descriptor desc , OperationType type ) 
 + public static Rebuilder createBuilder ( Descriptor desc , OperationType type ) 
 { 
 if ( ! desc . isLatestVersion ) 
 / / TODO : streaming between different versions will fail : need support for 
 @ @ - 240 , 83 + 241 , 7 @ @ public class SSTableWriter extends SSTable 
 throw new RuntimeException ( String . format ( " Cannot recover SSTable with version % s ( current version % s ) . " , 
 desc . version , Descriptor . CURRENT _ VERSION ) ) ; 
 
 - return new Builder ( desc , type ) ; 
 - } 
 - 
 - / * * 
 - * Removes the given SSTable from temporary status and opens it , rebuilding the 
 - * bloom filter and row index from the data file . 
 - * / 
 - public static class Builder implements CompactionInfo . Holder 
 - { 
 - private final Descriptor desc ; 
 - private final OperationType type ; 
 - private final ColumnFamilyStore cfs ; 
 - private RowIndexer indexer ; 
 - 
 - public Builder ( Descriptor desc , OperationType type ) 
 - { 
 - this . desc = desc ; 
 - this . type = type ; 
 - cfs = Table . open ( desc . ksname ) . getColumnFamilyStore ( desc . cfname ) ; 
 - } 
 - 
 - public CompactionInfo getCompactionInfo ( ) 
 - { 
 - maybeOpenIndexer ( ) ; 
 - try 
 - { 
 - / / both file offsets are still valid post - close 
 - return new CompactionInfo ( desc . ksname , 
 - desc . cfname , 
 - CompactionType . SSTABLE _ BUILD , 
 - indexer . dfile . getFilePointer ( ) , 
 - indexer . dfile . length ( ) ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - throw new IOError ( e ) ; 
 - } 
 - } 
 - 
 - / / lazy - initialize the file to avoid opening it until it ' s actually executing on the CompactionManager , 
 - / / since the 8MB buffers can use up heap quickly 
 - private void maybeOpenIndexer ( ) 
 - { 
 - if ( indexer ! = null ) 
 - return ; 
 - try 
 - { 
 - if ( cfs . metadata . getDefaultValidator ( ) . isCommutative ( ) ) 
 - indexer = new CommutativeRowIndexer ( desc , cfs , type ) ; 
 - else 
 - indexer = new RowIndexer ( desc , cfs , type ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - throw new IOError ( e ) ; 
 - } 
 - } 
 - 
 - public SSTableReader build ( ) throws IOException 
 - { 
 - if ( cfs . isInvalid ( ) ) 
 - return null ; 
 - maybeOpenIndexer ( ) ; 
 - 
 - File ifile = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; 
 - File ffile = new File ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; 
 - assert ! ifile . exists ( ) ; 
 - assert ! ffile . exists ( ) ; 
 - 
 - long estimatedRows = indexer . prepareIndexing ( ) ; 
 - 
 - / / build the index and filter 
 - long rows = indexer . index ( ) ; 
 - 
 - logger . debug ( " estimated row count was { } of real count " , ( ( double ) estimatedRows ) / rows ) ; 
 - return SSTableReader . open ( rename ( desc , SSTable . componentsFor ( desc , false ) ) ) ; 
 - } 
 + return new Rebuilder ( desc , type ) ; 
 } 
 
 static class RowIndexer 
 @ @ - 540 , 75 + 465 , 4 @ @ public class SSTableWriter extends SSTable 
 } 
 } 
 
 - / * * 
 - * Encapsulates writing the index and filter for an SSTable . The state of this object is not valid until it has been closed . 
 - * / 
 - static class IndexWriter 
 - { 
 - private final BufferedRandomAccessFile indexFile ; 
 - public final Descriptor desc ; 
 - public final IPartitioner partitioner ; 
 - public final SegmentedFile . Builder builder ; 
 - public final IndexSummary summary ; 
 - public final BloomFilter bf ; 
 - private FileMark mark ; 
 - 
 - IndexWriter ( Descriptor desc , IPartitioner part , long keyCount ) throws IOException 
 - { 
 - this . desc = desc ; 
 - this . partitioner = part ; 
 - indexFile = new BufferedRandomAccessFile ( new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) , " rw " , 8 * 1024 * 1024 , true ) ; 
 - builder = SegmentedFile . getBuilder ( DatabaseDescriptor . getIndexAccessMode ( ) ) ; 
 - summary = new IndexSummary ( keyCount ) ; 
 - bf = BloomFilter . getFilter ( keyCount , 15 ) ; 
 - } 
 - 
 - public void afterAppend ( DecoratedKey key , long dataPosition ) throws IOException 
 - { 
 - bf . add ( key . key ) ; 
 - long indexPosition = indexFile . getFilePointer ( ) ; 
 - ByteBufferUtil . writeWithShortLength ( key . key , indexFile ) ; 
 - indexFile . writeLong ( dataPosition ) ; 
 - if ( logger . isTraceEnabled ( ) ) 
 - logger . trace ( " wrote index of " + key + " at " + indexPosition ) ; 
 - 
 - summary . maybeAddEntry ( key , indexPosition ) ; 
 - builder . addPotentialBoundary ( indexPosition ) ; 
 - } 
 - 
 - / * * 
 - * Closes the index and bloomfilter , making the public state of this writer valid for consumption . 
 - * / 
 - public void close ( ) throws IOException 
 - { 
 - / / bloom filter 
 - FileOutputStream fos = new FileOutputStream ( desc . filenameFor ( SSTable . COMPONENT _ FILTER ) ) ; 
 - DataOutputStream stream = new DataOutputStream ( fos ) ; 
 - BloomFilter . serializer ( ) . serialize ( bf , stream ) ; 
 - stream . flush ( ) ; 
 - fos . getFD ( ) . sync ( ) ; 
 - stream . close ( ) ; 
 - 
 - / / index 
 - long position = indexFile . getFilePointer ( ) ; 
 - indexFile . close ( ) ; / / calls force 
 - FileUtils . truncate ( indexFile . getPath ( ) , position ) ; 
 - 
 - / / finalize in - memory index state 
 - summary . complete ( ) ; 
 - } 
 - 
 - public void mark ( ) 
 - { 
 - mark = indexFile . mark ( ) ; 
 - } 
 - 
 - public void reset ( ) throws IOException 
 - { 
 - / / we can ' t un - set the bloom filter addition , but extra keys in there are harmless . 
 - / / we can ' t reset dbuilder either , but that is the last thing called in afterappend so 
 - / / we assume that if that worked then we won ' t be trying to reset . 
 - indexFile . reset ( mark ) ; 
 - } 
 - } 
 }
