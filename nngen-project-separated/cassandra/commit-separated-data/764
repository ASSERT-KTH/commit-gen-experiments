BLEU SCORE: 0.006193628179172647

TEST MSG: CDC Follow - ups
GENERATED MSG: add CommitLog and RecoveryManager tests . patch by jbellis ; reviewed by goffinet for CASSANDRA - 237

TEST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / Directories . java b / src / java / org / apache / cassandra / db / Directories . java <nl> index 2a55992 . . 87527e8 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Directories . java <nl> + + + b / src / java / org / apache / cassandra / db / Directories . java <nl> @ @ - 920 , 7 + 920 , 7 @ @ public class Directories <nl> if ( ! input . isDirectory ( ) ) <nl> return 0 ; <nl> <nl> - SSTableSizeSummer visitor = new SSTableSizeSummer ( sstableLister ( Directories . OnTxnErr . THROW ) . listFiles ( ) ) ; <nl> + SSTableSizeSummer visitor = new SSTableSizeSummer ( input , sstableLister ( Directories . OnTxnErr . THROW ) . listFiles ( ) ) ; <nl> try <nl> { <nl> Files . walkFileTree ( input . toPath ( ) , visitor ) ; <nl> @ @ - 1006 , 9 + 1006 , 11 @ @ public class Directories <nl> <nl> private class SSTableSizeSummer extends DirectorySizeCalculator <nl> { <nl> - SSTableSizeSummer ( List < File > files ) <nl> + private final HashSet < File > toSkip ; <nl> + SSTableSizeSummer ( File path , List < File > files ) <nl> { <nl> - super ( files ) ; <nl> + super ( path ) ; <nl> + toSkip = new HashSet < > ( files ) ; <nl> } <nl> <nl> @ Override <nl> @ @ - 1019 , 8 + 1021 , 7 @ @ public class Directories <nl> return pair ! = null <nl> & & pair . left . ksname . equals ( metadata . ksName ) <nl> & & pair . left . cfname . equals ( metadata . cfName ) <nl> - & & ! visited . contains ( fileName ) <nl> - & & ! alive . contains ( fileName ) ; <nl> + & & ! toSkip . contains ( fileName ) ; <nl> } <nl> } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogReader . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogReader . java <nl> index 4594080 . . c797482 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogReader . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogReader . java <nl> @ @ - 190 , 7 + 190 , 8 @ @ public class CommitLogReader <nl> ReadStatusTracker statusTracker = new ReadStatusTracker ( mutationLimit , tolerateTruncation ) ; <nl> for ( CommitLogSegmentReader . SyncSegment syncSegment : segmentReader ) <nl> { <nl> - statusTracker . tolerateErrorsInSection & = syncSegment . toleratesErrorsInSection ; <nl> + / / Only tolerate truncation if we allow in both global and segment <nl> + statusTracker . tolerateErrorsInSection = tolerateTruncation & syncSegment . toleratesErrorsInSection ; <nl> <nl> / / Skip segments that are completely behind the desired minPosition <nl> if ( desc . id = = minPosition . segmentId & & syncSegment . endPosition < minPosition . position ) <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogSegmentManagerCDC . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegmentManagerCDC . java <nl> index 15944bd . . 1fac735 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogSegmentManagerCDC . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegmentManagerCDC . java <nl> @ @ - 20 , 9 + 20 , 11 @ @ package org . apache . cassandra . db . commitlog ; <nl> <nl> import java . io . File ; <nl> import java . io . IOException ; <nl> + import java . nio . file . FileVisitResult ; <nl> import java . nio . file . Files ; <nl> + import java . nio . file . Path ; <nl> + import java . nio . file . attribute . BasicFileAttributes ; <nl> import java . util . concurrent . * ; <nl> - import java . util . concurrent . atomic . AtomicLong ; <nl> <nl> import com . google . common . annotations . VisibleForTesting ; <nl> import com . google . common . util . concurrent . RateLimiter ; <nl> @ @ - 35 , 6 + 37 , 7 @ @ import org . apache . cassandra . db . commitlog . CommitLogSegment . CDCState ; <nl> import org . apache . cassandra . exceptions . WriteTimeoutException ; <nl> import org . apache . cassandra . io . util . FileUtils ; <nl> import org . apache . cassandra . utils . DirectorySizeCalculator ; <nl> + import org . apache . cassandra . utils . NoSpamLogger ; <nl> <nl> public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager <nl> { <nl> @ @ - 116 , 6 + 119 , 12 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager <nl> if ( mutation . trackedByCDC ( ) & & segment . getCDCState ( ) = = CDCState . FORBIDDEN ) <nl> { <nl> cdcSizeTracker . submitOverflowSizeRecalculation ( ) ; <nl> + NoSpamLogger . log ( logger , <nl> + NoSpamLogger . Level . WARN , <nl> + 10 , <nl> + TimeUnit . SECONDS , <nl> + " Rejecting Mutation containing CDC - enabled table . Free up space in { } . " , <nl> + DatabaseDescriptor . getCDCLogLocation ( ) ) ; <nl> throw new WriteTimeoutException ( WriteType . CDC , ConsistencyLevel . LOCAL _ ONE , 0 , 1 ) ; <nl> } <nl> } <nl> @ @ - 148 , 17 + 157 , 16 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager <nl> * <nl> * Allows atomic increment / decrement of unflushed size , however only allows increment on flushed and requires a full <nl> * directory walk to determine any potential deletions by CDC consumer . <nl> - * <nl> - * TODO : linux performs approximately 25 % better with the following one - liner instead of this walker : <nl> - * Arrays . stream ( path . listFiles ( ) ) . mapToLong ( File : : length ) . sum ( ) ; <nl> - * However this solution is 375 % slower on Windows . Revisit this and split logic to per - OS <nl> * / <nl> private static class CDCSizeTracker extends DirectorySizeCalculator <nl> { <nl> private final RateLimiter rateLimiter = RateLimiter . create ( 1000 . 0 / DatabaseDescriptor . getCDCDiskCheckInterval ( ) ) ; <nl> private ExecutorService cdcSizeCalculationExecutor ; <nl> private CommitLogSegmentManagerCDC segmentManager ; <nl> - private AtomicLong unflushedCDCSize = new AtomicLong ( 0 ) ; <nl> + private volatile long unflushedCDCSize ; <nl> + <nl> + / / Used instead of size during walk to remove chance of over - allocation <nl> + private volatile long sizeInProgress = 0 ; <nl> <nl> CDCSizeTracker ( CommitLogSegmentManagerCDC segmentManager , File path ) <nl> { <nl> @ @ - 193 , 7 + 201 , 7 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager <nl> ? CDCState . FORBIDDEN <nl> : CDCState . PERMITTED ) ; <nl> if ( segment . getCDCState ( ) = = CDCState . PERMITTED ) <nl> - unflushedCDCSize . addAndGet ( defaultSegmentSize ( ) ) ; <nl> + unflushedCDCSize + = defaultSegmentSize ( ) ; <nl> } <nl> <nl> / / Take this opportunity to kick off a recalc to pick up any consumer file deletion . <nl> @ @ - 207 , 9 + 215 , 9 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager <nl> { <nl> / / Add to flushed size before decrementing unflushed so we don ' t have a window of false generosity <nl> if ( segment . getCDCState ( ) = = CDCState . CONTAINS ) <nl> - size . addAndGet ( segment . onDiskSize ( ) ) ; <nl> + size + = segment . onDiskSize ( ) ; <nl> if ( segment . getCDCState ( ) ! = CDCState . FORBIDDEN ) <nl> - unflushedCDCSize . addAndGet ( - defaultSegmentSize ( ) ) ; <nl> + unflushedCDCSize - = defaultSegmentSize ( ) ; <nl> } <nl> <nl> / / Take this opportunity to kick off a recalc to pick up any consumer file deletion . <nl> @ @ - 251 , 14 + 259 , 10 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager <nl> { <nl> try <nl> { <nl> - / / Since we don ' t synchronize around either rebuilding our file list or walking the tree and adding to <nl> - / / size , it ' s possible we could have changes take place underneath us and end up with a slightly incorrect <nl> - / / view of our flushed size by the time this walking completes . Given that there ' s a linear growth in <nl> - / / runtime on both rebuildFileList and walkFileTree ( about 50 % for each one on runtime ) , and that the <nl> - / / window for this race should be very small , this is an acceptable trade - off since it will be resolved <nl> - / / on the next segment creation / deletion with a subsequent call to submitOverflowSizeRecalculation . <nl> - rebuildFileList ( ) ; <nl> + / / The Arrays . stream approach is considerably slower on Windows than linux <nl> + sizeInProgress = 0 ; <nl> Files . walkFileTree ( path . toPath ( ) , this ) ; <nl> + size = sizeInProgress ; <nl> } <nl> catch ( IOException ie ) <nl> { <nl> @ @ - 266 , 14 + 270 , 21 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager <nl> } <nl> } <nl> <nl> - private long addFlushedSize ( long toAdd ) <nl> + @ Override <nl> + public FileVisitResult visitFile ( Path file , BasicFileAttributes attrs ) throws IOException <nl> + { <nl> + sizeInProgress + = attrs . size ( ) ; <nl> + return FileVisitResult . CONTINUE ; <nl> + } <nl> + <nl> + private void addFlushedSize ( long toAdd ) <nl> { <nl> - return size . addAndGet ( toAdd ) ; <nl> + size + = toAdd ; <nl> } <nl> <nl> private long totalCDCSizeOnDisk ( ) <nl> { <nl> - return unflushedCDCSize . get ( ) + size . get ( ) ; <nl> + return unflushedCDCSize + size ; <nl> } <nl> <nl> public void shutdown ( ) <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CompressedSegment . java b / src / java / org / apache / cassandra / db / commitlog / CompressedSegment . java <nl> index e44dfdf . . 645eda9 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CompressedSegment . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CompressedSegment . java <nl> @ @ - 48 , 11 + 48 , 7 @ @ public class CompressedSegment extends FileDirectSegment <nl> { <nl> super ( commitLog , manager , onClose ) ; <nl> this . compressor = commitLog . configuration . getCompressor ( ) ; <nl> - } <nl> - <nl> - ByteBuffer allocate ( int size ) <nl> - { <nl> - return compressor . preferredBufferType ( ) . allocate ( size ) ; <nl> + manager . getBufferPool ( ) . setPreferredReusableBufferType ( compressor . preferredBufferType ( ) ) ; <nl> } <nl> <nl> ByteBuffer createBuffer ( CommitLog commitLog ) <nl> @ @ - 71 , 14 + 67 , 7 @ @ public class CompressedSegment extends FileDirectSegment <nl> try <nl> { <nl> int neededBufferSize = compressor . initialCompressedBufferLength ( length ) + COMPRESSED _ MARKER _ SIZE ; <nl> - ByteBuffer compressedBuffer = manager . getBufferPool ( ) . getThreadLocalReusableBuffer ( ) ; <nl> - if ( compressor . preferredBufferType ( ) ! = BufferType . typeOf ( compressedBuffer ) | | <nl> - compressedBuffer . capacity ( ) < neededBufferSize ) <nl> - { <nl> - FileUtils . clean ( compressedBuffer ) ; <nl> - compressedBuffer = allocate ( neededBufferSize ) ; <nl> - manager . getBufferPool ( ) . setThreadLocalReusableBuffer ( compressedBuffer ) ; <nl> - } <nl> + ByteBuffer compressedBuffer = manager . getBufferPool ( ) . getThreadLocalReusableBuffer ( neededBufferSize ) ; <nl> <nl> ByteBuffer inputBuffer = buffer . duplicate ( ) ; <nl> inputBuffer . limit ( contentStart + length ) . position ( contentStart ) ; <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / EncryptedSegment . java b / src / java / org / apache / cassandra / db / commitlog / EncryptedSegment . java <nl> index e13b20a . . 103351e 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / EncryptedSegment . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / EncryptedSegment . java <nl> @ @ - 25 , 12 + 25 , 12 @ @ import javax . crypto . Cipher ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . io . FSWriteError ; <nl> import org . apache . cassandra . io . compress . BufferType ; <nl> import org . apache . cassandra . io . compress . ICompressor ; <nl> import org . apache . cassandra . security . EncryptionUtils ; <nl> import org . apache . cassandra . security . EncryptionContext ; <nl> - import org . apache . cassandra . utils . ByteBufferUtil ; <nl> import org . apache . cassandra . utils . Hex ; <nl> import org . apache . cassandra . utils . SyncUtil ; <nl> <nl> @ @ - 79 , 6 + 79 , 8 @ @ public class EncryptedSegment extends FileDirectSegment <nl> throw new FSWriteError ( e , logFile ) ; <nl> } <nl> logger . debug ( " created a new encrypted commit log segment : { } " , logFile ) ; <nl> + / / Keep reusable buffers on - heap regardless of compression preference so we avoid copy off / on repeatedly during decryption <nl> + manager . getBufferPool ( ) . setPreferredReusableBufferType ( BufferType . ON _ HEAP ) ; <nl> } <nl> <nl> protected Map < String , String > additionalHeaderParameters ( ) <nl> @ @ - 108 , 7 + 110 , 7 @ @ public class EncryptedSegment extends FileDirectSegment <nl> { <nl> ByteBuffer inputBuffer = buffer . duplicate ( ) ; <nl> inputBuffer . limit ( contentStart + length ) . position ( contentStart ) ; <nl> - ByteBuffer buffer = manager . getBufferPool ( ) . getThreadLocalReusableBuffer ( ) ; <nl> + ByteBuffer buffer = manager . getBufferPool ( ) . getThreadLocalReusableBuffer ( DatabaseDescriptor . getCommitLogSegmentSize ( ) ) ; <nl> <nl> / / save space for the sync marker at the beginning of this section <nl> final long syncMarkerPosition = lastWrittenPos ; <nl> @ @ - 132 , 21 + 134 , 17 @ @ public class EncryptedSegment extends FileDirectSegment <nl> <nl> lastWrittenPos = channel . position ( ) ; <nl> <nl> - / / rewind to the beginning of the section and write out the sync marker , <nl> - / / reusing the one of the existing buffers <nl> - buffer = ByteBufferUtil . ensureCapacity ( buffer , ENCRYPTED _ SECTION _ HEADER _ SIZE , true ) ; <nl> + / / rewind to the beginning of the section and write out the sync marker <nl> + buffer . position ( 0 ) . limit ( ENCRYPTED _ SECTION _ HEADER _ SIZE ) ; <nl> writeSyncMarker ( buffer , 0 , ( int ) syncMarkerPosition , ( int ) lastWrittenPos ) ; <nl> buffer . putInt ( SYNC _ MARKER _ SIZE , length ) ; <nl> - buffer . position ( 0 ) . limit ( ENCRYPTED _ SECTION _ HEADER _ SIZE ) ; <nl> + buffer . rewind ( ) ; <nl> manager . addSize ( buffer . limit ( ) ) ; <nl> <nl> channel . position ( syncMarkerPosition ) ; <nl> channel . write ( buffer ) ; <nl> <nl> SyncUtil . force ( channel , true ) ; <nl> - <nl> - if ( manager . getBufferPool ( ) . getThreadLocalReusableBuffer ( ) . capacity ( ) < buffer . capacity ( ) ) <nl> - manager . getBufferPool ( ) . setThreadLocalReusableBuffer ( buffer ) ; <nl> } <nl> catch ( Exception e ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / SimpleCachedBufferPool . java b / src / java / org / apache / cassandra / db / commitlog / SimpleCachedBufferPool . java <nl> index 1c10c25 . . bdec3fc 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / SimpleCachedBufferPool . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / SimpleCachedBufferPool . java <nl> @ @ - 57 , 6 + 57 , 8 @ @ public class SimpleCachedBufferPool <nl> * / <nl> private final int bufferSize ; <nl> <nl> + private BufferType preferredReusableBufferType = BufferType . ON _ HEAP ; <nl> + <nl> public SimpleCachedBufferPool ( int maxBufferPoolSize , int bufferSize ) <nl> { <nl> this . maxBufferPoolSize = maxBufferPoolSize ; <nl> @ @ - 75 , 14 + 77 , 19 @ @ public class SimpleCachedBufferPool <nl> return bufferType . allocate ( bufferSize ) ; <nl> } <nl> <nl> - public ByteBuffer getThreadLocalReusableBuffer ( ) <nl> + public ByteBuffer getThreadLocalReusableBuffer ( int size ) <nl> { <nl> - return reusableBufferHolder . get ( ) ; <nl> + ByteBuffer result = reusableBufferHolder . get ( ) ; <nl> + if ( result . capacity ( ) < size | | BufferType . typeOf ( result ) ! = preferredReusableBufferType ) { <nl> + FileUtils . clean ( result ) ; <nl> + result = preferredReusableBufferType . allocate ( size ) ; <nl> + reusableBufferHolder . set ( result ) ; <nl> + } <nl> + return result ; <nl> } <nl> <nl> - public void setThreadLocalReusableBuffer ( ByteBuffer buffer ) <nl> - { <nl> - reusableBufferHolder . set ( buffer ) ; <nl> + public void setPreferredReusableBufferType ( BufferType type ) { <nl> + preferredReusableBufferType = type ; <nl> } <nl> <nl> public void releaseBuffer ( ByteBuffer buffer ) <nl> diff - - git a / src / java / org / apache / cassandra / utils / DirectorySizeCalculator . java b / src / java / org / apache / cassandra / utils / DirectorySizeCalculator . java <nl> index aa7898c . . c1fb6e0 100644 <nl> - - - a / src / java / org / apache / cassandra / utils / DirectorySizeCalculator . java <nl> + + + b / src / java / org / apache / cassandra / utils / DirectorySizeCalculator . java <nl> @ @ - 24 , 39 + 24 , 19 @ @ import java . nio . file . FileVisitResult ; <nl> import java . nio . file . Path ; <nl> import java . nio . file . SimpleFileVisitor ; <nl> import java . nio . file . attribute . BasicFileAttributes ; <nl> - import java . util . List ; <nl> - import java . util . Set ; <nl> - import java . util . concurrent . atomic . AtomicLong ; <nl> - <nl> - import com . google . common . collect . ImmutableSet ; <nl> - <nl> - import static com . google . common . collect . Sets . newHashSet ; <nl> <nl> / * * <nl> * Walks directory recursively , summing up total contents of files within . <nl> * / <nl> public class DirectorySizeCalculator extends SimpleFileVisitor < Path > <nl> { <nl> - protected final AtomicLong size = new AtomicLong ( 0 ) ; <nl> - protected Set < String > visited = newHashSet ( ) ; / / count each file only once <nl> - protected Set < String > alive = newHashSet ( ) ; <nl> + protected volatile long size = 0 ; <nl> protected final File path ; <nl> <nl> public DirectorySizeCalculator ( File path ) <nl> { <nl> super ( ) ; <nl> this . path = path ; <nl> - rebuildFileList ( ) ; <nl> - } <nl> - <nl> - public DirectorySizeCalculator ( List < File > files ) <nl> - { <nl> - super ( ) ; <nl> - this . path = null ; <nl> - ImmutableSet . Builder < String > builder = ImmutableSet . builder ( ) ; <nl> - for ( File file : files ) <nl> - builder . add ( file . getName ( ) ) ; <nl> - alive = builder . build ( ) ; <nl> } <nl> <nl> public boolean isAcceptable ( Path file ) <nl> @ @ - 64 , 24 + 44 , 11 @ @ public class DirectorySizeCalculator extends SimpleFileVisitor < Path > <nl> return true ; <nl> } <nl> <nl> - public void rebuildFileList ( ) <nl> - { <nl> - assert path ! = null ; <nl> - ImmutableSet . Builder < String > builder = ImmutableSet . builder ( ) ; <nl> - for ( File file : path . listFiles ( ) ) <nl> - builder . add ( file . getName ( ) ) ; <nl> - size . set ( 0 ) ; <nl> - alive = builder . build ( ) ; <nl> - } <nl> - <nl> @ Override <nl> public FileVisitResult visitFile ( Path file , BasicFileAttributes attrs ) throws IOException <nl> { <nl> if ( isAcceptable ( file ) ) <nl> - { <nl> - size . addAndGet ( attrs . size ( ) ) ; <nl> - visited . add ( file . toFile ( ) . getName ( ) ) ; <nl> - } <nl> + size + = attrs . size ( ) ; <nl> return FileVisitResult . CONTINUE ; <nl> } <nl> <nl> @ @ - 93 , 6 + 60 , 6 @ @ public class DirectorySizeCalculator extends SimpleFileVisitor < Path > <nl> <nl> public long getAllocatedSize ( ) <nl> { <nl> - return size . get ( ) ; <nl> + return size ; <nl> } <nl> } <nl> diff - - git a / test / microbench / org / apache / cassandra / test / microbench / DirectorySizerBench . java b / test / microbench / org / apache / cassandra / test / microbench / DirectorySizerBench . java <nl> index a653c81 . . 34cbb17 100644 <nl> - - - a / test / microbench / org / apache / cassandra / test / microbench / DirectorySizerBench . java <nl> + + + b / test / microbench / org / apache / cassandra / test / microbench / DirectorySizerBench . java <nl> @ @ - 99 , 7 + 99 , 6 @ @ public class DirectorySizerBench <nl> @ Benchmark <nl> public void countFiles ( final Blackhole bh ) throws IOException <nl> { <nl> - sizer . rebuildFileList ( ) ; <nl> Files . walkFileTree ( tempDir . toPath ( ) , sizer ) ; <nl> } <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / db / commitlog / CommitLogTest . java b / test / unit / org / apache / cassandra / db / commitlog / CommitLogTest . java <nl> index eff972d . . ebd868a 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / commitlog / CommitLogTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / commitlog / CommitLogTest . java <nl> @ @ - 508 , 19 + 508 , 6 @ @ public class CommitLogTest <nl> runExpecting ( ( ) - > testRecovery ( new CommitLogDescriptor ( 4 , commitLogCompression , encryptionContext ) , logData ) , expected ) ; <nl> } <nl> <nl> - protected void testRecovery ( byte [ ] logData ) throws Exception <nl> - { <nl> - Pair < File , Integer > pair = tmpFile ( ) ; <nl> - try ( RandomAccessFile raf = new RandomAccessFile ( pair . left , " rw " ) ) <nl> - { <nl> - raf . seek ( pair . right ) ; <nl> - raf . write ( logData ) ; <nl> - raf . close ( ) ; <nl> - <nl> - CommitLog . instance . recoverFiles ( pair . left ) ; / / CASSANDRA - 1119 / CASSANDRA - 1179 throw on failure * / <nl> - } <nl> - } <nl> - <nl> @ Test <nl> public void testTruncateWithoutSnapshot ( ) throws ExecutionException , InterruptedException , IOException <nl> {
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> index aa7d774 . . 98347ab 100644 <nl> - - - a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> + + + b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> @ @ - 24 , 10 + 24 , 7 @ @ import java . io . * ; <nl> <nl> import org . apache . log4j . Logger ; <nl> <nl> - import org . apache . cassandra . db . ColumnFamily ; <nl> - import org . apache . cassandra . db . Table ; <nl> - import org . apache . cassandra . db . TypeInfo ; <nl> - import org . apache . cassandra . db . SystemTable ; <nl> + import org . apache . cassandra . db . * ; <nl> import org . apache . cassandra . utils . FileUtils ; <nl> import org . apache . cassandra . utils . XMLUtils ; <nl> import org . w3c . dom . Node ; <nl> @ @ - 63 , 7 + 60 , 6 @ @ public class DatabaseDescriptor <nl> private static int currentIndex _ = 0 ; <nl> private static String logFileDirectory _ ; <nl> private static String bootstrapFileDirectory _ ; <nl> - private static int logRotationThreshold _ = 128 * 1024 * 1024 ; <nl> private static boolean fastSync _ = false ; <nl> private static boolean rackAware _ = false ; <nl> private static int threadsPerPool _ = 4 ; <nl> @ @ - 293 , 7 + 289 , 7 @ @ public class DatabaseDescriptor <nl> / * threshold after which commit log should be rotated . * / <nl> String value = xmlUtils . getNodeValue ( " / Storage / CommitLogRotationThresholdInMB " ) ; <nl> if ( value ! = null ) <nl> - logRotationThreshold _ = Integer . parseInt ( value ) * 1024 * 1024 ; <nl> + CommitLog . setSegmentSize ( Integer . parseInt ( value ) * 1024 * 1024 ) ; <nl> <nl> / * fast sync option * / <nl> value = xmlUtils . getNodeValue ( " / Storage / CommitLogFastSync " ) ; <nl> @ @ - 743 , 11 + 739 , 6 @ @ public class DatabaseDescriptor <nl> bootstrapFileDirectory _ = bfLocation ; <nl> } <nl> <nl> - public static int getLogFileSizeThreshold ( ) <nl> - { <nl> - return logRotationThreshold _ ; <nl> - } <nl> - <nl> public static String getLogFileLocation ( ) <nl> { <nl> return logFileDirectory _ ; <nl> diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> index 3324b89 . . bca4a56 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> @ @ - 1661 , 4 + 1661 , 17 @ @ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> lock _ . readLock ( ) . unlock ( ) ; <nl> } <nl> } <nl> + <nl> + void clearUnsafe ( ) <nl> + { <nl> + lock _ . writeLock ( ) . lock ( ) ; <nl> + try <nl> + { <nl> + memtable _ . clearUnsafe ( ) ; <nl> + } <nl> + finally <nl> + { <nl> + lock _ . writeLock ( ) . unlock ( ) ; <nl> + } <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / CommitLog . java b / src / java / org / apache / cassandra / db / CommitLog . java <nl> index 36f4521 . . 68e03cb 100644 <nl> - - - a / src / java / org / apache / cassandra / db / CommitLog . java <nl> + + + b / src / java / org / apache / cassandra / db / CommitLog . java <nl> @ @ - 61 , 7 + 61 , 7 @ @ import org . apache . commons . lang . StringUtils ; <nl> * / <nl> public class CommitLog <nl> { <nl> - private static final int bufSize _ = 128 * 1024 * 1024 ; <nl> + private static volatile int SEGMENT _ SIZE = 128 * 1024 * 1024 ; / / roll after log gets this big <nl> private static Map < String , CommitLog > instances _ = new HashMap < String , CommitLog > ( ) ; <nl> private static Lock lock _ = new ReentrantLock ( ) ; <nl> private static Logger logger _ = Logger . getLogger ( CommitLog . class ) ; <nl> @ @ - 112 , 6 + 112 , 16 @ @ public class CommitLog <nl> } <nl> } <nl> <nl> + public static void setSegmentSize ( int size ) <nl> + { <nl> + SEGMENT _ SIZE = size ; <nl> + } <nl> + <nl> + static int getSegmentCount ( ) <nl> + { <nl> + return clHeaders _ . size ( ) ; <nl> + } <nl> + <nl> static long getCreationTime ( String file ) <nl> { <nl> String [ ] entries = FBUtilities . strip ( file , " - . " ) ; <nl> @ @ - 134 , 9 + 144 , 7 @ @ public class CommitLog <nl> { <nl> if ( DatabaseDescriptor . isFastSync ( ) ) <nl> { <nl> - / * Add this to the threshold * / <nl> - int bufSize = 4 * 1024 * 1024 ; <nl> - return SequenceFile . fastWriter ( file , CommitLog . bufSize _ + bufSize ) ; <nl> + return SequenceFile . fastWriter ( file , 4 * 1024 * 1024 ) ; <nl> } <nl> else <nl> return SequenceFile . writer ( file ) ; <nl> @ @ - 178 , 9 + 186 , 6 @ @ public class CommitLog <nl> private CommitLogHeader clHeader _ ; <nl> private IFileWriter logWriter _ ; <nl> private long commitHeaderStartPos _ ; <nl> - / * Force rollover the commit log on the next insert * / <nl> - private boolean forcedRollOver _ = false ; <nl> - <nl> <nl> / * <nl> * Generates a file name of the format CommitLog - < table > - < timestamp > . log in the <nl> @ @ - 456 , 8 + 461 , 7 @ @ public class CommitLog <nl> / * Update the header * / <nl> updateHeader ( row ) ; <nl> logWriter _ . append ( table _ , cfBuffer ) ; <nl> - fileSize = logWriter _ . getFileSize ( ) ; <nl> - checkThresholdAndRollLog ( fileSize ) ; <nl> + checkThresholdAndRollLog ( ) ; <nl> } <nl> catch ( IOException e ) <nl> { <nl> @ @ - 573 , 50 + 577 , 38 @ @ public class CommitLog <nl> } <nl> } <nl> <nl> - private void checkThresholdAndRollLog ( long fileSize ) <nl> + private void checkThresholdAndRollLog ( ) <nl> { <nl> try <nl> { <nl> - if ( fileSize > = DatabaseDescriptor . getLogFileSizeThreshold ( ) | | forcedRollOver _ ) <nl> + if ( logWriter _ . getFileSize ( ) > = SEGMENT _ SIZE ) <nl> { <nl> - if ( logWriter _ . getFileSize ( ) > = DatabaseDescriptor . getLogFileSizeThreshold ( ) | | forcedRollOver _ ) <nl> - { <nl> - 	 / * Rolls the current log file over to a new one . * / <nl> - 	 setNextFileName ( ) ; <nl> - 	 String oldLogFile = logWriter _ . getFileName ( ) ; <nl> - 	 / / history _ . add ( oldLogFile ) ; <nl> - 	 logWriter _ . close ( ) ; <nl> - 	 <nl> - 	 / * point reader / writer to a new commit log file . * / <nl> - 	 / / logWriter _ = SequenceFile . writer ( logFile _ ) ; <nl> - 	 logWriter _ = CommitLog . createWriter ( logFile _ ) ; <nl> - 	 / * squirrel away the old commit log header * / <nl> - 	 clHeaders _ . put ( oldLogFile , new CommitLogHeader ( clHeader _ ) ) ; <nl> - 	 / * <nl> - 	 * We need to zero out positions because the positions in <nl> - 	 * the old file do not make sense in the new one . <nl> - 	 * / <nl> - 	 clHeader _ . zeroPositions ( ) ; <nl> - 	 writeCommitLogHeader ( clHeader _ . toByteArray ( ) , false ) ; <nl> - 	 / / Get the list of files in commit log directory if it is greater than a certain number <nl> - 	 / / Force flush all the column families that way we ensure that a slowly populated column family is not screwing up <nl> - 	 / / by accumulating the commit logs . <nl> - } <nl> + / * Rolls the current log file over to a new one . * / <nl> + setNextFileName ( ) ; <nl> + String oldLogFile = logWriter _ . getFileName ( ) ; <nl> + / / history _ . add ( oldLogFile ) ; <nl> + logWriter _ . close ( ) ; <nl> + <nl> + / * point reader / writer to a new commit log file . * / <nl> + / / logWriter _ = SequenceFile . writer ( logFile _ ) ; <nl> + logWriter _ = CommitLog . createWriter ( logFile _ ) ; <nl> + / * squirrel away the old commit log header * / <nl> + clHeaders _ . put ( oldLogFile , new CommitLogHeader ( clHeader _ ) ) ; <nl> + / * <nl> + * We need to zero out positions because the positions in <nl> + * the old file do not make sense in the new one . <nl> + * / <nl> + clHeader _ . zeroPositions ( ) ; <nl> + writeCommitLogHeader ( clHeader _ . toByteArray ( ) , false ) ; <nl> + / / Get the list of files in commit log directory if it is greater than a certain number <nl> + / / Force flush all the column families that way we ensure that a slowly populated column family is not screwing up <nl> + / / by accumulating the commit logs . <nl> } <nl> } <nl> - catch ( IOException e ) <nl> + catch ( IOException e ) <nl> { <nl> logger _ . info ( LogUtil . throwableToString ( e ) ) ; <nl> } <nl> - finally <nl> - { <nl> - 	 forcedRollOver _ = false ; <nl> - } <nl> - } <nl> - <nl> - public void setForcedRollOver ( ) <nl> - { <nl> - 	 forcedRollOver _ = true ; <nl> } <nl> <nl> public static void main ( String [ ] args ) throws Throwable <nl> diff - - git a / src / java / org / apache / cassandra / db / Memtable . java b / src / java / org / apache / cassandra / db / Memtable . java <nl> index be641a9 . . d82e5a0 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Memtable . java <nl> + + + b / src / java / org / apache / cassandra / db / Memtable . java <nl> @ @ - 388 , 4 + 388 , 9 @ @ public class Memtable implements Comparable < Memtable > <nl> } <nl> } ; <nl> } <nl> + <nl> + public void clearUnsafe ( ) <nl> + { <nl> + columnFamilies _ . clear ( ) ; <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / RecoveryManager . java b / src / java / org / apache / cassandra / db / RecoveryManager . java <nl> index f4e2d75 . . e6b3523 100644 <nl> - - - a / src / java / org / apache / cassandra / db / RecoveryManager . java <nl> + + + b / src / java / org / apache / cassandra / db / RecoveryManager . java <nl> @ @ - 70 , 7 + 70 , 7 @ @ public class RecoveryManager <nl> return tableToCommitLogs ; <nl> } <nl> <nl> - public void doRecovery ( ) throws IOException <nl> + public static void doRecovery ( ) throws IOException <nl> { <nl> File [ ] files = getListofCommitLogs ( ) ; <nl> Map < String , List < File > > tableToCommitLogs = getListOFCommitLogsPerTable ( ) ; <nl> @ @ - 78 , 7 + 78 , 7 @ @ public class RecoveryManager <nl> FileUtils . delete ( files ) ; <nl> } <nl> <nl> - private void recoverEachTable ( Map < String , List < File > > tableToCommitLogs ) throws IOException <nl> + private static void recoverEachTable ( Map < String , List < File > > tableToCommitLogs ) throws IOException <nl> { <nl> Comparator < File > fCmp = new FileUtils . FileComparator ( ) ; <nl> Set < String > tables = tableToCommitLogs . keySet ( ) ; <nl> @ @ - 90 , 12 + 90 , 4 @ @ public class RecoveryManager <nl> clog . recover ( clogs ) ; <nl> } <nl> } <nl> - <nl> - public static void main ( String [ ] args ) throws Throwable <nl> - { <nl> - long start = System . currentTimeMillis ( ) ; <nl> - RecoveryManager rm = RecoveryManager . instance ( ) ; <nl> - rm . doRecovery ( ) ; <nl> - logger _ . debug ( " Time taken : " + ( System . currentTimeMillis ( ) - start ) + " ms . " ) ; <nl> - } <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / db / CommitLogTest . java b / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> index c9b2b48 . . 5bbbb62 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> @ @ - 19 , 6 + 19 , 7 @ @ <nl> package org . apache . cassandra . db ; <nl> <nl> import java . io . IOException ; <nl> + import java . util . concurrent . ExecutionException ; <nl> <nl> import org . junit . Test ; <nl> <nl> @ @ - 27 , 32 + 28 , 33 @ @ import org . apache . cassandra . CleanupHelper ; <nl> public class CommitLogTest extends CleanupHelper <nl> { <nl> @ Test <nl> - public void testMain ( ) throws IOException { <nl> - / / TODO this is useless , since it assumes we have a working set of commit logs to parse <nl> - / * <nl> - File logDir = new File ( DatabaseDescriptor . getLogFileLocation ( ) ) ; <nl> - File [ ] files = logDir . listFiles ( ) ; <nl> - Arrays . sort ( files , new FileUtils . FileComparator ( ) ) ; <nl> - <nl> - byte [ ] bytes = new byte [ CommitLogHeader . size ( Integer . parseInt ( args [ 0 ] ) ) ] ; <nl> - for ( File file : files ) <nl> + public void testCleanup ( ) throws IOException , ExecutionException , InterruptedException <nl> + { <nl> + assert CommitLog . getSegmentCount ( ) = = 0 ; <nl> + CommitLog . setSegmentSize ( 1000 ) ; <nl> + <nl> + Table table = Table . open ( " Table1 " ) ; <nl> + ColumnFamilyStore store1 = table . getColumnFamilyStore ( " Standard1 " ) ; <nl> + ColumnFamilyStore store2 = table . getColumnFamilyStore ( " Standard2 " ) ; <nl> + RowMutation rm ; <nl> + byte [ ] value = new byte [ 501 ] ; <nl> + <nl> + / / add data . use relatively large values to force quick segment creation since we have a low flush threshold in the test config . <nl> + for ( int i = 0 ; i < 10 ; i + + ) <nl> { <nl> - CommitLog clog = new CommitLog ( file ) ; <nl> - clog . readCommitLogHeader ( file . getAbsolutePath ( ) , bytes ) ; <nl> - DataInputBuffer bufIn = new DataInputBuffer ( ) ; <nl> - bufIn . reset ( bytes , 0 , bytes . length ) ; <nl> - CommitLogHeader clHeader = CommitLogHeader . serializer ( ) . deserialize ( bufIn ) ; <nl> - <nl> - StringBuilder sb = new StringBuilder ( " " ) ; <nl> - for ( byte b : bytes ) <nl> - { <nl> - sb . append ( b ) ; <nl> - sb . append ( " " ) ; <nl> - } <nl> - <nl> - System . out . println ( " FILE : " + file ) ; <nl> - System . out . println ( clHeader . toString ( ) ) ; <nl> + rm = new RowMutation ( " Table1 " , " key1 " ) ; <nl> + rm . add ( " Standard1 : Column1 " , value , 0 ) ; <nl> + rm . add ( " Standard2 : Column1 " , value , 0 ) ; <nl> + rm . apply ( ) ; <nl> } <nl> - * / <nl> + assert CommitLog . getSegmentCount ( ) > 1 ; <nl> + <nl> + / / nothing should get removed after flushing just Standard1 <nl> + store1 . forceBlockingFlush ( ) ; <nl> + assert CommitLog . getSegmentCount ( ) > 1 ; <nl> + <nl> + / / after flushing Standard2 we should be able to clean out all segments <nl> + store2 . forceBlockingFlush ( ) ; <nl> + assert CommitLog . getSegmentCount ( ) = = 1 ; <nl> } <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java b / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java <nl> index 0e2c732 . . e2edc37 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java <nl> @ @ - 19 , 17 + 19 , 40 @ @ <nl> package org . apache . cassandra . db ; <nl> <nl> import java . io . IOException ; <nl> + import java . util . concurrent . ExecutionException ; <nl> <nl> import org . junit . Test ; <nl> <nl> import org . apache . cassandra . CleanupHelper ; <nl> + import static org . apache . cassandra . db . TableTest . assertColumns ; <nl> <nl> public class RecoveryManagerTest extends CleanupHelper <nl> { <nl> @ Test <nl> - public void testDoRecovery ( ) throws IOException { <nl> + public void testNothing ( ) throws IOException { <nl> / / TODO nothing to recover <nl> RecoveryManager rm = RecoveryManager . instance ( ) ; <nl> rm . doRecovery ( ) ; <nl> } <nl> + <nl> + @ Test <nl> + public void testSomething ( ) throws IOException , ExecutionException , InterruptedException <nl> + { <nl> + Table table1 = Table . open ( " Table1 " ) ; <nl> + <nl> + RowMutation rm ; <nl> + ColumnFamily cf ; <nl> + <nl> + rm = new RowMutation ( " Table1 " , " keymulti " ) ; <nl> + cf = new ColumnFamily ( " Standard1 " , " Standard " ) ; <nl> + cf . addColumn ( new Column ( " col1 " , " val1 " . getBytes ( ) , 1L ) ) ; <nl> + rm . add ( cf ) ; <nl> + rm . apply ( ) ; <nl> + <nl> + table1 . getColumnFamilyStore ( " Standard1 " ) . clearUnsafe ( ) ; <nl> + <nl> + RecoveryManager . doRecovery ( ) ; <nl> + <nl> + assertColumns ( table1 . get ( " keymulti " , " Standard1 " ) , " col1 " ) ; <nl> + } <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / db / TableTest . java b / test / unit / org / apache / cassandra / db / TableTest . java <nl> index 3b9bf2e . . 61b12ca 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / TableTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / TableTest . java <nl> @ @ - 349 , 7 + 349 , 7 @ @ public class TableTest extends CleanupHelper <nl> assertEquals ( new String ( cfres . getColumn ( " col1992 " ) . value ( ) ) , " vvvvvvvvvvvvvvvv1992 " ) ; <nl> } <nl> <nl> - private void assertColumns ( ColumnFamily columnFamily , String . . . columnNames ) <nl> + public static void assertColumns ( ColumnFamily columnFamily , String . . . columnNames ) <nl> { <nl> assertNotNull ( columnFamily ) ; <nl> SortedSet < IColumn > columns = columnFamily . getAllColumns ( ) ;

TEST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / Directories . java b / src / java / org / apache / cassandra / db / Directories . java 
 index 2a55992 . . 87527e8 100644 
 - - - a / src / java / org / apache / cassandra / db / Directories . java 
 + + + b / src / java / org / apache / cassandra / db / Directories . java 
 @ @ - 920 , 7 + 920 , 7 @ @ public class Directories 
 if ( ! input . isDirectory ( ) ) 
 return 0 ; 
 
 - SSTableSizeSummer visitor = new SSTableSizeSummer ( sstableLister ( Directories . OnTxnErr . THROW ) . listFiles ( ) ) ; 
 + SSTableSizeSummer visitor = new SSTableSizeSummer ( input , sstableLister ( Directories . OnTxnErr . THROW ) . listFiles ( ) ) ; 
 try 
 { 
 Files . walkFileTree ( input . toPath ( ) , visitor ) ; 
 @ @ - 1006 , 9 + 1006 , 11 @ @ public class Directories 
 
 private class SSTableSizeSummer extends DirectorySizeCalculator 
 { 
 - SSTableSizeSummer ( List < File > files ) 
 + private final HashSet < File > toSkip ; 
 + SSTableSizeSummer ( File path , List < File > files ) 
 { 
 - super ( files ) ; 
 + super ( path ) ; 
 + toSkip = new HashSet < > ( files ) ; 
 } 
 
 @ Override 
 @ @ - 1019 , 8 + 1021 , 7 @ @ public class Directories 
 return pair ! = null 
 & & pair . left . ksname . equals ( metadata . ksName ) 
 & & pair . left . cfname . equals ( metadata . cfName ) 
 - & & ! visited . contains ( fileName ) 
 - & & ! alive . contains ( fileName ) ; 
 + & & ! toSkip . contains ( fileName ) ; 
 } 
 } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogReader . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogReader . java 
 index 4594080 . . c797482 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogReader . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogReader . java 
 @ @ - 190 , 7 + 190 , 8 @ @ public class CommitLogReader 
 ReadStatusTracker statusTracker = new ReadStatusTracker ( mutationLimit , tolerateTruncation ) ; 
 for ( CommitLogSegmentReader . SyncSegment syncSegment : segmentReader ) 
 { 
 - statusTracker . tolerateErrorsInSection & = syncSegment . toleratesErrorsInSection ; 
 + / / Only tolerate truncation if we allow in both global and segment 
 + statusTracker . tolerateErrorsInSection = tolerateTruncation & syncSegment . toleratesErrorsInSection ; 
 
 / / Skip segments that are completely behind the desired minPosition 
 if ( desc . id = = minPosition . segmentId & & syncSegment . endPosition < minPosition . position ) 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogSegmentManagerCDC . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegmentManagerCDC . java 
 index 15944bd . . 1fac735 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogSegmentManagerCDC . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegmentManagerCDC . java 
 @ @ - 20 , 9 + 20 , 11 @ @ package org . apache . cassandra . db . commitlog ; 
 
 import java . io . File ; 
 import java . io . IOException ; 
 + import java . nio . file . FileVisitResult ; 
 import java . nio . file . Files ; 
 + import java . nio . file . Path ; 
 + import java . nio . file . attribute . BasicFileAttributes ; 
 import java . util . concurrent . * ; 
 - import java . util . concurrent . atomic . AtomicLong ; 
 
 import com . google . common . annotations . VisibleForTesting ; 
 import com . google . common . util . concurrent . RateLimiter ; 
 @ @ - 35 , 6 + 37 , 7 @ @ import org . apache . cassandra . db . commitlog . CommitLogSegment . CDCState ; 
 import org . apache . cassandra . exceptions . WriteTimeoutException ; 
 import org . apache . cassandra . io . util . FileUtils ; 
 import org . apache . cassandra . utils . DirectorySizeCalculator ; 
 + import org . apache . cassandra . utils . NoSpamLogger ; 
 
 public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager 
 { 
 @ @ - 116 , 6 + 119 , 12 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager 
 if ( mutation . trackedByCDC ( ) & & segment . getCDCState ( ) = = CDCState . FORBIDDEN ) 
 { 
 cdcSizeTracker . submitOverflowSizeRecalculation ( ) ; 
 + NoSpamLogger . log ( logger , 
 + NoSpamLogger . Level . WARN , 
 + 10 , 
 + TimeUnit . SECONDS , 
 + " Rejecting Mutation containing CDC - enabled table . Free up space in { } . " , 
 + DatabaseDescriptor . getCDCLogLocation ( ) ) ; 
 throw new WriteTimeoutException ( WriteType . CDC , ConsistencyLevel . LOCAL _ ONE , 0 , 1 ) ; 
 } 
 } 
 @ @ - 148 , 17 + 157 , 16 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager 
 * 
 * Allows atomic increment / decrement of unflushed size , however only allows increment on flushed and requires a full 
 * directory walk to determine any potential deletions by CDC consumer . 
 - * 
 - * TODO : linux performs approximately 25 % better with the following one - liner instead of this walker : 
 - * Arrays . stream ( path . listFiles ( ) ) . mapToLong ( File : : length ) . sum ( ) ; 
 - * However this solution is 375 % slower on Windows . Revisit this and split logic to per - OS 
 * / 
 private static class CDCSizeTracker extends DirectorySizeCalculator 
 { 
 private final RateLimiter rateLimiter = RateLimiter . create ( 1000 . 0 / DatabaseDescriptor . getCDCDiskCheckInterval ( ) ) ; 
 private ExecutorService cdcSizeCalculationExecutor ; 
 private CommitLogSegmentManagerCDC segmentManager ; 
 - private AtomicLong unflushedCDCSize = new AtomicLong ( 0 ) ; 
 + private volatile long unflushedCDCSize ; 
 + 
 + / / Used instead of size during walk to remove chance of over - allocation 
 + private volatile long sizeInProgress = 0 ; 
 
 CDCSizeTracker ( CommitLogSegmentManagerCDC segmentManager , File path ) 
 { 
 @ @ - 193 , 7 + 201 , 7 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager 
 ? CDCState . FORBIDDEN 
 : CDCState . PERMITTED ) ; 
 if ( segment . getCDCState ( ) = = CDCState . PERMITTED ) 
 - unflushedCDCSize . addAndGet ( defaultSegmentSize ( ) ) ; 
 + unflushedCDCSize + = defaultSegmentSize ( ) ; 
 } 
 
 / / Take this opportunity to kick off a recalc to pick up any consumer file deletion . 
 @ @ - 207 , 9 + 215 , 9 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager 
 { 
 / / Add to flushed size before decrementing unflushed so we don ' t have a window of false generosity 
 if ( segment . getCDCState ( ) = = CDCState . CONTAINS ) 
 - size . addAndGet ( segment . onDiskSize ( ) ) ; 
 + size + = segment . onDiskSize ( ) ; 
 if ( segment . getCDCState ( ) ! = CDCState . FORBIDDEN ) 
 - unflushedCDCSize . addAndGet ( - defaultSegmentSize ( ) ) ; 
 + unflushedCDCSize - = defaultSegmentSize ( ) ; 
 } 
 
 / / Take this opportunity to kick off a recalc to pick up any consumer file deletion . 
 @ @ - 251 , 14 + 259 , 10 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager 
 { 
 try 
 { 
 - / / Since we don ' t synchronize around either rebuilding our file list or walking the tree and adding to 
 - / / size , it ' s possible we could have changes take place underneath us and end up with a slightly incorrect 
 - / / view of our flushed size by the time this walking completes . Given that there ' s a linear growth in 
 - / / runtime on both rebuildFileList and walkFileTree ( about 50 % for each one on runtime ) , and that the 
 - / / window for this race should be very small , this is an acceptable trade - off since it will be resolved 
 - / / on the next segment creation / deletion with a subsequent call to submitOverflowSizeRecalculation . 
 - rebuildFileList ( ) ; 
 + / / The Arrays . stream approach is considerably slower on Windows than linux 
 + sizeInProgress = 0 ; 
 Files . walkFileTree ( path . toPath ( ) , this ) ; 
 + size = sizeInProgress ; 
 } 
 catch ( IOException ie ) 
 { 
 @ @ - 266 , 14 + 270 , 21 @ @ public class CommitLogSegmentManagerCDC extends AbstractCommitLogSegmentManager 
 } 
 } 
 
 - private long addFlushedSize ( long toAdd ) 
 + @ Override 
 + public FileVisitResult visitFile ( Path file , BasicFileAttributes attrs ) throws IOException 
 + { 
 + sizeInProgress + = attrs . size ( ) ; 
 + return FileVisitResult . CONTINUE ; 
 + } 
 + 
 + private void addFlushedSize ( long toAdd ) 
 { 
 - return size . addAndGet ( toAdd ) ; 
 + size + = toAdd ; 
 } 
 
 private long totalCDCSizeOnDisk ( ) 
 { 
 - return unflushedCDCSize . get ( ) + size . get ( ) ; 
 + return unflushedCDCSize + size ; 
 } 
 
 public void shutdown ( ) 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CompressedSegment . java b / src / java / org / apache / cassandra / db / commitlog / CompressedSegment . java 
 index e44dfdf . . 645eda9 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CompressedSegment . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CompressedSegment . java 
 @ @ - 48 , 11 + 48 , 7 @ @ public class CompressedSegment extends FileDirectSegment 
 { 
 super ( commitLog , manager , onClose ) ; 
 this . compressor = commitLog . configuration . getCompressor ( ) ; 
 - } 
 - 
 - ByteBuffer allocate ( int size ) 
 - { 
 - return compressor . preferredBufferType ( ) . allocate ( size ) ; 
 + manager . getBufferPool ( ) . setPreferredReusableBufferType ( compressor . preferredBufferType ( ) ) ; 
 } 
 
 ByteBuffer createBuffer ( CommitLog commitLog ) 
 @ @ - 71 , 14 + 67 , 7 @ @ public class CompressedSegment extends FileDirectSegment 
 try 
 { 
 int neededBufferSize = compressor . initialCompressedBufferLength ( length ) + COMPRESSED _ MARKER _ SIZE ; 
 - ByteBuffer compressedBuffer = manager . getBufferPool ( ) . getThreadLocalReusableBuffer ( ) ; 
 - if ( compressor . preferredBufferType ( ) ! = BufferType . typeOf ( compressedBuffer ) | | 
 - compressedBuffer . capacity ( ) < neededBufferSize ) 
 - { 
 - FileUtils . clean ( compressedBuffer ) ; 
 - compressedBuffer = allocate ( neededBufferSize ) ; 
 - manager . getBufferPool ( ) . setThreadLocalReusableBuffer ( compressedBuffer ) ; 
 - } 
 + ByteBuffer compressedBuffer = manager . getBufferPool ( ) . getThreadLocalReusableBuffer ( neededBufferSize ) ; 
 
 ByteBuffer inputBuffer = buffer . duplicate ( ) ; 
 inputBuffer . limit ( contentStart + length ) . position ( contentStart ) ; 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / EncryptedSegment . java b / src / java / org / apache / cassandra / db / commitlog / EncryptedSegment . java 
 index e13b20a . . 103351e 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / EncryptedSegment . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / EncryptedSegment . java 
 @ @ - 25 , 12 + 25 , 12 @ @ import javax . crypto . Cipher ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . io . FSWriteError ; 
 import org . apache . cassandra . io . compress . BufferType ; 
 import org . apache . cassandra . io . compress . ICompressor ; 
 import org . apache . cassandra . security . EncryptionUtils ; 
 import org . apache . cassandra . security . EncryptionContext ; 
 - import org . apache . cassandra . utils . ByteBufferUtil ; 
 import org . apache . cassandra . utils . Hex ; 
 import org . apache . cassandra . utils . SyncUtil ; 
 
 @ @ - 79 , 6 + 79 , 8 @ @ public class EncryptedSegment extends FileDirectSegment 
 throw new FSWriteError ( e , logFile ) ; 
 } 
 logger . debug ( " created a new encrypted commit log segment : { } " , logFile ) ; 
 + / / Keep reusable buffers on - heap regardless of compression preference so we avoid copy off / on repeatedly during decryption 
 + manager . getBufferPool ( ) . setPreferredReusableBufferType ( BufferType . ON _ HEAP ) ; 
 } 
 
 protected Map < String , String > additionalHeaderParameters ( ) 
 @ @ - 108 , 7 + 110 , 7 @ @ public class EncryptedSegment extends FileDirectSegment 
 { 
 ByteBuffer inputBuffer = buffer . duplicate ( ) ; 
 inputBuffer . limit ( contentStart + length ) . position ( contentStart ) ; 
 - ByteBuffer buffer = manager . getBufferPool ( ) . getThreadLocalReusableBuffer ( ) ; 
 + ByteBuffer buffer = manager . getBufferPool ( ) . getThreadLocalReusableBuffer ( DatabaseDescriptor . getCommitLogSegmentSize ( ) ) ; 
 
 / / save space for the sync marker at the beginning of this section 
 final long syncMarkerPosition = lastWrittenPos ; 
 @ @ - 132 , 21 + 134 , 17 @ @ public class EncryptedSegment extends FileDirectSegment 
 
 lastWrittenPos = channel . position ( ) ; 
 
 - / / rewind to the beginning of the section and write out the sync marker , 
 - / / reusing the one of the existing buffers 
 - buffer = ByteBufferUtil . ensureCapacity ( buffer , ENCRYPTED _ SECTION _ HEADER _ SIZE , true ) ; 
 + / / rewind to the beginning of the section and write out the sync marker 
 + buffer . position ( 0 ) . limit ( ENCRYPTED _ SECTION _ HEADER _ SIZE ) ; 
 writeSyncMarker ( buffer , 0 , ( int ) syncMarkerPosition , ( int ) lastWrittenPos ) ; 
 buffer . putInt ( SYNC _ MARKER _ SIZE , length ) ; 
 - buffer . position ( 0 ) . limit ( ENCRYPTED _ SECTION _ HEADER _ SIZE ) ; 
 + buffer . rewind ( ) ; 
 manager . addSize ( buffer . limit ( ) ) ; 
 
 channel . position ( syncMarkerPosition ) ; 
 channel . write ( buffer ) ; 
 
 SyncUtil . force ( channel , true ) ; 
 - 
 - if ( manager . getBufferPool ( ) . getThreadLocalReusableBuffer ( ) . capacity ( ) < buffer . capacity ( ) ) 
 - manager . getBufferPool ( ) . setThreadLocalReusableBuffer ( buffer ) ; 
 } 
 catch ( Exception e ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / SimpleCachedBufferPool . java b / src / java / org / apache / cassandra / db / commitlog / SimpleCachedBufferPool . java 
 index 1c10c25 . . bdec3fc 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / SimpleCachedBufferPool . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / SimpleCachedBufferPool . java 
 @ @ - 57 , 6 + 57 , 8 @ @ public class SimpleCachedBufferPool 
 * / 
 private final int bufferSize ; 
 
 + private BufferType preferredReusableBufferType = BufferType . ON _ HEAP ; 
 + 
 public SimpleCachedBufferPool ( int maxBufferPoolSize , int bufferSize ) 
 { 
 this . maxBufferPoolSize = maxBufferPoolSize ; 
 @ @ - 75 , 14 + 77 , 19 @ @ public class SimpleCachedBufferPool 
 return bufferType . allocate ( bufferSize ) ; 
 } 
 
 - public ByteBuffer getThreadLocalReusableBuffer ( ) 
 + public ByteBuffer getThreadLocalReusableBuffer ( int size ) 
 { 
 - return reusableBufferHolder . get ( ) ; 
 + ByteBuffer result = reusableBufferHolder . get ( ) ; 
 + if ( result . capacity ( ) < size | | BufferType . typeOf ( result ) ! = preferredReusableBufferType ) { 
 + FileUtils . clean ( result ) ; 
 + result = preferredReusableBufferType . allocate ( size ) ; 
 + reusableBufferHolder . set ( result ) ; 
 + } 
 + return result ; 
 } 
 
 - public void setThreadLocalReusableBuffer ( ByteBuffer buffer ) 
 - { 
 - reusableBufferHolder . set ( buffer ) ; 
 + public void setPreferredReusableBufferType ( BufferType type ) { 
 + preferredReusableBufferType = type ; 
 } 
 
 public void releaseBuffer ( ByteBuffer buffer ) 
 diff - - git a / src / java / org / apache / cassandra / utils / DirectorySizeCalculator . java b / src / java / org / apache / cassandra / utils / DirectorySizeCalculator . java 
 index aa7898c . . c1fb6e0 100644 
 - - - a / src / java / org / apache / cassandra / utils / DirectorySizeCalculator . java 
 + + + b / src / java / org / apache / cassandra / utils / DirectorySizeCalculator . java 
 @ @ - 24 , 39 + 24 , 19 @ @ import java . nio . file . FileVisitResult ; 
 import java . nio . file . Path ; 
 import java . nio . file . SimpleFileVisitor ; 
 import java . nio . file . attribute . BasicFileAttributes ; 
 - import java . util . List ; 
 - import java . util . Set ; 
 - import java . util . concurrent . atomic . AtomicLong ; 
 - 
 - import com . google . common . collect . ImmutableSet ; 
 - 
 - import static com . google . common . collect . Sets . newHashSet ; 
 
 / * * 
 * Walks directory recursively , summing up total contents of files within . 
 * / 
 public class DirectorySizeCalculator extends SimpleFileVisitor < Path > 
 { 
 - protected final AtomicLong size = new AtomicLong ( 0 ) ; 
 - protected Set < String > visited = newHashSet ( ) ; / / count each file only once 
 - protected Set < String > alive = newHashSet ( ) ; 
 + protected volatile long size = 0 ; 
 protected final File path ; 
 
 public DirectorySizeCalculator ( File path ) 
 { 
 super ( ) ; 
 this . path = path ; 
 - rebuildFileList ( ) ; 
 - } 
 - 
 - public DirectorySizeCalculator ( List < File > files ) 
 - { 
 - super ( ) ; 
 - this . path = null ; 
 - ImmutableSet . Builder < String > builder = ImmutableSet . builder ( ) ; 
 - for ( File file : files ) 
 - builder . add ( file . getName ( ) ) ; 
 - alive = builder . build ( ) ; 
 } 
 
 public boolean isAcceptable ( Path file ) 
 @ @ - 64 , 24 + 44 , 11 @ @ public class DirectorySizeCalculator extends SimpleFileVisitor < Path > 
 return true ; 
 } 
 
 - public void rebuildFileList ( ) 
 - { 
 - assert path ! = null ; 
 - ImmutableSet . Builder < String > builder = ImmutableSet . builder ( ) ; 
 - for ( File file : path . listFiles ( ) ) 
 - builder . add ( file . getName ( ) ) ; 
 - size . set ( 0 ) ; 
 - alive = builder . build ( ) ; 
 - } 
 - 
 @ Override 
 public FileVisitResult visitFile ( Path file , BasicFileAttributes attrs ) throws IOException 
 { 
 if ( isAcceptable ( file ) ) 
 - { 
 - size . addAndGet ( attrs . size ( ) ) ; 
 - visited . add ( file . toFile ( ) . getName ( ) ) ; 
 - } 
 + size + = attrs . size ( ) ; 
 return FileVisitResult . CONTINUE ; 
 } 
 
 @ @ - 93 , 6 + 60 , 6 @ @ public class DirectorySizeCalculator extends SimpleFileVisitor < Path > 
 
 public long getAllocatedSize ( ) 
 { 
 - return size . get ( ) ; 
 + return size ; 
 } 
 } 
 diff - - git a / test / microbench / org / apache / cassandra / test / microbench / DirectorySizerBench . java b / test / microbench / org / apache / cassandra / test / microbench / DirectorySizerBench . java 
 index a653c81 . . 34cbb17 100644 
 - - - a / test / microbench / org / apache / cassandra / test / microbench / DirectorySizerBench . java 
 + + + b / test / microbench / org / apache / cassandra / test / microbench / DirectorySizerBench . java 
 @ @ - 99 , 7 + 99 , 6 @ @ public class DirectorySizerBench 
 @ Benchmark 
 public void countFiles ( final Blackhole bh ) throws IOException 
 { 
 - sizer . rebuildFileList ( ) ; 
 Files . walkFileTree ( tempDir . toPath ( ) , sizer ) ; 
 } 
 } 
 diff - - git a / test / unit / org / apache / cassandra / db / commitlog / CommitLogTest . java b / test / unit / org / apache / cassandra / db / commitlog / CommitLogTest . java 
 index eff972d . . ebd868a 100644 
 - - - a / test / unit / org / apache / cassandra / db / commitlog / CommitLogTest . java 
 + + + b / test / unit / org / apache / cassandra / db / commitlog / CommitLogTest . java 
 @ @ - 508 , 19 + 508 , 6 @ @ public class CommitLogTest 
 runExpecting ( ( ) - > testRecovery ( new CommitLogDescriptor ( 4 , commitLogCompression , encryptionContext ) , logData ) , expected ) ; 
 } 
 
 - protected void testRecovery ( byte [ ] logData ) throws Exception 
 - { 
 - Pair < File , Integer > pair = tmpFile ( ) ; 
 - try ( RandomAccessFile raf = new RandomAccessFile ( pair . left , " rw " ) ) 
 - { 
 - raf . seek ( pair . right ) ; 
 - raf . write ( logData ) ; 
 - raf . close ( ) ; 
 - 
 - CommitLog . instance . recoverFiles ( pair . left ) ; / / CASSANDRA - 1119 / CASSANDRA - 1179 throw on failure * / 
 - } 
 - } 
 - 
 @ Test 
 public void testTruncateWithoutSnapshot ( ) throws ExecutionException , InterruptedException , IOException 
 {

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 index aa7d774 . . 98347ab 100644 
 - - - a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 + + + b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 @ @ - 24 , 10 + 24 , 7 @ @ import java . io . * ; 
 
 import org . apache . log4j . Logger ; 
 
 - import org . apache . cassandra . db . ColumnFamily ; 
 - import org . apache . cassandra . db . Table ; 
 - import org . apache . cassandra . db . TypeInfo ; 
 - import org . apache . cassandra . db . SystemTable ; 
 + import org . apache . cassandra . db . * ; 
 import org . apache . cassandra . utils . FileUtils ; 
 import org . apache . cassandra . utils . XMLUtils ; 
 import org . w3c . dom . Node ; 
 @ @ - 63 , 7 + 60 , 6 @ @ public class DatabaseDescriptor 
 private static int currentIndex _ = 0 ; 
 private static String logFileDirectory _ ; 
 private static String bootstrapFileDirectory _ ; 
 - private static int logRotationThreshold _ = 128 * 1024 * 1024 ; 
 private static boolean fastSync _ = false ; 
 private static boolean rackAware _ = false ; 
 private static int threadsPerPool _ = 4 ; 
 @ @ - 293 , 7 + 289 , 7 @ @ public class DatabaseDescriptor 
 / * threshold after which commit log should be rotated . * / 
 String value = xmlUtils . getNodeValue ( " / Storage / CommitLogRotationThresholdInMB " ) ; 
 if ( value ! = null ) 
 - logRotationThreshold _ = Integer . parseInt ( value ) * 1024 * 1024 ; 
 + CommitLog . setSegmentSize ( Integer . parseInt ( value ) * 1024 * 1024 ) ; 
 
 / * fast sync option * / 
 value = xmlUtils . getNodeValue ( " / Storage / CommitLogFastSync " ) ; 
 @ @ - 743 , 11 + 739 , 6 @ @ public class DatabaseDescriptor 
 bootstrapFileDirectory _ = bfLocation ; 
 } 
 
 - public static int getLogFileSizeThreshold ( ) 
 - { 
 - return logRotationThreshold _ ; 
 - } 
 - 
 public static String getLogFileLocation ( ) 
 { 
 return logFileDirectory _ ; 
 diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 index 3324b89 . . bca4a56 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 @ @ - 1661 , 4 + 1661 , 17 @ @ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 lock _ . readLock ( ) . unlock ( ) ; 
 } 
 } 
 + 
 + void clearUnsafe ( ) 
 + { 
 + lock _ . writeLock ( ) . lock ( ) ; 
 + try 
 + { 
 + memtable _ . clearUnsafe ( ) ; 
 + } 
 + finally 
 + { 
 + lock _ . writeLock ( ) . unlock ( ) ; 
 + } 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / CommitLog . java b / src / java / org / apache / cassandra / db / CommitLog . java 
 index 36f4521 . . 68e03cb 100644 
 - - - a / src / java / org / apache / cassandra / db / CommitLog . java 
 + + + b / src / java / org / apache / cassandra / db / CommitLog . java 
 @ @ - 61 , 7 + 61 , 7 @ @ import org . apache . commons . lang . StringUtils ; 
 * / 
 public class CommitLog 
 { 
 - private static final int bufSize _ = 128 * 1024 * 1024 ; 
 + private static volatile int SEGMENT _ SIZE = 128 * 1024 * 1024 ; / / roll after log gets this big 
 private static Map < String , CommitLog > instances _ = new HashMap < String , CommitLog > ( ) ; 
 private static Lock lock _ = new ReentrantLock ( ) ; 
 private static Logger logger _ = Logger . getLogger ( CommitLog . class ) ; 
 @ @ - 112 , 6 + 112 , 16 @ @ public class CommitLog 
 } 
 } 
 
 + public static void setSegmentSize ( int size ) 
 + { 
 + SEGMENT _ SIZE = size ; 
 + } 
 + 
 + static int getSegmentCount ( ) 
 + { 
 + return clHeaders _ . size ( ) ; 
 + } 
 + 
 static long getCreationTime ( String file ) 
 { 
 String [ ] entries = FBUtilities . strip ( file , " - . " ) ; 
 @ @ - 134 , 9 + 144 , 7 @ @ public class CommitLog 
 { 
 if ( DatabaseDescriptor . isFastSync ( ) ) 
 { 
 - / * Add this to the threshold * / 
 - int bufSize = 4 * 1024 * 1024 ; 
 - return SequenceFile . fastWriter ( file , CommitLog . bufSize _ + bufSize ) ; 
 + return SequenceFile . fastWriter ( file , 4 * 1024 * 1024 ) ; 
 } 
 else 
 return SequenceFile . writer ( file ) ; 
 @ @ - 178 , 9 + 186 , 6 @ @ public class CommitLog 
 private CommitLogHeader clHeader _ ; 
 private IFileWriter logWriter _ ; 
 private long commitHeaderStartPos _ ; 
 - / * Force rollover the commit log on the next insert * / 
 - private boolean forcedRollOver _ = false ; 
 - 
 
 / * 
 * Generates a file name of the format CommitLog - < table > - < timestamp > . log in the 
 @ @ - 456 , 8 + 461 , 7 @ @ public class CommitLog 
 / * Update the header * / 
 updateHeader ( row ) ; 
 logWriter _ . append ( table _ , cfBuffer ) ; 
 - fileSize = logWriter _ . getFileSize ( ) ; 
 - checkThresholdAndRollLog ( fileSize ) ; 
 + checkThresholdAndRollLog ( ) ; 
 } 
 catch ( IOException e ) 
 { 
 @ @ - 573 , 50 + 577 , 38 @ @ public class CommitLog 
 } 
 } 
 
 - private void checkThresholdAndRollLog ( long fileSize ) 
 + private void checkThresholdAndRollLog ( ) 
 { 
 try 
 { 
 - if ( fileSize > = DatabaseDescriptor . getLogFileSizeThreshold ( ) | | forcedRollOver _ ) 
 + if ( logWriter _ . getFileSize ( ) > = SEGMENT _ SIZE ) 
 { 
 - if ( logWriter _ . getFileSize ( ) > = DatabaseDescriptor . getLogFileSizeThreshold ( ) | | forcedRollOver _ ) 
 - { 
 - 	 / * Rolls the current log file over to a new one . * / 
 - 	 setNextFileName ( ) ; 
 - 	 String oldLogFile = logWriter _ . getFileName ( ) ; 
 - 	 / / history _ . add ( oldLogFile ) ; 
 - 	 logWriter _ . close ( ) ; 
 - 	 
 - 	 / * point reader / writer to a new commit log file . * / 
 - 	 / / logWriter _ = SequenceFile . writer ( logFile _ ) ; 
 - 	 logWriter _ = CommitLog . createWriter ( logFile _ ) ; 
 - 	 / * squirrel away the old commit log header * / 
 - 	 clHeaders _ . put ( oldLogFile , new CommitLogHeader ( clHeader _ ) ) ; 
 - 	 / * 
 - 	 * We need to zero out positions because the positions in 
 - 	 * the old file do not make sense in the new one . 
 - 	 * / 
 - 	 clHeader _ . zeroPositions ( ) ; 
 - 	 writeCommitLogHeader ( clHeader _ . toByteArray ( ) , false ) ; 
 - 	 / / Get the list of files in commit log directory if it is greater than a certain number 
 - 	 / / Force flush all the column families that way we ensure that a slowly populated column family is not screwing up 
 - 	 / / by accumulating the commit logs . 
 - } 
 + / * Rolls the current log file over to a new one . * / 
 + setNextFileName ( ) ; 
 + String oldLogFile = logWriter _ . getFileName ( ) ; 
 + / / history _ . add ( oldLogFile ) ; 
 + logWriter _ . close ( ) ; 
 + 
 + / * point reader / writer to a new commit log file . * / 
 + / / logWriter _ = SequenceFile . writer ( logFile _ ) ; 
 + logWriter _ = CommitLog . createWriter ( logFile _ ) ; 
 + / * squirrel away the old commit log header * / 
 + clHeaders _ . put ( oldLogFile , new CommitLogHeader ( clHeader _ ) ) ; 
 + / * 
 + * We need to zero out positions because the positions in 
 + * the old file do not make sense in the new one . 
 + * / 
 + clHeader _ . zeroPositions ( ) ; 
 + writeCommitLogHeader ( clHeader _ . toByteArray ( ) , false ) ; 
 + / / Get the list of files in commit log directory if it is greater than a certain number 
 + / / Force flush all the column families that way we ensure that a slowly populated column family is not screwing up 
 + / / by accumulating the commit logs . 
 } 
 } 
 - catch ( IOException e ) 
 + catch ( IOException e ) 
 { 
 logger _ . info ( LogUtil . throwableToString ( e ) ) ; 
 } 
 - finally 
 - { 
 - 	 forcedRollOver _ = false ; 
 - } 
 - } 
 - 
 - public void setForcedRollOver ( ) 
 - { 
 - 	 forcedRollOver _ = true ; 
 } 
 
 public static void main ( String [ ] args ) throws Throwable 
 diff - - git a / src / java / org / apache / cassandra / db / Memtable . java b / src / java / org / apache / cassandra / db / Memtable . java 
 index be641a9 . . d82e5a0 100644 
 - - - a / src / java / org / apache / cassandra / db / Memtable . java 
 + + + b / src / java / org / apache / cassandra / db / Memtable . java 
 @ @ - 388 , 4 + 388 , 9 @ @ public class Memtable implements Comparable < Memtable > 
 } 
 } ; 
 } 
 + 
 + public void clearUnsafe ( ) 
 + { 
 + columnFamilies _ . clear ( ) ; 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / RecoveryManager . java b / src / java / org / apache / cassandra / db / RecoveryManager . java 
 index f4e2d75 . . e6b3523 100644 
 - - - a / src / java / org / apache / cassandra / db / RecoveryManager . java 
 + + + b / src / java / org / apache / cassandra / db / RecoveryManager . java 
 @ @ - 70 , 7 + 70 , 7 @ @ public class RecoveryManager 
 return tableToCommitLogs ; 
 } 
 
 - public void doRecovery ( ) throws IOException 
 + public static void doRecovery ( ) throws IOException 
 { 
 File [ ] files = getListofCommitLogs ( ) ; 
 Map < String , List < File > > tableToCommitLogs = getListOFCommitLogsPerTable ( ) ; 
 @ @ - 78 , 7 + 78 , 7 @ @ public class RecoveryManager 
 FileUtils . delete ( files ) ; 
 } 
 
 - private void recoverEachTable ( Map < String , List < File > > tableToCommitLogs ) throws IOException 
 + private static void recoverEachTable ( Map < String , List < File > > tableToCommitLogs ) throws IOException 
 { 
 Comparator < File > fCmp = new FileUtils . FileComparator ( ) ; 
 Set < String > tables = tableToCommitLogs . keySet ( ) ; 
 @ @ - 90 , 12 + 90 , 4 @ @ public class RecoveryManager 
 clog . recover ( clogs ) ; 
 } 
 } 
 - 
 - public static void main ( String [ ] args ) throws Throwable 
 - { 
 - long start = System . currentTimeMillis ( ) ; 
 - RecoveryManager rm = RecoveryManager . instance ( ) ; 
 - rm . doRecovery ( ) ; 
 - logger _ . debug ( " Time taken : " + ( System . currentTimeMillis ( ) - start ) + " ms . " ) ; 
 - } 
 } 
 diff - - git a / test / unit / org / apache / cassandra / db / CommitLogTest . java b / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 index c9b2b48 . . 5bbbb62 100644 
 - - - a / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 + + + b / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 @ @ - 19 , 6 + 19 , 7 @ @ 
 package org . apache . cassandra . db ; 
 
 import java . io . IOException ; 
 + import java . util . concurrent . ExecutionException ; 
 
 import org . junit . Test ; 
 
 @ @ - 27 , 32 + 28 , 33 @ @ import org . apache . cassandra . CleanupHelper ; 
 public class CommitLogTest extends CleanupHelper 
 { 
 @ Test 
 - public void testMain ( ) throws IOException { 
 - / / TODO this is useless , since it assumes we have a working set of commit logs to parse 
 - / * 
 - File logDir = new File ( DatabaseDescriptor . getLogFileLocation ( ) ) ; 
 - File [ ] files = logDir . listFiles ( ) ; 
 - Arrays . sort ( files , new FileUtils . FileComparator ( ) ) ; 
 - 
 - byte [ ] bytes = new byte [ CommitLogHeader . size ( Integer . parseInt ( args [ 0 ] ) ) ] ; 
 - for ( File file : files ) 
 + public void testCleanup ( ) throws IOException , ExecutionException , InterruptedException 
 + { 
 + assert CommitLog . getSegmentCount ( ) = = 0 ; 
 + CommitLog . setSegmentSize ( 1000 ) ; 
 + 
 + Table table = Table . open ( " Table1 " ) ; 
 + ColumnFamilyStore store1 = table . getColumnFamilyStore ( " Standard1 " ) ; 
 + ColumnFamilyStore store2 = table . getColumnFamilyStore ( " Standard2 " ) ; 
 + RowMutation rm ; 
 + byte [ ] value = new byte [ 501 ] ; 
 + 
 + / / add data . use relatively large values to force quick segment creation since we have a low flush threshold in the test config . 
 + for ( int i = 0 ; i < 10 ; i + + ) 
 { 
 - CommitLog clog = new CommitLog ( file ) ; 
 - clog . readCommitLogHeader ( file . getAbsolutePath ( ) , bytes ) ; 
 - DataInputBuffer bufIn = new DataInputBuffer ( ) ; 
 - bufIn . reset ( bytes , 0 , bytes . length ) ; 
 - CommitLogHeader clHeader = CommitLogHeader . serializer ( ) . deserialize ( bufIn ) ; 
 - 
 - StringBuilder sb = new StringBuilder ( " " ) ; 
 - for ( byte b : bytes ) 
 - { 
 - sb . append ( b ) ; 
 - sb . append ( " " ) ; 
 - } 
 - 
 - System . out . println ( " FILE : " + file ) ; 
 - System . out . println ( clHeader . toString ( ) ) ; 
 + rm = new RowMutation ( " Table1 " , " key1 " ) ; 
 + rm . add ( " Standard1 : Column1 " , value , 0 ) ; 
 + rm . add ( " Standard2 : Column1 " , value , 0 ) ; 
 + rm . apply ( ) ; 
 } 
 - * / 
 + assert CommitLog . getSegmentCount ( ) > 1 ; 
 + 
 + / / nothing should get removed after flushing just Standard1 
 + store1 . forceBlockingFlush ( ) ; 
 + assert CommitLog . getSegmentCount ( ) > 1 ; 
 + 
 + / / after flushing Standard2 we should be able to clean out all segments 
 + store2 . forceBlockingFlush ( ) ; 
 + assert CommitLog . getSegmentCount ( ) = = 1 ; 
 } 
 } 
 diff - - git a / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java b / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java 
 index 0e2c732 . . e2edc37 100644 
 - - - a / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java 
 + + + b / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java 
 @ @ - 19 , 17 + 19 , 40 @ @ 
 package org . apache . cassandra . db ; 
 
 import java . io . IOException ; 
 + import java . util . concurrent . ExecutionException ; 
 
 import org . junit . Test ; 
 
 import org . apache . cassandra . CleanupHelper ; 
 + import static org . apache . cassandra . db . TableTest . assertColumns ; 
 
 public class RecoveryManagerTest extends CleanupHelper 
 { 
 @ Test 
 - public void testDoRecovery ( ) throws IOException { 
 + public void testNothing ( ) throws IOException { 
 / / TODO nothing to recover 
 RecoveryManager rm = RecoveryManager . instance ( ) ; 
 rm . doRecovery ( ) ; 
 } 
 + 
 + @ Test 
 + public void testSomething ( ) throws IOException , ExecutionException , InterruptedException 
 + { 
 + Table table1 = Table . open ( " Table1 " ) ; 
 + 
 + RowMutation rm ; 
 + ColumnFamily cf ; 
 + 
 + rm = new RowMutation ( " Table1 " , " keymulti " ) ; 
 + cf = new ColumnFamily ( " Standard1 " , " Standard " ) ; 
 + cf . addColumn ( new Column ( " col1 " , " val1 " . getBytes ( ) , 1L ) ) ; 
 + rm . add ( cf ) ; 
 + rm . apply ( ) ; 
 + 
 + table1 . getColumnFamilyStore ( " Standard1 " ) . clearUnsafe ( ) ; 
 + 
 + RecoveryManager . doRecovery ( ) ; 
 + 
 + assertColumns ( table1 . get ( " keymulti " , " Standard1 " ) , " col1 " ) ; 
 + } 
 } 
 diff - - git a / test / unit / org / apache / cassandra / db / TableTest . java b / test / unit / org / apache / cassandra / db / TableTest . java 
 index 3b9bf2e . . 61b12ca 100644 
 - - - a / test / unit / org / apache / cassandra / db / TableTest . java 
 + + + b / test / unit / org / apache / cassandra / db / TableTest . java 
 @ @ - 349 , 7 + 349 , 7 @ @ public class TableTest extends CleanupHelper 
 assertEquals ( new String ( cfres . getColumn ( " col1992 " ) . value ( ) ) , " vvvvvvvvvvvvvvvv1992 " ) ; 
 } 
 
 - private void assertColumns ( ColumnFamily columnFamily , String . . . columnNames ) 
 + public static void assertColumns ( ColumnFamily columnFamily , String . . . columnNames ) 
 { 
 assertNotNull ( columnFamily ) ; 
 SortedSet < IColumn > columns = columnFamily . getAllColumns ( ) ;
