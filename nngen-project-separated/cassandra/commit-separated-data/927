BLEU SCORE: 0.013902193611199336

TEST MSG: LogAwareFileLister should only use OLD sstable files in current folder to determine disk consistency
GENERATED MSG: clean up compaction code and refactor to allow user - specified threadhold ( minimum number of CFs to compact ) . patch by jbellis

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 26ab66d . . 47e6105 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 3 . 0 . 6 <nl> + * LogAwareFileLister should only use OLD sstable files in current folder to determine disk consistency ( CASSANDRA - 11470 ) <nl> * Notify indexers of expired rows during compaction ( CASSANDRA - 11329 ) <nl> * Properly respond with ProtocolError when a v1 / v2 native protocol <nl> header is received ( CASSANDRA - 11464 ) <nl> diff - - git a / src / java / org / apache / cassandra / db / lifecycle / LogAwareFileLister . java b / src / java / org / apache / cassandra / db / lifecycle / LogAwareFileLister . java <nl> index 3393b5c . . 4d3d46d 100644 <nl> - - - a / src / java / org / apache / cassandra / db / lifecycle / LogAwareFileLister . java <nl> + + + b / src / java / org / apache / cassandra / db / lifecycle / LogAwareFileLister . java <nl> @ @ - 8 , 11 + 8 , 13 @ @ import java . nio . file . Path ; <nl> import java . util . * ; <nl> import java . util . function . BiFunction ; <nl> import java . util . stream . Collectors ; <nl> - import java . util . stream . Stream ; <nl> import java . util . stream . StreamSupport ; <nl> <nl> import com . google . common . annotations . VisibleForTesting ; <nl> <nl> + import org . slf4j . Logger ; <nl> + import org . slf4j . LoggerFactory ; <nl> + <nl> import org . apache . cassandra . db . Directories ; <nl> <nl> import static org . apache . cassandra . db . Directories . * ; <nl> @ @ - 22 , 6 + 24 , 8 @ @ import static org . apache . cassandra . db . Directories . * ; <nl> * / <nl> final class LogAwareFileLister <nl> { <nl> + private static final Logger logger = LoggerFactory . getLogger ( LogAwareFileLister . class ) ; <nl> + <nl> / / The folder to scan <nl> private final Path folder ; <nl> <nl> @ @ - 112 , 8 + 116 , 8 @ @ final class LogAwareFileLister <nl> <nl> void classifyFiles ( LogFile txnFile ) <nl> { <nl> - Map < LogRecord , Set < File > > oldFiles = txnFile . getFilesOfType ( files . navigableKeySet ( ) , LogRecord . Type . REMOVE ) ; <nl> - Map < LogRecord , Set < File > > newFiles = txnFile . getFilesOfType ( files . navigableKeySet ( ) , LogRecord . Type . ADD ) ; <nl> + Map < LogRecord , Set < File > > oldFiles = txnFile . getFilesOfType ( folder , files . navigableKeySet ( ) , LogRecord . Type . REMOVE ) ; <nl> + Map < LogRecord , Set < File > > newFiles = txnFile . getFilesOfType ( folder , files . navigableKeySet ( ) , LogRecord . Type . ADD ) ; <nl> <nl> if ( txnFile . completed ( ) ) <nl> { / / last record present , filter regardless of disk status <nl> @ @ - 121 , 13 + 125 , 13 @ @ final class LogAwareFileLister <nl> return ; <nl> } <nl> <nl> - if ( allFilesPresent ( txnFile , oldFiles , newFiles ) ) <nl> - { / / all files present , transaction is in progress , this will filter as aborted <nl> + if ( allFilesPresent ( oldFiles ) ) <nl> + { / / all old files present , transaction is in progress , this will filter as aborted <nl> setTemporary ( txnFile , oldFiles . values ( ) , newFiles . values ( ) ) ; <nl> return ; <nl> } <nl> <nl> - / / some files are missing , we expect the txn file to either also be missing or completed , so check <nl> + / / some old files are missing , we expect the txn file to either also be missing or completed , so check <nl> / / disk state again to resolve any previous races on non - atomic directory listing platforms <nl> <nl> / / if txn file also gone , then do nothing ( all temporary should be gone , we could remove them if any ) <nl> @ @ - 143 , 23 + 147 , 30 @ @ final class LogAwareFileLister <nl> return ; <nl> } <nl> <nl> - / / some files are missing and yet the txn is still there and not completed <nl> - / / something must be wrong ( see comment at the top of this file requiring txn to be <nl> + logger . error ( " Failed to classify files in { } \ n " + <nl> + " Some old files are missing but the txn log is still there and not completed \ n " + <nl> + " Files in folder : \ n { } \ nTxn : { } \ n { } " , <nl> + folder , <nl> + files . isEmpty ( ) <nl> + ? " \ t - " <nl> + : String . join ( " \ n " , files . keySet ( ) . stream ( ) . map ( f - > String . format ( " \ t % s " , f ) ) . collect ( Collectors . toList ( ) ) ) , <nl> + txnFile . toString ( ) , <nl> + String . join ( " \ n " , txnFile . getRecords ( ) . stream ( ) . map ( r - > String . format ( " \ t % s " , r ) ) . collect ( Collectors . toList ( ) ) ) ) ; <nl> + <nl> + / / some old files are missing and yet the txn is still there and not completed <nl> + / / something must be wrong ( see comment at the top of LogTransaction requiring txn to be <nl> / / completed before obsoleting or aborting sstables ) <nl> throw new RuntimeException ( String . format ( " Failed to list directory files in % s , inconsistent disk state for transaction % s " , <nl> folder , <nl> txnFile ) ) ; <nl> } <nl> <nl> - / * * See if all files are present or if only the last record files are missing and it ' s a NEW record * / <nl> - private static boolean allFilesPresent ( LogFile txnFile , Map < LogRecord , Set < File > > oldFiles , Map < LogRecord , Set < File > > newFiles ) <nl> + / * * See if all files are present * / <nl> + private static boolean allFilesPresent ( Map < LogRecord , Set < File > > oldFiles ) <nl> { <nl> - LogRecord lastRecord = txnFile . getLastRecord ( ) ; <nl> - return ! Stream . concat ( oldFiles . entrySet ( ) . stream ( ) , <nl> - newFiles . entrySet ( ) . stream ( ) <nl> - . filter ( ( e ) - > e . getKey ( ) ! = lastRecord ) ) <nl> - . filter ( ( e ) - > e . getKey ( ) . numFiles > e . getValue ( ) . size ( ) ) <nl> - . findFirst ( ) . isPresent ( ) ; <nl> + return ! oldFiles . entrySet ( ) . stream ( ) <nl> + . filter ( ( e ) - > e . getKey ( ) . numFiles > e . getValue ( ) . size ( ) ) <nl> + . findFirst ( ) . isPresent ( ) ; <nl> } <nl> <nl> private void setTemporary ( LogFile txnFile , Collection < Set < File > > oldFiles , Collection < Set < File > > newFiles ) <nl> diff - - git a / src / java / org / apache / cassandra / db / lifecycle / LogFile . java b / src / java / org / apache / cassandra / db / lifecycle / LogFile . java <nl> index 9064e5f . . 4c3e550 100644 <nl> - - - a / src / java / org / apache / cassandra / db / lifecycle / LogFile . java <nl> + + + b / src / java / org / apache / cassandra / db / lifecycle / LogFile . java <nl> @ @ - 1 , 6 + 1 , 7 @ @ <nl> package org . apache . cassandra . db . lifecycle ; <nl> <nl> import java . io . File ; <nl> + import java . nio . file . Path ; <nl> import java . util . * ; <nl> import java . util . regex . Matcher ; <nl> import java . util . regex . Pattern ; <nl> @ @ - 315 , 13 + 316 , 23 @ @ final class LogFile <nl> files . forEach ( LogTransaction : : delete ) ; <nl> } <nl> <nl> - Map < LogRecord , Set < File > > getFilesOfType ( NavigableSet < File > files , Type type ) <nl> + / * * <nl> + * Extract from the files passed in all those that are of the given type . <nl> + * <nl> + * Scan all records and select those that are of the given type , valid , and <nl> + * located in the same folder . For each such record extract from the files passed in <nl> + * those that belong to this record . <nl> + * <nl> + * @ return a map linking each mapped record to its files , where the files where passed in as parameters . <nl> + * / <nl> + Map < LogRecord , Set < File > > getFilesOfType ( Path folder , NavigableSet < File > files , Type type ) <nl> { <nl> Map < LogRecord , Set < File > > ret = new HashMap < > ( ) ; <nl> <nl> records . stream ( ) <nl> . filter ( type : : matches ) <nl> . filter ( LogRecord : : isValid ) <nl> + . filter ( r - > r . isInFolder ( folder ) ) <nl> . forEach ( ( r ) - > ret . put ( r , getRecordFiles ( files , r ) ) ) ; <nl> <nl> return ret ; <nl> @ @ - 378 , 4 + 389 , 9 @ @ final class LogFile <nl> LogFile . EXT ) ; <nl> return StringUtils . join ( folder , File . separator , fileName ) ; <nl> } <nl> + <nl> + Collection < LogRecord > getRecords ( ) <nl> + { <nl> + return records ; <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / lifecycle / LogRecord . java b / src / java / org / apache / cassandra / db / lifecycle / LogRecord . java <nl> index 9e606fc . . 9b7d59e 100644 <nl> - - - a / src / java / org / apache / cassandra / db / lifecycle / LogRecord . java <nl> + + + b / src / java / org / apache / cassandra / db / lifecycle / LogRecord . java <nl> @ @ - 255 , 6 + 255 , 13 @ @ final class LogRecord <nl> return absolutePath . isPresent ( ) ? Paths . get ( absolutePath . get ( ) ) . getFileName ( ) . toString ( ) : " " ; <nl> } <nl> <nl> + boolean isInFolder ( Path folder ) <nl> + { <nl> + return absolutePath . isPresent ( ) <nl> + ? FileUtils . isContained ( folder . toFile ( ) , Paths . get ( absolutePath . get ( ) ) . toFile ( ) ) <nl> + : false ; <nl> + } <nl> + <nl> String absolutePath ( ) <nl> { <nl> return absolutePath . isPresent ( ) ? absolutePath . get ( ) : " " ; <nl> diff - - git a / test / unit / org / apache / cassandra / db / lifecycle / LogTransactionTest . java b / test / unit / org / apache / cassandra / db / lifecycle / LogTransactionTest . java <nl> index 45b5844 . . 0f03baf 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / lifecycle / LogTransactionTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / lifecycle / LogTransactionTest . java <nl> @ @ - 831 , 6 + 831 , 72 @ @ public class LogTransactionTest extends AbstractTransactionalTest <nl> } <nl> <nl> @ Test <nl> + public void testGetTemporaryFilesMultipleFolders ( ) throws IOException <nl> + { <nl> + ColumnFamilyStore cfs = MockSchema . newCFS ( KEYSPACE ) ; <nl> + <nl> + File origiFolder = new Directories ( cfs . metadata ) . getDirectoryForNewSSTables ( ) ; <nl> + File dataFolder1 = new File ( origiFolder , " 1 " ) ; <nl> + File dataFolder2 = new File ( origiFolder , " 2 " ) ; <nl> + Files . createDirectories ( dataFolder1 . toPath ( ) ) ; <nl> + Files . createDirectories ( dataFolder2 . toPath ( ) ) ; <nl> + <nl> + SSTableReader [ ] sstables = { sstable ( dataFolder1 , cfs , 0 , 128 ) , <nl> + sstable ( dataFolder1 , cfs , 1 , 128 ) , <nl> + sstable ( dataFolder2 , cfs , 2 , 128 ) , <nl> + sstable ( dataFolder2 , cfs , 3 , 128 ) <nl> + } ; <nl> + <nl> + / / they should all have the same number of files since they are created in the same way <nl> + int numSStableFiles = sstables [ 0 ] . getAllFilePaths ( ) . size ( ) ; <nl> + <nl> + LogTransaction log = new LogTransaction ( OperationType . COMPACTION ) ; <nl> + assertNotNull ( log ) ; <nl> + <nl> + for ( File dataFolder : new File [ ] { dataFolder1 , dataFolder2 } ) <nl> + { <nl> + Set < File > tmpFiles = getTemporaryFiles ( dataFolder ) ; <nl> + assertNotNull ( tmpFiles ) ; <nl> + assertEquals ( 0 , tmpFiles . size ( ) ) ; <nl> + } <nl> + <nl> + LogTransaction . SSTableTidier [ ] tidiers = { log . obsoleted ( sstables [ 0 ] ) , log . obsoleted ( sstables [ 2 ] ) } ; <nl> + <nl> + log . trackNew ( sstables [ 1 ] ) ; <nl> + log . trackNew ( sstables [ 3 ] ) ; <nl> + <nl> + for ( File dataFolder : new File [ ] { dataFolder1 , dataFolder2 } ) <nl> + { <nl> + Set < File > tmpFiles = getTemporaryFiles ( dataFolder ) ; <nl> + assertNotNull ( tmpFiles ) ; <nl> + assertEquals ( numSStableFiles , tmpFiles . size ( ) ) ; <nl> + } <nl> + <nl> + log . finish ( ) ; <nl> + <nl> + for ( File dataFolder : new File [ ] { dataFolder1 , dataFolder2 } ) <nl> + { <nl> + Set < File > tmpFiles = getTemporaryFiles ( dataFolder ) ; <nl> + assertNotNull ( tmpFiles ) ; <nl> + assertEquals ( numSStableFiles , tmpFiles . size ( ) ) ; <nl> + } <nl> + <nl> + sstables [ 0 ] . markObsolete ( tidiers [ 0 ] ) ; <nl> + sstables [ 2 ] . markObsolete ( tidiers [ 1 ] ) ; <nl> + <nl> + Arrays . stream ( sstables ) . forEach ( s - > s . selfRef ( ) . release ( ) ) ; <nl> + LogTransaction . waitForDeletions ( ) ; <nl> + <nl> + for ( File dataFolder : new File [ ] { dataFolder1 , dataFolder2 } ) <nl> + { <nl> + Set < File > tmpFiles = getTemporaryFiles ( dataFolder ) ; <nl> + assertNotNull ( tmpFiles ) ; <nl> + assertEquals ( 0 , tmpFiles . size ( ) ) ; <nl> + } <nl> + <nl> + } <nl> + <nl> + @ Test <nl> public void testWrongChecksumLastLine ( ) throws IOException <nl> { <nl> testCorruptRecord ( ( t , s ) - >
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> index fc8aa29 . . e3008d2 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> @ @ - 55 , 6 + 55 , 7 @ @ import org . apache . cassandra . utils . BloomFilter ; <nl> import org . apache . cassandra . utils . FileUtils ; <nl> import org . apache . cassandra . utils . LogUtil ; <nl> import org . apache . cassandra . utils . TimedStatsDeque ; <nl> + import org . apache . commons . lang . StringUtils ; <nl> <nl> / * * <nl> * Author : Avinash Lakshman ( alakshman @ facebook . com ) & Prashant Malik ( pmalik @ facebook . com ) <nl> @ @ - 62 , 9 + 63 , 9 @ @ import org . apache . cassandra . utils . TimedStatsDeque ; <nl> <nl> public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> { <nl> - private static int threshHold _ = 4 ; <nl> - private static final int bufSize _ = 128 * 1024 * 1024 ; <nl> - private static int compactionMemoryThreshold _ = 1 < < 30 ; <nl> + private static int COMPACTION _ THRESHOLD = 4 ; / / compact this many sstables at a time <nl> + private static final int BUFSIZE = 128 * 1024 * 1024 ; <nl> + private static final int COMPACTION _ MEMORY _ THRESHOLD = 1 < < 30 ; <nl> private static Logger logger _ = Logger . getLogger ( ColumnFamilyStore . class ) ; <nl> <nl> private final String table _ ; <nl> @ @ - 717 , 8 + 718 , 8 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> 	 lock _ . writeLock ( ) . unlock ( ) ; <nl> } <nl> <nl> - if ( ( ssTableSize > = threshHold _ & & ! isCompacting _ . get ( ) ) <nl> - | | ( isCompacting _ . get ( ) & & ssTableSize % threshHold _ = = 0 ) ) <nl> + if ( ( ssTableSize > = COMPACTION _ THRESHOLD & & ! isCompacting _ . get ( ) ) <nl> + | | ( isCompacting _ . get ( ) & & ssTableSize % COMPACTION _ THRESHOLD = = 0 ) ) <nl> { <nl> logger _ . debug ( " Submitting for compaction . . . " ) ; <nl> MinorCompactionManager . instance ( ) . submit ( ColumnFamilyStore . this ) ; <nl> @ @ - 731 , 7 + 732 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> PriorityQueue < FileStruct > pq = new PriorityQueue < FileStruct > ( ) ; <nl> if ( files . size ( ) > 1 | | ( ranges ! = null & & files . size ( ) > 0 ) ) <nl> { <nl> - int bufferSize = Math . min ( ( ColumnFamilyStore . compactionMemoryThreshold _ / files . size ( ) ) , minBufferSize ) ; <nl> + int bufferSize = Math . min ( ( ColumnFamilyStore . COMPACTION _ MEMORY _ THRESHOLD / files . size ( ) ) , minBufferSize ) ; <nl> FileStruct fs = null ; <nl> for ( String file : files ) <nl> { <nl> @ @ - 805 , 40 + 806 , 47 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> return buckets . keySet ( ) ; <nl> } <nl> <nl> + public void doCompaction ( ) throws IOException <nl> + { <nl> + doCompaction ( COMPACTION _ THRESHOLD ) ; <nl> + } <nl> + <nl> / * <nl> * Break the files into buckets and then compact . <nl> * / <nl> - void doCompaction ( ) throws IOException <nl> + public void doCompaction ( int threshold ) throws IOException <nl> { <nl> isCompacting _ . set ( true ) ; <nl> List < String > files = new ArrayList < String > ( ssTables _ ) ; <nl> try <nl> { <nl> - 	 int count ; <nl> - 	 	 for ( List < String > fileList : getCompactionBuckets ( files , 50L * 1024L * 1024L ) ) <nl> + int count ; <nl> + for ( List < String > fileList : getCompactionBuckets ( files , 50L * 1024L * 1024L ) ) <nl> { <nl> - 	 	 	 Collections . sort ( fileList , new FileNameComparator ( FileNameComparator . Ascending ) ) ; <nl> - 	 	 	 if ( fileList . size ( ) > = threshHold _ ) <nl> - 	 	 	 { <nl> - 	 	 	 	 files . clear ( ) ; <nl> - 	 	 	 	 count = 0 ; <nl> - 	 	 	 	 for ( String file : fileList ) <nl> - 	 	 	 	 { <nl> - 	 	 	 	 	 files . add ( file ) ; <nl> - 	 	 	 	 	 count + + ; <nl> - 	 	 	 	 	 if ( count = = threshHold _ ) <nl> - 	 	 	 	 	 	 break ; <nl> - 	 	 	 	 } <nl> - / / For each bucket if it has crossed the threshhold do the compaction <nl> - / / In case of range compaction merge the counting bloom filters also . <nl> - if ( count = = threshHold _ ) <nl> - doFileCompaction ( files , bufSize _ ) ; <nl> - 	 	 	 } <nl> - 	 	 } <nl> + Collections . sort ( fileList , new FileNameComparator ( FileNameComparator . Ascending ) ) ; <nl> + if ( fileList . size ( ) < threshold ) <nl> + { <nl> + continue ; <nl> + } <nl> + / / For each bucket if it has crossed the threshhold do the compaction <nl> + / / In case of range compaction merge the counting bloom filters also . <nl> + files . clear ( ) ; <nl> + count = 0 ; <nl> + for ( String file : fileList ) <nl> + { <nl> + files . add ( file ) ; <nl> + count + + ; <nl> + if ( count = = threshold ) <nl> + { <nl> + doFileCompaction ( files , BUFSIZE ) ; <nl> + break ; <nl> + } <nl> + } <nl> + } <nl> } <nl> finally <nl> { <nl> - 	 isCompacting _ . set ( false ) ; <nl> + isCompacting _ . set ( false ) ; <nl> } <nl> } <nl> <nl> @ @ - 876 , 11 + 884 , 11 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> 	 { <nl> 	 	 files = filesInternal ; <nl> 	 } <nl> - 	 doFileCompaction ( files , bufSize _ ) ; <nl> + 	 doFileCompaction ( files , BUFSIZE ) ; <nl> } <nl> - catch ( Exception ex ) <nl> + catch ( IOException ex ) <nl> { <nl> - 	 ex . printStackTrace ( ) ; <nl> + logger _ . error ( ex ) ; <nl> } <nl> finally <nl> { <nl> @ @ - 932 , 10 + 940 , 6 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> { <nl> 	 result = doFileAntiCompaction ( files , ranges , target , fileList , null ) ; <nl> } <nl> - catch ( Exception ex ) <nl> - { <nl> - 	 ex . printStackTrace ( ) ; <nl> - } <nl> finally <nl> { <nl> 	 isCompacting _ . set ( false ) ; <nl> @ @ - 958 , 18 + 962 , 17 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> { <nl> isCompacting _ . set ( true ) ; <nl> List < String > files = new ArrayList < String > ( ssTables _ ) ; <nl> - for ( String file : files ) <nl> + try <nl> { <nl> - 	 try <nl> - 	 { <nl> - 	 	 doCleanup ( file ) ; <nl> - 	 } <nl> - 	 catch ( Exception ex ) <nl> - 	 { <nl> - 	 	 ex . printStackTrace ( ) ; <nl> - 	 } <nl> + for ( String file : files ) <nl> + { <nl> + doCleanup ( file ) ; <nl> + } <nl> + } <nl> + finally <nl> + { <nl> + 	 isCompacting _ . set ( false ) ; <nl> } <nl> - 	 isCompacting _ . set ( false ) ; <nl> } <nl> / * * <nl> * cleans up one particular file by removing keys that this node is not responsible for . <nl> @ @ - 1048 , 7 + 1051 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> 	 + expectedRangeFileSize + " is greater than the safe limit of the disk space available . " ) ; <nl> 	 return result ; <nl> 	 } <nl> - 	 PriorityQueue < FileStruct > pq = initializePriorityQueue ( files , ranges , ColumnFamilyStore . bufSize _ ) ; <nl> + 	 PriorityQueue < FileStruct > pq = initializePriorityQueue ( files , ranges , ColumnFamilyStore . BUFSIZE ) ; <nl> 	 if ( pq . size ( ) > 0 ) <nl> 	 { <nl> 	 mergedFileName = getTempFileName ( ) ; <nl> @ @ - 1234 , 18 + 1237 , 11 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> * to get the latest data . <nl> * <nl> * / <nl> - void doFileCompaction ( List < String > files , int minBufferSize ) throws IOException <nl> + private void doFileCompaction ( List < String > files , int minBufferSize ) throws IOException <nl> { <nl> - 	 String newfile = null ; <nl> - long startTime = System . currentTimeMillis ( ) ; <nl> - long totalBytesRead = 0 ; <nl> - long totalBytesWritten = 0 ; <nl> - long totalkeysRead = 0 ; <nl> - long totalkeysWritten = 0 ; <nl> - / / Calculate the expected compacted filesize <nl> - long expectedCompactedFileSize = getExpectedCompactedFileSize ( files ) ; <nl> - String compactionFileLocation = DatabaseDescriptor . getCompactionFileLocation ( expectedCompactedFileSize ) ; <nl> + String compactionFileLocation = DatabaseDescriptor . getCompactionFileLocation ( getExpectedCompactedFileSize ( files ) ) ; <nl> / / If the compaction file path is null that means we have no space left for this compaction . <nl> + / / try again w / o the largest one . <nl> if ( compactionFileLocation = = null ) <nl> { <nl> String maxFile = getMaxSizeFile ( files ) ; <nl> @ @ - 1253 , 7 + 1249 , 15 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> doFileCompaction ( files , minBufferSize ) ; <nl> return ; <nl> } <nl> + <nl> + String newfile = null ; <nl> + long startTime = System . currentTimeMillis ( ) ; <nl> + long totalBytesRead = 0 ; <nl> + long totalBytesWritten = 0 ; <nl> + long totalkeysRead = 0 ; <nl> + long totalkeysWritten = 0 ; <nl> PriorityQueue < FileStruct > pq = initializePriorityQueue ( files , null , minBufferSize ) ; <nl> + <nl> if ( pq . size ( ) > 0 ) <nl> { <nl> String mergedFileName = getTempFileName ( files ) ; <nl> @ @ - 1331 , 7 + 1335 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> } <nl> catch ( Exception ex ) <nl> { <nl> - ex . printStackTrace ( ) ; <nl> + logger _ . error ( " empty sstable file " + filestruct . getFileName ( ) , ex ) ; <nl> filestruct . close ( ) ; <nl> continue ; <nl> } <nl> @ @ - 1390 , 10 + 1394 , 10 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> } <nl> if ( newfile ! = null ) <nl> { <nl> - ssTables _ . add ( newfile ) ; <nl> logger _ . debug ( " Inserting bloom filter for file " + newfile ) ; <nl> SSTable . storeBloomFilter ( newfile , compactedBloomFilter ) ; <nl> - totalBytesWritten = ( new File ( newfile ) ) . length ( ) ; <nl> + ssTables _ . add ( newfile ) ; <nl> + totalBytesWritten + = ( new File ( newfile ) ) . length ( ) ; <nl> } <nl> } <nl> finally <nl> @ @ - 1405 , 11 + 1409 , 9 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> SSTable . delete ( file ) ; <nl> } <nl> } <nl> - logger _ . debug ( " Total time taken for compaction . . . " <nl> - + ( System . currentTimeMillis ( ) - startTime ) ) ; <nl> - logger _ . debug ( " Total bytes Read for compaction . . . " + totalBytesRead ) ; <nl> - logger _ . debug ( " Total bytes written for compaction . . . " <nl> - + totalBytesWritten + " Total keys read . . . " + totalkeysRead ) ; <nl> + String format = " Compacted [ % s ] to % s . % d / % d bytes for % d / % d keys read / written . Time : % dms . " ; <nl> + long dTime = System . currentTimeMillis ( ) - startTime ; <nl> + logger _ . info ( String . format ( format , StringUtils . join ( files , " , " ) , newfile , totalBytesRead , totalBytesWritten , totalkeysRead , totalkeysWritten , dTime ) ) ; <nl> } <nl> <nl> public boolean isSuper ( ) <nl> diff - - git a / src / java / org / apache / cassandra / db / DBManager . java b / src / java / org / apache / cassandra / db / DBManager . java <nl> index 009ca06 . . 85c988a 100644 <nl> - - - a / src / java / org / apache / cassandra / db / DBManager . java <nl> + + + b / src / java / org / apache / cassandra / db / DBManager . java <nl> @ @ - 154 , 9 + 154 , 4 @ @ public class DBManager <nl> } <nl> return storageMetadata ; <nl> } <nl> - <nl> - public static void main ( String [ ] args ) throws Throwable <nl> - { <nl> - DBManager . instance ( ) . start ( ) ; <nl> - } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / FileStruct . java b / src / java / org / apache / cassandra / db / FileStruct . java <nl> index e382720 . . fc7eec8 100644 <nl> - - - a / src / java / org / apache / cassandra / db / FileStruct . java <nl> + + + b / src / java / org / apache / cassandra / db / FileStruct . java <nl> @ @ - 27 , 10 + 27 , 13 @ @ import org . apache . cassandra . io . IFileReader ; <nl> import org . apache . cassandra . io . SSTable ; <nl> import org . apache . cassandra . io . Coordinate ; <nl> import org . apache . cassandra . dht . IPartitioner ; <nl> + import org . apache . log4j . Logger ; <nl> <nl> <nl> public class FileStruct implements Comparable < FileStruct > , Iterator < String > <nl> { <nl> + private static Logger logger = Logger . getLogger ( FileStruct . class ) ; <nl> + <nl> private String key = null ; / / decorated ! <nl> private boolean exhausted = false ; <nl> private IFileReader reader ; <nl> diff - - git a / src / java / org / apache / cassandra / db / MinorCompactionManager . java b / src / java / org / apache / cassandra / db / MinorCompactionManager . java <nl> index c27bdf9 . . fda2c02 100644 <nl> - - - a / src / java / org / apache / cassandra / db / MinorCompactionManager . java <nl> + + + b / src / java / org / apache / cassandra / db / MinorCompactionManager . java <nl> @ @ - 72 , 12 + 72 , 12 @ @ class MinorCompactionManager implements IComponentShutdown <nl> <nl> FileCompactor ( ColumnFamilyStore columnFamilyStore ) <nl> { <nl> - 	 columnFamilyStore _ = columnFamilyStore ; <nl> + columnFamilyStore _ = columnFamilyStore ; <nl> } <nl> <nl> public void run ( ) <nl> { <nl> - logger _ . debug ( " Started compaction . . . " + columnFamilyStore _ . columnFamily _ ) ; <nl> + logger _ . debug ( " Started compaction . . . " + columnFamilyStore _ . columnFamily _ ) ; <nl> try <nl> { <nl> columnFamilyStore _ . doCompaction ( ) ; <nl> @ @ - 86 , 7 + 86 , 7 @ @ class MinorCompactionManager implements IComponentShutdown <nl> { <nl> throw new RuntimeException ( e ) ; <nl> } <nl> - logger _ . debug ( " Finished compaction . . . " + columnFamilyStore _ . columnFamily _ ) ; <nl> + logger _ . debug ( " Finished compaction . . . " + columnFamilyStore _ . columnFamily _ ) ; <nl> } <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / io / Coordinate . java b / src / java / org / apache / cassandra / io / Coordinate . java <nl> index 7a544b9 . . 23dff6e 100644 <nl> - - - a / src / java / org / apache / cassandra / io / Coordinate . java <nl> + + + b / src / java / org / apache / cassandra / io / Coordinate . java <nl> @ @ - 31 , 4 + 31 , 12 @ @ public class Coordinate <nl> start _ = start ; <nl> end _ = end ; <nl> } <nl> + <nl> + public String toString ( ) <nl> + { <nl> + return " Coordinate ( " + <nl> + " start _ = " + start _ + <nl> + " , end _ = " + end _ + <nl> + ' ) ' ; <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / SSTable . java b / src / java / org / apache / cassandra / io / SSTable . java <nl> index 493ce94 . . dd5082f 100644 <nl> - - - a / src / java / org / apache / cassandra / io / SSTable . java <nl> + + + b / src / java / org / apache / cassandra / io / SSTable . java <nl> @ @ - 221 , 17 + 221 , 11 @ @ public class SSTable <nl> } <nl> <nl> File file = new File ( dataFile ) ; <nl> - if ( file . exists ( ) ) <nl> + assert file . exists ( ) ; <nl> + / * delete the data file * / <nl> + if ( ! file . delete ( ) ) <nl> { <nl> - / * delete the data file * / <nl> - 	 	 	 if ( file . delete ( ) ) <nl> - 	 	 	 { 	 	 	 <nl> - 	 	 	 logger _ . info ( " * * Deleted " + file . getName ( ) + " * * " ) ; <nl> - 	 	 	 } <nl> - 	 	 	 else <nl> - 	 	 	 { 	 	 	 <nl> - 	 	 	 logger _ . error ( " Failed to delete " + file . getName ( ) ) ; <nl> - 	 	 	 } <nl> + logger _ . error ( " Failed to delete " + file . getName ( ) ) ; <nl> } <nl> } <nl> <nl> @ @ - 600 , 9 + 594 , 8 @ @ public class SSTable <nl> * @ throws IOException <nl> * / <nl> private void dumpBlockIndexes ( ) throws IOException <nl> - { 	 <nl> - long position = dataWriter _ . getCurrentPosition ( ) ; <nl> - firstBlockPosition _ = position ; <nl> + { <nl> + firstBlockPosition _ = dataWriter _ . getCurrentPosition ( ) ; <nl> 	 for ( SortedMap < String , BlockMetadata > block : blockIndexes _ ) <nl> 	 { <nl> 	 	 dumpBlockIndex ( block ) ; <nl> @ @ - 660 , 12 + 653 , 16 @ @ public class SSTable <nl> afterAppend ( decoratedKey , currentPosition , value . length ) ; <nl> } <nl> <nl> + / * <nl> + TODO only the end _ part of the returned Coordinate is ever used . Apparently this code works , but it ' s definitely due for some cleanup <nl> + since the code fooling about with start _ appears to be irrelevant . <nl> + * / <nl> public static Coordinate getCoordinates ( String decoratedKey , IFileReader dataReader , IPartitioner partitioner ) throws IOException <nl> { <nl> 	 List < KeyPositionInfo > indexInfo = indexMetadataMap _ . get ( dataReader . getFileName ( ) ) ; <nl> 	 int size = ( indexInfo = = null ) ? 0 : indexInfo . size ( ) ; <nl> 	 long start = 0L ; <nl> - 	 long end = dataReader . getEOF ( ) ; <nl> + 	 long end ; <nl> if ( size > 0 ) <nl> { <nl> int index = Collections . binarySearch ( indexInfo , new KeyPositionInfo ( decoratedKey , partitioner ) ) ;

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 26ab66d . . 47e6105 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 3 . 0 . 6 
 + * LogAwareFileLister should only use OLD sstable files in current folder to determine disk consistency ( CASSANDRA - 11470 ) 
 * Notify indexers of expired rows during compaction ( CASSANDRA - 11329 ) 
 * Properly respond with ProtocolError when a v1 / v2 native protocol 
 header is received ( CASSANDRA - 11464 ) 
 diff - - git a / src / java / org / apache / cassandra / db / lifecycle / LogAwareFileLister . java b / src / java / org / apache / cassandra / db / lifecycle / LogAwareFileLister . java 
 index 3393b5c . . 4d3d46d 100644 
 - - - a / src / java / org / apache / cassandra / db / lifecycle / LogAwareFileLister . java 
 + + + b / src / java / org / apache / cassandra / db / lifecycle / LogAwareFileLister . java 
 @ @ - 8 , 11 + 8 , 13 @ @ import java . nio . file . Path ; 
 import java . util . * ; 
 import java . util . function . BiFunction ; 
 import java . util . stream . Collectors ; 
 - import java . util . stream . Stream ; 
 import java . util . stream . StreamSupport ; 
 
 import com . google . common . annotations . VisibleForTesting ; 
 
 + import org . slf4j . Logger ; 
 + import org . slf4j . LoggerFactory ; 
 + 
 import org . apache . cassandra . db . Directories ; 
 
 import static org . apache . cassandra . db . Directories . * ; 
 @ @ - 22 , 6 + 24 , 8 @ @ import static org . apache . cassandra . db . Directories . * ; 
 * / 
 final class LogAwareFileLister 
 { 
 + private static final Logger logger = LoggerFactory . getLogger ( LogAwareFileLister . class ) ; 
 + 
 / / The folder to scan 
 private final Path folder ; 
 
 @ @ - 112 , 8 + 116 , 8 @ @ final class LogAwareFileLister 
 
 void classifyFiles ( LogFile txnFile ) 
 { 
 - Map < LogRecord , Set < File > > oldFiles = txnFile . getFilesOfType ( files . navigableKeySet ( ) , LogRecord . Type . REMOVE ) ; 
 - Map < LogRecord , Set < File > > newFiles = txnFile . getFilesOfType ( files . navigableKeySet ( ) , LogRecord . Type . ADD ) ; 
 + Map < LogRecord , Set < File > > oldFiles = txnFile . getFilesOfType ( folder , files . navigableKeySet ( ) , LogRecord . Type . REMOVE ) ; 
 + Map < LogRecord , Set < File > > newFiles = txnFile . getFilesOfType ( folder , files . navigableKeySet ( ) , LogRecord . Type . ADD ) ; 
 
 if ( txnFile . completed ( ) ) 
 { / / last record present , filter regardless of disk status 
 @ @ - 121 , 13 + 125 , 13 @ @ final class LogAwareFileLister 
 return ; 
 } 
 
 - if ( allFilesPresent ( txnFile , oldFiles , newFiles ) ) 
 - { / / all files present , transaction is in progress , this will filter as aborted 
 + if ( allFilesPresent ( oldFiles ) ) 
 + { / / all old files present , transaction is in progress , this will filter as aborted 
 setTemporary ( txnFile , oldFiles . values ( ) , newFiles . values ( ) ) ; 
 return ; 
 } 
 
 - / / some files are missing , we expect the txn file to either also be missing or completed , so check 
 + / / some old files are missing , we expect the txn file to either also be missing or completed , so check 
 / / disk state again to resolve any previous races on non - atomic directory listing platforms 
 
 / / if txn file also gone , then do nothing ( all temporary should be gone , we could remove them if any ) 
 @ @ - 143 , 23 + 147 , 30 @ @ final class LogAwareFileLister 
 return ; 
 } 
 
 - / / some files are missing and yet the txn is still there and not completed 
 - / / something must be wrong ( see comment at the top of this file requiring txn to be 
 + logger . error ( " Failed to classify files in { } \ n " + 
 + " Some old files are missing but the txn log is still there and not completed \ n " + 
 + " Files in folder : \ n { } \ nTxn : { } \ n { } " , 
 + folder , 
 + files . isEmpty ( ) 
 + ? " \ t - " 
 + : String . join ( " \ n " , files . keySet ( ) . stream ( ) . map ( f - > String . format ( " \ t % s " , f ) ) . collect ( Collectors . toList ( ) ) ) , 
 + txnFile . toString ( ) , 
 + String . join ( " \ n " , txnFile . getRecords ( ) . stream ( ) . map ( r - > String . format ( " \ t % s " , r ) ) . collect ( Collectors . toList ( ) ) ) ) ; 
 + 
 + / / some old files are missing and yet the txn is still there and not completed 
 + / / something must be wrong ( see comment at the top of LogTransaction requiring txn to be 
 / / completed before obsoleting or aborting sstables ) 
 throw new RuntimeException ( String . format ( " Failed to list directory files in % s , inconsistent disk state for transaction % s " , 
 folder , 
 txnFile ) ) ; 
 } 
 
 - / * * See if all files are present or if only the last record files are missing and it ' s a NEW record * / 
 - private static boolean allFilesPresent ( LogFile txnFile , Map < LogRecord , Set < File > > oldFiles , Map < LogRecord , Set < File > > newFiles ) 
 + / * * See if all files are present * / 
 + private static boolean allFilesPresent ( Map < LogRecord , Set < File > > oldFiles ) 
 { 
 - LogRecord lastRecord = txnFile . getLastRecord ( ) ; 
 - return ! Stream . concat ( oldFiles . entrySet ( ) . stream ( ) , 
 - newFiles . entrySet ( ) . stream ( ) 
 - . filter ( ( e ) - > e . getKey ( ) ! = lastRecord ) ) 
 - . filter ( ( e ) - > e . getKey ( ) . numFiles > e . getValue ( ) . size ( ) ) 
 - . findFirst ( ) . isPresent ( ) ; 
 + return ! oldFiles . entrySet ( ) . stream ( ) 
 + . filter ( ( e ) - > e . getKey ( ) . numFiles > e . getValue ( ) . size ( ) ) 
 + . findFirst ( ) . isPresent ( ) ; 
 } 
 
 private void setTemporary ( LogFile txnFile , Collection < Set < File > > oldFiles , Collection < Set < File > > newFiles ) 
 diff - - git a / src / java / org / apache / cassandra / db / lifecycle / LogFile . java b / src / java / org / apache / cassandra / db / lifecycle / LogFile . java 
 index 9064e5f . . 4c3e550 100644 
 - - - a / src / java / org / apache / cassandra / db / lifecycle / LogFile . java 
 + + + b / src / java / org / apache / cassandra / db / lifecycle / LogFile . java 
 @ @ - 1 , 6 + 1 , 7 @ @ 
 package org . apache . cassandra . db . lifecycle ; 
 
 import java . io . File ; 
 + import java . nio . file . Path ; 
 import java . util . * ; 
 import java . util . regex . Matcher ; 
 import java . util . regex . Pattern ; 
 @ @ - 315 , 13 + 316 , 23 @ @ final class LogFile 
 files . forEach ( LogTransaction : : delete ) ; 
 } 
 
 - Map < LogRecord , Set < File > > getFilesOfType ( NavigableSet < File > files , Type type ) 
 + / * * 
 + * Extract from the files passed in all those that are of the given type . 
 + * 
 + * Scan all records and select those that are of the given type , valid , and 
 + * located in the same folder . For each such record extract from the files passed in 
 + * those that belong to this record . 
 + * 
 + * @ return a map linking each mapped record to its files , where the files where passed in as parameters . 
 + * / 
 + Map < LogRecord , Set < File > > getFilesOfType ( Path folder , NavigableSet < File > files , Type type ) 
 { 
 Map < LogRecord , Set < File > > ret = new HashMap < > ( ) ; 
 
 records . stream ( ) 
 . filter ( type : : matches ) 
 . filter ( LogRecord : : isValid ) 
 + . filter ( r - > r . isInFolder ( folder ) ) 
 . forEach ( ( r ) - > ret . put ( r , getRecordFiles ( files , r ) ) ) ; 
 
 return ret ; 
 @ @ - 378 , 4 + 389 , 9 @ @ final class LogFile 
 LogFile . EXT ) ; 
 return StringUtils . join ( folder , File . separator , fileName ) ; 
 } 
 + 
 + Collection < LogRecord > getRecords ( ) 
 + { 
 + return records ; 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / lifecycle / LogRecord . java b / src / java / org / apache / cassandra / db / lifecycle / LogRecord . java 
 index 9e606fc . . 9b7d59e 100644 
 - - - a / src / java / org / apache / cassandra / db / lifecycle / LogRecord . java 
 + + + b / src / java / org / apache / cassandra / db / lifecycle / LogRecord . java 
 @ @ - 255 , 6 + 255 , 13 @ @ final class LogRecord 
 return absolutePath . isPresent ( ) ? Paths . get ( absolutePath . get ( ) ) . getFileName ( ) . toString ( ) : " " ; 
 } 
 
 + boolean isInFolder ( Path folder ) 
 + { 
 + return absolutePath . isPresent ( ) 
 + ? FileUtils . isContained ( folder . toFile ( ) , Paths . get ( absolutePath . get ( ) ) . toFile ( ) ) 
 + : false ; 
 + } 
 + 
 String absolutePath ( ) 
 { 
 return absolutePath . isPresent ( ) ? absolutePath . get ( ) : " " ; 
 diff - - git a / test / unit / org / apache / cassandra / db / lifecycle / LogTransactionTest . java b / test / unit / org / apache / cassandra / db / lifecycle / LogTransactionTest . java 
 index 45b5844 . . 0f03baf 100644 
 - - - a / test / unit / org / apache / cassandra / db / lifecycle / LogTransactionTest . java 
 + + + b / test / unit / org / apache / cassandra / db / lifecycle / LogTransactionTest . java 
 @ @ - 831 , 6 + 831 , 72 @ @ public class LogTransactionTest extends AbstractTransactionalTest 
 } 
 
 @ Test 
 + public void testGetTemporaryFilesMultipleFolders ( ) throws IOException 
 + { 
 + ColumnFamilyStore cfs = MockSchema . newCFS ( KEYSPACE ) ; 
 + 
 + File origiFolder = new Directories ( cfs . metadata ) . getDirectoryForNewSSTables ( ) ; 
 + File dataFolder1 = new File ( origiFolder , " 1 " ) ; 
 + File dataFolder2 = new File ( origiFolder , " 2 " ) ; 
 + Files . createDirectories ( dataFolder1 . toPath ( ) ) ; 
 + Files . createDirectories ( dataFolder2 . toPath ( ) ) ; 
 + 
 + SSTableReader [ ] sstables = { sstable ( dataFolder1 , cfs , 0 , 128 ) , 
 + sstable ( dataFolder1 , cfs , 1 , 128 ) , 
 + sstable ( dataFolder2 , cfs , 2 , 128 ) , 
 + sstable ( dataFolder2 , cfs , 3 , 128 ) 
 + } ; 
 + 
 + / / they should all have the same number of files since they are created in the same way 
 + int numSStableFiles = sstables [ 0 ] . getAllFilePaths ( ) . size ( ) ; 
 + 
 + LogTransaction log = new LogTransaction ( OperationType . COMPACTION ) ; 
 + assertNotNull ( log ) ; 
 + 
 + for ( File dataFolder : new File [ ] { dataFolder1 , dataFolder2 } ) 
 + { 
 + Set < File > tmpFiles = getTemporaryFiles ( dataFolder ) ; 
 + assertNotNull ( tmpFiles ) ; 
 + assertEquals ( 0 , tmpFiles . size ( ) ) ; 
 + } 
 + 
 + LogTransaction . SSTableTidier [ ] tidiers = { log . obsoleted ( sstables [ 0 ] ) , log . obsoleted ( sstables [ 2 ] ) } ; 
 + 
 + log . trackNew ( sstables [ 1 ] ) ; 
 + log . trackNew ( sstables [ 3 ] ) ; 
 + 
 + for ( File dataFolder : new File [ ] { dataFolder1 , dataFolder2 } ) 
 + { 
 + Set < File > tmpFiles = getTemporaryFiles ( dataFolder ) ; 
 + assertNotNull ( tmpFiles ) ; 
 + assertEquals ( numSStableFiles , tmpFiles . size ( ) ) ; 
 + } 
 + 
 + log . finish ( ) ; 
 + 
 + for ( File dataFolder : new File [ ] { dataFolder1 , dataFolder2 } ) 
 + { 
 + Set < File > tmpFiles = getTemporaryFiles ( dataFolder ) ; 
 + assertNotNull ( tmpFiles ) ; 
 + assertEquals ( numSStableFiles , tmpFiles . size ( ) ) ; 
 + } 
 + 
 + sstables [ 0 ] . markObsolete ( tidiers [ 0 ] ) ; 
 + sstables [ 2 ] . markObsolete ( tidiers [ 1 ] ) ; 
 + 
 + Arrays . stream ( sstables ) . forEach ( s - > s . selfRef ( ) . release ( ) ) ; 
 + LogTransaction . waitForDeletions ( ) ; 
 + 
 + for ( File dataFolder : new File [ ] { dataFolder1 , dataFolder2 } ) 
 + { 
 + Set < File > tmpFiles = getTemporaryFiles ( dataFolder ) ; 
 + assertNotNull ( tmpFiles ) ; 
 + assertEquals ( 0 , tmpFiles . size ( ) ) ; 
 + } 
 + 
 + } 
 + 
 + @ Test 
 public void testWrongChecksumLastLine ( ) throws IOException 
 { 
 testCorruptRecord ( ( t , s ) - >

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 index fc8aa29 . . e3008d2 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 @ @ - 55 , 6 + 55 , 7 @ @ import org . apache . cassandra . utils . BloomFilter ; 
 import org . apache . cassandra . utils . FileUtils ; 
 import org . apache . cassandra . utils . LogUtil ; 
 import org . apache . cassandra . utils . TimedStatsDeque ; 
 + import org . apache . commons . lang . StringUtils ; 
 
 / * * 
 * Author : Avinash Lakshman ( alakshman @ facebook . com ) & Prashant Malik ( pmalik @ facebook . com ) 
 @ @ - 62 , 9 + 63 , 9 @ @ import org . apache . cassandra . utils . TimedStatsDeque ; 
 
 public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 { 
 - private static int threshHold _ = 4 ; 
 - private static final int bufSize _ = 128 * 1024 * 1024 ; 
 - private static int compactionMemoryThreshold _ = 1 < < 30 ; 
 + private static int COMPACTION _ THRESHOLD = 4 ; / / compact this many sstables at a time 
 + private static final int BUFSIZE = 128 * 1024 * 1024 ; 
 + private static final int COMPACTION _ MEMORY _ THRESHOLD = 1 < < 30 ; 
 private static Logger logger _ = Logger . getLogger ( ColumnFamilyStore . class ) ; 
 
 private final String table _ ; 
 @ @ - 717 , 8 + 718 , 8 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 	 lock _ . writeLock ( ) . unlock ( ) ; 
 } 
 
 - if ( ( ssTableSize > = threshHold _ & & ! isCompacting _ . get ( ) ) 
 - | | ( isCompacting _ . get ( ) & & ssTableSize % threshHold _ = = 0 ) ) 
 + if ( ( ssTableSize > = COMPACTION _ THRESHOLD & & ! isCompacting _ . get ( ) ) 
 + | | ( isCompacting _ . get ( ) & & ssTableSize % COMPACTION _ THRESHOLD = = 0 ) ) 
 { 
 logger _ . debug ( " Submitting for compaction . . . " ) ; 
 MinorCompactionManager . instance ( ) . submit ( ColumnFamilyStore . this ) ; 
 @ @ - 731 , 7 + 732 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 PriorityQueue < FileStruct > pq = new PriorityQueue < FileStruct > ( ) ; 
 if ( files . size ( ) > 1 | | ( ranges ! = null & & files . size ( ) > 0 ) ) 
 { 
 - int bufferSize = Math . min ( ( ColumnFamilyStore . compactionMemoryThreshold _ / files . size ( ) ) , minBufferSize ) ; 
 + int bufferSize = Math . min ( ( ColumnFamilyStore . COMPACTION _ MEMORY _ THRESHOLD / files . size ( ) ) , minBufferSize ) ; 
 FileStruct fs = null ; 
 for ( String file : files ) 
 { 
 @ @ - 805 , 40 + 806 , 47 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 return buckets . keySet ( ) ; 
 } 
 
 + public void doCompaction ( ) throws IOException 
 + { 
 + doCompaction ( COMPACTION _ THRESHOLD ) ; 
 + } 
 + 
 / * 
 * Break the files into buckets and then compact . 
 * / 
 - void doCompaction ( ) throws IOException 
 + public void doCompaction ( int threshold ) throws IOException 
 { 
 isCompacting _ . set ( true ) ; 
 List < String > files = new ArrayList < String > ( ssTables _ ) ; 
 try 
 { 
 - 	 int count ; 
 - 	 	 for ( List < String > fileList : getCompactionBuckets ( files , 50L * 1024L * 1024L ) ) 
 + int count ; 
 + for ( List < String > fileList : getCompactionBuckets ( files , 50L * 1024L * 1024L ) ) 
 { 
 - 	 	 	 Collections . sort ( fileList , new FileNameComparator ( FileNameComparator . Ascending ) ) ; 
 - 	 	 	 if ( fileList . size ( ) > = threshHold _ ) 
 - 	 	 	 { 
 - 	 	 	 	 files . clear ( ) ; 
 - 	 	 	 	 count = 0 ; 
 - 	 	 	 	 for ( String file : fileList ) 
 - 	 	 	 	 { 
 - 	 	 	 	 	 files . add ( file ) ; 
 - 	 	 	 	 	 count + + ; 
 - 	 	 	 	 	 if ( count = = threshHold _ ) 
 - 	 	 	 	 	 	 break ; 
 - 	 	 	 	 } 
 - / / For each bucket if it has crossed the threshhold do the compaction 
 - / / In case of range compaction merge the counting bloom filters also . 
 - if ( count = = threshHold _ ) 
 - doFileCompaction ( files , bufSize _ ) ; 
 - 	 	 	 } 
 - 	 	 } 
 + Collections . sort ( fileList , new FileNameComparator ( FileNameComparator . Ascending ) ) ; 
 + if ( fileList . size ( ) < threshold ) 
 + { 
 + continue ; 
 + } 
 + / / For each bucket if it has crossed the threshhold do the compaction 
 + / / In case of range compaction merge the counting bloom filters also . 
 + files . clear ( ) ; 
 + count = 0 ; 
 + for ( String file : fileList ) 
 + { 
 + files . add ( file ) ; 
 + count + + ; 
 + if ( count = = threshold ) 
 + { 
 + doFileCompaction ( files , BUFSIZE ) ; 
 + break ; 
 + } 
 + } 
 + } 
 } 
 finally 
 { 
 - 	 isCompacting _ . set ( false ) ; 
 + isCompacting _ . set ( false ) ; 
 } 
 } 
 
 @ @ - 876 , 11 + 884 , 11 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 	 { 
 	 	 files = filesInternal ; 
 	 } 
 - 	 doFileCompaction ( files , bufSize _ ) ; 
 + 	 doFileCompaction ( files , BUFSIZE ) ; 
 } 
 - catch ( Exception ex ) 
 + catch ( IOException ex ) 
 { 
 - 	 ex . printStackTrace ( ) ; 
 + logger _ . error ( ex ) ; 
 } 
 finally 
 { 
 @ @ - 932 , 10 + 940 , 6 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 { 
 	 result = doFileAntiCompaction ( files , ranges , target , fileList , null ) ; 
 } 
 - catch ( Exception ex ) 
 - { 
 - 	 ex . printStackTrace ( ) ; 
 - } 
 finally 
 { 
 	 isCompacting _ . set ( false ) ; 
 @ @ - 958 , 18 + 962 , 17 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 { 
 isCompacting _ . set ( true ) ; 
 List < String > files = new ArrayList < String > ( ssTables _ ) ; 
 - for ( String file : files ) 
 + try 
 { 
 - 	 try 
 - 	 { 
 - 	 	 doCleanup ( file ) ; 
 - 	 } 
 - 	 catch ( Exception ex ) 
 - 	 { 
 - 	 	 ex . printStackTrace ( ) ; 
 - 	 } 
 + for ( String file : files ) 
 + { 
 + doCleanup ( file ) ; 
 + } 
 + } 
 + finally 
 + { 
 + 	 isCompacting _ . set ( false ) ; 
 } 
 - 	 isCompacting _ . set ( false ) ; 
 } 
 / * * 
 * cleans up one particular file by removing keys that this node is not responsible for . 
 @ @ - 1048 , 7 + 1051 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 	 + expectedRangeFileSize + " is greater than the safe limit of the disk space available . " ) ; 
 	 return result ; 
 	 } 
 - 	 PriorityQueue < FileStruct > pq = initializePriorityQueue ( files , ranges , ColumnFamilyStore . bufSize _ ) ; 
 + 	 PriorityQueue < FileStruct > pq = initializePriorityQueue ( files , ranges , ColumnFamilyStore . BUFSIZE ) ; 
 	 if ( pq . size ( ) > 0 ) 
 	 { 
 	 mergedFileName = getTempFileName ( ) ; 
 @ @ - 1234 , 18 + 1237 , 11 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 * to get the latest data . 
 * 
 * / 
 - void doFileCompaction ( List < String > files , int minBufferSize ) throws IOException 
 + private void doFileCompaction ( List < String > files , int minBufferSize ) throws IOException 
 { 
 - 	 String newfile = null ; 
 - long startTime = System . currentTimeMillis ( ) ; 
 - long totalBytesRead = 0 ; 
 - long totalBytesWritten = 0 ; 
 - long totalkeysRead = 0 ; 
 - long totalkeysWritten = 0 ; 
 - / / Calculate the expected compacted filesize 
 - long expectedCompactedFileSize = getExpectedCompactedFileSize ( files ) ; 
 - String compactionFileLocation = DatabaseDescriptor . getCompactionFileLocation ( expectedCompactedFileSize ) ; 
 + String compactionFileLocation = DatabaseDescriptor . getCompactionFileLocation ( getExpectedCompactedFileSize ( files ) ) ; 
 / / If the compaction file path is null that means we have no space left for this compaction . 
 + / / try again w / o the largest one . 
 if ( compactionFileLocation = = null ) 
 { 
 String maxFile = getMaxSizeFile ( files ) ; 
 @ @ - 1253 , 7 + 1249 , 15 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 doFileCompaction ( files , minBufferSize ) ; 
 return ; 
 } 
 + 
 + String newfile = null ; 
 + long startTime = System . currentTimeMillis ( ) ; 
 + long totalBytesRead = 0 ; 
 + long totalBytesWritten = 0 ; 
 + long totalkeysRead = 0 ; 
 + long totalkeysWritten = 0 ; 
 PriorityQueue < FileStruct > pq = initializePriorityQueue ( files , null , minBufferSize ) ; 
 + 
 if ( pq . size ( ) > 0 ) 
 { 
 String mergedFileName = getTempFileName ( files ) ; 
 @ @ - 1331 , 7 + 1335 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 } 
 catch ( Exception ex ) 
 { 
 - ex . printStackTrace ( ) ; 
 + logger _ . error ( " empty sstable file " + filestruct . getFileName ( ) , ex ) ; 
 filestruct . close ( ) ; 
 continue ; 
 } 
 @ @ - 1390 , 10 + 1394 , 10 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 } 
 if ( newfile ! = null ) 
 { 
 - ssTables _ . add ( newfile ) ; 
 logger _ . debug ( " Inserting bloom filter for file " + newfile ) ; 
 SSTable . storeBloomFilter ( newfile , compactedBloomFilter ) ; 
 - totalBytesWritten = ( new File ( newfile ) ) . length ( ) ; 
 + ssTables _ . add ( newfile ) ; 
 + totalBytesWritten + = ( new File ( newfile ) ) . length ( ) ; 
 } 
 } 
 finally 
 @ @ - 1405 , 11 + 1409 , 9 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 SSTable . delete ( file ) ; 
 } 
 } 
 - logger _ . debug ( " Total time taken for compaction . . . " 
 - + ( System . currentTimeMillis ( ) - startTime ) ) ; 
 - logger _ . debug ( " Total bytes Read for compaction . . . " + totalBytesRead ) ; 
 - logger _ . debug ( " Total bytes written for compaction . . . " 
 - + totalBytesWritten + " Total keys read . . . " + totalkeysRead ) ; 
 + String format = " Compacted [ % s ] to % s . % d / % d bytes for % d / % d keys read / written . Time : % dms . " ; 
 + long dTime = System . currentTimeMillis ( ) - startTime ; 
 + logger _ . info ( String . format ( format , StringUtils . join ( files , " , " ) , newfile , totalBytesRead , totalBytesWritten , totalkeysRead , totalkeysWritten , dTime ) ) ; 
 } 
 
 public boolean isSuper ( ) 
 diff - - git a / src / java / org / apache / cassandra / db / DBManager . java b / src / java / org / apache / cassandra / db / DBManager . java 
 index 009ca06 . . 85c988a 100644 
 - - - a / src / java / org / apache / cassandra / db / DBManager . java 
 + + + b / src / java / org / apache / cassandra / db / DBManager . java 
 @ @ - 154 , 9 + 154 , 4 @ @ public class DBManager 
 } 
 return storageMetadata ; 
 } 
 - 
 - public static void main ( String [ ] args ) throws Throwable 
 - { 
 - DBManager . instance ( ) . start ( ) ; 
 - } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / FileStruct . java b / src / java / org / apache / cassandra / db / FileStruct . java 
 index e382720 . . fc7eec8 100644 
 - - - a / src / java / org / apache / cassandra / db / FileStruct . java 
 + + + b / src / java / org / apache / cassandra / db / FileStruct . java 
 @ @ - 27 , 10 + 27 , 13 @ @ import org . apache . cassandra . io . IFileReader ; 
 import org . apache . cassandra . io . SSTable ; 
 import org . apache . cassandra . io . Coordinate ; 
 import org . apache . cassandra . dht . IPartitioner ; 
 + import org . apache . log4j . Logger ; 
 
 
 public class FileStruct implements Comparable < FileStruct > , Iterator < String > 
 { 
 + private static Logger logger = Logger . getLogger ( FileStruct . class ) ; 
 + 
 private String key = null ; / / decorated ! 
 private boolean exhausted = false ; 
 private IFileReader reader ; 
 diff - - git a / src / java / org / apache / cassandra / db / MinorCompactionManager . java b / src / java / org / apache / cassandra / db / MinorCompactionManager . java 
 index c27bdf9 . . fda2c02 100644 
 - - - a / src / java / org / apache / cassandra / db / MinorCompactionManager . java 
 + + + b / src / java / org / apache / cassandra / db / MinorCompactionManager . java 
 @ @ - 72 , 12 + 72 , 12 @ @ class MinorCompactionManager implements IComponentShutdown 
 
 FileCompactor ( ColumnFamilyStore columnFamilyStore ) 
 { 
 - 	 columnFamilyStore _ = columnFamilyStore ; 
 + columnFamilyStore _ = columnFamilyStore ; 
 } 
 
 public void run ( ) 
 { 
 - logger _ . debug ( " Started compaction . . . " + columnFamilyStore _ . columnFamily _ ) ; 
 + logger _ . debug ( " Started compaction . . . " + columnFamilyStore _ . columnFamily _ ) ; 
 try 
 { 
 columnFamilyStore _ . doCompaction ( ) ; 
 @ @ - 86 , 7 + 86 , 7 @ @ class MinorCompactionManager implements IComponentShutdown 
 { 
 throw new RuntimeException ( e ) ; 
 } 
 - logger _ . debug ( " Finished compaction . . . " + columnFamilyStore _ . columnFamily _ ) ; 
 + logger _ . debug ( " Finished compaction . . . " + columnFamilyStore _ . columnFamily _ ) ; 
 } 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / io / Coordinate . java b / src / java / org / apache / cassandra / io / Coordinate . java 
 index 7a544b9 . . 23dff6e 100644 
 - - - a / src / java / org / apache / cassandra / io / Coordinate . java 
 + + + b / src / java / org / apache / cassandra / io / Coordinate . java 
 @ @ - 31 , 4 + 31 , 12 @ @ public class Coordinate 
 start _ = start ; 
 end _ = end ; 
 } 
 + 
 + public String toString ( ) 
 + { 
 + return " Coordinate ( " + 
 + " start _ = " + start _ + 
 + " , end _ = " + end _ + 
 + ' ) ' ; 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / SSTable . java b / src / java / org / apache / cassandra / io / SSTable . java 
 index 493ce94 . . dd5082f 100644 
 - - - a / src / java / org / apache / cassandra / io / SSTable . java 
 + + + b / src / java / org / apache / cassandra / io / SSTable . java 
 @ @ - 221 , 17 + 221 , 11 @ @ public class SSTable 
 } 
 
 File file = new File ( dataFile ) ; 
 - if ( file . exists ( ) ) 
 + assert file . exists ( ) ; 
 + / * delete the data file * / 
 + if ( ! file . delete ( ) ) 
 { 
 - / * delete the data file * / 
 - 	 	 	 if ( file . delete ( ) ) 
 - 	 	 	 { 	 	 	 
 - 	 	 	 logger _ . info ( " * * Deleted " + file . getName ( ) + " * * " ) ; 
 - 	 	 	 } 
 - 	 	 	 else 
 - 	 	 	 { 	 	 	 
 - 	 	 	 logger _ . error ( " Failed to delete " + file . getName ( ) ) ; 
 - 	 	 	 } 
 + logger _ . error ( " Failed to delete " + file . getName ( ) ) ; 
 } 
 } 
 
 @ @ - 600 , 9 + 594 , 8 @ @ public class SSTable 
 * @ throws IOException 
 * / 
 private void dumpBlockIndexes ( ) throws IOException 
 - { 	 
 - long position = dataWriter _ . getCurrentPosition ( ) ; 
 - firstBlockPosition _ = position ; 
 + { 
 + firstBlockPosition _ = dataWriter _ . getCurrentPosition ( ) ; 
 	 for ( SortedMap < String , BlockMetadata > block : blockIndexes _ ) 
 	 { 
 	 	 dumpBlockIndex ( block ) ; 
 @ @ - 660 , 12 + 653 , 16 @ @ public class SSTable 
 afterAppend ( decoratedKey , currentPosition , value . length ) ; 
 } 
 
 + / * 
 + TODO only the end _ part of the returned Coordinate is ever used . Apparently this code works , but it ' s definitely due for some cleanup 
 + since the code fooling about with start _ appears to be irrelevant . 
 + * / 
 public static Coordinate getCoordinates ( String decoratedKey , IFileReader dataReader , IPartitioner partitioner ) throws IOException 
 { 
 	 List < KeyPositionInfo > indexInfo = indexMetadataMap _ . get ( dataReader . getFileName ( ) ) ; 
 	 int size = ( indexInfo = = null ) ? 0 : indexInfo . size ( ) ; 
 	 long start = 0L ; 
 - 	 long end = dataReader . getEOF ( ) ; 
 + 	 long end ; 
 if ( size > 0 ) 
 { 
 int index = Collections . binarySearch ( indexInfo , new KeyPositionInfo ( decoratedKey , partitioner ) ) ;
