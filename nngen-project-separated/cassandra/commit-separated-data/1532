BLEU SCORE: 0.08116697886877475

TEST MSG: Expand upgrade testing for commitlog changes
GENERATED MSG: encapsulate commitlog file operations in CommitLogSegment

TEST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java <nl> index a59e70e . . 176f64b 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java <nl> @ @ - 281 , 7 + 281 , 7 @ @ public class CommitLogReplayer <nl> return ; <nl> if ( globalPosition . segment = = desc . id ) <nl> reader . seek ( globalPosition . position ) ; <nl> - replaySyncSection ( reader , - 1 , desc ) ; <nl> + replaySyncSection ( reader , ( int ) reader . getPositionLimit ( ) , desc ) ; <nl> return ; <nl> } <nl> <nl> diff - - git a / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750790 . log b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750790 . log <nl> new file mode 100644 <nl> index 0000000 . . 3301331 <nl> Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750790 . log differ <nl> diff - - git a / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750791 . log b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750791 . log <nl> new file mode 100644 <nl> index 0000000 . . 04314d6 <nl> Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750791 . log differ <nl> diff - - git a / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750792 . log b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750792 . log <nl> new file mode 100644 <nl> index 0000000 . . a9af9e4 <nl> Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750792 . log differ <nl> diff - - git a / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750793 . log b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750793 . log <nl> new file mode 100644 <nl> index 0000000 . . 3301331 <nl> Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750793 . log differ <nl> diff - - git a / test / data / legacy - commitlog / 2 . 0 / hash . txt b / test / data / legacy - commitlog / 2 . 0 / hash . txt <nl> new file mode 100644 <nl> index 0000000 . . 4bbec02 <nl> - - - / dev / null <nl> + + + b / test / data / legacy - commitlog / 2 . 0 / hash . txt <nl> @ @ - 0 , 0 + 1 , 3 @ @ <nl> + cfid = 4d331c44 - f018 - 302b - 91c2 - 2dcf94c4bfad <nl> + cells = 9724 <nl> + hash = - 682777064 <nl> diff - - git a / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069529 . log b / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069529 . log <nl> new file mode 100644 <nl> index 0000000 . . 60064ee <nl> Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069529 . log differ <nl> diff - - git a / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069530 . log b / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069530 . log <nl> new file mode 100644 <nl> index 0000000 . . fdf7071 <nl> Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069530 . log differ <nl> diff - - git a / test / data / legacy - commitlog / 2 . 1 / hash . txt b / test / data / legacy - commitlog / 2 . 1 / hash . txt <nl> new file mode 100644 <nl> index 0000000 . . f05cf97 <nl> - - - / dev / null <nl> + + + b / test / data / legacy - commitlog / 2 . 1 / hash . txt <nl> @ @ - 0 , 0 + 1 , 3 @ @ <nl> + cfid = 6c622920 - f980 - 11e4 - b8a0 - e7d448d5e26d <nl> + cells = 5165 <nl> + hash = - 1915888171 <nl> diff - - git a / test / long / org / apache / cassandra / db / commitlog / CommitLogStressTest . java b / test / long / org / apache / cassandra / db / commitlog / CommitLogStressTest . java <nl> index f5fd2cf . . 5897dec 100644 <nl> - - - a / test / long / org / apache / cassandra / db / commitlog / CommitLogStressTest . java <nl> + + + b / test / long / org / apache / cassandra / db / commitlog / CommitLogStressTest . java <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> package org . apache . cassandra . db . commitlog ; <nl> + <nl> / * <nl> * <nl> * Licensed to the Apache Software Foundation ( ASF ) under one <nl> @ @ - 20 , 7 + 21 , 6 @ @ package org . apache . cassandra . db . commitlog ; <nl> * <nl> * / <nl> <nl> - <nl> import java . io . DataInputStream ; <nl> import java . io . File ; <nl> import java . io . FileInputStream ; <nl> @ @ - 60 , 54 + 60 , 56 @ @ import org . apache . cassandra . io . util . FastByteArrayInputStream ; <nl> <nl> public class CommitLogStressTest <nl> { <nl> - <nl> public static ByteBuffer dataSource ; <nl> - <nl> - public static int NUM _ THREADS = 4 * Runtime . getRuntime ( ) . availableProcessors ( ) - 1 ; <nl> <nl> + public static int NUM _ THREADS = 4 * Runtime . getRuntime ( ) . availableProcessors ( ) - 1 ; <nl> public static int numCells = 1 ; <nl> - <nl> public static int cellSize = 1024 ; <nl> - <nl> public static int rateLimit = 0 ; <nl> - <nl> public static int runTimeMs = 10000 ; <nl> - <nl> + <nl> public static String location = DatabaseDescriptor . getCommitLogLocation ( ) + " / stress " ; <nl> - <nl> + <nl> public static int hash ( int hash , ByteBuffer bytes ) <nl> { <nl> int shift = 0 ; <nl> - for ( int i = 0 ; i < bytes . limit ( ) ; i + + ) { <nl> + for ( int i = 0 ; i < bytes . limit ( ) ; i + + ) <nl> + { <nl> hash + = ( bytes . get ( i ) & 0xFF ) < < shift ; <nl> shift = ( shift + 8 ) & 0x1F ; <nl> } <nl> return hash ; <nl> } <nl> - <nl> - public static void main ( String [ ] args ) throws Exception { <nl> - try { <nl> - if ( args . length > = 1 ) { <nl> + <nl> + public static void main ( String [ ] args ) throws Exception <nl> + { <nl> + try <nl> + { <nl> + if ( args . length > = 1 ) <nl> + { <nl> NUM _ THREADS = Integer . parseInt ( args [ 0 ] ) ; <nl> System . out . println ( " Setting num threads to : " + NUM _ THREADS ) ; <nl> } <nl> - <nl> - if ( args . length > = 2 ) { <nl> + <nl> + if ( args . length > = 2 ) <nl> + { <nl> numCells = Integer . parseInt ( args [ 1 ] ) ; <nl> System . out . println ( " Setting num cells to : " + numCells ) ; <nl> } <nl> - <nl> - if ( args . length > = 3 ) { <nl> + <nl> + if ( args . length > = 3 ) <nl> + { <nl> cellSize = Integer . parseInt ( args [ 1 ] ) ; <nl> System . out . println ( " Setting cell size to : " + cellSize + " be aware the source corpus may be small " ) ; <nl> } <nl> - <nl> - if ( args . length > = 4 ) { <nl> + <nl> + if ( args . length > = 4 ) <nl> + { <nl> rateLimit = Integer . parseInt ( args [ 1 ] ) ; <nl> System . out . println ( " Setting per thread rate limit to : " + rateLimit ) ; <nl> } <nl> initialize ( ) ; <nl> - <nl> + <nl> CommitLogStressTest tester = new CommitLogStressTest ( ) ; <nl> tester . testFixedSize ( ) ; <nl> } <nl> @ @ - 115 , 24 + 117 , 26 @ @ public class CommitLogStressTest <nl> { <nl> e . printStackTrace ( System . err ) ; <nl> } <nl> - finally { <nl> + finally <nl> + { <nl> System . exit ( 0 ) ; <nl> } <nl> } <nl> - <nl> + <nl> boolean failed = false ; <nl> volatile boolean stop = false ; <nl> boolean randomSize = false ; <nl> boolean discardedRun = false ; <nl> ReplayPosition discardedPos ; <nl> - <nl> + <nl> @ BeforeClass <nl> - static public void initialize ( ) throws FileNotFoundException , IOException , InterruptedException <nl> + static public void initialize ( ) throws IOException <nl> { <nl> try ( FileInputStream fis = new FileInputStream ( " CHANGES . txt " ) ) <nl> { <nl> - dataSource = ByteBuffer . allocateDirect ( ( int ) fis . getChannel ( ) . size ( ) ) ; <nl> - while ( dataSource . hasRemaining ( ) ) { <nl> + dataSource = ByteBuffer . allocateDirect ( ( int ) fis . getChannel ( ) . size ( ) ) ; <nl> + while ( dataSource . hasRemaining ( ) ) <nl> + { <nl> fis . getChannel ( ) . read ( dataSource ) ; <nl> } <nl> dataSource . flip ( ) ; <nl> @ @ - 141 , 7 + 145 , 7 @ @ public class CommitLogStressTest <nl> SchemaLoader . loadSchema ( ) ; <nl> SchemaLoader . schemaDefinition ( " " ) ; / / leave def . blank to maintain old behaviour <nl> } <nl> - <nl> + <nl> @ Before <nl> public void cleanDir ( ) <nl> { <nl> @ @ - 149 , 11 + 153 , 13 @ @ public class CommitLogStressTest <nl> if ( dir . isDirectory ( ) ) <nl> { <nl> File [ ] files = dir . listFiles ( ) ; <nl> - <nl> + <nl> for ( File f : files ) <nl> if ( ! f . delete ( ) ) <nl> Assert . fail ( " Failed to delete " + f ) ; <nl> - } else { <nl> + } <nl> + else <nl> + { <nl> dir . mkdir ( ) ; <nl> } <nl> } <nl> @ @ - 194 , 7 + 200 , 8 @ @ public class CommitLogStressTest <nl> null , <nl> new ParameterizedClass ( " LZ4Compressor " , null ) , <nl> new ParameterizedClass ( " SnappyCompressor " , null ) , <nl> - new ParameterizedClass ( " DeflateCompressor " , null ) } ) { <nl> + new ParameterizedClass ( " DeflateCompressor " , null ) } ) <nl> + { <nl> DatabaseDescriptor . setCommitLogCompression ( compressor ) ; <nl> for ( CommitLogSync sync : CommitLogSync . values ( ) ) <nl> { <nl> @ @ - 206 , 27 + 213 , 29 @ @ public class CommitLogStressTest <nl> assert ! failed ; <nl> } <nl> <nl> - public void testLog ( CommitLog commitLog ) throws IOException , InterruptedException { <nl> + public void testLog ( CommitLog commitLog ) throws IOException , InterruptedException <nl> + { <nl> System . out . format ( " \ nTesting commit log size % . 0fmb , compressor % s , sync % s % s % s \ n " , <nl> - mb ( DatabaseDescriptor . getCommitLogSegmentSize ( ) ) , <nl> - commitLog . compressor ! = null ? commitLog . compressor . getClass ( ) . getSimpleName ( ) : " none " , <nl> - commitLog . executor . getClass ( ) . getSimpleName ( ) , <nl> - randomSize ? " random size " : " " , <nl> - discardedRun ? " with discarded run " : " " ) ; <nl> + mb ( DatabaseDescriptor . getCommitLogSegmentSize ( ) ) , <nl> + commitLog . compressor ! = null ? commitLog . compressor . getClass ( ) . getSimpleName ( ) : " none " , <nl> + commitLog . executor . getClass ( ) . getSimpleName ( ) , <nl> + randomSize ? " random size " : " " , <nl> + discardedRun ? " with discarded run " : " " ) ; <nl> commitLog . allocator . enableReserveSegmentCreation ( ) ; <nl> - <nl> + <nl> final List < CommitlogExecutor > threads = new ArrayList < > ( ) ; <nl> ScheduledExecutorService scheduled = startThreads ( commitLog , threads ) ; <nl> <nl> discardedPos = ReplayPosition . NONE ; <nl> - if ( discardedRun ) { <nl> + if ( discardedRun ) <nl> + { <nl> / / Makes sure post - break data is not deleted , and that replayer correctly rejects earlier mutations . <nl> Thread . sleep ( runTimeMs / 3 ) ; <nl> stop = true ; <nl> scheduled . shutdown ( ) ; <nl> scheduled . awaitTermination ( 2 , TimeUnit . SECONDS ) ; <nl> <nl> - for ( CommitlogExecutor t : threads ) <nl> + for ( CommitlogExecutor t : threads ) <nl> { <nl> t . join ( ) ; <nl> if ( t . rp . compareTo ( discardedPos ) > 0 ) <nl> @ @ - 234 , 15 + 243 , 15 @ @ public class CommitLogStressTest <nl> } <nl> verifySizes ( commitLog ) ; <nl> <nl> - commitLog . discardCompletedSegments ( Schema . instance . getCFMetaData ( " Keyspace1 " , " Standard1 " ) . cfId , discardedPos ) ; <nl> + commitLog . discardCompletedSegments ( Schema . instance . getCFMetaData ( " Keyspace1 " , " Standard1 " ) . cfId , <nl> + discardedPos ) ; <nl> threads . clear ( ) ; <nl> System . out . format ( " Discarded at % s \ n " , discardedPos ) ; <nl> verifySizes ( commitLog ) ; <nl> - <nl> + <nl> scheduled = startThreads ( commitLog , threads ) ; <nl> } <nl> <nl> - <nl> Thread . sleep ( runTimeMs ) ; <nl> stop = true ; <nl> scheduled . shutdown ( ) ; <nl> @ @ - 250 , 16 + 259 , 18 @ @ public class CommitLogStressTest <nl> <nl> int hash = 0 ; <nl> int cells = 0 ; <nl> - for ( CommitlogExecutor t : threads ) { <nl> + for ( CommitlogExecutor t : threads ) <nl> + { <nl> t . join ( ) ; <nl> hash + = t . hash ; <nl> cells + = t . cells ; <nl> } <nl> verifySizes ( commitLog ) ; <nl> - <nl> + <nl> commitLog . shutdownBlocking ( ) ; <nl> <nl> - System . out . print ( " Stopped . Replaying . . . " ) ; System . out . flush ( ) ; <nl> + System . out . print ( " Stopped . Replaying . . . " ) ; <nl> + System . out . flush ( ) ; <nl> Replayer repl = new Replayer ( ) ; <nl> File [ ] files = new File ( location ) . listFiles ( ) ; <nl> repl . recover ( files ) ; <nl> @ @ - 267 , 12 + 278 , 16 @ @ public class CommitLogStressTest <nl> for ( File f : files ) <nl> if ( ! f . delete ( ) ) <nl> Assert . fail ( " Failed to delete " + f ) ; <nl> - <nl> + <nl> if ( hash = = repl . hash & & cells = = repl . cells ) <nl> System . out . println ( " Test success . " ) ; <nl> else <nl> { <nl> - System . out . format ( " Test failed . Cells % d expected % d , hash % d expected % d . \ n " , repl . cells , cells , repl . hash , hash ) ; <nl> + System . out . format ( " Test failed . Cells % d expected % d , hash % d expected % d . \ n " , <nl> + repl . cells , <nl> + cells , <nl> + repl . hash , <nl> + hash ) ; <nl> failed = true ; <nl> } <nl> } <nl> @ @ - 287 , 7 + 302 , 7 @ @ public class CommitLogStressTest <nl> commitLog . executor . requestExtraSync ( ) . awaitUninterruptibly ( ) ; <nl> / / Wait for any pending deletes or segment allocations to complete . <nl> commitLog . allocator . awaitManagementTasksCompletion ( ) ; <nl> - <nl> + <nl> long combinedSize = 0 ; <nl> for ( File f : new File ( commitLog . location ) . listFiles ( ) ) <nl> combinedSize + = f . length ( ) ; <nl> @ @ - 297 , 11 + 312 , 11 @ @ public class CommitLogStressTest <nl> Map < String , Double > ratios = commitLog . getActiveSegmentCompressionRatios ( ) ; <nl> Collection < CommitLogSegment > segments = commitLog . allocator . getActiveSegments ( ) ; <nl> <nl> - for ( CommitLogSegment segment : segments ) <nl> + for ( CommitLogSegment segment : segments ) <nl> { <nl> Assert . assertTrue ( logFileNames . remove ( segment . getName ( ) ) ) ; <nl> Double ratio = ratios . remove ( segment . getName ( ) ) ; <nl> - <nl> + <nl> Assert . assertEquals ( segment . logFile . length ( ) , segment . onDiskSize ( ) ) ; <nl> Assert . assertEquals ( segment . onDiskSize ( ) * 1 . 0 / segment . contentSize ( ) , ratio , 0 . 01 ) ; <nl> } <nl> @ @ - 312 , 35 + 327 , 47 @ @ public class CommitLogStressTest <nl> public ScheduledExecutorService startThreads ( final CommitLog commitLog , final List < CommitlogExecutor > threads ) <nl> { <nl> stop = false ; <nl> - for ( int ii = 0 ; ii < NUM _ THREADS ; ii + + ) { <nl> + for ( int ii = 0 ; ii < NUM _ THREADS ; ii + + ) <nl> + { <nl> final CommitlogExecutor t = new CommitlogExecutor ( commitLog , new Random ( ii ) ) ; <nl> threads . add ( t ) ; <nl> t . start ( ) ; <nl> } <nl> <nl> final long start = System . currentTimeMillis ( ) ; <nl> - Runnable printRunnable = new Runnable ( ) { <nl> + Runnable printRunnable = new Runnable ( ) <nl> + { <nl> long lastUpdate = 0 ; <nl> <nl> - public void run ( ) { <nl> - Runtime runtime = Runtime . getRuntime ( ) ; <nl> - long maxMemory = runtime . maxMemory ( ) ; <nl> - long allocatedMemory = runtime . totalMemory ( ) ; <nl> - long freeMemory = runtime . freeMemory ( ) ; <nl> - long temp = 0 ; <nl> - long sz = 0 ; <nl> - for ( CommitlogExecutor cle : threads ) { <nl> - temp + = cle . counter . get ( ) ; <nl> - sz + = cle . dataSize ; <nl> - } <nl> - double time = ( System . currentTimeMillis ( ) - start ) / 1000 . 0 ; <nl> - double avg = ( temp / time ) ; <nl> - System . out . println ( <nl> - String . format ( " second % d mem max % . 0fmb allocated % . 0fmb free % . 0fmb mutations % d since start % d avg % . 3f content % . 1fmb ondisk % . 1fmb transfer % . 3fmb " , <nl> - ( ( System . currentTimeMillis ( ) - start ) / 1000 ) , <nl> - mb ( maxMemory ) , mb ( allocatedMemory ) , mb ( freeMemory ) , ( temp - lastUpdate ) , lastUpdate , avg , <nl> - mb ( commitLog . getActiveContentSize ( ) ) , mb ( commitLog . getActiveOnDiskSize ( ) ) , mb ( sz / time ) ) ) ; <nl> - lastUpdate = temp ; <nl> + public void run ( ) <nl> + { <nl> + Runtime runtime = Runtime . getRuntime ( ) ; <nl> + long maxMemory = runtime . maxMemory ( ) ; <nl> + long allocatedMemory = runtime . totalMemory ( ) ; <nl> + long freeMemory = runtime . freeMemory ( ) ; <nl> + long temp = 0 ; <nl> + long sz = 0 ; <nl> + for ( CommitlogExecutor cle : threads ) <nl> + { <nl> + temp + = cle . counter . get ( ) ; <nl> + sz + = cle . dataSize ; <nl> + } <nl> + double time = ( System . currentTimeMillis ( ) - start ) / 1000 . 0 ; <nl> + double avg = ( temp / time ) ; <nl> + System . out <nl> + . println ( <nl> + String . format ( " second % d mem max % . 0fmb allocated % . 0fmb free % . 0fmb mutations % d since start % d avg % . 3f content % . 1fmb ondisk % . 1fmb transfer % . 3fmb " , <nl> + ( ( System . currentTimeMillis ( ) - start ) / 1000 ) , <nl> + mb ( maxMemory ) , <nl> + mb ( allocatedMemory ) , <nl> + mb ( freeMemory ) , <nl> + ( temp - lastUpdate ) , <nl> + lastUpdate , <nl> + avg , <nl> + mb ( commitLog . getActiveContentSize ( ) ) , <nl> + mb ( commitLog . getActiveOnDiskSize ( ) ) , <nl> + mb ( sz / time ) ) ) ; <nl> + lastUpdate = temp ; <nl> } <nl> } ; <nl> ScheduledExecutorService scheduled = Executors . newScheduledThreadPool ( 1 ) ; <nl> @ @ - 348 , 15 + 375 , 18 @ @ public class CommitLogStressTest <nl> return scheduled ; <nl> } <nl> <nl> - private static double mb ( long maxMemory ) { <nl> + private static double mb ( long maxMemory ) <nl> + { <nl> return maxMemory / ( 1024 . 0 * 1024 ) ; <nl> } <nl> <nl> - private static double mb ( double maxMemory ) { <nl> + private static double mb ( double maxMemory ) <nl> + { <nl> return maxMemory / ( 1024 * 1024 ) ; <nl> } <nl> <nl> - public static ByteBuffer randomBytes ( int quantity , Random tlr ) { <nl> + public static ByteBuffer randomBytes ( int quantity , Random tlr ) <nl> + { <nl> ByteBuffer slice = ByteBuffer . allocate ( quantity ) ; <nl> ByteBuffer source = dataSource . duplicate ( ) ; <nl> source . position ( tlr . nextInt ( source . capacity ( ) - quantity ) ) ; <nl> @ @ - 366 , 7 + 396 , 8 @ @ public class CommitLogStressTest <nl> return slice ; <nl> } <nl> <nl> - public class CommitlogExecutor extends Thread { <nl> + public class CommitlogExecutor extends Thread <nl> + { <nl> final AtomicLong counter = new AtomicLong ( ) ; <nl> int hash = 0 ; <nl> int cells = 0 ; <nl> @ @ - 382 , 21 + 413 , 23 @ @ public class CommitLogStressTest <nl> this . random = rand ; <nl> } <nl> <nl> - public void run ( ) { <nl> + public void run ( ) <nl> + { <nl> RateLimiter rl = rateLimit ! = 0 ? RateLimiter . create ( rateLimit ) : null ; <nl> final Random rand = random ! = null ? random : ThreadLocalRandom . current ( ) ; <nl> - while ( ! stop ) { <nl> + while ( ! stop ) <nl> + { <nl> if ( rl ! = null ) <nl> rl . acquire ( ) ; <nl> String ks = " Keyspace1 " ; <nl> ByteBuffer key = randomBytes ( 16 , rand ) ; <nl> Mutation mutation = new Mutation ( ks , key ) ; <nl> <nl> - for ( int ii = 0 ; ii < numCells ; ii + + ) { <nl> + for ( int ii = 0 ; ii < numCells ; ii + + ) <nl> + { <nl> int sz = randomSize ? rand . nextInt ( cellSize ) : cellSize ; <nl> ByteBuffer bytes = randomBytes ( sz , rand ) ; <nl> - mutation . add ( " Standard1 " , Util . cellname ( " name " + ii ) , bytes , <nl> - System . currentTimeMillis ( ) ) ; <nl> + mutation . add ( " Standard1 " , Util . cellname ( " name " + ii ) , bytes , System . currentTimeMillis ( ) ) ; <nl> hash = hash ( hash , bytes ) ; <nl> + + cells ; <nl> dataSize + = sz ; <nl> @ @ - 406 , 7 + 439 , 7 @ @ public class CommitLogStressTest <nl> } <nl> } <nl> } <nl> - <nl> + <nl> class Replayer extends CommitLogReplayer <nl> { <nl> Replayer ( ) <nl> @ @ - 420 , 20 + 453 , 22 @ @ public class CommitLogStressTest <nl> @ Override <nl> void replayMutation ( byte [ ] inputBuffer , int size , final long entryLocation , final CommitLogDescriptor desc ) <nl> { <nl> - if ( desc . id < discardedPos . segment ) { <nl> + if ( desc . id < discardedPos . segment ) <nl> + { <nl> System . out . format ( " Mutation from discarded segment , segment % d pos % d \ n " , desc . id , entryLocation ) ; <nl> return ; <nl> - } else if ( desc . id = = discardedPos . segment & & entryLocation < = discardedPos . position ) <nl> + } <nl> + else if ( desc . id = = discardedPos . segment & & entryLocation < = discardedPos . position ) <nl> / / Skip over this mutation . <nl> return ; <nl> - <nl> + <nl> FastByteArrayInputStream bufIn = new FastByteArrayInputStream ( inputBuffer , 0 , size ) ; <nl> Mutation mutation ; <nl> try <nl> { <nl> mutation = Mutation . serializer . deserialize ( new DataInputStream ( bufIn ) , <nl> - desc . getMessagingVersion ( ) , <nl> - ColumnSerializer . Flag . LOCAL ) ; <nl> + desc . getMessagingVersion ( ) , <nl> + ColumnSerializer . Flag . LOCAL ) ; <nl> } <nl> catch ( IOException e ) <nl> { <nl> @ @ - 441 , 8 + 476 , 10 @ @ public class CommitLogStressTest <nl> throw new AssertionError ( e ) ; <nl> } <nl> <nl> - for ( ColumnFamily cf : mutation . getColumnFamilies ( ) ) { <nl> - for ( Cell c : cf . getSortedColumns ( ) ) { <nl> + for ( ColumnFamily cf : mutation . getColumnFamilies ( ) ) <nl> + { <nl> + for ( Cell c : cf . getSortedColumns ( ) ) <nl> + { <nl> if ( new String ( c . name ( ) . toByteBuffer ( ) . array ( ) , StandardCharsets . UTF _ 8 ) . startsWith ( " name " ) ) <nl> { <nl> hash = hash ( hash , c . value ( ) ) ; <nl> @ @ - 451 , 6 + 488 , 6 @ @ public class CommitLogStressTest <nl> } <nl> } <nl> } <nl> - <nl> + <nl> } <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTest . java b / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTest . java <nl> new file mode 100644 <nl> index 0000000 . . 1655078 <nl> - - - / dev / null <nl> + + + b / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTest . java <nl> @ @ - 0 , 0 + 1 , 143 @ @ <nl> + package org . apache . cassandra . db . commitlog ; <nl> + <nl> + / * <nl> + * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , <nl> + * software distributed under the License is distributed on an <nl> + * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY <nl> + * KIND , either express or implied . See the License for the <nl> + * specific language governing permissions and limitations <nl> + * under the License . <nl> + * <nl> + * / <nl> + <nl> + import java . io . * ; <nl> + import java . nio . ByteBuffer ; <nl> + import java . nio . charset . StandardCharsets ; <nl> + import java . util . Properties ; <nl> + import java . util . UUID ; <nl> + <nl> + import junit . framework . Assert ; <nl> + <nl> + import com . google . common . base . Predicate ; <nl> + <nl> + import org . junit . BeforeClass ; <nl> + import org . junit . Test ; <nl> + <nl> + import org . apache . cassandra . SchemaLoader ; <nl> + import org . apache . cassandra . config . CFMetaData ; <nl> + import org . apache . cassandra . config . Schema ; <nl> + import org . apache . cassandra . db . Cell ; <nl> + import org . apache . cassandra . db . ColumnFamily ; <nl> + import org . apache . cassandra . db . Mutation ; <nl> + <nl> + public class CommitLogUpgradeTest <nl> + { <nl> + static final String DATA _ DIR = " test / data / legacy - commitlog / " ; <nl> + static final String PROPERTIES _ FILE = " hash . txt " ; <nl> + static final String CFID _ PROPERTY = " cfid " ; <nl> + static final String CELLS _ PROPERTY = " cells " ; <nl> + static final String HASH _ PROPERTY = " hash " ; <nl> + <nl> + static final String TABLE = " Standard1 " ; <nl> + static final String KEYSPACE = " Keyspace1 " ; <nl> + static final String CELLNAME = " name " ; <nl> + <nl> + @ Test <nl> + public void test20 ( ) throws Exception <nl> + { <nl> + testRestore ( DATA _ DIR + " 2 . 0 " ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void test21 ( ) throws Exception <nl> + { <nl> + testRestore ( DATA _ DIR + " 2 . 1 " ) ; <nl> + } <nl> + <nl> + @ BeforeClass <nl> + static public void initialize ( ) throws FileNotFoundException , IOException , InterruptedException <nl> + { <nl> + SchemaLoader . loadSchema ( ) ; <nl> + SchemaLoader . schemaDefinition ( " " ) ; <nl> + } <nl> + <nl> + public void testRestore ( String location ) throws IOException , InterruptedException <nl> + { <nl> + Properties prop = new Properties ( ) ; <nl> + prop . load ( new FileInputStream ( new File ( location + File . separatorChar + PROPERTIES _ FILE ) ) ) ; <nl> + int hash = Integer . parseInt ( prop . getProperty ( HASH _ PROPERTY ) ) ; <nl> + int cells = Integer . parseInt ( prop . getProperty ( CELLS _ PROPERTY ) ) ; <nl> + <nl> + String cfidString = prop . getProperty ( CFID _ PROPERTY ) ; <nl> + if ( cfidString ! = null ) <nl> + { <nl> + UUID cfid = UUID . fromString ( cfidString ) ; <nl> + if ( Schema . instance . getCF ( cfid ) = = null ) <nl> + { <nl> + CFMetaData cfm = Schema . instance . getCFMetaData ( KEYSPACE , TABLE ) ; <nl> + Schema . instance . purge ( cfm ) ; <nl> + Schema . instance . load ( cfm . copy ( cfid ) ) ; <nl> + } <nl> + } <nl> + <nl> + Hasher hasher = new Hasher ( ) ; <nl> + CommitLogTestReplayer replayer = new CommitLogTestReplayer ( hasher ) ; <nl> + File [ ] files = new File ( location ) . listFiles ( new FilenameFilter ( ) <nl> + { <nl> + @ Override <nl> + public boolean accept ( File dir , String name ) <nl> + { <nl> + return name . endsWith ( " . log " ) ; <nl> + } <nl> + } ) ; <nl> + replayer . recover ( files ) ; <nl> + <nl> + Assert . assertEquals ( cells , hasher . cells ) ; <nl> + Assert . assertEquals ( hash , hasher . hash ) ; <nl> + } <nl> + <nl> + public static int hash ( int hash , ByteBuffer bytes ) <nl> + { <nl> + int shift = 0 ; <nl> + for ( int i = 0 ; i < bytes . limit ( ) ; i + + ) <nl> + { <nl> + hash + = ( bytes . get ( i ) & 0xFF ) < < shift ; <nl> + shift = ( shift + 8 ) & 0x1F ; <nl> + } <nl> + return hash ; <nl> + } <nl> + <nl> + class Hasher implements Predicate < Mutation > <nl> + { <nl> + int hash = 0 ; <nl> + int cells = 0 ; <nl> + <nl> + @ Override <nl> + public boolean apply ( Mutation mutation ) <nl> + { <nl> + for ( ColumnFamily cf : mutation . getColumnFamilies ( ) ) <nl> + { <nl> + for ( Cell c : cf . getSortedColumns ( ) ) <nl> + { <nl> + if ( new String ( c . name ( ) . toByteBuffer ( ) . array ( ) , StandardCharsets . UTF _ 8 ) . startsWith ( CELLNAME ) ) <nl> + { <nl> + hash = hash ( hash , c . value ( ) ) ; <nl> + + + cells ; <nl> + } <nl> + } <nl> + } <nl> + return true ; <nl> + } <nl> + } <nl> + } <nl> diff - - git a / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTestMaker . java b / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTestMaker . java <nl> new file mode 100644 <nl> index 0000000 . . 7b07c8e <nl> - - - / dev / null <nl> + + + b / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTestMaker . java <nl> @ @ - 0 , 0 + 1 , 250 @ @ <nl> + package org . apache . cassandra . db . commitlog ; <nl> + <nl> + / * <nl> + * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , <nl> + * software distributed under the License is distributed on an <nl> + * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY <nl> + * KIND , either express or implied . See the License for the <nl> + * specific language governing permissions and limitations <nl> + * under the License . <nl> + * <nl> + * / <nl> + <nl> + import java . io . * ; <nl> + import java . nio . ByteBuffer ; <nl> + import java . util . ArrayList ; <nl> + import java . util . List ; <nl> + import java . util . Properties ; <nl> + import java . util . concurrent . Executors ; <nl> + import java . util . concurrent . ScheduledExecutorService ; <nl> + import java . util . concurrent . ThreadLocalRandom ; <nl> + import java . util . concurrent . TimeUnit ; <nl> + import java . util . concurrent . atomic . AtomicLong ; <nl> + <nl> + import com . google . common . util . concurrent . RateLimiter ; <nl> + <nl> + import org . apache . cassandra . SchemaLoader ; <nl> + import org . apache . cassandra . Util ; <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + import org . apache . cassandra . config . Schema ; <nl> + import org . apache . cassandra . db . Mutation ; <nl> + import org . apache . cassandra . exceptions . ConfigurationException ; <nl> + import org . apache . cassandra . io . util . FileUtils ; <nl> + import org . apache . cassandra . utils . FBUtilities ; <nl> + <nl> + import static org . apache . cassandra . db . commitlog . CommitLogUpgradeTest . * ; <nl> + <nl> + public class CommitLogUpgradeTestMaker <nl> + { <nl> + public static ByteBuffer dataSource ; <nl> + <nl> + private static int NUM _ THREADS = 4 * Runtime . getRuntime ( ) . availableProcessors ( ) - 1 ; <nl> + public static int numCells = 1 ; <nl> + public static int cellSize = 256 ; <nl> + public static int rateLimit = 0 ; <nl> + public static int runTimeMs = 1000 ; <nl> + <nl> + public static void main ( String [ ] args ) throws Exception <nl> + { <nl> + try <nl> + { <nl> + initialize ( ) ; <nl> + <nl> + CommitLogUpgradeTestMaker tester = new CommitLogUpgradeTestMaker ( ) ; <nl> + tester . makeLog ( ) ; <nl> + } <nl> + catch ( Throwable e ) <nl> + { <nl> + e . printStackTrace ( System . err ) ; <nl> + } <nl> + finally <nl> + { <nl> + System . exit ( 0 ) ; <nl> + } <nl> + } <nl> + <nl> + volatile boolean stop = false ; <nl> + boolean randomSize = true ; <nl> + <nl> + static public void initialize ( ) throws IOException , ConfigurationException <nl> + { <nl> + try ( FileInputStream fis = new FileInputStream ( " CHANGES . txt " ) ) <nl> + { <nl> + dataSource = ByteBuffer . allocateDirect ( ( int ) fis . getChannel ( ) . size ( ) ) ; <nl> + while ( dataSource . hasRemaining ( ) ) <nl> + { <nl> + fis . getChannel ( ) . read ( dataSource ) ; <nl> + } <nl> + dataSource . flip ( ) ; <nl> + } <nl> + <nl> + SchemaLoader . loadSchema ( ) ; <nl> + SchemaLoader . schemaDefinition ( " " ) ; <nl> + } <nl> + <nl> + public void makeLog ( ) throws IOException , InterruptedException <nl> + { <nl> + CommitLog commitLog = CommitLog . instance ; <nl> + System . out . format ( " \ nUsing commit log size % dmb , compressor % s , sync % s % s \ n " , <nl> + mb ( DatabaseDescriptor . getCommitLogSegmentSize ( ) ) , <nl> + commitLog . compressor ! = null ? commitLog . compressor . getClass ( ) . getSimpleName ( ) : " none " , <nl> + commitLog . executor . getClass ( ) . getSimpleName ( ) , <nl> + randomSize ? " random size " : " " ) ; <nl> + final List < CommitlogExecutor > threads = new ArrayList < > ( ) ; <nl> + ScheduledExecutorService scheduled = startThreads ( commitLog , threads ) ; <nl> + <nl> + Thread . sleep ( runTimeMs ) ; <nl> + stop = true ; <nl> + scheduled . shutdown ( ) ; <nl> + scheduled . awaitTermination ( 2 , TimeUnit . SECONDS ) ; <nl> + <nl> + int hash = 0 ; <nl> + int cells = 0 ; <nl> + for ( CommitlogExecutor t : threads ) <nl> + { <nl> + t . join ( ) ; <nl> + hash + = t . hash ; <nl> + cells + = t . cells ; <nl> + } <nl> + commitLog . shutdownBlocking ( ) ; <nl> + <nl> + File dataDir = new File ( CommitLogUpgradeTest . DATA _ DIR + FBUtilities . getReleaseVersionString ( ) ) ; <nl> + System . out . format ( " Data will be stored in % s \ n " , dataDir ) ; <nl> + if ( dataDir . exists ( ) ) <nl> + FileUtils . deleteRecursive ( dataDir ) ; <nl> + <nl> + dataDir . mkdirs ( ) ; <nl> + for ( File f : new File ( DatabaseDescriptor . getCommitLogLocation ( ) ) . listFiles ( ) ) <nl> + FileUtils . createHardLink ( f , new File ( dataDir , f . getName ( ) ) ) ; <nl> + <nl> + Properties prop = new Properties ( ) ; <nl> + prop . setProperty ( CFID _ PROPERTY , Schema . instance . getId ( KEYSPACE , TABLE ) . toString ( ) ) ; <nl> + prop . setProperty ( CELLS _ PROPERTY , Integer . toString ( cells ) ) ; <nl> + prop . setProperty ( HASH _ PROPERTY , Integer . toString ( hash ) ) ; <nl> + prop . store ( new FileOutputStream ( new File ( dataDir , PROPERTIES _ FILE ) ) , <nl> + " CommitLog upgrade test , version " + FBUtilities . getReleaseVersionString ( ) ) ; <nl> + System . out . println ( " Done " ) ; <nl> + } <nl> + <nl> + public ScheduledExecutorService startThreads ( CommitLog commitLog , final List < CommitlogExecutor > threads ) <nl> + { <nl> + stop = false ; <nl> + for ( int ii = 0 ; ii < NUM _ THREADS ; ii + + ) <nl> + { <nl> + final CommitlogExecutor t = new CommitlogExecutor ( commitLog ) ; <nl> + threads . add ( t ) ; <nl> + t . start ( ) ; <nl> + } <nl> + <nl> + final long start = System . currentTimeMillis ( ) ; <nl> + Runnable printRunnable = new Runnable ( ) <nl> + { <nl> + long lastUpdate = 0 ; <nl> + <nl> + public void run ( ) <nl> + { <nl> + Runtime runtime = Runtime . getRuntime ( ) ; <nl> + long maxMemory = mb ( runtime . maxMemory ( ) ) ; <nl> + long allocatedMemory = mb ( runtime . totalMemory ( ) ) ; <nl> + long freeMemory = mb ( runtime . freeMemory ( ) ) ; <nl> + long temp = 0 ; <nl> + long sz = 0 ; <nl> + for ( CommitlogExecutor cle : threads ) <nl> + { <nl> + temp + = cle . counter . get ( ) ; <nl> + sz + = cle . dataSize ; <nl> + } <nl> + double time = ( System . currentTimeMillis ( ) - start ) / 1000 . 0 ; <nl> + double avg = ( temp / time ) ; <nl> + System . out . println ( <nl> + String . format ( " second % d mem max % dmb allocated % dmb free % dmb mutations % d since start % d avg % . 3f transfer % . 3fmb " , <nl> + ( ( System . currentTimeMillis ( ) - start ) / 1000 ) , <nl> + maxMemory , <nl> + allocatedMemory , <nl> + freeMemory , <nl> + ( temp - lastUpdate ) , <nl> + lastUpdate , <nl> + avg , <nl> + mb ( sz / time ) ) ) ; <nl> + lastUpdate = temp ; <nl> + } <nl> + } ; <nl> + ScheduledExecutorService scheduled = Executors . newScheduledThreadPool ( 1 ) ; <nl> + scheduled . scheduleAtFixedRate ( printRunnable , 1 , 1 , TimeUnit . SECONDS ) ; <nl> + return scheduled ; <nl> + } <nl> + <nl> + private static long mb ( long maxMemory ) <nl> + { <nl> + return maxMemory / ( 1024 * 1024 ) ; <nl> + } <nl> + <nl> + private static double mb ( double maxMemory ) <nl> + { <nl> + return maxMemory / ( 1024 * 1024 ) ; <nl> + } <nl> + <nl> + public static ByteBuffer randomBytes ( int quantity , ThreadLocalRandom tlr ) <nl> + { <nl> + ByteBuffer slice = ByteBuffer . allocate ( quantity ) ; <nl> + ByteBuffer source = dataSource . duplicate ( ) ; <nl> + source . position ( tlr . nextInt ( source . capacity ( ) - quantity ) ) ; <nl> + source . limit ( source . position ( ) + quantity ) ; <nl> + slice . put ( source ) ; <nl> + slice . flip ( ) ; <nl> + return slice ; <nl> + } <nl> + <nl> + public class CommitlogExecutor extends Thread <nl> + { <nl> + final AtomicLong counter = new AtomicLong ( ) ; <nl> + int hash = 0 ; <nl> + int cells = 0 ; <nl> + int dataSize = 0 ; <nl> + final CommitLog commitLog ; <nl> + <nl> + volatile ReplayPosition rp ; <nl> + <nl> + public CommitlogExecutor ( CommitLog commitLog ) <nl> + { <nl> + this . commitLog = commitLog ; <nl> + } <nl> + <nl> + public void run ( ) <nl> + { <nl> + RateLimiter rl = rateLimit ! = 0 ? RateLimiter . create ( rateLimit ) : null ; <nl> + final ThreadLocalRandom tlr = ThreadLocalRandom . current ( ) ; <nl> + while ( ! stop ) <nl> + { <nl> + if ( rl ! = null ) <nl> + rl . acquire ( ) ; <nl> + String ks = KEYSPACE ; <nl> + ByteBuffer key = randomBytes ( 16 , tlr ) ; <nl> + Mutation mutation = new Mutation ( ks , key ) ; <nl> + <nl> + for ( int ii = 0 ; ii < numCells ; ii + + ) <nl> + { <nl> + int sz = randomSize ? tlr . nextInt ( cellSize ) : cellSize ; <nl> + ByteBuffer bytes = randomBytes ( sz , tlr ) ; <nl> + mutation . add ( TABLE , Util . cellname ( CELLNAME + ii ) , bytes , System . currentTimeMillis ( ) ) ; <nl> + hash = hash ( hash , bytes ) ; <nl> + + + cells ; <nl> + dataSize + = sz ; <nl> + } <nl> + rp = commitLog . add ( mutation ) ; <nl> + counter . incrementAndGet ( ) ; <nl> + } <nl> + } <nl> + } <nl> + }
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> index d3b6a84 . . 8ef0b0e 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> @ @ - 40 , 6 + 40 , 7 @ @ import org . apache . log4j . Logger ; <nl> <nl> import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . db . commitlog . CommitLog ; <nl> + import org . apache . cassandra . db . commitlog . CommitLogSegment ; <nl> import org . apache . cassandra . dht . AbstractBounds ; <nl> import org . apache . cassandra . dht . Bounds ; <nl> import org . apache . cassandra . dht . Range ; <nl> @ @ - 366 , 14 + 367 , 14 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> Table . flusherLock . writeLock ( ) . lock ( ) ; <nl> try <nl> { <nl> - final CommitLog . CommitLogContext ctx = CommitLog . instance ( ) . getContext ( ) ; / / this is harmless if ! writeCommitLog <nl> - <nl> if ( oldMemtable . isFrozen ( ) ) <nl> { <nl> return null ; <nl> } <nl> - logger _ . info ( columnFamily _ + " has reached its threshold ; switching in a fresh Memtable " ) ; <nl> oldMemtable . freeze ( ) ; <nl> + <nl> + final CommitLogSegment . CommitLogContext ctx = writeCommitLog ? CommitLog . instance ( ) . getContext ( ) : null ; <nl> + logger _ . info ( columnFamily _ + " has reached its threshold ; switching in a fresh Memtable at " + ctx ) ; <nl> final Condition condition = submitFlush ( oldMemtable ) ; <nl> memtable _ = new Memtable ( table _ , columnFamily _ ) ; <nl> / / a second executor that makes sure the onMemtableFlushes get called in the right order , <nl> @ @ - 387 , 7 + 388 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> { <nl> / / if we ' re not writing to the commit log , we are replaying the log , so marking <nl> / / the log header with " you can discard anything written before the context " is not valid <nl> - onMemtableFlush ( ctx ) ; <nl> + CommitLog . instance ( ) . discardCompletedSegments ( table _ , columnFamily _ , ctx ) ; <nl> } <nl> } <nl> } ) ; <nl> @ @ - 533 , 19 + 534 , 6 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> } <nl> <nl> / * <nl> - * This method is called when the Memtable is frozen and ready to be flushed <nl> - * to disk . This method informs the CommitLog that a particular ColumnFamily <nl> - * is being flushed to disk . <nl> - * / <nl> - void onMemtableFlush ( CommitLog . CommitLogContext cLogCtx ) throws IOException <nl> - { <nl> - if ( cLogCtx . isValidContext ( ) ) <nl> - { <nl> - CommitLog . instance ( ) . onMemtableFlush ( table _ , columnFamily _ , cLogCtx ) ; <nl> - } <nl> - } <nl> - <nl> - / * <nl> * Called after the Memtable flushes its in - memory data , or we add a file <nl> * via bootstrap . This information is <nl> * cached in the ColumnFamilyStore . This is useful for reads because the <nl> diff - - git a / src / java / org / apache / cassandra / db / RecoveryManager . java b / src / java / org / apache / cassandra / db / RecoveryManager . java <nl> index 1a1e2e1 . . 115a521 100644 <nl> - - - a / src / java / org / apache / cassandra / db / RecoveryManager . java <nl> + + + b / src / java / org / apache / cassandra / db / RecoveryManager . java <nl> @ @ - 53 , 7 + 53 , 7 @ @ public class RecoveryManager <nl> <nl> Arrays . sort ( files , new FileUtils . FileComparator ( ) ) ; <nl> logger _ . info ( " Replaying " + StringUtils . join ( files , " , " ) ) ; <nl> - CommitLog . instance ( ) . recover ( files ) ; <nl> + CommitLog . recover ( files ) ; <nl> FileUtils . delete ( files ) ; <nl> logger _ . info ( " Log replay complete " ) ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / Table . java b / src / java / org / apache / cassandra / db / Table . java <nl> index f17d1bf . . 652dea5 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Table . java <nl> + + + b / src / java / org / apache / cassandra / db / Table . java <nl> @ @ - 28 , 6 + 28 , 7 @ @ import com . google . common . base . Function ; <nl> import com . google . common . collect . Iterables ; <nl> import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . db . commitlog . CommitLog ; <nl> + import org . apache . cassandra . db . commitlog . CommitLogSegment ; <nl> import org . apache . cassandra . dht . Range ; <nl> import org . apache . cassandra . io . SSTableDeletingReference ; <nl> import org . apache . cassandra . io . SSTableReader ; <nl> @ @ - 396 , 7 + 397 , 7 @ @ public class Table <nl> { <nl> if ( writeCommitLog ) <nl> { <nl> - Future < CommitLog . CommitLogContext > future = CommitLog . instance ( ) . add ( mutation , serializedMutation ) ; <nl> + Future < CommitLogSegment . CommitLogContext > future = CommitLog . instance ( ) . add ( mutation , serializedMutation ) ; <nl> if ( waitForCommitLog ) <nl> { <nl> try <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLog . java b / src / java / org / apache / cassandra / db / commitlog / CommitLog . java <nl> index 0aac0ae . . 63ab353 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLog . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLog . java <nl> @ @ - 23 , 9 + 23 , 7 @ @ import org . apache . cassandra . db . ColumnFamily ; <nl> import org . apache . cassandra . db . RowMutation ; <nl> import org . apache . cassandra . db . Table ; <nl> import org . apache . cassandra . io . util . BufferedRandomAccessFile ; <nl> - import org . apache . cassandra . io . util . DataOutputBuffer ; <nl> import org . apache . cassandra . io . DeletionService ; <nl> - import org . apache . cassandra . utils . FBUtilities ; <nl> import org . apache . cassandra . utils . WrappedRunnable ; <nl> import org . apache . cassandra . concurrent . StageManager ; <nl> <nl> @ @ - 68 , 15 + 66 , 12 @ @ import java . util . concurrent . Future ; <nl> * inherited the old ' s dirty bitflags , getting a zero for any given bit in the anding <nl> * means that either the CF was clean in the old CL or it has been flushed since the <nl> * switch in the new . ) <nl> - * <nl> - * The CommitLog class itself is " mostly a singleton . " open ( ) always returns one <nl> - * instance , but log replay will bypass that . <nl> * / <nl> public class CommitLog <nl> { <nl> private static volatile int SEGMENT _ SIZE = 128 * 1024 * 1024 ; / / roll after log gets this big <nl> + <nl> private static final Logger logger = Logger . getLogger ( CommitLog . class ) ; <nl> - private static final Map < String , CommitLogHeader > clHeaders = new HashMap < String , CommitLogHeader > ( ) ; <nl> <nl> public static CommitLog instance ( ) <nl> { <nl> @ @ - 88 , 81 + 83 , 21 @ @ public class CommitLog <nl> public static final CommitLog instance = new CommitLog ( ) ; <nl> } <nl> <nl> - public static class CommitLogContext <nl> - { <nl> - / * Commit Log associated with this operation * / <nl> - public final String file ; <nl> - / * Offset within the Commit Log where this row as added * / <nl> - public final long position ; <nl> - <nl> - public CommitLogContext ( String file , long position ) <nl> - { <nl> - this . file = file ; <nl> - this . position = position ; <nl> - } <nl> - <nl> - public boolean isValidContext ( ) <nl> - { <nl> - return ( position ! = - 1L ) ; <nl> - } <nl> - <nl> - @ Override <nl> - public String toString ( ) <nl> - { <nl> - return " CommitLogContext ( " + <nl> - " file = ' " + file + ' \ ' ' + <nl> - " , position = " + position + <nl> - ' ) ' ; <nl> - } <nl> - } <nl> - <nl> - public static class CommitLogFileComparator implements Comparator < String > <nl> - { <nl> - public int compare ( String f , String f2 ) <nl> - { <nl> - return ( int ) ( getCreationTime ( f ) - getCreationTime ( f2 ) ) ; <nl> - } <nl> - } <nl> + private final Deque < CommitLogSegment > segments = new ArrayDeque < CommitLogSegment > ( ) ; <nl> <nl> public static void setSegmentSize ( int size ) <nl> { <nl> SEGMENT _ SIZE = size ; <nl> } <nl> <nl> - public static int getSegmentCount ( ) <nl> + public int getSegmentCount ( ) <nl> { <nl> - return clHeaders . size ( ) ; <nl> - } <nl> - <nl> - static long getCreationTime ( String file ) <nl> - { <nl> - String [ ] entries = FBUtilities . strip ( file , " - . " ) ; <nl> - return Long . parseLong ( entries [ entries . length - 2 ] ) ; <nl> - } <nl> - <nl> - private static BufferedRandomAccessFile createWriter ( String file ) throws IOException <nl> - { <nl> - return new BufferedRandomAccessFile ( file , " rw " ) ; <nl> + return segments . size ( ) ; <nl> } <nl> <nl> - / * Current commit log file * / <nl> - private String logFile ; <nl> - / * header for current commit log * / <nl> - private CommitLogHeader clHeader ; <nl> - private BufferedRandomAccessFile logWriter ; <nl> private final ExecutorService executor = new CommitLogExecutorService ( ) ; <nl> <nl> - / * <nl> - * Generates a file name of the format CommitLog - < table > - < timestamp > . log in the <nl> - * directory specified by the Database Descriptor . <nl> - * / <nl> - private void setNextFileName ( ) <nl> - { <nl> - logFile = DatabaseDescriptor . getLogFileLocation ( ) + File . separator + <nl> - " CommitLog - " + System . currentTimeMillis ( ) + " . log " ; <nl> - } <nl> - <nl> - / * <nl> + / * * <nl> * param @ table - name of table for which we are maintaining <nl> * this commit log . <nl> * param @ recoverymode - is commit log being instantiated in <nl> @ @ - 170 , 17 + 105 , 11 @ @ public class CommitLog <nl> * / <nl> private CommitLog ( ) <nl> { <nl> - setNextFileName ( ) ; <nl> - try <nl> - { <nl> - logWriter = CommitLog . createWriter ( logFile ) ; <nl> - writeCommitLogHeader ( ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - throw new IOError ( e ) ; <nl> - } <nl> - <nl> + / / all old segments are recovered and deleted before CommitLog is instantiated . <nl> + / / All we need to do is create a new one . <nl> + int cfSize = Table . TableMetadata . getColumnFamilyCount ( ) ; <nl> + segments . add ( new CommitLogSegment ( cfSize ) ) ; <nl> + <nl> if ( DatabaseDescriptor . getCommitLogSync ( ) = = DatabaseDescriptor . CommitLogSync . periodic ) <nl> { <nl> final Runnable syncer = new WrappedRunnable ( ) <nl> @ @ - 216 , 50 + 145 , 7 @ @ public class CommitLog <nl> } <nl> } <nl> <nl> - String getLogFile ( ) <nl> - { <nl> - return logFile ; <nl> - } <nl> - <nl> - private CommitLogHeader readCommitLogHeader ( BufferedRandomAccessFile logReader ) throws IOException <nl> - { <nl> - int size = ( int ) logReader . readLong ( ) ; <nl> - byte [ ] bytes = new byte [ size ] ; <nl> - logReader . read ( bytes ) ; <nl> - ByteArrayInputStream byteStream = new ByteArrayInputStream ( bytes ) ; <nl> - return CommitLogHeader . serializer ( ) . deserialize ( new DataInputStream ( byteStream ) ) ; <nl> - } <nl> - <nl> - / * <nl> - * This is invoked on startup via the ctor . It basically <nl> - * writes a header with all bits set to zero . <nl> - * / <nl> - private void writeCommitLogHeader ( ) throws IOException <nl> - { <nl> - int cfSize = Table . TableMetadata . getColumnFamilyCount ( ) ; <nl> - clHeader = new CommitLogHeader ( cfSize ) ; <nl> - writeCommitLogHeader ( logWriter , clHeader . toByteArray ( ) ) ; <nl> - } <nl> - <nl> - / * * writes header at the beginning of the file , then seeks back to current position * / <nl> - private void seekAndWriteCommitLogHeader ( byte [ ] bytes ) throws IOException <nl> - { <nl> - long currentPos = logWriter . getFilePointer ( ) ; <nl> - logWriter . seek ( 0 ) ; <nl> - <nl> - writeCommitLogHeader ( logWriter , bytes ) ; <nl> - <nl> - logWriter . seek ( currentPos ) ; <nl> - } <nl> - <nl> - private static void writeCommitLogHeader ( BufferedRandomAccessFile logWriter , byte [ ] bytes ) throws IOException <nl> - { <nl> - logWriter . writeLong ( bytes . length ) ; <nl> - logWriter . write ( bytes ) ; <nl> - logWriter . sync ( ) ; <nl> - } <nl> - <nl> - public void recover ( File [ ] clogs ) throws IOException <nl> + public static void recover ( File [ ] clogs ) throws IOException <nl> { <nl> Set < Table > tablesRecovered = new HashSet < Table > ( ) ; <nl> assert StageManager . getStage ( StageManager . MUTATION _ STAGE ) . getCompletedTaskCount ( ) = = 0 ; <nl> @ @ - 268 , 7 + 154 , 7 @ @ public class CommitLog <nl> { <nl> int bufferSize = ( int ) Math . min ( file . length ( ) , 32 * 1024 * 1024 ) ; <nl> BufferedRandomAccessFile reader = new BufferedRandomAccessFile ( file . getAbsolutePath ( ) , " r " , bufferSize ) ; <nl> - final CommitLogHeader clHeader = readCommitLogHeader ( reader ) ; <nl> + final CommitLogHeader clHeader = CommitLogHeader . readCommitLogHeader ( reader ) ; <nl> / * seek to the lowest position where any CF has non - flushed data * / <nl> int lowPos = CommitLogHeader . getLowestPosition ( clHeader ) ; <nl> if ( lowPos = = 0 ) <nl> @ @ - 357 , 13 + 243 , 13 @ @ public class CommitLog <nl> } <nl> } <nl> <nl> - / / flush replayed tables , allowing commitlog segments to be removed <nl> + / / flush replayed tables <nl> List < Future < ? > > futures = new ArrayList < Future < ? > > ( ) ; <nl> for ( Table table : tablesRecovered ) <nl> { <nl> futures . addAll ( table . flush ( ) ) ; <nl> } <nl> - / / wait for flushes to finish before continuing with startup <nl> + / / wait for flushes to finish <nl> for ( Future < ? > future : futures ) <nl> { <nl> try <nl> @ @ - 377 , 31 + 263 , 18 @ @ public class CommitLog <nl> } <nl> } <nl> <nl> - / * <nl> - * Update the header of the commit log if a new column family <nl> - * is encountered for the first time . <nl> - * / <nl> - private void maybeUpdateHeader ( RowMutation rm ) throws IOException <nl> + private CommitLogSegment currentSegment ( ) <nl> { <nl> - Table table = Table . open ( rm . getTable ( ) ) ; <nl> - for ( ColumnFamily columnFamily : rm . getColumnFamilies ( ) ) <nl> - { <nl> - int id = table . getColumnFamilyId ( columnFamily . name ( ) ) ; <nl> - if ( ! clHeader . isDirty ( id ) ) <nl> - { <nl> - clHeader . turnOn ( id , logWriter . getFilePointer ( ) ) ; <nl> - seekAndWriteCommitLogHeader ( clHeader . toByteArray ( ) ) ; <nl> - } <nl> - } <nl> + return segments . getLast ( ) ; <nl> } <nl> <nl> - public CommitLogContext getContext ( ) throws IOException <nl> + public CommitLogSegment . CommitLogContext getContext ( ) throws IOException <nl> { <nl> - Callable < CommitLogContext > task = new Callable < CommitLogContext > ( ) <nl> + Callable < CommitLogSegment . CommitLogContext > task = new Callable < CommitLogSegment . CommitLogContext > ( ) <nl> { <nl> - public CommitLogContext call ( ) throws Exception <nl> + public CommitLogSegment . CommitLogContext call ( ) throws Exception <nl> { <nl> - return new CommitLogContext ( logFile , logWriter . getFilePointer ( ) ) ; <nl> + return currentSegment ( ) . getContext ( ) ; <nl> } <nl> } ; <nl> try <nl> @ @ - 424 , 9 + 297 , 9 @ @ public class CommitLog <nl> * of any problems . This way we can assume that the subsequent commit log <nl> * entry will override the garbage left over by the previous write . <nl> * / <nl> - public Future < CommitLogContext > add ( RowMutation rowMutation , Object serializedRow ) throws IOException <nl> + public Future < CommitLogSegment . CommitLogContext > add ( RowMutation rowMutation , Object serializedRow ) throws IOException <nl> { <nl> - Callable < CommitLogContext > task = new LogRecordAdder ( rowMutation , serializedRow ) ; <nl> + Callable < CommitLogSegment . CommitLogContext > task = new LogRecordAdder ( rowMutation , serializedRow ) ; <nl> return executor . submit ( task ) ; <nl> } <nl> <nl> @ @ - 436 , 15 + 309 , 14 @ @ public class CommitLog <nl> * The bit flag associated with this column family is set in the <nl> * header and this is used to decide if the log file can be deleted . <nl> * / <nl> - public void onMemtableFlush ( final String tableName , final String cf , final CommitLog . CommitLogContext cLogCtx ) throws IOException <nl> + public void discardCompletedSegments ( final String tableName , final String cf , final CommitLogSegment . CommitLogContext context ) throws IOException <nl> { <nl> Callable task = new Callable ( ) <nl> { <nl> public Object call ( ) throws IOException <nl> { <nl> - Table table = Table . open ( tableName ) ; <nl> - int id = table . getColumnFamilyId ( cf ) ; <nl> - discardCompletedSegments ( cLogCtx , id ) ; <nl> + int id = Table . open ( tableName ) . getColumnFamilyId ( cf ) ; <nl> + discardCompletedSegmentsInternal ( context , id ) ; <nl> return null ; <nl> } <nl> } ; <nl> @ @ - 462 , 41 + 334 , 23 @ @ public class CommitLog <nl> } <nl> } <nl> <nl> - / * <nl> - * Delete log segments whose contents have been turned into SSTables . <nl> + / * * <nl> + * Delete log segments whose contents have been turned into SSTables . NOT threadsafe . <nl> * <nl> - * param @ cLogCtx The commitLog context . <nl> + * param @ context The commitLog context . <nl> * param @ id id of the columnFamily being flushed to disk . <nl> * <nl> * / <nl> - private void discardCompletedSegments ( CommitLog . CommitLogContext cLogCtx , int id ) throws IOException <nl> + private void discardCompletedSegmentsInternal ( CommitLogSegment . CommitLogContext context , int id ) throws IOException <nl> { <nl> if ( logger . isDebugEnabled ( ) ) <nl> - logger . debug ( " discard completed log segments for " + cLogCtx + " , column family " + id + " . CFIDs are " + Table . TableMetadata . getColumnFamilyIDString ( ) ) ; <nl> - / * retrieve the commit log header associated with the file in the context * / <nl> - if ( clHeaders . get ( cLogCtx . file ) = = null ) <nl> - { <nl> - if ( logFile . equals ( cLogCtx . file ) ) <nl> - { <nl> - / * this means we are dealing with the current commit log . * / <nl> - clHeaders . put ( cLogCtx . file , clHeader ) ; <nl> - } <nl> - else <nl> - { <nl> - logger . error ( " Unknown commitlog file " + cLogCtx . file ) ; <nl> - return ; <nl> - } <nl> - } <nl> + logger . debug ( " discard completed log segments for " + context + " , column family " + id + " . CFIDs are " + Table . TableMetadata . getColumnFamilyIDString ( ) ) ; <nl> <nl> / * <nl> * log replay assumes that we only have to look at entries past the last <nl> * flush position , so verify that this flush happens after the last . <nl> * / <nl> - assert cLogCtx . position > = clHeaders . get ( cLogCtx . file ) . getPosition ( id ) ; <nl> - <nl> - / * Sort the commit logs based on creation time * / <nl> - List < String > oldFiles = new ArrayList < String > ( clHeaders . keySet ( ) ) ; <nl> - Collections . sort ( oldFiles , new CommitLogFileComparator ( ) ) ; <nl> + assert context . position > context . getSegment ( ) . getHeader ( ) . getPosition ( id ) : " discard called on obsolete context " + context ; <nl> <nl> / * <nl> * Loop through all the commit log files in the history . Now process <nl> @ @ - 504 , 78 + 358 , 47 @ @ public class CommitLog <nl> * these files the header needs to modified by resetting the dirty <nl> * bit corresponding to the flushed CF . <nl> * / <nl> - for ( String oldFile : oldFiles ) <nl> + for ( CommitLogSegment segment : segments ) <nl> { <nl> - CommitLogHeader header = clHeaders . get ( oldFile ) ; <nl> - if ( oldFile . equals ( cLogCtx . file ) ) <nl> + CommitLogHeader header = segment . getHeader ( ) ; <nl> + if ( segment . equals ( context . getSegment ( ) ) ) <nl> { <nl> / / we can ' t just mark the segment where the flush happened clean , <nl> / / since there may have been writes to it between when the flush <nl> / / started and when it finished . so mark the flush position as <nl> / / the replay point for this CF , instead . <nl> if ( logger . isDebugEnabled ( ) ) <nl> - logger . debug ( " Marking replay position " + cLogCtx . position + " on commit log " + oldFile ) ; <nl> - header . turnOn ( id , cLogCtx . position ) ; <nl> - if ( oldFile . equals ( logFile ) ) <nl> - { <nl> - seekAndWriteCommitLogHeader ( header . toByteArray ( ) ) ; <nl> - } <nl> - else <nl> - { <nl> - writeOldCommitLogHeader ( oldFile , header ) ; <nl> - } <nl> + logger . debug ( " Marking replay position " + context . position + " on commit log " + segment ) ; <nl> + header . turnOn ( id , context . position ) ; <nl> + segment . writeHeader ( ) ; <nl> break ; <nl> } <nl> <nl> header . turnOff ( id ) ; <nl> if ( header . isSafeToDelete ( ) ) <nl> { <nl> - logger . info ( " Deleting obsolete commit log : " + oldFile ) ; <nl> - DeletionService . submitDelete ( oldFile ) ; <nl> - clHeaders . remove ( oldFile ) ; <nl> + logger . info ( " Discarding obsolete commit log : " + segment ) ; <nl> + segment . close ( ) ; <nl> + DeletionService . submitDelete ( segment . getPath ( ) ) ; <nl> + / / usually this will be the first ( remaining ) segment , but not always , if segment A contains <nl> + / / writes to a CF that is unflushed but is followed by segment B whose CFs are all flushed . <nl> + segments . remove ( segment ) ; <nl> } <nl> else <nl> { <nl> if ( logger . isDebugEnabled ( ) ) <nl> - logger . debug ( " Not safe to delete commit log " + oldFile + " ; dirty is " + header . dirtyString ( ) ) ; <nl> - writeOldCommitLogHeader ( oldFile , header ) ; <nl> + logger . debug ( " Not safe to delete commit log " + segment + " ; dirty is " + header . dirtyString ( ) ) ; <nl> + segment . writeHeader ( ) ; <nl> } <nl> } <nl> } <nl> <nl> - private void writeOldCommitLogHeader ( String oldFile , CommitLogHeader header ) throws IOException <nl> - { <nl> - BufferedRandomAccessFile logWriter = CommitLog . createWriter ( oldFile ) ; <nl> - writeCommitLogHeader ( logWriter , header . toByteArray ( ) ) ; <nl> - logWriter . close ( ) ; <nl> - } <nl> - <nl> - private boolean maybeRollLog ( ) throws IOException <nl> - { <nl> - if ( logWriter . length ( ) > = SEGMENT _ SIZE ) <nl> - { <nl> - / * Rolls the current log file over to a new one . * / <nl> - setNextFileName ( ) ; <nl> - String oldLogFile = logWriter . getPath ( ) ; <nl> - logWriter . close ( ) ; <nl> - <nl> - / * point reader / writer to a new commit log file . * / <nl> - logWriter = CommitLog . createWriter ( logFile ) ; <nl> - / * squirrel away the old commit log header * / <nl> - clHeaders . put ( oldLogFile , new CommitLogHeader ( clHeader ) ) ; <nl> - clHeader . clear ( ) ; <nl> - writeCommitLogHeader ( logWriter , clHeader . toByteArray ( ) ) ; <nl> - return true ; <nl> - } <nl> - return false ; <nl> - } <nl> - <nl> void sync ( ) throws IOException <nl> { <nl> - logWriter . sync ( ) ; <nl> + currentSegment ( ) . sync ( ) ; <nl> } <nl> <nl> - class LogRecordAdder implements Callable < CommitLog . CommitLogContext > <nl> + class LogRecordAdder implements Callable < CommitLogSegment . CommitLogContext > <nl> { <nl> final RowMutation rowMutation ; <nl> final Object serializedRow ; <nl> @ @ - 586 , 40 + 409 , 18 @ @ public class CommitLog <nl> this . serializedRow = serializedRow ; <nl> } <nl> <nl> - public CommitLog . CommitLogContext call ( ) throws Exception <nl> + public CommitLogSegment . CommitLogContext call ( ) throws Exception <nl> { <nl> - long currentPosition = - 1L ; <nl> - try <nl> - { <nl> - currentPosition = logWriter . getFilePointer ( ) ; <nl> - CommitLogContext cLogCtx = new CommitLogContext ( logFile , currentPosition ) ; <nl> - maybeUpdateHeader ( rowMutation ) ; <nl> - Checksum checkum = new CRC32 ( ) ; <nl> - if ( serializedRow instanceof DataOutputBuffer ) <nl> - { <nl> - DataOutputBuffer buffer = ( DataOutputBuffer ) serializedRow ; <nl> - logWriter . writeLong ( buffer . getLength ( ) ) ; <nl> - logWriter . write ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; <nl> - checkum . update ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; <nl> - } <nl> - else <nl> - { <nl> - assert serializedRow instanceof byte [ ] ; <nl> - byte [ ] bytes = ( byte [ ] ) serializedRow ; <nl> - logWriter . writeLong ( bytes . length ) ; <nl> - logWriter . write ( bytes ) ; <nl> - checkum . update ( bytes , 0 , bytes . length ) ; <nl> - } <nl> - logWriter . writeLong ( checkum . getValue ( ) ) ; <nl> - maybeRollLog ( ) ; <nl> - return cLogCtx ; <nl> - } <nl> - catch ( IOException e ) <nl> + CommitLogSegment . CommitLogContext context = currentSegment ( ) . write ( rowMutation , serializedRow ) ; <nl> + <nl> + / / roll log if necessary <nl> + if ( currentSegment ( ) . length ( ) > = SEGMENT _ SIZE ) <nl> { <nl> - if ( currentPosition ! = - 1 ) <nl> - logWriter . seek ( currentPosition ) ; <nl> - throw e ; <nl> + sync ( ) ; <nl> + segments . add ( new CommitLogSegment ( currentSegment ( ) . getHeader ( ) . getColumnFamilyCount ( ) ) ) ; <nl> } <nl> + <nl> + return context ; <nl> } <nl> } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogExecutorService . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogExecutorService . java <nl> index f0675c0 . . b9145a7 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogExecutorService . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogExecutorService . java <nl> @ @ - 11 , 9 + 11 , 9 @ @ import javax . management . ObjectName ; <nl> import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . utils . WrappedRunnable ; <nl> <nl> - public class CommitLogExecutorService extends AbstractExecutorService implements CommitLogExecutorServiceMBean <nl> + class CommitLogExecutorService extends AbstractExecutorService implements CommitLogExecutorServiceMBean <nl> { <nl> - BlockingQueue < CheaterFutureTask > queue ; <nl> + private final BlockingQueue < CheaterFutureTask > queue ; <nl> <nl> private volatile long completedTaskCount = 0 ; <nl> <nl> @ @ - 92 , 8 + 92 , 8 @ @ public class CommitLogExecutorService extends AbstractExecutorService implements <nl> queue . take ( ) . run ( ) ; <nl> } <nl> <nl> - private ArrayList < CheaterFutureTask > incompleteTasks = new ArrayList < CheaterFutureTask > ( ) ; <nl> - private ArrayList taskValues = new ArrayList ( ) ; / / TODO not sure how to generify this <nl> + private final ArrayList < CheaterFutureTask > incompleteTasks = new ArrayList < CheaterFutureTask > ( ) ; <nl> + private final ArrayList taskValues = new ArrayList ( ) ; / / TODO not sure how to generify this <nl> private void processWithSyncBatch ( ) throws Exception <nl> { <nl> CheaterFutureTask firstTask = queue . take ( ) ; <nl> @ @ - 188 , 26 + 188 , 26 @ @ public class CommitLogExecutorService extends AbstractExecutorService implements <nl> { <nl> throw new UnsupportedOperationException ( ) ; <nl> } <nl> - } <nl> - <nl> - class CheaterFutureTask < V > extends FutureTask < V > <nl> - { <nl> - private Callable rawCallable ; <nl> <nl> - public CheaterFutureTask ( Callable < V > callable ) <nl> + private static class CheaterFutureTask < V > extends FutureTask < V > <nl> { <nl> - super ( callable ) ; <nl> - rawCallable = callable ; <nl> - } <nl> + private final Callable rawCallable ; <nl> <nl> - public Callable getRawCallable ( ) <nl> - { <nl> - return rawCallable ; <nl> - } <nl> + public CheaterFutureTask ( Callable < V > callable ) <nl> + { <nl> + super ( callable ) ; <nl> + rawCallable = callable ; <nl> + } <nl> <nl> - @ Override <nl> - public void set ( V v ) <nl> - { <nl> - super . set ( v ) ; <nl> + public Callable getRawCallable ( ) <nl> + { <nl> + return rawCallable ; <nl> + } <nl> + <nl> + @ Override <nl> + public void set ( V v ) <nl> + { <nl> + super . set ( v ) ; <nl> + } <nl> } <nl> - } <nl> \ No newline at end of file <nl> + } <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java <nl> index 91adaea . . a80a55a 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java <nl> @ @ - 24 , 6 + 24 , 7 @ @ import java . util . Arrays ; <nl> <nl> import org . apache . cassandra . db . Table ; <nl> import org . apache . cassandra . io . ICompactSerializer ; <nl> + import org . apache . cassandra . io . util . BufferedRandomAccessFile ; <nl> import org . apache . cassandra . utils . BitSetSerializer ; <nl> <nl> class CommitLogHeader <nl> @ @ - 70 , 14 + 71 , 7 @ @ class CommitLogHeader <nl> this . dirty = dirty ; <nl> this . lastFlushedAt = lastFlushedAt ; <nl> } <nl> - <nl> - CommitLogHeader ( CommitLogHeader clHeader ) <nl> - { <nl> - dirty = ( BitSet ) clHeader . dirty . clone ( ) ; <nl> - lastFlushedAt = new int [ clHeader . lastFlushedAt . length ] ; <nl> - System . arraycopy ( clHeader . lastFlushedAt , 0 , lastFlushedAt , 0 , lastFlushedAt . length ) ; <nl> - } <nl> - <nl> + <nl> boolean isDirty ( int index ) <nl> { <nl> return dirty . get ( index ) ; <nl> @ @ - 105 , 12 + 99 , 6 @ @ class CommitLogHeader <nl> return dirty . isEmpty ( ) ; <nl> } <nl> <nl> - void clear ( ) <nl> - { <nl> - dirty . clear ( ) ; <nl> - Arrays . fill ( lastFlushedAt , 0 ) ; <nl> - } <nl> - <nl> byte [ ] toByteArray ( ) throws IOException <nl> { <nl> ByteArrayOutputStream bos = new ByteArrayOutputStream ( ) ; <nl> @ @ - 152 , 6 + 140 , 20 @ @ class CommitLogHeader <nl> return sb . toString ( ) ; <nl> } <nl> <nl> + static CommitLogHeader readCommitLogHeader ( BufferedRandomAccessFile logReader ) throws IOException <nl> + { <nl> + int size = ( int ) logReader . readLong ( ) ; <nl> + byte [ ] bytes = new byte [ size ] ; <nl> + logReader . read ( bytes ) ; <nl> + ByteArrayInputStream byteStream = new ByteArrayInputStream ( bytes ) ; <nl> + return serializer ( ) . deserialize ( new DataInputStream ( byteStream ) ) ; <nl> + } <nl> + <nl> + public int getColumnFamilyCount ( ) <nl> + { <nl> + return lastFlushedAt . length ; <nl> + } <nl> + <nl> static class CommitLogHeaderSerializer implements ICompactSerializer < CommitLogHeader > <nl> { <nl> public void serialize ( CommitLogHeader clHeader , DataOutputStream dos ) throws IOException <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java <nl> new file mode 100644 <nl> index 0000000 . . d0e3d50 <nl> - - - / dev / null <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java <nl> @ @ - 0 , 0 + 1 , 193 @ @ <nl> + package org . apache . cassandra . db . commitlog ; <nl> + <nl> + import java . io . File ; <nl> + import java . io . IOError ; <nl> + import java . io . IOException ; <nl> + import java . util . zip . CRC32 ; <nl> + import java . util . zip . Checksum ; <nl> + <nl> + import org . apache . log4j . Logger ; <nl> + <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + import org . apache . cassandra . db . ColumnFamily ; <nl> + import org . apache . cassandra . db . RowMutation ; <nl> + import org . apache . cassandra . db . Table ; <nl> + import org . apache . cassandra . io . util . BufferedRandomAccessFile ; <nl> + import org . apache . cassandra . io . util . DataOutputBuffer ; <nl> + <nl> + public class CommitLogSegment <nl> + { <nl> + private static final Logger logger = Logger . getLogger ( CommitLogSegment . class ) ; <nl> + <nl> + private final BufferedRandomAccessFile logWriter ; <nl> + private final CommitLogHeader header ; <nl> + <nl> + public CommitLogSegment ( int cfCount ) <nl> + { <nl> + this . header = new CommitLogHeader ( cfCount ) ; <nl> + String logFile = DatabaseDescriptor . getLogFileLocation ( ) + File . separator + " CommitLog - " + System . currentTimeMillis ( ) + " . log " ; <nl> + logger . info ( " Creating new commitlog segment " + logFile ) ; <nl> + <nl> + try <nl> + { <nl> + logWriter = createWriter ( logFile ) ; <nl> + writeCommitLogHeader ( header . toByteArray ( ) ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + throw new IOError ( e ) ; <nl> + } <nl> + } <nl> + <nl> + public void writeHeader ( ) throws IOException <nl> + { <nl> + seekAndWriteCommitLogHeader ( header . toByteArray ( ) ) ; <nl> + } <nl> + <nl> + / * * writes header at the beginning of the file , then seeks back to current position * / <nl> + void seekAndWriteCommitLogHeader ( byte [ ] bytes ) throws IOException <nl> + { <nl> + long currentPos = logWriter . getFilePointer ( ) ; <nl> + logWriter . seek ( 0 ) ; <nl> + <nl> + writeCommitLogHeader ( bytes ) ; <nl> + <nl> + logWriter . seek ( currentPos ) ; <nl> + } <nl> + <nl> + private void writeCommitLogHeader ( byte [ ] bytes ) throws IOException <nl> + { <nl> + logWriter . writeLong ( bytes . length ) ; <nl> + logWriter . write ( bytes ) ; <nl> + logWriter . sync ( ) ; <nl> + } <nl> + <nl> + private static BufferedRandomAccessFile createWriter ( String file ) throws IOException <nl> + { <nl> + return new BufferedRandomAccessFile ( file , " rw " , 128 * 1024 ) ; <nl> + } <nl> + <nl> + public CommitLogSegment . CommitLogContext write ( RowMutation rowMutation , Object serializedRow ) throws IOException <nl> + { <nl> + long currentPosition = - 1L ; <nl> + try <nl> + { <nl> + currentPosition = logWriter . getFilePointer ( ) ; <nl> + CommitLogSegment . CommitLogContext cLogCtx = new CommitLogSegment . CommitLogContext ( currentPosition ) ; <nl> + Table table = Table . open ( rowMutation . getTable ( ) ) ; <nl> + <nl> + / / update header <nl> + for ( ColumnFamily columnFamily : rowMutation . getColumnFamilies ( ) ) <nl> + { <nl> + int id = table . getColumnFamilyId ( columnFamily . name ( ) ) ; <nl> + if ( ! header . isDirty ( id ) ) <nl> + { <nl> + header . turnOn ( id , logWriter . getFilePointer ( ) ) ; <nl> + seekAndWriteCommitLogHeader ( header . toByteArray ( ) ) ; <nl> + } <nl> + } <nl> + <nl> + / / write mutation , w / checksum <nl> + Checksum checkum = new CRC32 ( ) ; <nl> + if ( serializedRow instanceof DataOutputBuffer ) <nl> + { <nl> + DataOutputBuffer buffer = ( DataOutputBuffer ) serializedRow ; <nl> + logWriter . writeLong ( buffer . getLength ( ) ) ; <nl> + logWriter . write ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; <nl> + checkum . update ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; <nl> + } <nl> + else <nl> + { <nl> + assert serializedRow instanceof byte [ ] ; <nl> + byte [ ] bytes = ( byte [ ] ) serializedRow ; <nl> + logWriter . writeLong ( bytes . length ) ; <nl> + logWriter . write ( bytes ) ; <nl> + checkum . update ( bytes , 0 , bytes . length ) ; <nl> + } <nl> + logWriter . writeLong ( checkum . getValue ( ) ) ; <nl> + <nl> + return cLogCtx ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + if ( currentPosition ! = - 1 ) <nl> + logWriter . seek ( currentPosition ) ; <nl> + throw e ; <nl> + } <nl> + } <nl> + <nl> + public void sync ( ) throws IOException <nl> + { <nl> + logWriter . sync ( ) ; <nl> + } <nl> + <nl> + public CommitLogContext getContext ( ) <nl> + { <nl> + return new CommitLogContext ( logWriter . getFilePointer ( ) ) ; <nl> + } <nl> + <nl> + public CommitLogHeader getHeader ( ) <nl> + { <nl> + return header ; <nl> + } <nl> + <nl> + public String getPath ( ) <nl> + { <nl> + return logWriter . getPath ( ) ; <nl> + } <nl> + <nl> + public long length ( ) <nl> + { <nl> + try <nl> + { <nl> + return logWriter . length ( ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + throw new IOError ( e ) ; <nl> + } <nl> + } <nl> + <nl> + public void close ( ) <nl> + { <nl> + try <nl> + { <nl> + logWriter . close ( ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + throw new IOError ( e ) ; <nl> + } <nl> + } <nl> + <nl> + @ Override <nl> + public String toString ( ) <nl> + { <nl> + return " CommitLogSegment ( " + logWriter . getPath ( ) + ' ) ' ; <nl> + } <nl> + <nl> + public class CommitLogContext <nl> + { <nl> + public final long position ; <nl> + <nl> + public CommitLogContext ( long position ) <nl> + { <nl> + assert position > = 0 ; <nl> + this . position = position ; <nl> + } <nl> + <nl> + public CommitLogSegment getSegment ( ) <nl> + { <nl> + return CommitLogSegment . this ; <nl> + } <nl> + <nl> + @ Override <nl> + public String toString ( ) <nl> + { <nl> + return " CommitLogContext ( " + <nl> + " file = ' " + logWriter . getPath ( ) + ' \ ' ' + <nl> + " , position = " + position + <nl> + ' ) ' ; <nl> + } <nl> + } <nl> + } <nl> diff - - git a / test / unit / org / apache / cassandra / db / CommitLogTest . java b / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> index 24f81f9 . . dcff17d 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> @ @ - 32 , 7 + 32 , 7 @ @ public class CommitLogTest extends CleanupHelper <nl> @ Test <nl> public void testCleanup ( ) throws IOException , ExecutionException , InterruptedException <nl> { <nl> - assert CommitLog . getSegmentCount ( ) = = 0 ; <nl> + assert CommitLog . instance ( ) . getSegmentCount ( ) = = 1 ; <nl> CommitLog . setSegmentSize ( 1000 ) ; <nl> <nl> Table table = Table . open ( " Keyspace1 " ) ; <nl> @ @ - 49 , 14 + 49 , 14 @ @ public class CommitLogTest extends CleanupHelper <nl> rm . add ( new QueryPath ( " Standard2 " , null , " Column1 " . getBytes ( ) ) , value , 0 ) ; <nl> rm . apply ( ) ; <nl> } <nl> - assert CommitLog . getSegmentCount ( ) > 1 ; <nl> + assert CommitLog . instance ( ) . getSegmentCount ( ) > 1 ; <nl> <nl> / / nothing should get removed after flushing just Standard1 <nl> store1 . forceBlockingFlush ( ) ; <nl> - assert CommitLog . getSegmentCount ( ) > 1 ; <nl> + assert CommitLog . instance ( ) . getSegmentCount ( ) > 1 ; <nl> <nl> / / after flushing Standard2 we should be able to clean out all segments <nl> store2 . forceBlockingFlush ( ) ; <nl> - assert CommitLog . getSegmentCount ( ) = = 1 ; <nl> + assert CommitLog . instance ( ) . getSegmentCount ( ) = = 1 ; <nl> } <nl> }

TEST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java 
 index a59e70e . . 176f64b 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java 
 @ @ - 281 , 7 + 281 , 7 @ @ public class CommitLogReplayer 
 return ; 
 if ( globalPosition . segment = = desc . id ) 
 reader . seek ( globalPosition . position ) ; 
 - replaySyncSection ( reader , - 1 , desc ) ; 
 + replaySyncSection ( reader , ( int ) reader . getPositionLimit ( ) , desc ) ; 
 return ; 
 } 
 
 diff - - git a / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750790 . log b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750790 . log 
 new file mode 100644 
 index 0000000 . . 3301331 
 Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750790 . log differ 
 diff - - git a / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750791 . log b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750791 . log 
 new file mode 100644 
 index 0000000 . . 04314d6 
 Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750791 . log differ 
 diff - - git a / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750792 . log b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750792 . log 
 new file mode 100644 
 index 0000000 . . a9af9e4 
 Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750792 . log differ 
 diff - - git a / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750793 . log b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750793 . log 
 new file mode 100644 
 index 0000000 . . 3301331 
 Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 0 / CommitLog - 3 - 1431528750793 . log differ 
 diff - - git a / test / data / legacy - commitlog / 2 . 0 / hash . txt b / test / data / legacy - commitlog / 2 . 0 / hash . txt 
 new file mode 100644 
 index 0000000 . . 4bbec02 
 - - - / dev / null 
 + + + b / test / data / legacy - commitlog / 2 . 0 / hash . txt 
 @ @ - 0 , 0 + 1 , 3 @ @ 
 + cfid = 4d331c44 - f018 - 302b - 91c2 - 2dcf94c4bfad 
 + cells = 9724 
 + hash = - 682777064 
 diff - - git a / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069529 . log b / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069529 . log 
 new file mode 100644 
 index 0000000 . . 60064ee 
 Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069529 . log differ 
 diff - - git a / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069530 . log b / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069530 . log 
 new file mode 100644 
 index 0000000 . . fdf7071 
 Binary files / dev / null and b / test / data / legacy - commitlog / 2 . 1 / CommitLog - 4 - 1431529069530 . log differ 
 diff - - git a / test / data / legacy - commitlog / 2 . 1 / hash . txt b / test / data / legacy - commitlog / 2 . 1 / hash . txt 
 new file mode 100644 
 index 0000000 . . f05cf97 
 - - - / dev / null 
 + + + b / test / data / legacy - commitlog / 2 . 1 / hash . txt 
 @ @ - 0 , 0 + 1 , 3 @ @ 
 + cfid = 6c622920 - f980 - 11e4 - b8a0 - e7d448d5e26d 
 + cells = 5165 
 + hash = - 1915888171 
 diff - - git a / test / long / org / apache / cassandra / db / commitlog / CommitLogStressTest . java b / test / long / org / apache / cassandra / db / commitlog / CommitLogStressTest . java 
 index f5fd2cf . . 5897dec 100644 
 - - - a / test / long / org / apache / cassandra / db / commitlog / CommitLogStressTest . java 
 + + + b / test / long / org / apache / cassandra / db / commitlog / CommitLogStressTest . java 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 package org . apache . cassandra . db . commitlog ; 
 + 
 / * 
 * 
 * Licensed to the Apache Software Foundation ( ASF ) under one 
 @ @ - 20 , 7 + 21 , 6 @ @ package org . apache . cassandra . db . commitlog ; 
 * 
 * / 
 
 - 
 import java . io . DataInputStream ; 
 import java . io . File ; 
 import java . io . FileInputStream ; 
 @ @ - 60 , 54 + 60 , 56 @ @ import org . apache . cassandra . io . util . FastByteArrayInputStream ; 
 
 public class CommitLogStressTest 
 { 
 - 
 public static ByteBuffer dataSource ; 
 - 
 - public static int NUM _ THREADS = 4 * Runtime . getRuntime ( ) . availableProcessors ( ) - 1 ; 
 
 + public static int NUM _ THREADS = 4 * Runtime . getRuntime ( ) . availableProcessors ( ) - 1 ; 
 public static int numCells = 1 ; 
 - 
 public static int cellSize = 1024 ; 
 - 
 public static int rateLimit = 0 ; 
 - 
 public static int runTimeMs = 10000 ; 
 - 
 + 
 public static String location = DatabaseDescriptor . getCommitLogLocation ( ) + " / stress " ; 
 - 
 + 
 public static int hash ( int hash , ByteBuffer bytes ) 
 { 
 int shift = 0 ; 
 - for ( int i = 0 ; i < bytes . limit ( ) ; i + + ) { 
 + for ( int i = 0 ; i < bytes . limit ( ) ; i + + ) 
 + { 
 hash + = ( bytes . get ( i ) & 0xFF ) < < shift ; 
 shift = ( shift + 8 ) & 0x1F ; 
 } 
 return hash ; 
 } 
 - 
 - public static void main ( String [ ] args ) throws Exception { 
 - try { 
 - if ( args . length > = 1 ) { 
 + 
 + public static void main ( String [ ] args ) throws Exception 
 + { 
 + try 
 + { 
 + if ( args . length > = 1 ) 
 + { 
 NUM _ THREADS = Integer . parseInt ( args [ 0 ] ) ; 
 System . out . println ( " Setting num threads to : " + NUM _ THREADS ) ; 
 } 
 - 
 - if ( args . length > = 2 ) { 
 + 
 + if ( args . length > = 2 ) 
 + { 
 numCells = Integer . parseInt ( args [ 1 ] ) ; 
 System . out . println ( " Setting num cells to : " + numCells ) ; 
 } 
 - 
 - if ( args . length > = 3 ) { 
 + 
 + if ( args . length > = 3 ) 
 + { 
 cellSize = Integer . parseInt ( args [ 1 ] ) ; 
 System . out . println ( " Setting cell size to : " + cellSize + " be aware the source corpus may be small " ) ; 
 } 
 - 
 - if ( args . length > = 4 ) { 
 + 
 + if ( args . length > = 4 ) 
 + { 
 rateLimit = Integer . parseInt ( args [ 1 ] ) ; 
 System . out . println ( " Setting per thread rate limit to : " + rateLimit ) ; 
 } 
 initialize ( ) ; 
 - 
 + 
 CommitLogStressTest tester = new CommitLogStressTest ( ) ; 
 tester . testFixedSize ( ) ; 
 } 
 @ @ - 115 , 24 + 117 , 26 @ @ public class CommitLogStressTest 
 { 
 e . printStackTrace ( System . err ) ; 
 } 
 - finally { 
 + finally 
 + { 
 System . exit ( 0 ) ; 
 } 
 } 
 - 
 + 
 boolean failed = false ; 
 volatile boolean stop = false ; 
 boolean randomSize = false ; 
 boolean discardedRun = false ; 
 ReplayPosition discardedPos ; 
 - 
 + 
 @ BeforeClass 
 - static public void initialize ( ) throws FileNotFoundException , IOException , InterruptedException 
 + static public void initialize ( ) throws IOException 
 { 
 try ( FileInputStream fis = new FileInputStream ( " CHANGES . txt " ) ) 
 { 
 - dataSource = ByteBuffer . allocateDirect ( ( int ) fis . getChannel ( ) . size ( ) ) ; 
 - while ( dataSource . hasRemaining ( ) ) { 
 + dataSource = ByteBuffer . allocateDirect ( ( int ) fis . getChannel ( ) . size ( ) ) ; 
 + while ( dataSource . hasRemaining ( ) ) 
 + { 
 fis . getChannel ( ) . read ( dataSource ) ; 
 } 
 dataSource . flip ( ) ; 
 @ @ - 141 , 7 + 145 , 7 @ @ public class CommitLogStressTest 
 SchemaLoader . loadSchema ( ) ; 
 SchemaLoader . schemaDefinition ( " " ) ; / / leave def . blank to maintain old behaviour 
 } 
 - 
 + 
 @ Before 
 public void cleanDir ( ) 
 { 
 @ @ - 149 , 11 + 153 , 13 @ @ public class CommitLogStressTest 
 if ( dir . isDirectory ( ) ) 
 { 
 File [ ] files = dir . listFiles ( ) ; 
 - 
 + 
 for ( File f : files ) 
 if ( ! f . delete ( ) ) 
 Assert . fail ( " Failed to delete " + f ) ; 
 - } else { 
 + } 
 + else 
 + { 
 dir . mkdir ( ) ; 
 } 
 } 
 @ @ - 194 , 7 + 200 , 8 @ @ public class CommitLogStressTest 
 null , 
 new ParameterizedClass ( " LZ4Compressor " , null ) , 
 new ParameterizedClass ( " SnappyCompressor " , null ) , 
 - new ParameterizedClass ( " DeflateCompressor " , null ) } ) { 
 + new ParameterizedClass ( " DeflateCompressor " , null ) } ) 
 + { 
 DatabaseDescriptor . setCommitLogCompression ( compressor ) ; 
 for ( CommitLogSync sync : CommitLogSync . values ( ) ) 
 { 
 @ @ - 206 , 27 + 213 , 29 @ @ public class CommitLogStressTest 
 assert ! failed ; 
 } 
 
 - public void testLog ( CommitLog commitLog ) throws IOException , InterruptedException { 
 + public void testLog ( CommitLog commitLog ) throws IOException , InterruptedException 
 + { 
 System . out . format ( " \ nTesting commit log size % . 0fmb , compressor % s , sync % s % s % s \ n " , 
 - mb ( DatabaseDescriptor . getCommitLogSegmentSize ( ) ) , 
 - commitLog . compressor ! = null ? commitLog . compressor . getClass ( ) . getSimpleName ( ) : " none " , 
 - commitLog . executor . getClass ( ) . getSimpleName ( ) , 
 - randomSize ? " random size " : " " , 
 - discardedRun ? " with discarded run " : " " ) ; 
 + mb ( DatabaseDescriptor . getCommitLogSegmentSize ( ) ) , 
 + commitLog . compressor ! = null ? commitLog . compressor . getClass ( ) . getSimpleName ( ) : " none " , 
 + commitLog . executor . getClass ( ) . getSimpleName ( ) , 
 + randomSize ? " random size " : " " , 
 + discardedRun ? " with discarded run " : " " ) ; 
 commitLog . allocator . enableReserveSegmentCreation ( ) ; 
 - 
 + 
 final List < CommitlogExecutor > threads = new ArrayList < > ( ) ; 
 ScheduledExecutorService scheduled = startThreads ( commitLog , threads ) ; 
 
 discardedPos = ReplayPosition . NONE ; 
 - if ( discardedRun ) { 
 + if ( discardedRun ) 
 + { 
 / / Makes sure post - break data is not deleted , and that replayer correctly rejects earlier mutations . 
 Thread . sleep ( runTimeMs / 3 ) ; 
 stop = true ; 
 scheduled . shutdown ( ) ; 
 scheduled . awaitTermination ( 2 , TimeUnit . SECONDS ) ; 
 
 - for ( CommitlogExecutor t : threads ) 
 + for ( CommitlogExecutor t : threads ) 
 { 
 t . join ( ) ; 
 if ( t . rp . compareTo ( discardedPos ) > 0 ) 
 @ @ - 234 , 15 + 243 , 15 @ @ public class CommitLogStressTest 
 } 
 verifySizes ( commitLog ) ; 
 
 - commitLog . discardCompletedSegments ( Schema . instance . getCFMetaData ( " Keyspace1 " , " Standard1 " ) . cfId , discardedPos ) ; 
 + commitLog . discardCompletedSegments ( Schema . instance . getCFMetaData ( " Keyspace1 " , " Standard1 " ) . cfId , 
 + discardedPos ) ; 
 threads . clear ( ) ; 
 System . out . format ( " Discarded at % s \ n " , discardedPos ) ; 
 verifySizes ( commitLog ) ; 
 - 
 + 
 scheduled = startThreads ( commitLog , threads ) ; 
 } 
 
 - 
 Thread . sleep ( runTimeMs ) ; 
 stop = true ; 
 scheduled . shutdown ( ) ; 
 @ @ - 250 , 16 + 259 , 18 @ @ public class CommitLogStressTest 
 
 int hash = 0 ; 
 int cells = 0 ; 
 - for ( CommitlogExecutor t : threads ) { 
 + for ( CommitlogExecutor t : threads ) 
 + { 
 t . join ( ) ; 
 hash + = t . hash ; 
 cells + = t . cells ; 
 } 
 verifySizes ( commitLog ) ; 
 - 
 + 
 commitLog . shutdownBlocking ( ) ; 
 
 - System . out . print ( " Stopped . Replaying . . . " ) ; System . out . flush ( ) ; 
 + System . out . print ( " Stopped . Replaying . . . " ) ; 
 + System . out . flush ( ) ; 
 Replayer repl = new Replayer ( ) ; 
 File [ ] files = new File ( location ) . listFiles ( ) ; 
 repl . recover ( files ) ; 
 @ @ - 267 , 12 + 278 , 16 @ @ public class CommitLogStressTest 
 for ( File f : files ) 
 if ( ! f . delete ( ) ) 
 Assert . fail ( " Failed to delete " + f ) ; 
 - 
 + 
 if ( hash = = repl . hash & & cells = = repl . cells ) 
 System . out . println ( " Test success . " ) ; 
 else 
 { 
 - System . out . format ( " Test failed . Cells % d expected % d , hash % d expected % d . \ n " , repl . cells , cells , repl . hash , hash ) ; 
 + System . out . format ( " Test failed . Cells % d expected % d , hash % d expected % d . \ n " , 
 + repl . cells , 
 + cells , 
 + repl . hash , 
 + hash ) ; 
 failed = true ; 
 } 
 } 
 @ @ - 287 , 7 + 302 , 7 @ @ public class CommitLogStressTest 
 commitLog . executor . requestExtraSync ( ) . awaitUninterruptibly ( ) ; 
 / / Wait for any pending deletes or segment allocations to complete . 
 commitLog . allocator . awaitManagementTasksCompletion ( ) ; 
 - 
 + 
 long combinedSize = 0 ; 
 for ( File f : new File ( commitLog . location ) . listFiles ( ) ) 
 combinedSize + = f . length ( ) ; 
 @ @ - 297 , 11 + 312 , 11 @ @ public class CommitLogStressTest 
 Map < String , Double > ratios = commitLog . getActiveSegmentCompressionRatios ( ) ; 
 Collection < CommitLogSegment > segments = commitLog . allocator . getActiveSegments ( ) ; 
 
 - for ( CommitLogSegment segment : segments ) 
 + for ( CommitLogSegment segment : segments ) 
 { 
 Assert . assertTrue ( logFileNames . remove ( segment . getName ( ) ) ) ; 
 Double ratio = ratios . remove ( segment . getName ( ) ) ; 
 - 
 + 
 Assert . assertEquals ( segment . logFile . length ( ) , segment . onDiskSize ( ) ) ; 
 Assert . assertEquals ( segment . onDiskSize ( ) * 1 . 0 / segment . contentSize ( ) , ratio , 0 . 01 ) ; 
 } 
 @ @ - 312 , 35 + 327 , 47 @ @ public class CommitLogStressTest 
 public ScheduledExecutorService startThreads ( final CommitLog commitLog , final List < CommitlogExecutor > threads ) 
 { 
 stop = false ; 
 - for ( int ii = 0 ; ii < NUM _ THREADS ; ii + + ) { 
 + for ( int ii = 0 ; ii < NUM _ THREADS ; ii + + ) 
 + { 
 final CommitlogExecutor t = new CommitlogExecutor ( commitLog , new Random ( ii ) ) ; 
 threads . add ( t ) ; 
 t . start ( ) ; 
 } 
 
 final long start = System . currentTimeMillis ( ) ; 
 - Runnable printRunnable = new Runnable ( ) { 
 + Runnable printRunnable = new Runnable ( ) 
 + { 
 long lastUpdate = 0 ; 
 
 - public void run ( ) { 
 - Runtime runtime = Runtime . getRuntime ( ) ; 
 - long maxMemory = runtime . maxMemory ( ) ; 
 - long allocatedMemory = runtime . totalMemory ( ) ; 
 - long freeMemory = runtime . freeMemory ( ) ; 
 - long temp = 0 ; 
 - long sz = 0 ; 
 - for ( CommitlogExecutor cle : threads ) { 
 - temp + = cle . counter . get ( ) ; 
 - sz + = cle . dataSize ; 
 - } 
 - double time = ( System . currentTimeMillis ( ) - start ) / 1000 . 0 ; 
 - double avg = ( temp / time ) ; 
 - System . out . println ( 
 - String . format ( " second % d mem max % . 0fmb allocated % . 0fmb free % . 0fmb mutations % d since start % d avg % . 3f content % . 1fmb ondisk % . 1fmb transfer % . 3fmb " , 
 - ( ( System . currentTimeMillis ( ) - start ) / 1000 ) , 
 - mb ( maxMemory ) , mb ( allocatedMemory ) , mb ( freeMemory ) , ( temp - lastUpdate ) , lastUpdate , avg , 
 - mb ( commitLog . getActiveContentSize ( ) ) , mb ( commitLog . getActiveOnDiskSize ( ) ) , mb ( sz / time ) ) ) ; 
 - lastUpdate = temp ; 
 + public void run ( ) 
 + { 
 + Runtime runtime = Runtime . getRuntime ( ) ; 
 + long maxMemory = runtime . maxMemory ( ) ; 
 + long allocatedMemory = runtime . totalMemory ( ) ; 
 + long freeMemory = runtime . freeMemory ( ) ; 
 + long temp = 0 ; 
 + long sz = 0 ; 
 + for ( CommitlogExecutor cle : threads ) 
 + { 
 + temp + = cle . counter . get ( ) ; 
 + sz + = cle . dataSize ; 
 + } 
 + double time = ( System . currentTimeMillis ( ) - start ) / 1000 . 0 ; 
 + double avg = ( temp / time ) ; 
 + System . out 
 + . println ( 
 + String . format ( " second % d mem max % . 0fmb allocated % . 0fmb free % . 0fmb mutations % d since start % d avg % . 3f content % . 1fmb ondisk % . 1fmb transfer % . 3fmb " , 
 + ( ( System . currentTimeMillis ( ) - start ) / 1000 ) , 
 + mb ( maxMemory ) , 
 + mb ( allocatedMemory ) , 
 + mb ( freeMemory ) , 
 + ( temp - lastUpdate ) , 
 + lastUpdate , 
 + avg , 
 + mb ( commitLog . getActiveContentSize ( ) ) , 
 + mb ( commitLog . getActiveOnDiskSize ( ) ) , 
 + mb ( sz / time ) ) ) ; 
 + lastUpdate = temp ; 
 } 
 } ; 
 ScheduledExecutorService scheduled = Executors . newScheduledThreadPool ( 1 ) ; 
 @ @ - 348 , 15 + 375 , 18 @ @ public class CommitLogStressTest 
 return scheduled ; 
 } 
 
 - private static double mb ( long maxMemory ) { 
 + private static double mb ( long maxMemory ) 
 + { 
 return maxMemory / ( 1024 . 0 * 1024 ) ; 
 } 
 
 - private static double mb ( double maxMemory ) { 
 + private static double mb ( double maxMemory ) 
 + { 
 return maxMemory / ( 1024 * 1024 ) ; 
 } 
 
 - public static ByteBuffer randomBytes ( int quantity , Random tlr ) { 
 + public static ByteBuffer randomBytes ( int quantity , Random tlr ) 
 + { 
 ByteBuffer slice = ByteBuffer . allocate ( quantity ) ; 
 ByteBuffer source = dataSource . duplicate ( ) ; 
 source . position ( tlr . nextInt ( source . capacity ( ) - quantity ) ) ; 
 @ @ - 366 , 7 + 396 , 8 @ @ public class CommitLogStressTest 
 return slice ; 
 } 
 
 - public class CommitlogExecutor extends Thread { 
 + public class CommitlogExecutor extends Thread 
 + { 
 final AtomicLong counter = new AtomicLong ( ) ; 
 int hash = 0 ; 
 int cells = 0 ; 
 @ @ - 382 , 21 + 413 , 23 @ @ public class CommitLogStressTest 
 this . random = rand ; 
 } 
 
 - public void run ( ) { 
 + public void run ( ) 
 + { 
 RateLimiter rl = rateLimit ! = 0 ? RateLimiter . create ( rateLimit ) : null ; 
 final Random rand = random ! = null ? random : ThreadLocalRandom . current ( ) ; 
 - while ( ! stop ) { 
 + while ( ! stop ) 
 + { 
 if ( rl ! = null ) 
 rl . acquire ( ) ; 
 String ks = " Keyspace1 " ; 
 ByteBuffer key = randomBytes ( 16 , rand ) ; 
 Mutation mutation = new Mutation ( ks , key ) ; 
 
 - for ( int ii = 0 ; ii < numCells ; ii + + ) { 
 + for ( int ii = 0 ; ii < numCells ; ii + + ) 
 + { 
 int sz = randomSize ? rand . nextInt ( cellSize ) : cellSize ; 
 ByteBuffer bytes = randomBytes ( sz , rand ) ; 
 - mutation . add ( " Standard1 " , Util . cellname ( " name " + ii ) , bytes , 
 - System . currentTimeMillis ( ) ) ; 
 + mutation . add ( " Standard1 " , Util . cellname ( " name " + ii ) , bytes , System . currentTimeMillis ( ) ) ; 
 hash = hash ( hash , bytes ) ; 
 + + cells ; 
 dataSize + = sz ; 
 @ @ - 406 , 7 + 439 , 7 @ @ public class CommitLogStressTest 
 } 
 } 
 } 
 - 
 + 
 class Replayer extends CommitLogReplayer 
 { 
 Replayer ( ) 
 @ @ - 420 , 20 + 453 , 22 @ @ public class CommitLogStressTest 
 @ Override 
 void replayMutation ( byte [ ] inputBuffer , int size , final long entryLocation , final CommitLogDescriptor desc ) 
 { 
 - if ( desc . id < discardedPos . segment ) { 
 + if ( desc . id < discardedPos . segment ) 
 + { 
 System . out . format ( " Mutation from discarded segment , segment % d pos % d \ n " , desc . id , entryLocation ) ; 
 return ; 
 - } else if ( desc . id = = discardedPos . segment & & entryLocation < = discardedPos . position ) 
 + } 
 + else if ( desc . id = = discardedPos . segment & & entryLocation < = discardedPos . position ) 
 / / Skip over this mutation . 
 return ; 
 - 
 + 
 FastByteArrayInputStream bufIn = new FastByteArrayInputStream ( inputBuffer , 0 , size ) ; 
 Mutation mutation ; 
 try 
 { 
 mutation = Mutation . serializer . deserialize ( new DataInputStream ( bufIn ) , 
 - desc . getMessagingVersion ( ) , 
 - ColumnSerializer . Flag . LOCAL ) ; 
 + desc . getMessagingVersion ( ) , 
 + ColumnSerializer . Flag . LOCAL ) ; 
 } 
 catch ( IOException e ) 
 { 
 @ @ - 441 , 8 + 476 , 10 @ @ public class CommitLogStressTest 
 throw new AssertionError ( e ) ; 
 } 
 
 - for ( ColumnFamily cf : mutation . getColumnFamilies ( ) ) { 
 - for ( Cell c : cf . getSortedColumns ( ) ) { 
 + for ( ColumnFamily cf : mutation . getColumnFamilies ( ) ) 
 + { 
 + for ( Cell c : cf . getSortedColumns ( ) ) 
 + { 
 if ( new String ( c . name ( ) . toByteBuffer ( ) . array ( ) , StandardCharsets . UTF _ 8 ) . startsWith ( " name " ) ) 
 { 
 hash = hash ( hash , c . value ( ) ) ; 
 @ @ - 451 , 6 + 488 , 6 @ @ public class CommitLogStressTest 
 } 
 } 
 } 
 - 
 + 
 } 
 } 
 diff - - git a / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTest . java b / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTest . java 
 new file mode 100644 
 index 0000000 . . 1655078 
 - - - / dev / null 
 + + + b / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTest . java 
 @ @ - 0 , 0 + 1 , 143 @ @ 
 + package org . apache . cassandra . db . commitlog ; 
 + 
 + / * 
 + * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , 
 + * software distributed under the License is distributed on an 
 + * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY 
 + * KIND , either express or implied . See the License for the 
 + * specific language governing permissions and limitations 
 + * under the License . 
 + * 
 + * / 
 + 
 + import java . io . * ; 
 + import java . nio . ByteBuffer ; 
 + import java . nio . charset . StandardCharsets ; 
 + import java . util . Properties ; 
 + import java . util . UUID ; 
 + 
 + import junit . framework . Assert ; 
 + 
 + import com . google . common . base . Predicate ; 
 + 
 + import org . junit . BeforeClass ; 
 + import org . junit . Test ; 
 + 
 + import org . apache . cassandra . SchemaLoader ; 
 + import org . apache . cassandra . config . CFMetaData ; 
 + import org . apache . cassandra . config . Schema ; 
 + import org . apache . cassandra . db . Cell ; 
 + import org . apache . cassandra . db . ColumnFamily ; 
 + import org . apache . cassandra . db . Mutation ; 
 + 
 + public class CommitLogUpgradeTest 
 + { 
 + static final String DATA _ DIR = " test / data / legacy - commitlog / " ; 
 + static final String PROPERTIES _ FILE = " hash . txt " ; 
 + static final String CFID _ PROPERTY = " cfid " ; 
 + static final String CELLS _ PROPERTY = " cells " ; 
 + static final String HASH _ PROPERTY = " hash " ; 
 + 
 + static final String TABLE = " Standard1 " ; 
 + static final String KEYSPACE = " Keyspace1 " ; 
 + static final String CELLNAME = " name " ; 
 + 
 + @ Test 
 + public void test20 ( ) throws Exception 
 + { 
 + testRestore ( DATA _ DIR + " 2 . 0 " ) ; 
 + } 
 + 
 + @ Test 
 + public void test21 ( ) throws Exception 
 + { 
 + testRestore ( DATA _ DIR + " 2 . 1 " ) ; 
 + } 
 + 
 + @ BeforeClass 
 + static public void initialize ( ) throws FileNotFoundException , IOException , InterruptedException 
 + { 
 + SchemaLoader . loadSchema ( ) ; 
 + SchemaLoader . schemaDefinition ( " " ) ; 
 + } 
 + 
 + public void testRestore ( String location ) throws IOException , InterruptedException 
 + { 
 + Properties prop = new Properties ( ) ; 
 + prop . load ( new FileInputStream ( new File ( location + File . separatorChar + PROPERTIES _ FILE ) ) ) ; 
 + int hash = Integer . parseInt ( prop . getProperty ( HASH _ PROPERTY ) ) ; 
 + int cells = Integer . parseInt ( prop . getProperty ( CELLS _ PROPERTY ) ) ; 
 + 
 + String cfidString = prop . getProperty ( CFID _ PROPERTY ) ; 
 + if ( cfidString ! = null ) 
 + { 
 + UUID cfid = UUID . fromString ( cfidString ) ; 
 + if ( Schema . instance . getCF ( cfid ) = = null ) 
 + { 
 + CFMetaData cfm = Schema . instance . getCFMetaData ( KEYSPACE , TABLE ) ; 
 + Schema . instance . purge ( cfm ) ; 
 + Schema . instance . load ( cfm . copy ( cfid ) ) ; 
 + } 
 + } 
 + 
 + Hasher hasher = new Hasher ( ) ; 
 + CommitLogTestReplayer replayer = new CommitLogTestReplayer ( hasher ) ; 
 + File [ ] files = new File ( location ) . listFiles ( new FilenameFilter ( ) 
 + { 
 + @ Override 
 + public boolean accept ( File dir , String name ) 
 + { 
 + return name . endsWith ( " . log " ) ; 
 + } 
 + } ) ; 
 + replayer . recover ( files ) ; 
 + 
 + Assert . assertEquals ( cells , hasher . cells ) ; 
 + Assert . assertEquals ( hash , hasher . hash ) ; 
 + } 
 + 
 + public static int hash ( int hash , ByteBuffer bytes ) 
 + { 
 + int shift = 0 ; 
 + for ( int i = 0 ; i < bytes . limit ( ) ; i + + ) 
 + { 
 + hash + = ( bytes . get ( i ) & 0xFF ) < < shift ; 
 + shift = ( shift + 8 ) & 0x1F ; 
 + } 
 + return hash ; 
 + } 
 + 
 + class Hasher implements Predicate < Mutation > 
 + { 
 + int hash = 0 ; 
 + int cells = 0 ; 
 + 
 + @ Override 
 + public boolean apply ( Mutation mutation ) 
 + { 
 + for ( ColumnFamily cf : mutation . getColumnFamilies ( ) ) 
 + { 
 + for ( Cell c : cf . getSortedColumns ( ) ) 
 + { 
 + if ( new String ( c . name ( ) . toByteBuffer ( ) . array ( ) , StandardCharsets . UTF _ 8 ) . startsWith ( CELLNAME ) ) 
 + { 
 + hash = hash ( hash , c . value ( ) ) ; 
 + + + cells ; 
 + } 
 + } 
 + } 
 + return true ; 
 + } 
 + } 
 + } 
 diff - - git a / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTestMaker . java b / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTestMaker . java 
 new file mode 100644 
 index 0000000 . . 7b07c8e 
 - - - / dev / null 
 + + + b / test / unit / org / apache / cassandra / db / commitlog / CommitLogUpgradeTestMaker . java 
 @ @ - 0 , 0 + 1 , 250 @ @ 
 + package org . apache . cassandra . db . commitlog ; 
 + 
 + / * 
 + * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , 
 + * software distributed under the License is distributed on an 
 + * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY 
 + * KIND , either express or implied . See the License for the 
 + * specific language governing permissions and limitations 
 + * under the License . 
 + * 
 + * / 
 + 
 + import java . io . * ; 
 + import java . nio . ByteBuffer ; 
 + import java . util . ArrayList ; 
 + import java . util . List ; 
 + import java . util . Properties ; 
 + import java . util . concurrent . Executors ; 
 + import java . util . concurrent . ScheduledExecutorService ; 
 + import java . util . concurrent . ThreadLocalRandom ; 
 + import java . util . concurrent . TimeUnit ; 
 + import java . util . concurrent . atomic . AtomicLong ; 
 + 
 + import com . google . common . util . concurrent . RateLimiter ; 
 + 
 + import org . apache . cassandra . SchemaLoader ; 
 + import org . apache . cassandra . Util ; 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 + import org . apache . cassandra . config . Schema ; 
 + import org . apache . cassandra . db . Mutation ; 
 + import org . apache . cassandra . exceptions . ConfigurationException ; 
 + import org . apache . cassandra . io . util . FileUtils ; 
 + import org . apache . cassandra . utils . FBUtilities ; 
 + 
 + import static org . apache . cassandra . db . commitlog . CommitLogUpgradeTest . * ; 
 + 
 + public class CommitLogUpgradeTestMaker 
 + { 
 + public static ByteBuffer dataSource ; 
 + 
 + private static int NUM _ THREADS = 4 * Runtime . getRuntime ( ) . availableProcessors ( ) - 1 ; 
 + public static int numCells = 1 ; 
 + public static int cellSize = 256 ; 
 + public static int rateLimit = 0 ; 
 + public static int runTimeMs = 1000 ; 
 + 
 + public static void main ( String [ ] args ) throws Exception 
 + { 
 + try 
 + { 
 + initialize ( ) ; 
 + 
 + CommitLogUpgradeTestMaker tester = new CommitLogUpgradeTestMaker ( ) ; 
 + tester . makeLog ( ) ; 
 + } 
 + catch ( Throwable e ) 
 + { 
 + e . printStackTrace ( System . err ) ; 
 + } 
 + finally 
 + { 
 + System . exit ( 0 ) ; 
 + } 
 + } 
 + 
 + volatile boolean stop = false ; 
 + boolean randomSize = true ; 
 + 
 + static public void initialize ( ) throws IOException , ConfigurationException 
 + { 
 + try ( FileInputStream fis = new FileInputStream ( " CHANGES . txt " ) ) 
 + { 
 + dataSource = ByteBuffer . allocateDirect ( ( int ) fis . getChannel ( ) . size ( ) ) ; 
 + while ( dataSource . hasRemaining ( ) ) 
 + { 
 + fis . getChannel ( ) . read ( dataSource ) ; 
 + } 
 + dataSource . flip ( ) ; 
 + } 
 + 
 + SchemaLoader . loadSchema ( ) ; 
 + SchemaLoader . schemaDefinition ( " " ) ; 
 + } 
 + 
 + public void makeLog ( ) throws IOException , InterruptedException 
 + { 
 + CommitLog commitLog = CommitLog . instance ; 
 + System . out . format ( " \ nUsing commit log size % dmb , compressor % s , sync % s % s \ n " , 
 + mb ( DatabaseDescriptor . getCommitLogSegmentSize ( ) ) , 
 + commitLog . compressor ! = null ? commitLog . compressor . getClass ( ) . getSimpleName ( ) : " none " , 
 + commitLog . executor . getClass ( ) . getSimpleName ( ) , 
 + randomSize ? " random size " : " " ) ; 
 + final List < CommitlogExecutor > threads = new ArrayList < > ( ) ; 
 + ScheduledExecutorService scheduled = startThreads ( commitLog , threads ) ; 
 + 
 + Thread . sleep ( runTimeMs ) ; 
 + stop = true ; 
 + scheduled . shutdown ( ) ; 
 + scheduled . awaitTermination ( 2 , TimeUnit . SECONDS ) ; 
 + 
 + int hash = 0 ; 
 + int cells = 0 ; 
 + for ( CommitlogExecutor t : threads ) 
 + { 
 + t . join ( ) ; 
 + hash + = t . hash ; 
 + cells + = t . cells ; 
 + } 
 + commitLog . shutdownBlocking ( ) ; 
 + 
 + File dataDir = new File ( CommitLogUpgradeTest . DATA _ DIR + FBUtilities . getReleaseVersionString ( ) ) ; 
 + System . out . format ( " Data will be stored in % s \ n " , dataDir ) ; 
 + if ( dataDir . exists ( ) ) 
 + FileUtils . deleteRecursive ( dataDir ) ; 
 + 
 + dataDir . mkdirs ( ) ; 
 + for ( File f : new File ( DatabaseDescriptor . getCommitLogLocation ( ) ) . listFiles ( ) ) 
 + FileUtils . createHardLink ( f , new File ( dataDir , f . getName ( ) ) ) ; 
 + 
 + Properties prop = new Properties ( ) ; 
 + prop . setProperty ( CFID _ PROPERTY , Schema . instance . getId ( KEYSPACE , TABLE ) . toString ( ) ) ; 
 + prop . setProperty ( CELLS _ PROPERTY , Integer . toString ( cells ) ) ; 
 + prop . setProperty ( HASH _ PROPERTY , Integer . toString ( hash ) ) ; 
 + prop . store ( new FileOutputStream ( new File ( dataDir , PROPERTIES _ FILE ) ) , 
 + " CommitLog upgrade test , version " + FBUtilities . getReleaseVersionString ( ) ) ; 
 + System . out . println ( " Done " ) ; 
 + } 
 + 
 + public ScheduledExecutorService startThreads ( CommitLog commitLog , final List < CommitlogExecutor > threads ) 
 + { 
 + stop = false ; 
 + for ( int ii = 0 ; ii < NUM _ THREADS ; ii + + ) 
 + { 
 + final CommitlogExecutor t = new CommitlogExecutor ( commitLog ) ; 
 + threads . add ( t ) ; 
 + t . start ( ) ; 
 + } 
 + 
 + final long start = System . currentTimeMillis ( ) ; 
 + Runnable printRunnable = new Runnable ( ) 
 + { 
 + long lastUpdate = 0 ; 
 + 
 + public void run ( ) 
 + { 
 + Runtime runtime = Runtime . getRuntime ( ) ; 
 + long maxMemory = mb ( runtime . maxMemory ( ) ) ; 
 + long allocatedMemory = mb ( runtime . totalMemory ( ) ) ; 
 + long freeMemory = mb ( runtime . freeMemory ( ) ) ; 
 + long temp = 0 ; 
 + long sz = 0 ; 
 + for ( CommitlogExecutor cle : threads ) 
 + { 
 + temp + = cle . counter . get ( ) ; 
 + sz + = cle . dataSize ; 
 + } 
 + double time = ( System . currentTimeMillis ( ) - start ) / 1000 . 0 ; 
 + double avg = ( temp / time ) ; 
 + System . out . println ( 
 + String . format ( " second % d mem max % dmb allocated % dmb free % dmb mutations % d since start % d avg % . 3f transfer % . 3fmb " , 
 + ( ( System . currentTimeMillis ( ) - start ) / 1000 ) , 
 + maxMemory , 
 + allocatedMemory , 
 + freeMemory , 
 + ( temp - lastUpdate ) , 
 + lastUpdate , 
 + avg , 
 + mb ( sz / time ) ) ) ; 
 + lastUpdate = temp ; 
 + } 
 + } ; 
 + ScheduledExecutorService scheduled = Executors . newScheduledThreadPool ( 1 ) ; 
 + scheduled . scheduleAtFixedRate ( printRunnable , 1 , 1 , TimeUnit . SECONDS ) ; 
 + return scheduled ; 
 + } 
 + 
 + private static long mb ( long maxMemory ) 
 + { 
 + return maxMemory / ( 1024 * 1024 ) ; 
 + } 
 + 
 + private static double mb ( double maxMemory ) 
 + { 
 + return maxMemory / ( 1024 * 1024 ) ; 
 + } 
 + 
 + public static ByteBuffer randomBytes ( int quantity , ThreadLocalRandom tlr ) 
 + { 
 + ByteBuffer slice = ByteBuffer . allocate ( quantity ) ; 
 + ByteBuffer source = dataSource . duplicate ( ) ; 
 + source . position ( tlr . nextInt ( source . capacity ( ) - quantity ) ) ; 
 + source . limit ( source . position ( ) + quantity ) ; 
 + slice . put ( source ) ; 
 + slice . flip ( ) ; 
 + return slice ; 
 + } 
 + 
 + public class CommitlogExecutor extends Thread 
 + { 
 + final AtomicLong counter = new AtomicLong ( ) ; 
 + int hash = 0 ; 
 + int cells = 0 ; 
 + int dataSize = 0 ; 
 + final CommitLog commitLog ; 
 + 
 + volatile ReplayPosition rp ; 
 + 
 + public CommitlogExecutor ( CommitLog commitLog ) 
 + { 
 + this . commitLog = commitLog ; 
 + } 
 + 
 + public void run ( ) 
 + { 
 + RateLimiter rl = rateLimit ! = 0 ? RateLimiter . create ( rateLimit ) : null ; 
 + final ThreadLocalRandom tlr = ThreadLocalRandom . current ( ) ; 
 + while ( ! stop ) 
 + { 
 + if ( rl ! = null ) 
 + rl . acquire ( ) ; 
 + String ks = KEYSPACE ; 
 + ByteBuffer key = randomBytes ( 16 , tlr ) ; 
 + Mutation mutation = new Mutation ( ks , key ) ; 
 + 
 + for ( int ii = 0 ; ii < numCells ; ii + + ) 
 + { 
 + int sz = randomSize ? tlr . nextInt ( cellSize ) : cellSize ; 
 + ByteBuffer bytes = randomBytes ( sz , tlr ) ; 
 + mutation . add ( TABLE , Util . cellname ( CELLNAME + ii ) , bytes , System . currentTimeMillis ( ) ) ; 
 + hash = hash ( hash , bytes ) ; 
 + + + cells ; 
 + dataSize + = sz ; 
 + } 
 + rp = commitLog . add ( mutation ) ; 
 + counter . incrementAndGet ( ) ; 
 + } 
 + } 
 + } 
 + }

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 index d3b6a84 . . 8ef0b0e 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 @ @ - 40 , 6 + 40 , 7 @ @ import org . apache . log4j . Logger ; 
 
 import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . db . commitlog . CommitLog ; 
 + import org . apache . cassandra . db . commitlog . CommitLogSegment ; 
 import org . apache . cassandra . dht . AbstractBounds ; 
 import org . apache . cassandra . dht . Bounds ; 
 import org . apache . cassandra . dht . Range ; 
 @ @ - 366 , 14 + 367 , 14 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 Table . flusherLock . writeLock ( ) . lock ( ) ; 
 try 
 { 
 - final CommitLog . CommitLogContext ctx = CommitLog . instance ( ) . getContext ( ) ; / / this is harmless if ! writeCommitLog 
 - 
 if ( oldMemtable . isFrozen ( ) ) 
 { 
 return null ; 
 } 
 - logger _ . info ( columnFamily _ + " has reached its threshold ; switching in a fresh Memtable " ) ; 
 oldMemtable . freeze ( ) ; 
 + 
 + final CommitLogSegment . CommitLogContext ctx = writeCommitLog ? CommitLog . instance ( ) . getContext ( ) : null ; 
 + logger _ . info ( columnFamily _ + " has reached its threshold ; switching in a fresh Memtable at " + ctx ) ; 
 final Condition condition = submitFlush ( oldMemtable ) ; 
 memtable _ = new Memtable ( table _ , columnFamily _ ) ; 
 / / a second executor that makes sure the onMemtableFlushes get called in the right order , 
 @ @ - 387 , 7 + 388 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 { 
 / / if we ' re not writing to the commit log , we are replaying the log , so marking 
 / / the log header with " you can discard anything written before the context " is not valid 
 - onMemtableFlush ( ctx ) ; 
 + CommitLog . instance ( ) . discardCompletedSegments ( table _ , columnFamily _ , ctx ) ; 
 } 
 } 
 } ) ; 
 @ @ - 533 , 19 + 534 , 6 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 } 
 
 / * 
 - * This method is called when the Memtable is frozen and ready to be flushed 
 - * to disk . This method informs the CommitLog that a particular ColumnFamily 
 - * is being flushed to disk . 
 - * / 
 - void onMemtableFlush ( CommitLog . CommitLogContext cLogCtx ) throws IOException 
 - { 
 - if ( cLogCtx . isValidContext ( ) ) 
 - { 
 - CommitLog . instance ( ) . onMemtableFlush ( table _ , columnFamily _ , cLogCtx ) ; 
 - } 
 - } 
 - 
 - / * 
 * Called after the Memtable flushes its in - memory data , or we add a file 
 * via bootstrap . This information is 
 * cached in the ColumnFamilyStore . This is useful for reads because the 
 diff - - git a / src / java / org / apache / cassandra / db / RecoveryManager . java b / src / java / org / apache / cassandra / db / RecoveryManager . java 
 index 1a1e2e1 . . 115a521 100644 
 - - - a / src / java / org / apache / cassandra / db / RecoveryManager . java 
 + + + b / src / java / org / apache / cassandra / db / RecoveryManager . java 
 @ @ - 53 , 7 + 53 , 7 @ @ public class RecoveryManager 
 
 Arrays . sort ( files , new FileUtils . FileComparator ( ) ) ; 
 logger _ . info ( " Replaying " + StringUtils . join ( files , " , " ) ) ; 
 - CommitLog . instance ( ) . recover ( files ) ; 
 + CommitLog . recover ( files ) ; 
 FileUtils . delete ( files ) ; 
 logger _ . info ( " Log replay complete " ) ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / Table . java b / src / java / org / apache / cassandra / db / Table . java 
 index f17d1bf . . 652dea5 100644 
 - - - a / src / java / org / apache / cassandra / db / Table . java 
 + + + b / src / java / org / apache / cassandra / db / Table . java 
 @ @ - 28 , 6 + 28 , 7 @ @ import com . google . common . base . Function ; 
 import com . google . common . collect . Iterables ; 
 import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . db . commitlog . CommitLog ; 
 + import org . apache . cassandra . db . commitlog . CommitLogSegment ; 
 import org . apache . cassandra . dht . Range ; 
 import org . apache . cassandra . io . SSTableDeletingReference ; 
 import org . apache . cassandra . io . SSTableReader ; 
 @ @ - 396 , 7 + 397 , 7 @ @ public class Table 
 { 
 if ( writeCommitLog ) 
 { 
 - Future < CommitLog . CommitLogContext > future = CommitLog . instance ( ) . add ( mutation , serializedMutation ) ; 
 + Future < CommitLogSegment . CommitLogContext > future = CommitLog . instance ( ) . add ( mutation , serializedMutation ) ; 
 if ( waitForCommitLog ) 
 { 
 try 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLog . java b / src / java / org / apache / cassandra / db / commitlog / CommitLog . java 
 index 0aac0ae . . 63ab353 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLog . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLog . java 
 @ @ - 23 , 9 + 23 , 7 @ @ import org . apache . cassandra . db . ColumnFamily ; 
 import org . apache . cassandra . db . RowMutation ; 
 import org . apache . cassandra . db . Table ; 
 import org . apache . cassandra . io . util . BufferedRandomAccessFile ; 
 - import org . apache . cassandra . io . util . DataOutputBuffer ; 
 import org . apache . cassandra . io . DeletionService ; 
 - import org . apache . cassandra . utils . FBUtilities ; 
 import org . apache . cassandra . utils . WrappedRunnable ; 
 import org . apache . cassandra . concurrent . StageManager ; 
 
 @ @ - 68 , 15 + 66 , 12 @ @ import java . util . concurrent . Future ; 
 * inherited the old ' s dirty bitflags , getting a zero for any given bit in the anding 
 * means that either the CF was clean in the old CL or it has been flushed since the 
 * switch in the new . ) 
 - * 
 - * The CommitLog class itself is " mostly a singleton . " open ( ) always returns one 
 - * instance , but log replay will bypass that . 
 * / 
 public class CommitLog 
 { 
 private static volatile int SEGMENT _ SIZE = 128 * 1024 * 1024 ; / / roll after log gets this big 
 + 
 private static final Logger logger = Logger . getLogger ( CommitLog . class ) ; 
 - private static final Map < String , CommitLogHeader > clHeaders = new HashMap < String , CommitLogHeader > ( ) ; 
 
 public static CommitLog instance ( ) 
 { 
 @ @ - 88 , 81 + 83 , 21 @ @ public class CommitLog 
 public static final CommitLog instance = new CommitLog ( ) ; 
 } 
 
 - public static class CommitLogContext 
 - { 
 - / * Commit Log associated with this operation * / 
 - public final String file ; 
 - / * Offset within the Commit Log where this row as added * / 
 - public final long position ; 
 - 
 - public CommitLogContext ( String file , long position ) 
 - { 
 - this . file = file ; 
 - this . position = position ; 
 - } 
 - 
 - public boolean isValidContext ( ) 
 - { 
 - return ( position ! = - 1L ) ; 
 - } 
 - 
 - @ Override 
 - public String toString ( ) 
 - { 
 - return " CommitLogContext ( " + 
 - " file = ' " + file + ' \ ' ' + 
 - " , position = " + position + 
 - ' ) ' ; 
 - } 
 - } 
 - 
 - public static class CommitLogFileComparator implements Comparator < String > 
 - { 
 - public int compare ( String f , String f2 ) 
 - { 
 - return ( int ) ( getCreationTime ( f ) - getCreationTime ( f2 ) ) ; 
 - } 
 - } 
 + private final Deque < CommitLogSegment > segments = new ArrayDeque < CommitLogSegment > ( ) ; 
 
 public static void setSegmentSize ( int size ) 
 { 
 SEGMENT _ SIZE = size ; 
 } 
 
 - public static int getSegmentCount ( ) 
 + public int getSegmentCount ( ) 
 { 
 - return clHeaders . size ( ) ; 
 - } 
 - 
 - static long getCreationTime ( String file ) 
 - { 
 - String [ ] entries = FBUtilities . strip ( file , " - . " ) ; 
 - return Long . parseLong ( entries [ entries . length - 2 ] ) ; 
 - } 
 - 
 - private static BufferedRandomAccessFile createWriter ( String file ) throws IOException 
 - { 
 - return new BufferedRandomAccessFile ( file , " rw " ) ; 
 + return segments . size ( ) ; 
 } 
 
 - / * Current commit log file * / 
 - private String logFile ; 
 - / * header for current commit log * / 
 - private CommitLogHeader clHeader ; 
 - private BufferedRandomAccessFile logWriter ; 
 private final ExecutorService executor = new CommitLogExecutorService ( ) ; 
 
 - / * 
 - * Generates a file name of the format CommitLog - < table > - < timestamp > . log in the 
 - * directory specified by the Database Descriptor . 
 - * / 
 - private void setNextFileName ( ) 
 - { 
 - logFile = DatabaseDescriptor . getLogFileLocation ( ) + File . separator + 
 - " CommitLog - " + System . currentTimeMillis ( ) + " . log " ; 
 - } 
 - 
 - / * 
 + / * * 
 * param @ table - name of table for which we are maintaining 
 * this commit log . 
 * param @ recoverymode - is commit log being instantiated in 
 @ @ - 170 , 17 + 105 , 11 @ @ public class CommitLog 
 * / 
 private CommitLog ( ) 
 { 
 - setNextFileName ( ) ; 
 - try 
 - { 
 - logWriter = CommitLog . createWriter ( logFile ) ; 
 - writeCommitLogHeader ( ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - throw new IOError ( e ) ; 
 - } 
 - 
 + / / all old segments are recovered and deleted before CommitLog is instantiated . 
 + / / All we need to do is create a new one . 
 + int cfSize = Table . TableMetadata . getColumnFamilyCount ( ) ; 
 + segments . add ( new CommitLogSegment ( cfSize ) ) ; 
 + 
 if ( DatabaseDescriptor . getCommitLogSync ( ) = = DatabaseDescriptor . CommitLogSync . periodic ) 
 { 
 final Runnable syncer = new WrappedRunnable ( ) 
 @ @ - 216 , 50 + 145 , 7 @ @ public class CommitLog 
 } 
 } 
 
 - String getLogFile ( ) 
 - { 
 - return logFile ; 
 - } 
 - 
 - private CommitLogHeader readCommitLogHeader ( BufferedRandomAccessFile logReader ) throws IOException 
 - { 
 - int size = ( int ) logReader . readLong ( ) ; 
 - byte [ ] bytes = new byte [ size ] ; 
 - logReader . read ( bytes ) ; 
 - ByteArrayInputStream byteStream = new ByteArrayInputStream ( bytes ) ; 
 - return CommitLogHeader . serializer ( ) . deserialize ( new DataInputStream ( byteStream ) ) ; 
 - } 
 - 
 - / * 
 - * This is invoked on startup via the ctor . It basically 
 - * writes a header with all bits set to zero . 
 - * / 
 - private void writeCommitLogHeader ( ) throws IOException 
 - { 
 - int cfSize = Table . TableMetadata . getColumnFamilyCount ( ) ; 
 - clHeader = new CommitLogHeader ( cfSize ) ; 
 - writeCommitLogHeader ( logWriter , clHeader . toByteArray ( ) ) ; 
 - } 
 - 
 - / * * writes header at the beginning of the file , then seeks back to current position * / 
 - private void seekAndWriteCommitLogHeader ( byte [ ] bytes ) throws IOException 
 - { 
 - long currentPos = logWriter . getFilePointer ( ) ; 
 - logWriter . seek ( 0 ) ; 
 - 
 - writeCommitLogHeader ( logWriter , bytes ) ; 
 - 
 - logWriter . seek ( currentPos ) ; 
 - } 
 - 
 - private static void writeCommitLogHeader ( BufferedRandomAccessFile logWriter , byte [ ] bytes ) throws IOException 
 - { 
 - logWriter . writeLong ( bytes . length ) ; 
 - logWriter . write ( bytes ) ; 
 - logWriter . sync ( ) ; 
 - } 
 - 
 - public void recover ( File [ ] clogs ) throws IOException 
 + public static void recover ( File [ ] clogs ) throws IOException 
 { 
 Set < Table > tablesRecovered = new HashSet < Table > ( ) ; 
 assert StageManager . getStage ( StageManager . MUTATION _ STAGE ) . getCompletedTaskCount ( ) = = 0 ; 
 @ @ - 268 , 7 + 154 , 7 @ @ public class CommitLog 
 { 
 int bufferSize = ( int ) Math . min ( file . length ( ) , 32 * 1024 * 1024 ) ; 
 BufferedRandomAccessFile reader = new BufferedRandomAccessFile ( file . getAbsolutePath ( ) , " r " , bufferSize ) ; 
 - final CommitLogHeader clHeader = readCommitLogHeader ( reader ) ; 
 + final CommitLogHeader clHeader = CommitLogHeader . readCommitLogHeader ( reader ) ; 
 / * seek to the lowest position where any CF has non - flushed data * / 
 int lowPos = CommitLogHeader . getLowestPosition ( clHeader ) ; 
 if ( lowPos = = 0 ) 
 @ @ - 357 , 13 + 243 , 13 @ @ public class CommitLog 
 } 
 } 
 
 - / / flush replayed tables , allowing commitlog segments to be removed 
 + / / flush replayed tables 
 List < Future < ? > > futures = new ArrayList < Future < ? > > ( ) ; 
 for ( Table table : tablesRecovered ) 
 { 
 futures . addAll ( table . flush ( ) ) ; 
 } 
 - / / wait for flushes to finish before continuing with startup 
 + / / wait for flushes to finish 
 for ( Future < ? > future : futures ) 
 { 
 try 
 @ @ - 377 , 31 + 263 , 18 @ @ public class CommitLog 
 } 
 } 
 
 - / * 
 - * Update the header of the commit log if a new column family 
 - * is encountered for the first time . 
 - * / 
 - private void maybeUpdateHeader ( RowMutation rm ) throws IOException 
 + private CommitLogSegment currentSegment ( ) 
 { 
 - Table table = Table . open ( rm . getTable ( ) ) ; 
 - for ( ColumnFamily columnFamily : rm . getColumnFamilies ( ) ) 
 - { 
 - int id = table . getColumnFamilyId ( columnFamily . name ( ) ) ; 
 - if ( ! clHeader . isDirty ( id ) ) 
 - { 
 - clHeader . turnOn ( id , logWriter . getFilePointer ( ) ) ; 
 - seekAndWriteCommitLogHeader ( clHeader . toByteArray ( ) ) ; 
 - } 
 - } 
 + return segments . getLast ( ) ; 
 } 
 
 - public CommitLogContext getContext ( ) throws IOException 
 + public CommitLogSegment . CommitLogContext getContext ( ) throws IOException 
 { 
 - Callable < CommitLogContext > task = new Callable < CommitLogContext > ( ) 
 + Callable < CommitLogSegment . CommitLogContext > task = new Callable < CommitLogSegment . CommitLogContext > ( ) 
 { 
 - public CommitLogContext call ( ) throws Exception 
 + public CommitLogSegment . CommitLogContext call ( ) throws Exception 
 { 
 - return new CommitLogContext ( logFile , logWriter . getFilePointer ( ) ) ; 
 + return currentSegment ( ) . getContext ( ) ; 
 } 
 } ; 
 try 
 @ @ - 424 , 9 + 297 , 9 @ @ public class CommitLog 
 * of any problems . This way we can assume that the subsequent commit log 
 * entry will override the garbage left over by the previous write . 
 * / 
 - public Future < CommitLogContext > add ( RowMutation rowMutation , Object serializedRow ) throws IOException 
 + public Future < CommitLogSegment . CommitLogContext > add ( RowMutation rowMutation , Object serializedRow ) throws IOException 
 { 
 - Callable < CommitLogContext > task = new LogRecordAdder ( rowMutation , serializedRow ) ; 
 + Callable < CommitLogSegment . CommitLogContext > task = new LogRecordAdder ( rowMutation , serializedRow ) ; 
 return executor . submit ( task ) ; 
 } 
 
 @ @ - 436 , 15 + 309 , 14 @ @ public class CommitLog 
 * The bit flag associated with this column family is set in the 
 * header and this is used to decide if the log file can be deleted . 
 * / 
 - public void onMemtableFlush ( final String tableName , final String cf , final CommitLog . CommitLogContext cLogCtx ) throws IOException 
 + public void discardCompletedSegments ( final String tableName , final String cf , final CommitLogSegment . CommitLogContext context ) throws IOException 
 { 
 Callable task = new Callable ( ) 
 { 
 public Object call ( ) throws IOException 
 { 
 - Table table = Table . open ( tableName ) ; 
 - int id = table . getColumnFamilyId ( cf ) ; 
 - discardCompletedSegments ( cLogCtx , id ) ; 
 + int id = Table . open ( tableName ) . getColumnFamilyId ( cf ) ; 
 + discardCompletedSegmentsInternal ( context , id ) ; 
 return null ; 
 } 
 } ; 
 @ @ - 462 , 41 + 334 , 23 @ @ public class CommitLog 
 } 
 } 
 
 - / * 
 - * Delete log segments whose contents have been turned into SSTables . 
 + / * * 
 + * Delete log segments whose contents have been turned into SSTables . NOT threadsafe . 
 * 
 - * param @ cLogCtx The commitLog context . 
 + * param @ context The commitLog context . 
 * param @ id id of the columnFamily being flushed to disk . 
 * 
 * / 
 - private void discardCompletedSegments ( CommitLog . CommitLogContext cLogCtx , int id ) throws IOException 
 + private void discardCompletedSegmentsInternal ( CommitLogSegment . CommitLogContext context , int id ) throws IOException 
 { 
 if ( logger . isDebugEnabled ( ) ) 
 - logger . debug ( " discard completed log segments for " + cLogCtx + " , column family " + id + " . CFIDs are " + Table . TableMetadata . getColumnFamilyIDString ( ) ) ; 
 - / * retrieve the commit log header associated with the file in the context * / 
 - if ( clHeaders . get ( cLogCtx . file ) = = null ) 
 - { 
 - if ( logFile . equals ( cLogCtx . file ) ) 
 - { 
 - / * this means we are dealing with the current commit log . * / 
 - clHeaders . put ( cLogCtx . file , clHeader ) ; 
 - } 
 - else 
 - { 
 - logger . error ( " Unknown commitlog file " + cLogCtx . file ) ; 
 - return ; 
 - } 
 - } 
 + logger . debug ( " discard completed log segments for " + context + " , column family " + id + " . CFIDs are " + Table . TableMetadata . getColumnFamilyIDString ( ) ) ; 
 
 / * 
 * log replay assumes that we only have to look at entries past the last 
 * flush position , so verify that this flush happens after the last . 
 * / 
 - assert cLogCtx . position > = clHeaders . get ( cLogCtx . file ) . getPosition ( id ) ; 
 - 
 - / * Sort the commit logs based on creation time * / 
 - List < String > oldFiles = new ArrayList < String > ( clHeaders . keySet ( ) ) ; 
 - Collections . sort ( oldFiles , new CommitLogFileComparator ( ) ) ; 
 + assert context . position > context . getSegment ( ) . getHeader ( ) . getPosition ( id ) : " discard called on obsolete context " + context ; 
 
 / * 
 * Loop through all the commit log files in the history . Now process 
 @ @ - 504 , 78 + 358 , 47 @ @ public class CommitLog 
 * these files the header needs to modified by resetting the dirty 
 * bit corresponding to the flushed CF . 
 * / 
 - for ( String oldFile : oldFiles ) 
 + for ( CommitLogSegment segment : segments ) 
 { 
 - CommitLogHeader header = clHeaders . get ( oldFile ) ; 
 - if ( oldFile . equals ( cLogCtx . file ) ) 
 + CommitLogHeader header = segment . getHeader ( ) ; 
 + if ( segment . equals ( context . getSegment ( ) ) ) 
 { 
 / / we can ' t just mark the segment where the flush happened clean , 
 / / since there may have been writes to it between when the flush 
 / / started and when it finished . so mark the flush position as 
 / / the replay point for this CF , instead . 
 if ( logger . isDebugEnabled ( ) ) 
 - logger . debug ( " Marking replay position " + cLogCtx . position + " on commit log " + oldFile ) ; 
 - header . turnOn ( id , cLogCtx . position ) ; 
 - if ( oldFile . equals ( logFile ) ) 
 - { 
 - seekAndWriteCommitLogHeader ( header . toByteArray ( ) ) ; 
 - } 
 - else 
 - { 
 - writeOldCommitLogHeader ( oldFile , header ) ; 
 - } 
 + logger . debug ( " Marking replay position " + context . position + " on commit log " + segment ) ; 
 + header . turnOn ( id , context . position ) ; 
 + segment . writeHeader ( ) ; 
 break ; 
 } 
 
 header . turnOff ( id ) ; 
 if ( header . isSafeToDelete ( ) ) 
 { 
 - logger . info ( " Deleting obsolete commit log : " + oldFile ) ; 
 - DeletionService . submitDelete ( oldFile ) ; 
 - clHeaders . remove ( oldFile ) ; 
 + logger . info ( " Discarding obsolete commit log : " + segment ) ; 
 + segment . close ( ) ; 
 + DeletionService . submitDelete ( segment . getPath ( ) ) ; 
 + / / usually this will be the first ( remaining ) segment , but not always , if segment A contains 
 + / / writes to a CF that is unflushed but is followed by segment B whose CFs are all flushed . 
 + segments . remove ( segment ) ; 
 } 
 else 
 { 
 if ( logger . isDebugEnabled ( ) ) 
 - logger . debug ( " Not safe to delete commit log " + oldFile + " ; dirty is " + header . dirtyString ( ) ) ; 
 - writeOldCommitLogHeader ( oldFile , header ) ; 
 + logger . debug ( " Not safe to delete commit log " + segment + " ; dirty is " + header . dirtyString ( ) ) ; 
 + segment . writeHeader ( ) ; 
 } 
 } 
 } 
 
 - private void writeOldCommitLogHeader ( String oldFile , CommitLogHeader header ) throws IOException 
 - { 
 - BufferedRandomAccessFile logWriter = CommitLog . createWriter ( oldFile ) ; 
 - writeCommitLogHeader ( logWriter , header . toByteArray ( ) ) ; 
 - logWriter . close ( ) ; 
 - } 
 - 
 - private boolean maybeRollLog ( ) throws IOException 
 - { 
 - if ( logWriter . length ( ) > = SEGMENT _ SIZE ) 
 - { 
 - / * Rolls the current log file over to a new one . * / 
 - setNextFileName ( ) ; 
 - String oldLogFile = logWriter . getPath ( ) ; 
 - logWriter . close ( ) ; 
 - 
 - / * point reader / writer to a new commit log file . * / 
 - logWriter = CommitLog . createWriter ( logFile ) ; 
 - / * squirrel away the old commit log header * / 
 - clHeaders . put ( oldLogFile , new CommitLogHeader ( clHeader ) ) ; 
 - clHeader . clear ( ) ; 
 - writeCommitLogHeader ( logWriter , clHeader . toByteArray ( ) ) ; 
 - return true ; 
 - } 
 - return false ; 
 - } 
 - 
 void sync ( ) throws IOException 
 { 
 - logWriter . sync ( ) ; 
 + currentSegment ( ) . sync ( ) ; 
 } 
 
 - class LogRecordAdder implements Callable < CommitLog . CommitLogContext > 
 + class LogRecordAdder implements Callable < CommitLogSegment . CommitLogContext > 
 { 
 final RowMutation rowMutation ; 
 final Object serializedRow ; 
 @ @ - 586 , 40 + 409 , 18 @ @ public class CommitLog 
 this . serializedRow = serializedRow ; 
 } 
 
 - public CommitLog . CommitLogContext call ( ) throws Exception 
 + public CommitLogSegment . CommitLogContext call ( ) throws Exception 
 { 
 - long currentPosition = - 1L ; 
 - try 
 - { 
 - currentPosition = logWriter . getFilePointer ( ) ; 
 - CommitLogContext cLogCtx = new CommitLogContext ( logFile , currentPosition ) ; 
 - maybeUpdateHeader ( rowMutation ) ; 
 - Checksum checkum = new CRC32 ( ) ; 
 - if ( serializedRow instanceof DataOutputBuffer ) 
 - { 
 - DataOutputBuffer buffer = ( DataOutputBuffer ) serializedRow ; 
 - logWriter . writeLong ( buffer . getLength ( ) ) ; 
 - logWriter . write ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; 
 - checkum . update ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; 
 - } 
 - else 
 - { 
 - assert serializedRow instanceof byte [ ] ; 
 - byte [ ] bytes = ( byte [ ] ) serializedRow ; 
 - logWriter . writeLong ( bytes . length ) ; 
 - logWriter . write ( bytes ) ; 
 - checkum . update ( bytes , 0 , bytes . length ) ; 
 - } 
 - logWriter . writeLong ( checkum . getValue ( ) ) ; 
 - maybeRollLog ( ) ; 
 - return cLogCtx ; 
 - } 
 - catch ( IOException e ) 
 + CommitLogSegment . CommitLogContext context = currentSegment ( ) . write ( rowMutation , serializedRow ) ; 
 + 
 + / / roll log if necessary 
 + if ( currentSegment ( ) . length ( ) > = SEGMENT _ SIZE ) 
 { 
 - if ( currentPosition ! = - 1 ) 
 - logWriter . seek ( currentPosition ) ; 
 - throw e ; 
 + sync ( ) ; 
 + segments . add ( new CommitLogSegment ( currentSegment ( ) . getHeader ( ) . getColumnFamilyCount ( ) ) ) ; 
 } 
 + 
 + return context ; 
 } 
 } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogExecutorService . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogExecutorService . java 
 index f0675c0 . . b9145a7 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogExecutorService . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogExecutorService . java 
 @ @ - 11 , 9 + 11 , 9 @ @ import javax . management . ObjectName ; 
 import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . utils . WrappedRunnable ; 
 
 - public class CommitLogExecutorService extends AbstractExecutorService implements CommitLogExecutorServiceMBean 
 + class CommitLogExecutorService extends AbstractExecutorService implements CommitLogExecutorServiceMBean 
 { 
 - BlockingQueue < CheaterFutureTask > queue ; 
 + private final BlockingQueue < CheaterFutureTask > queue ; 
 
 private volatile long completedTaskCount = 0 ; 
 
 @ @ - 92 , 8 + 92 , 8 @ @ public class CommitLogExecutorService extends AbstractExecutorService implements 
 queue . take ( ) . run ( ) ; 
 } 
 
 - private ArrayList < CheaterFutureTask > incompleteTasks = new ArrayList < CheaterFutureTask > ( ) ; 
 - private ArrayList taskValues = new ArrayList ( ) ; / / TODO not sure how to generify this 
 + private final ArrayList < CheaterFutureTask > incompleteTasks = new ArrayList < CheaterFutureTask > ( ) ; 
 + private final ArrayList taskValues = new ArrayList ( ) ; / / TODO not sure how to generify this 
 private void processWithSyncBatch ( ) throws Exception 
 { 
 CheaterFutureTask firstTask = queue . take ( ) ; 
 @ @ - 188 , 26 + 188 , 26 @ @ public class CommitLogExecutorService extends AbstractExecutorService implements 
 { 
 throw new UnsupportedOperationException ( ) ; 
 } 
 - } 
 - 
 - class CheaterFutureTask < V > extends FutureTask < V > 
 - { 
 - private Callable rawCallable ; 
 
 - public CheaterFutureTask ( Callable < V > callable ) 
 + private static class CheaterFutureTask < V > extends FutureTask < V > 
 { 
 - super ( callable ) ; 
 - rawCallable = callable ; 
 - } 
 + private final Callable rawCallable ; 
 
 - public Callable getRawCallable ( ) 
 - { 
 - return rawCallable ; 
 - } 
 + public CheaterFutureTask ( Callable < V > callable ) 
 + { 
 + super ( callable ) ; 
 + rawCallable = callable ; 
 + } 
 
 - @ Override 
 - public void set ( V v ) 
 - { 
 - super . set ( v ) ; 
 + public Callable getRawCallable ( ) 
 + { 
 + return rawCallable ; 
 + } 
 + 
 + @ Override 
 + public void set ( V v ) 
 + { 
 + super . set ( v ) ; 
 + } 
 } 
 - } 
 \ No newline at end of file 
 + } 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java 
 index 91adaea . . a80a55a 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java 
 @ @ - 24 , 6 + 24 , 7 @ @ import java . util . Arrays ; 
 
 import org . apache . cassandra . db . Table ; 
 import org . apache . cassandra . io . ICompactSerializer ; 
 + import org . apache . cassandra . io . util . BufferedRandomAccessFile ; 
 import org . apache . cassandra . utils . BitSetSerializer ; 
 
 class CommitLogHeader 
 @ @ - 70 , 14 + 71 , 7 @ @ class CommitLogHeader 
 this . dirty = dirty ; 
 this . lastFlushedAt = lastFlushedAt ; 
 } 
 - 
 - CommitLogHeader ( CommitLogHeader clHeader ) 
 - { 
 - dirty = ( BitSet ) clHeader . dirty . clone ( ) ; 
 - lastFlushedAt = new int [ clHeader . lastFlushedAt . length ] ; 
 - System . arraycopy ( clHeader . lastFlushedAt , 0 , lastFlushedAt , 0 , lastFlushedAt . length ) ; 
 - } 
 - 
 + 
 boolean isDirty ( int index ) 
 { 
 return dirty . get ( index ) ; 
 @ @ - 105 , 12 + 99 , 6 @ @ class CommitLogHeader 
 return dirty . isEmpty ( ) ; 
 } 
 
 - void clear ( ) 
 - { 
 - dirty . clear ( ) ; 
 - Arrays . fill ( lastFlushedAt , 0 ) ; 
 - } 
 - 
 byte [ ] toByteArray ( ) throws IOException 
 { 
 ByteArrayOutputStream bos = new ByteArrayOutputStream ( ) ; 
 @ @ - 152 , 6 + 140 , 20 @ @ class CommitLogHeader 
 return sb . toString ( ) ; 
 } 
 
 + static CommitLogHeader readCommitLogHeader ( BufferedRandomAccessFile logReader ) throws IOException 
 + { 
 + int size = ( int ) logReader . readLong ( ) ; 
 + byte [ ] bytes = new byte [ size ] ; 
 + logReader . read ( bytes ) ; 
 + ByteArrayInputStream byteStream = new ByteArrayInputStream ( bytes ) ; 
 + return serializer ( ) . deserialize ( new DataInputStream ( byteStream ) ) ; 
 + } 
 + 
 + public int getColumnFamilyCount ( ) 
 + { 
 + return lastFlushedAt . length ; 
 + } 
 + 
 static class CommitLogHeaderSerializer implements ICompactSerializer < CommitLogHeader > 
 { 
 public void serialize ( CommitLogHeader clHeader , DataOutputStream dos ) throws IOException 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java 
 new file mode 100644 
 index 0000000 . . d0e3d50 
 - - - / dev / null 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java 
 @ @ - 0 , 0 + 1 , 193 @ @ 
 + package org . apache . cassandra . db . commitlog ; 
 + 
 + import java . io . File ; 
 + import java . io . IOError ; 
 + import java . io . IOException ; 
 + import java . util . zip . CRC32 ; 
 + import java . util . zip . Checksum ; 
 + 
 + import org . apache . log4j . Logger ; 
 + 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 + import org . apache . cassandra . db . ColumnFamily ; 
 + import org . apache . cassandra . db . RowMutation ; 
 + import org . apache . cassandra . db . Table ; 
 + import org . apache . cassandra . io . util . BufferedRandomAccessFile ; 
 + import org . apache . cassandra . io . util . DataOutputBuffer ; 
 + 
 + public class CommitLogSegment 
 + { 
 + private static final Logger logger = Logger . getLogger ( CommitLogSegment . class ) ; 
 + 
 + private final BufferedRandomAccessFile logWriter ; 
 + private final CommitLogHeader header ; 
 + 
 + public CommitLogSegment ( int cfCount ) 
 + { 
 + this . header = new CommitLogHeader ( cfCount ) ; 
 + String logFile = DatabaseDescriptor . getLogFileLocation ( ) + File . separator + " CommitLog - " + System . currentTimeMillis ( ) + " . log " ; 
 + logger . info ( " Creating new commitlog segment " + logFile ) ; 
 + 
 + try 
 + { 
 + logWriter = createWriter ( logFile ) ; 
 + writeCommitLogHeader ( header . toByteArray ( ) ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + throw new IOError ( e ) ; 
 + } 
 + } 
 + 
 + public void writeHeader ( ) throws IOException 
 + { 
 + seekAndWriteCommitLogHeader ( header . toByteArray ( ) ) ; 
 + } 
 + 
 + / * * writes header at the beginning of the file , then seeks back to current position * / 
 + void seekAndWriteCommitLogHeader ( byte [ ] bytes ) throws IOException 
 + { 
 + long currentPos = logWriter . getFilePointer ( ) ; 
 + logWriter . seek ( 0 ) ; 
 + 
 + writeCommitLogHeader ( bytes ) ; 
 + 
 + logWriter . seek ( currentPos ) ; 
 + } 
 + 
 + private void writeCommitLogHeader ( byte [ ] bytes ) throws IOException 
 + { 
 + logWriter . writeLong ( bytes . length ) ; 
 + logWriter . write ( bytes ) ; 
 + logWriter . sync ( ) ; 
 + } 
 + 
 + private static BufferedRandomAccessFile createWriter ( String file ) throws IOException 
 + { 
 + return new BufferedRandomAccessFile ( file , " rw " , 128 * 1024 ) ; 
 + } 
 + 
 + public CommitLogSegment . CommitLogContext write ( RowMutation rowMutation , Object serializedRow ) throws IOException 
 + { 
 + long currentPosition = - 1L ; 
 + try 
 + { 
 + currentPosition = logWriter . getFilePointer ( ) ; 
 + CommitLogSegment . CommitLogContext cLogCtx = new CommitLogSegment . CommitLogContext ( currentPosition ) ; 
 + Table table = Table . open ( rowMutation . getTable ( ) ) ; 
 + 
 + / / update header 
 + for ( ColumnFamily columnFamily : rowMutation . getColumnFamilies ( ) ) 
 + { 
 + int id = table . getColumnFamilyId ( columnFamily . name ( ) ) ; 
 + if ( ! header . isDirty ( id ) ) 
 + { 
 + header . turnOn ( id , logWriter . getFilePointer ( ) ) ; 
 + seekAndWriteCommitLogHeader ( header . toByteArray ( ) ) ; 
 + } 
 + } 
 + 
 + / / write mutation , w / checksum 
 + Checksum checkum = new CRC32 ( ) ; 
 + if ( serializedRow instanceof DataOutputBuffer ) 
 + { 
 + DataOutputBuffer buffer = ( DataOutputBuffer ) serializedRow ; 
 + logWriter . writeLong ( buffer . getLength ( ) ) ; 
 + logWriter . write ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; 
 + checkum . update ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; 
 + } 
 + else 
 + { 
 + assert serializedRow instanceof byte [ ] ; 
 + byte [ ] bytes = ( byte [ ] ) serializedRow ; 
 + logWriter . writeLong ( bytes . length ) ; 
 + logWriter . write ( bytes ) ; 
 + checkum . update ( bytes , 0 , bytes . length ) ; 
 + } 
 + logWriter . writeLong ( checkum . getValue ( ) ) ; 
 + 
 + return cLogCtx ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + if ( currentPosition ! = - 1 ) 
 + logWriter . seek ( currentPosition ) ; 
 + throw e ; 
 + } 
 + } 
 + 
 + public void sync ( ) throws IOException 
 + { 
 + logWriter . sync ( ) ; 
 + } 
 + 
 + public CommitLogContext getContext ( ) 
 + { 
 + return new CommitLogContext ( logWriter . getFilePointer ( ) ) ; 
 + } 
 + 
 + public CommitLogHeader getHeader ( ) 
 + { 
 + return header ; 
 + } 
 + 
 + public String getPath ( ) 
 + { 
 + return logWriter . getPath ( ) ; 
 + } 
 + 
 + public long length ( ) 
 + { 
 + try 
 + { 
 + return logWriter . length ( ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + throw new IOError ( e ) ; 
 + } 
 + } 
 + 
 + public void close ( ) 
 + { 
 + try 
 + { 
 + logWriter . close ( ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + throw new IOError ( e ) ; 
 + } 
 + } 
 + 
 + @ Override 
 + public String toString ( ) 
 + { 
 + return " CommitLogSegment ( " + logWriter . getPath ( ) + ' ) ' ; 
 + } 
 + 
 + public class CommitLogContext 
 + { 
 + public final long position ; 
 + 
 + public CommitLogContext ( long position ) 
 + { 
 + assert position > = 0 ; 
 + this . position = position ; 
 + } 
 + 
 + public CommitLogSegment getSegment ( ) 
 + { 
 + return CommitLogSegment . this ; 
 + } 
 + 
 + @ Override 
 + public String toString ( ) 
 + { 
 + return " CommitLogContext ( " + 
 + " file = ' " + logWriter . getPath ( ) + ' \ ' ' + 
 + " , position = " + position + 
 + ' ) ' ; 
 + } 
 + } 
 + } 
 diff - - git a / test / unit / org / apache / cassandra / db / CommitLogTest . java b / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 index 24f81f9 . . dcff17d 100644 
 - - - a / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 + + + b / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 @ @ - 32 , 7 + 32 , 7 @ @ public class CommitLogTest extends CleanupHelper 
 @ Test 
 public void testCleanup ( ) throws IOException , ExecutionException , InterruptedException 
 { 
 - assert CommitLog . getSegmentCount ( ) = = 0 ; 
 + assert CommitLog . instance ( ) . getSegmentCount ( ) = = 1 ; 
 CommitLog . setSegmentSize ( 1000 ) ; 
 
 Table table = Table . open ( " Keyspace1 " ) ; 
 @ @ - 49 , 14 + 49 , 14 @ @ public class CommitLogTest extends CleanupHelper 
 rm . add ( new QueryPath ( " Standard2 " , null , " Column1 " . getBytes ( ) ) , value , 0 ) ; 
 rm . apply ( ) ; 
 } 
 - assert CommitLog . getSegmentCount ( ) > 1 ; 
 + assert CommitLog . instance ( ) . getSegmentCount ( ) > 1 ; 
 
 / / nothing should get removed after flushing just Standard1 
 store1 . forceBlockingFlush ( ) ; 
 - assert CommitLog . getSegmentCount ( ) > 1 ; 
 + assert CommitLog . instance ( ) . getSegmentCount ( ) > 1 ; 
 
 / / after flushing Standard2 we should be able to clean out all segments 
 store2 . forceBlockingFlush ( ) ; 
 - assert CommitLog . getSegmentCount ( ) = = 1 ; 
 + assert CommitLog . instance ( ) . getSegmentCount ( ) = = 1 ; 
 } 
 }
