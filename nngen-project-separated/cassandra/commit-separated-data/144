BLEU SCORE: 0.041961149062965476

TEST MSG: Fix compaction failure caused by reading un - flushed data
GENERATED MSG: Add block level checksum for compressed data

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 5f6189f . . 22ee346 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 2 . 2 . 13 <nl> + * Fix compaction failure caused by reading un - flushed data ( CASSANDRA - 12743 ) <nl> * Use Bounds instead of Range for sstables in anticompaction ( CASSANDRA - 14411 ) <nl> * Fix JSON queries with IN restrictions and ORDER BY clause ( CASSANDRA - 14286 ) <nl> * CQL fromJson ( null ) throws NullPointerException ( CASSANDRA - 13891 ) <nl> diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> index 9c7c776 . . a7f9bb4 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> @ @ - 129 , 7 + 129 , 7 @ @ public class CompressedSequentialWriter extends SequentialWriter <nl> / / write corresponding checksum <nl> compressed . rewind ( ) ; <nl> crcMetadata . appendDirect ( compressed , true ) ; <nl> - lastFlushOffset + = compressedLength + 4 ; <nl> + lastFlushOffset = uncompressedSize ; <nl> <nl> / / adjust our bufferOffset to account for the new uncompressed data we ' ve now written out <nl> resetBuffer ( ) ; <nl> @ @ - 235 , 10 + 235 , 23 @ @ public class CompressedSequentialWriter extends SequentialWriter <nl> chunkCount = realMark . nextChunkIndex - 1 ; <nl> <nl> / / truncate data and index file <nl> - truncate ( chunkOffset ) ; <nl> + truncate ( chunkOffset , bufferOffset ) ; <nl> metadataWriter . resetAndTruncate ( realMark . nextChunkIndex - 1 ) ; <nl> } <nl> <nl> + private void truncate ( long toFileSize , long toBufferOffset ) <nl> + { <nl> + try <nl> + { <nl> + channel . truncate ( toFileSize ) ; <nl> + lastFlushOffset = toBufferOffset ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + throw new FSWriteError ( e , getPath ( ) ) ; <nl> + } <nl> + } <nl> + <nl> / * * <nl> * Seek to the offset where next compressed data chunk should be stored . <nl> * / <nl> diff - - git a / src / java / org / apache / cassandra / io / util / SequentialWriter . java b / src / java / org / apache / cassandra / io / util / SequentialWriter . java <nl> index 0c39469 . . 452318e 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / SequentialWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / util / SequentialWriter . java <nl> @ @ - 430 , 6 + 430 , 7 @ @ public class SequentialWriter extends OutputStream implements WritableByteChanne <nl> throw new FSReadError ( e , getPath ( ) ) ; <nl> } <nl> <nl> + bufferOffset = truncateTarget ; <nl> resetBuffer ( ) ; <nl> } <nl> <nl> @ @ - 443 , 6 + 444 , 7 @ @ public class SequentialWriter extends OutputStream implements WritableByteChanne <nl> try <nl> { <nl> channel . truncate ( toSize ) ; <nl> + lastFlushOffset = toSize ; <nl> } <nl> catch ( IOException e ) <nl> { <nl> diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterReopenTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterReopenTest . java <nl> new file mode 100644 <nl> index 0000000 . . 33b4957 <nl> - - - / dev / null <nl> + + + b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterReopenTest . java <nl> @ @ - 0 , 0 + 1 , 153 @ @ <nl> + / * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , software <nl> + * distributed under the License is distributed on an " AS IS " BASIS , <nl> + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> + * See the License for the specific language governing permissions and <nl> + * limitations under the License . <nl> + * / <nl> + <nl> + package org . apache . cassandra . io . compress ; <nl> + <nl> + import java . io . IOException ; <nl> + import java . nio . ByteBuffer ; <nl> + import java . util . Arrays ; <nl> + import java . util . HashSet ; <nl> + import java . util . Map ; <nl> + import java . util . Random ; <nl> + import java . util . Set ; <nl> + <nl> + import org . junit . Test ; <nl> + <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + import org . apache . cassandra . cql3 . CQLTester ; <nl> + import org . apache . cassandra . db . ColumnFamilyStore ; <nl> + import org . apache . cassandra . db . Keyspace ; <nl> + import org . apache . cassandra . utils . ByteBufferUtil ; <nl> + <nl> + import static org . junit . Assert . assertEquals ; <nl> + <nl> + public class CompressedSequentialWriterReopenTest extends CQLTester <nl> + { <nl> + @ Test <nl> + public void badCompressor1 ( ) throws IOException <nl> + { <nl> + BadCompressor bad = new BadCompressor ( ) ; <nl> + byte [ ] test = new byte [ ] { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 } ; <nl> + byte [ ] out = new byte [ 10 ] ; <nl> + bad . uncompress ( test , 0 , 20 , out , 0 ) ; <nl> + for ( int i = 0 ; i < 10 ; i + + ) <nl> + assertEquals ( out [ i ] , ( byte ) i ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void badCompressor2 ( ) throws IOException <nl> + { <nl> + BadCompressor bad = new BadCompressor ( ) ; <nl> + ByteBuffer input = ByteBuffer . wrap ( new byte [ ] { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 } ) ; <nl> + ByteBuffer output = ByteBuffer . allocate ( 40 ) ; <nl> + bad . compress ( input , output ) ; <nl> + for ( int i = 0 ; i < 40 ; i + + ) <nl> + assertEquals ( i % 20 , output . get ( i ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void badCompressor3 ( ) throws IOException <nl> + { <nl> + BadCompressor bad = new BadCompressor ( ) ; <nl> + ByteBuffer input = ByteBuffer . wrap ( new byte [ ] { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 } ) ; <nl> + ByteBuffer output = ByteBuffer . allocate ( 10 ) ; <nl> + bad . uncompress ( input , output ) ; <nl> + for ( int i = 0 ; i < 10 ; i + + ) <nl> + assertEquals ( i , output . get ( i ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void compressionEnabled ( ) throws Throwable <nl> + { <nl> + createTable ( " create table % s ( id int primary key , t blob ) with compression = { ' sstable _ compression ' : ' org . apache . cassandra . io . compress . CompressedSequentialWriterReopenTest $ BadCompressor ' } " ) ; <nl> + byte [ ] blob = new byte [ 1000 ] ; <nl> + ( new Random ( ) ) . nextBytes ( blob ) ; <nl> + Keyspace keyspace = Keyspace . open ( keyspace ( ) ) ; <nl> + ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( currentTable ( ) ) ; <nl> + cfs . disableAutoCompaction ( ) ; <nl> + for ( int i = 0 ; i < 10000 ; i + + ) <nl> + { <nl> + execute ( " insert into % s ( id , t ) values ( ? , ? ) " , i , ByteBuffer . wrap ( blob ) ) ; <nl> + } <nl> + cfs . forceBlockingFlush ( ) ; <nl> + for ( int i = 0 ; i < 10000 ; i + + ) <nl> + { <nl> + execute ( " insert into % s ( id , t ) values ( ? , ? ) " , i , ByteBuffer . wrap ( blob ) ) ; <nl> + } <nl> + cfs . forceBlockingFlush ( ) ; <nl> + DatabaseDescriptor . setSSTablePreempiveOpenIntervalInMB ( 1 ) ; <nl> + cfs . forceMajorCompaction ( ) ; <nl> + } <nl> + <nl> + public static class BadCompressor implements ICompressor <nl> + { <nl> + public static ICompressor create ( Map < String , String > options ) <nl> + { <nl> + return new BadCompressor ( ) ; <nl> + } <nl> + <nl> + @ Override <nl> + public int initialCompressedBufferLength ( int chunkLength ) <nl> + { <nl> + return chunkLength * 2 ; <nl> + } <nl> + <nl> + @ Override <nl> + public int uncompress ( byte [ ] input , int inputOffset , int inputLength , byte [ ] output , int outputOffset ) throws IOException <nl> + { <nl> + System . arraycopy ( input , inputOffset , output , outputOffset , inputLength / 2 ) ; <nl> + return inputLength / 2 ; <nl> + } <nl> + <nl> + @ Override <nl> + public void compress ( ByteBuffer input , ByteBuffer output ) throws IOException <nl> + { <nl> + int len = input . remaining ( ) ; <nl> + byte [ ] arr = ByteBufferUtil . getArray ( input ) ; <nl> + output . put ( arr ) ; <nl> + output . put ( arr ) ; <nl> + input . position ( len ) ; <nl> + } <nl> + <nl> + @ Override <nl> + public void uncompress ( ByteBuffer input , ByteBuffer output ) throws IOException <nl> + { <nl> + byte [ ] arr = ByteBufferUtil . getArray ( input ) ; <nl> + output . put ( arr , 0 , arr . length / 2 ) ; <nl> + input . position ( arr . length ) ; <nl> + } <nl> + <nl> + @ Override <nl> + public BufferType preferredBufferType ( ) <nl> + { <nl> + return BufferType . ON _ HEAP ; <nl> + } <nl> + <nl> + @ Override <nl> + public boolean supports ( BufferType bufferType ) <nl> + { <nl> + return true ; <nl> + } <nl> + <nl> + @ Override <nl> + public Set < String > supportedOptions ( ) <nl> + { <nl> + return new HashSet < > ( Arrays . asList ( CompressionParameters . CRC _ CHECK _ CHANCE ) ) ; <nl> + } <nl> + } <nl> + } <nl> diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java <nl> index 43c44fd . . bca0354 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java <nl> @ @ - 17 , 6 + 17 , 7 @ @ <nl> * / <nl> package org . apache . cassandra . io . compress ; <nl> <nl> + import com . google . common . io . Files ; <nl> import java . io . ByteArrayInputStream ; <nl> import java . io . DataInputStream ; <nl> import java . io . File ; <nl> @ @ - 26 , 6 + 27 , 7 @ @ import java . util . * ; <nl> <nl> import static org . apache . commons . io . FileUtils . readFileToByteArray ; <nl> import static org . junit . Assert . assertEquals ; <nl> + import static org . junit . Assert . assertTrue ; <nl> <nl> import org . junit . After ; <nl> import org . junit . Test ; <nl> @ @ - 38 , 7 + 40 , 9 @ @ import org . apache . cassandra . db . marshal . UTF8Type ; <nl> import org . apache . cassandra . io . sstable . metadata . MetadataCollector ; <nl> import org . apache . cassandra . io . util . ChannelProxy ; <nl> import org . apache . cassandra . io . util . FileMark ; <nl> + import org . apache . cassandra . io . util . FileUtils ; <nl> import org . apache . cassandra . io . util . RandomAccessReader ; <nl> + import org . apache . cassandra . io . util . SequentialWriter ; <nl> import org . apache . cassandra . io . util . SequentialWriterTest ; <nl> <nl> public class CompressedSequentialWriterTest extends SequentialWriterTest <nl> @ @ - 107 , 6 + 111 , 12 @ @ public class CompressedSequentialWriterTest extends SequentialWriterTest <nl> { <nl> writer . write ( ( byte ) i ) ; <nl> } <nl> + <nl> + if ( bytesToTest < = CompressionParameters . DEFAULT _ CHUNK _ LENGTH ) <nl> + assertEquals ( writer . getLastFlushOffset ( ) , CompressionParameters . DEFAULT _ CHUNK _ LENGTH ) ; <nl> + else <nl> + assertTrue ( writer . getLastFlushOffset ( ) % CompressionParameters . DEFAULT _ CHUNK _ LENGTH = = 0 ) ; <nl> + <nl> writer . resetAndTruncate ( mark ) ; <nl> writer . write ( dataPost ) ; <nl> writer . finish ( ) ; <nl> @ @ - 155 , 6 + 165 , 48 @ @ public class CompressedSequentialWriterTest extends SequentialWriterTest <nl> writers . clear ( ) ; <nl> } <nl> <nl> + @ Test <nl> + @ Override <nl> + public void resetAndTruncateTest ( ) <nl> + { <nl> + File tempFile = new File ( Files . createTempDir ( ) , " reset . txt " ) ; <nl> + File offsetsFile = FileUtils . createTempFile ( " compressedsequentialwriter . offset " , " test " ) ; <nl> + final int bufferSize = 48 ; <nl> + final int writeSize = 64 ; <nl> + byte [ ] toWrite = new byte [ writeSize ] ; <nl> + <nl> + try ( SequentialWriter writer = new CompressedSequentialWriter ( tempFile , offsetsFile . getPath ( ) , <nl> + new CompressionParameters ( LZ4Compressor . instance ) , new MetadataCollector ( CellNames . fromAbstractType ( UTF8Type . instance , false ) ) ) ) <nl> + { <nl> + / / write bytes greather than buffer <nl> + writer . write ( toWrite ) ; <nl> + long flushedOffset = writer . getLastFlushOffset ( ) ; <nl> + assertEquals ( writeSize , writer . getFilePointer ( ) ) ; <nl> + / / mark thi position <nl> + FileMark pos = writer . mark ( ) ; <nl> + / / write another <nl> + writer . write ( toWrite ) ; <nl> + / / another buffer should be flushed <nl> + assertEquals ( flushedOffset * 2 , writer . getLastFlushOffset ( ) ) ; <nl> + assertEquals ( writeSize * 2 , writer . getFilePointer ( ) ) ; <nl> + / / reset writer <nl> + writer . resetAndTruncate ( pos ) ; <nl> + / / current position and flushed size should be changed <nl> + assertEquals ( writeSize , writer . getFilePointer ( ) ) ; <nl> + assertEquals ( flushedOffset , writer . getLastFlushOffset ( ) ) ; <nl> + / / write another byte less than buffer <nl> + writer . write ( new byte [ ] { 0 } ) ; <nl> + assertEquals ( writeSize + 1 , writer . getFilePointer ( ) ) ; <nl> + / / flush off set should not be increase <nl> + assertEquals ( flushedOffset , writer . getLastFlushOffset ( ) ) ; <nl> + writer . finish ( ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + Assert . fail ( ) ; <nl> + } <nl> + } <nl> + <nl> protected TestableTransaction newTest ( ) throws IOException <nl> { <nl> TestableCSW sw = new TestableCSW ( ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / io / util / SequentialWriterTest . java b / test / unit / org / apache / cassandra / io / util / SequentialWriterTest . java <nl> index fd38427 . . 15d6160 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / util / SequentialWriterTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / util / SequentialWriterTest . java <nl> @ @ - 36 , 6 + 36 , 7 @ @ import org . apache . cassandra . io . compress . BufferType ; <nl> import org . apache . cassandra . utils . concurrent . AbstractTransactionalTest ; <nl> <nl> import static org . apache . commons . io . FileUtils . * ; <nl> + import static org . junit . Assert . assertEquals ; <nl> <nl> public class SequentialWriterTest extends AbstractTransactionalTest <nl> { <nl> @ @ - 119 , 6 + 120 , 46 @ @ public class SequentialWriterTest extends AbstractTransactionalTest <nl> } <nl> } <nl> <nl> + @ Test <nl> + public void resetAndTruncateTest ( ) <nl> + { <nl> + File tempFile = new File ( Files . createTempDir ( ) , " reset . txt " ) ; <nl> + final int bufferSize = 48 ; <nl> + final int writeSize = 64 ; <nl> + byte [ ] toWrite = new byte [ writeSize ] ; <nl> + try ( SequentialWriter writer = new SequentialWriter ( tempFile , bufferSize , BufferType . OFF _ HEAP ) ) <nl> + { <nl> + / / write bytes greather than buffer <nl> + writer . write ( toWrite ) ; <nl> + assertEquals ( bufferSize , writer . getLastFlushOffset ( ) ) ; <nl> + assertEquals ( writeSize , writer . getFilePointer ( ) ) ; <nl> + / / mark thi position <nl> + FileMark pos = writer . mark ( ) ; <nl> + / / write another <nl> + writer . write ( toWrite ) ; <nl> + / / another buffer should be flushed <nl> + assertEquals ( bufferSize * 2 , writer . getLastFlushOffset ( ) ) ; <nl> + assertEquals ( writeSize * 2 , writer . getFilePointer ( ) ) ; <nl> + / / reset writer <nl> + writer . resetAndTruncate ( pos ) ; <nl> + / / current position and flushed size should be changed <nl> + assertEquals ( writeSize , writer . getFilePointer ( ) ) ; <nl> + assertEquals ( writeSize , writer . getLastFlushOffset ( ) ) ; <nl> + / / write another byte less than buffer <nl> + writer . write ( new byte [ ] { 0 } ) ; <nl> + assertEquals ( writeSize + 1 , writer . getFilePointer ( ) ) ; <nl> + / / flush off set should not be increase <nl> + assertEquals ( writeSize , writer . getLastFlushOffset ( ) ) ; <nl> + writer . finish ( ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + Assert . fail ( ) ; <nl> + } <nl> + / / final file size check <nl> + assertEquals ( writeSize + 1 , tempFile . length ( ) ) ; <nl> + } <nl> + <nl> / * * <nl> * Tests that the output stream exposed by SequentialWriter behaves as expected <nl> * /
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 9e22617 . . 5920283 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 6 + 1 , 7 @ @ <nl> 0 . 8 . 4 <nl> * include files - to - be - streamed in StreamInSession . getSources ( CASSANDRA - 2972 ) <nl> * use JAVA env var in cassandra - env . sh ( CASSANDRA - 2785 , 2992 ) <nl> + * avoid doing read for no - op replicate - on - write at CL = 1 ( CASSANDRA - 2892 ) <nl> <nl> <nl> 0 . 8 . 3 <nl> diff - - git a / src / java / org / apache / cassandra / service / StorageProxy . java b / src / java / org / apache / cassandra / service / StorageProxy . java <nl> index d295224 . . 17255ba 100644 <nl> - - - a / src / java / org / apache / cassandra / service / StorageProxy . java <nl> + + + b / src / java / org / apache / cassandra / service / StorageProxy . java <nl> @ @ - 96 , 7 + 96 , 7 @ @ public class StorageProxy implements StorageProxyMBean <nl> public void apply ( IMutation mutation , Multimap < InetAddress , InetAddress > hintedEndpoints , IWriteResponseHandler responseHandler , String localDataCenter , ConsistencyLevel consistency _ level ) throws IOException <nl> { <nl> assert mutation instanceof RowMutation ; <nl> - sendToHintedEndpoints ( ( RowMutation ) mutation , hintedEndpoints , responseHandler , localDataCenter , true , consistency _ level ) ; <nl> + sendToHintedEndpoints ( ( RowMutation ) mutation , hintedEndpoints , responseHandler , localDataCenter , consistency _ level ) ; <nl> } <nl> } ; <nl> <nl> @ @ - 110 , 7 + 110 , 11 @ @ public class StorageProxy implements StorageProxyMBean <nl> { <nl> public void apply ( IMutation mutation , Multimap < InetAddress , InetAddress > hintedEndpoints , IWriteResponseHandler responseHandler , String localDataCenter , ConsistencyLevel consistency _ level ) throws IOException <nl> { <nl> - applyCounterMutation ( mutation , hintedEndpoints , responseHandler , localDataCenter , consistency _ level , false ) ; <nl> + if ( logger . isDebugEnabled ( ) ) <nl> + logger . debug ( " insert writing local & replicate " + mutation . toString ( true ) ) ; <nl> + <nl> + Runnable runnable = counterWriteTask ( mutation , hintedEndpoints , responseHandler , localDataCenter , consistency _ level ) ; <nl> + runnable . run ( ) ; <nl> } <nl> } ; <nl> <nl> @ @ - 118 , 7 + 122 , 11 @ @ public class StorageProxy implements StorageProxyMBean <nl> { <nl> public void apply ( IMutation mutation , Multimap < InetAddress , InetAddress > hintedEndpoints , IWriteResponseHandler responseHandler , String localDataCenter , ConsistencyLevel consistency _ level ) throws IOException <nl> { <nl> - applyCounterMutation ( mutation , hintedEndpoints , responseHandler , localDataCenter , consistency _ level , true ) ; <nl> + if ( logger . isDebugEnabled ( ) ) <nl> + logger . debug ( " insert writing local & replicate " + mutation . toString ( true ) ) ; <nl> + <nl> + Runnable runnable = counterWriteTask ( mutation , hintedEndpoints , responseHandler , localDataCenter , consistency _ level ) ; <nl> + StageManager . getStage ( Stage . MUTATION ) . execute ( runnable ) ; <nl> } <nl> } ; <nl> } <nl> @ @ - 218 , 7 + 226 , 7 @ @ public class StorageProxy implements StorageProxyMBean <nl> return ss . getTokenMetadata ( ) . getWriteEndpoints ( StorageService . getPartitioner ( ) . getToken ( key ) , table , naturalEndpoints ) ; <nl> } <nl> <nl> - private static void sendToHintedEndpoints ( final RowMutation rm , Multimap < InetAddress , InetAddress > hintedEndpoints , IWriteResponseHandler responseHandler , String localDataCenter , boolean insertLocalMessages , ConsistencyLevel consistency _ level ) <nl> + private static void sendToHintedEndpoints ( final RowMutation rm , Multimap < InetAddress , InetAddress > hintedEndpoints , IWriteResponseHandler responseHandler , String localDataCenter , ConsistencyLevel consistency _ level ) <nl> throws IOException <nl> { <nl> / / Multimap that holds onto all the messages and addresses meant for a specific datacenter <nl> @ @ - 237 , 8 + 245 , 7 @ @ public class StorageProxy implements StorageProxyMBean <nl> / / unhinted writes <nl> if ( destination . equals ( FBUtilities . getLocalAddress ( ) ) ) <nl> { <nl> - if ( insertLocalMessages ) <nl> - insertLocal ( rm , responseHandler ) ; <nl> + insertLocal ( rm , responseHandler ) ; <nl> } <nl> else <nl> { <nl> @ @ - 425 , 13 + 432 , 9 @ @ public class StorageProxy implements StorageProxyMBean <nl> return performWrite ( cm , cm . consistency ( ) , localDataCenter , counterWriteOnCoordinatorPerformer ) ; <nl> } <nl> <nl> - private static void applyCounterMutation ( final IMutation mutation , final Multimap < InetAddress , InetAddress > hintedEndpoints , final IWriteResponseHandler responseHandler , final String localDataCenter , final ConsistencyLevel consistency _ level , boolean executeOnMutationStage ) <nl> + private static Runnable counterWriteTask ( final IMutation mutation , final Multimap < InetAddress , InetAddress > hintedEndpoints , final IWriteResponseHandler responseHandler , final String localDataCenter , final ConsistencyLevel consistency _ level ) <nl> { <nl> - / / we apply locally first , then send it to other replica <nl> - if ( logger . isDebugEnabled ( ) ) <nl> - logger . debug ( " insert writing local & replicate " + mutation . toString ( true ) ) ; <nl> - <nl> - Runnable runnable = new DroppableRunnable ( StorageService . Verb . MUTATION ) <nl> + return new DroppableRunnable ( StorageService . Verb . MUTATION ) <nl> { <nl> public void runMayThrow ( ) throws IOException <nl> { <nl> @ @ - 440 , 10 + 443 , 11 @ @ public class StorageProxy implements StorageProxyMBean <nl> <nl> / / apply mutation <nl> cm . apply ( ) ; <nl> - <nl> responseHandler . response ( null ) ; <nl> <nl> - if ( cm . shouldReplicateOnWrite ( ) ) <nl> + / / then send to replicas , if any <nl> + hintedEndpoints . removeAll ( FBUtilities . getLocalAddress ( ) ) ; <nl> + if ( cm . shouldReplicateOnWrite ( ) & & ! hintedEndpoints . isEmpty ( ) ) <nl> { <nl> / / We do the replication on another stage because it involves a read ( see CM . makeReplicationMutation ) <nl> / / and we want to avoid blocking too much the MUTATION stage <nl> @ @ - 452 , 16 + 456 , 12 @ @ public class StorageProxy implements StorageProxyMBean <nl> public void runMayThrow ( ) throws IOException <nl> { <nl> / / send mutation to other replica <nl> - sendToHintedEndpoints ( cm . makeReplicationMutation ( ) , hintedEndpoints , responseHandler , localDataCenter , false , consistency _ level ) ; <nl> + sendToHintedEndpoints ( cm . makeReplicationMutation ( ) , hintedEndpoints , responseHandler , localDataCenter , consistency _ level ) ; <nl> } <nl> } ) ; <nl> } <nl> } <nl> } ; <nl> - if ( executeOnMutationStage ) <nl> - StageManager . getStage ( Stage . MUTATION ) . execute ( runnable ) ; <nl> - else <nl> - runnable . run ( ) ; <nl> } <nl> <nl> / * *

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 5f6189f . . 22ee346 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 2 . 2 . 13 
 + * Fix compaction failure caused by reading un - flushed data ( CASSANDRA - 12743 ) 
 * Use Bounds instead of Range for sstables in anticompaction ( CASSANDRA - 14411 ) 
 * Fix JSON queries with IN restrictions and ORDER BY clause ( CASSANDRA - 14286 ) 
 * CQL fromJson ( null ) throws NullPointerException ( CASSANDRA - 13891 ) 
 diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 index 9c7c776 . . a7f9bb4 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 + + + b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 @ @ - 129 , 7 + 129 , 7 @ @ public class CompressedSequentialWriter extends SequentialWriter 
 / / write corresponding checksum 
 compressed . rewind ( ) ; 
 crcMetadata . appendDirect ( compressed , true ) ; 
 - lastFlushOffset + = compressedLength + 4 ; 
 + lastFlushOffset = uncompressedSize ; 
 
 / / adjust our bufferOffset to account for the new uncompressed data we ' ve now written out 
 resetBuffer ( ) ; 
 @ @ - 235 , 10 + 235 , 23 @ @ public class CompressedSequentialWriter extends SequentialWriter 
 chunkCount = realMark . nextChunkIndex - 1 ; 
 
 / / truncate data and index file 
 - truncate ( chunkOffset ) ; 
 + truncate ( chunkOffset , bufferOffset ) ; 
 metadataWriter . resetAndTruncate ( realMark . nextChunkIndex - 1 ) ; 
 } 
 
 + private void truncate ( long toFileSize , long toBufferOffset ) 
 + { 
 + try 
 + { 
 + channel . truncate ( toFileSize ) ; 
 + lastFlushOffset = toBufferOffset ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + throw new FSWriteError ( e , getPath ( ) ) ; 
 + } 
 + } 
 + 
 / * * 
 * Seek to the offset where next compressed data chunk should be stored . 
 * / 
 diff - - git a / src / java / org / apache / cassandra / io / util / SequentialWriter . java b / src / java / org / apache / cassandra / io / util / SequentialWriter . java 
 index 0c39469 . . 452318e 100644 
 - - - a / src / java / org / apache / cassandra / io / util / SequentialWriter . java 
 + + + b / src / java / org / apache / cassandra / io / util / SequentialWriter . java 
 @ @ - 430 , 6 + 430 , 7 @ @ public class SequentialWriter extends OutputStream implements WritableByteChanne 
 throw new FSReadError ( e , getPath ( ) ) ; 
 } 
 
 + bufferOffset = truncateTarget ; 
 resetBuffer ( ) ; 
 } 
 
 @ @ - 443 , 6 + 444 , 7 @ @ public class SequentialWriter extends OutputStream implements WritableByteChanne 
 try 
 { 
 channel . truncate ( toSize ) ; 
 + lastFlushOffset = toSize ; 
 } 
 catch ( IOException e ) 
 { 
 diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterReopenTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterReopenTest . java 
 new file mode 100644 
 index 0000000 . . 33b4957 
 - - - / dev / null 
 + + + b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterReopenTest . java 
 @ @ - 0 , 0 + 1 , 153 @ @ 
 + / * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , software 
 + * distributed under the License is distributed on an " AS IS " BASIS , 
 + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 + * See the License for the specific language governing permissions and 
 + * limitations under the License . 
 + * / 
 + 
 + package org . apache . cassandra . io . compress ; 
 + 
 + import java . io . IOException ; 
 + import java . nio . ByteBuffer ; 
 + import java . util . Arrays ; 
 + import java . util . HashSet ; 
 + import java . util . Map ; 
 + import java . util . Random ; 
 + import java . util . Set ; 
 + 
 + import org . junit . Test ; 
 + 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 + import org . apache . cassandra . cql3 . CQLTester ; 
 + import org . apache . cassandra . db . ColumnFamilyStore ; 
 + import org . apache . cassandra . db . Keyspace ; 
 + import org . apache . cassandra . utils . ByteBufferUtil ; 
 + 
 + import static org . junit . Assert . assertEquals ; 
 + 
 + public class CompressedSequentialWriterReopenTest extends CQLTester 
 + { 
 + @ Test 
 + public void badCompressor1 ( ) throws IOException 
 + { 
 + BadCompressor bad = new BadCompressor ( ) ; 
 + byte [ ] test = new byte [ ] { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 } ; 
 + byte [ ] out = new byte [ 10 ] ; 
 + bad . uncompress ( test , 0 , 20 , out , 0 ) ; 
 + for ( int i = 0 ; i < 10 ; i + + ) 
 + assertEquals ( out [ i ] , ( byte ) i ) ; 
 + } 
 + 
 + @ Test 
 + public void badCompressor2 ( ) throws IOException 
 + { 
 + BadCompressor bad = new BadCompressor ( ) ; 
 + ByteBuffer input = ByteBuffer . wrap ( new byte [ ] { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 } ) ; 
 + ByteBuffer output = ByteBuffer . allocate ( 40 ) ; 
 + bad . compress ( input , output ) ; 
 + for ( int i = 0 ; i < 40 ; i + + ) 
 + assertEquals ( i % 20 , output . get ( i ) ) ; 
 + } 
 + 
 + @ Test 
 + public void badCompressor3 ( ) throws IOException 
 + { 
 + BadCompressor bad = new BadCompressor ( ) ; 
 + ByteBuffer input = ByteBuffer . wrap ( new byte [ ] { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 } ) ; 
 + ByteBuffer output = ByteBuffer . allocate ( 10 ) ; 
 + bad . uncompress ( input , output ) ; 
 + for ( int i = 0 ; i < 10 ; i + + ) 
 + assertEquals ( i , output . get ( i ) ) ; 
 + } 
 + 
 + @ Test 
 + public void compressionEnabled ( ) throws Throwable 
 + { 
 + createTable ( " create table % s ( id int primary key , t blob ) with compression = { ' sstable _ compression ' : ' org . apache . cassandra . io . compress . CompressedSequentialWriterReopenTest $ BadCompressor ' } " ) ; 
 + byte [ ] blob = new byte [ 1000 ] ; 
 + ( new Random ( ) ) . nextBytes ( blob ) ; 
 + Keyspace keyspace = Keyspace . open ( keyspace ( ) ) ; 
 + ColumnFamilyStore cfs = keyspace . getColumnFamilyStore ( currentTable ( ) ) ; 
 + cfs . disableAutoCompaction ( ) ; 
 + for ( int i = 0 ; i < 10000 ; i + + ) 
 + { 
 + execute ( " insert into % s ( id , t ) values ( ? , ? ) " , i , ByteBuffer . wrap ( blob ) ) ; 
 + } 
 + cfs . forceBlockingFlush ( ) ; 
 + for ( int i = 0 ; i < 10000 ; i + + ) 
 + { 
 + execute ( " insert into % s ( id , t ) values ( ? , ? ) " , i , ByteBuffer . wrap ( blob ) ) ; 
 + } 
 + cfs . forceBlockingFlush ( ) ; 
 + DatabaseDescriptor . setSSTablePreempiveOpenIntervalInMB ( 1 ) ; 
 + cfs . forceMajorCompaction ( ) ; 
 + } 
 + 
 + public static class BadCompressor implements ICompressor 
 + { 
 + public static ICompressor create ( Map < String , String > options ) 
 + { 
 + return new BadCompressor ( ) ; 
 + } 
 + 
 + @ Override 
 + public int initialCompressedBufferLength ( int chunkLength ) 
 + { 
 + return chunkLength * 2 ; 
 + } 
 + 
 + @ Override 
 + public int uncompress ( byte [ ] input , int inputOffset , int inputLength , byte [ ] output , int outputOffset ) throws IOException 
 + { 
 + System . arraycopy ( input , inputOffset , output , outputOffset , inputLength / 2 ) ; 
 + return inputLength / 2 ; 
 + } 
 + 
 + @ Override 
 + public void compress ( ByteBuffer input , ByteBuffer output ) throws IOException 
 + { 
 + int len = input . remaining ( ) ; 
 + byte [ ] arr = ByteBufferUtil . getArray ( input ) ; 
 + output . put ( arr ) ; 
 + output . put ( arr ) ; 
 + input . position ( len ) ; 
 + } 
 + 
 + @ Override 
 + public void uncompress ( ByteBuffer input , ByteBuffer output ) throws IOException 
 + { 
 + byte [ ] arr = ByteBufferUtil . getArray ( input ) ; 
 + output . put ( arr , 0 , arr . length / 2 ) ; 
 + input . position ( arr . length ) ; 
 + } 
 + 
 + @ Override 
 + public BufferType preferredBufferType ( ) 
 + { 
 + return BufferType . ON _ HEAP ; 
 + } 
 + 
 + @ Override 
 + public boolean supports ( BufferType bufferType ) 
 + { 
 + return true ; 
 + } 
 + 
 + @ Override 
 + public Set < String > supportedOptions ( ) 
 + { 
 + return new HashSet < > ( Arrays . asList ( CompressionParameters . CRC _ CHECK _ CHANCE ) ) ; 
 + } 
 + } 
 + } 
 diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java 
 index 43c44fd . . bca0354 100644 
 - - - a / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java 
 + + + b / test / unit / org / apache / cassandra / io / compress / CompressedSequentialWriterTest . java 
 @ @ - 17 , 6 + 17 , 7 @ @ 
 * / 
 package org . apache . cassandra . io . compress ; 
 
 + import com . google . common . io . Files ; 
 import java . io . ByteArrayInputStream ; 
 import java . io . DataInputStream ; 
 import java . io . File ; 
 @ @ - 26 , 6 + 27 , 7 @ @ import java . util . * ; 
 
 import static org . apache . commons . io . FileUtils . readFileToByteArray ; 
 import static org . junit . Assert . assertEquals ; 
 + import static org . junit . Assert . assertTrue ; 
 
 import org . junit . After ; 
 import org . junit . Test ; 
 @ @ - 38 , 7 + 40 , 9 @ @ import org . apache . cassandra . db . marshal . UTF8Type ; 
 import org . apache . cassandra . io . sstable . metadata . MetadataCollector ; 
 import org . apache . cassandra . io . util . ChannelProxy ; 
 import org . apache . cassandra . io . util . FileMark ; 
 + import org . apache . cassandra . io . util . FileUtils ; 
 import org . apache . cassandra . io . util . RandomAccessReader ; 
 + import org . apache . cassandra . io . util . SequentialWriter ; 
 import org . apache . cassandra . io . util . SequentialWriterTest ; 
 
 public class CompressedSequentialWriterTest extends SequentialWriterTest 
 @ @ - 107 , 6 + 111 , 12 @ @ public class CompressedSequentialWriterTest extends SequentialWriterTest 
 { 
 writer . write ( ( byte ) i ) ; 
 } 
 + 
 + if ( bytesToTest < = CompressionParameters . DEFAULT _ CHUNK _ LENGTH ) 
 + assertEquals ( writer . getLastFlushOffset ( ) , CompressionParameters . DEFAULT _ CHUNK _ LENGTH ) ; 
 + else 
 + assertTrue ( writer . getLastFlushOffset ( ) % CompressionParameters . DEFAULT _ CHUNK _ LENGTH = = 0 ) ; 
 + 
 writer . resetAndTruncate ( mark ) ; 
 writer . write ( dataPost ) ; 
 writer . finish ( ) ; 
 @ @ - 155 , 6 + 165 , 48 @ @ public class CompressedSequentialWriterTest extends SequentialWriterTest 
 writers . clear ( ) ; 
 } 
 
 + @ Test 
 + @ Override 
 + public void resetAndTruncateTest ( ) 
 + { 
 + File tempFile = new File ( Files . createTempDir ( ) , " reset . txt " ) ; 
 + File offsetsFile = FileUtils . createTempFile ( " compressedsequentialwriter . offset " , " test " ) ; 
 + final int bufferSize = 48 ; 
 + final int writeSize = 64 ; 
 + byte [ ] toWrite = new byte [ writeSize ] ; 
 + 
 + try ( SequentialWriter writer = new CompressedSequentialWriter ( tempFile , offsetsFile . getPath ( ) , 
 + new CompressionParameters ( LZ4Compressor . instance ) , new MetadataCollector ( CellNames . fromAbstractType ( UTF8Type . instance , false ) ) ) ) 
 + { 
 + / / write bytes greather than buffer 
 + writer . write ( toWrite ) ; 
 + long flushedOffset = writer . getLastFlushOffset ( ) ; 
 + assertEquals ( writeSize , writer . getFilePointer ( ) ) ; 
 + / / mark thi position 
 + FileMark pos = writer . mark ( ) ; 
 + / / write another 
 + writer . write ( toWrite ) ; 
 + / / another buffer should be flushed 
 + assertEquals ( flushedOffset * 2 , writer . getLastFlushOffset ( ) ) ; 
 + assertEquals ( writeSize * 2 , writer . getFilePointer ( ) ) ; 
 + / / reset writer 
 + writer . resetAndTruncate ( pos ) ; 
 + / / current position and flushed size should be changed 
 + assertEquals ( writeSize , writer . getFilePointer ( ) ) ; 
 + assertEquals ( flushedOffset , writer . getLastFlushOffset ( ) ) ; 
 + / / write another byte less than buffer 
 + writer . write ( new byte [ ] { 0 } ) ; 
 + assertEquals ( writeSize + 1 , writer . getFilePointer ( ) ) ; 
 + / / flush off set should not be increase 
 + assertEquals ( flushedOffset , writer . getLastFlushOffset ( ) ) ; 
 + writer . finish ( ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + Assert . fail ( ) ; 
 + } 
 + } 
 + 
 protected TestableTransaction newTest ( ) throws IOException 
 { 
 TestableCSW sw = new TestableCSW ( ) ; 
 diff - - git a / test / unit / org / apache / cassandra / io / util / SequentialWriterTest . java b / test / unit / org / apache / cassandra / io / util / SequentialWriterTest . java 
 index fd38427 . . 15d6160 100644 
 - - - a / test / unit / org / apache / cassandra / io / util / SequentialWriterTest . java 
 + + + b / test / unit / org / apache / cassandra / io / util / SequentialWriterTest . java 
 @ @ - 36 , 6 + 36 , 7 @ @ import org . apache . cassandra . io . compress . BufferType ; 
 import org . apache . cassandra . utils . concurrent . AbstractTransactionalTest ; 
 
 import static org . apache . commons . io . FileUtils . * ; 
 + import static org . junit . Assert . assertEquals ; 
 
 public class SequentialWriterTest extends AbstractTransactionalTest 
 { 
 @ @ - 119 , 6 + 120 , 46 @ @ public class SequentialWriterTest extends AbstractTransactionalTest 
 } 
 } 
 
 + @ Test 
 + public void resetAndTruncateTest ( ) 
 + { 
 + File tempFile = new File ( Files . createTempDir ( ) , " reset . txt " ) ; 
 + final int bufferSize = 48 ; 
 + final int writeSize = 64 ; 
 + byte [ ] toWrite = new byte [ writeSize ] ; 
 + try ( SequentialWriter writer = new SequentialWriter ( tempFile , bufferSize , BufferType . OFF _ HEAP ) ) 
 + { 
 + / / write bytes greather than buffer 
 + writer . write ( toWrite ) ; 
 + assertEquals ( bufferSize , writer . getLastFlushOffset ( ) ) ; 
 + assertEquals ( writeSize , writer . getFilePointer ( ) ) ; 
 + / / mark thi position 
 + FileMark pos = writer . mark ( ) ; 
 + / / write another 
 + writer . write ( toWrite ) ; 
 + / / another buffer should be flushed 
 + assertEquals ( bufferSize * 2 , writer . getLastFlushOffset ( ) ) ; 
 + assertEquals ( writeSize * 2 , writer . getFilePointer ( ) ) ; 
 + / / reset writer 
 + writer . resetAndTruncate ( pos ) ; 
 + / / current position and flushed size should be changed 
 + assertEquals ( writeSize , writer . getFilePointer ( ) ) ; 
 + assertEquals ( writeSize , writer . getLastFlushOffset ( ) ) ; 
 + / / write another byte less than buffer 
 + writer . write ( new byte [ ] { 0 } ) ; 
 + assertEquals ( writeSize + 1 , writer . getFilePointer ( ) ) ; 
 + / / flush off set should not be increase 
 + assertEquals ( writeSize , writer . getLastFlushOffset ( ) ) ; 
 + writer . finish ( ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + Assert . fail ( ) ; 
 + } 
 + / / final file size check 
 + assertEquals ( writeSize + 1 , tempFile . length ( ) ) ; 
 + } 
 + 
 / * * 
 * Tests that the output stream exposed by SequentialWriter behaves as expected 
 * /

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 9e22617 . . 5920283 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 6 + 1 , 7 @ @ 
 0 . 8 . 4 
 * include files - to - be - streamed in StreamInSession . getSources ( CASSANDRA - 2972 ) 
 * use JAVA env var in cassandra - env . sh ( CASSANDRA - 2785 , 2992 ) 
 + * avoid doing read for no - op replicate - on - write at CL = 1 ( CASSANDRA - 2892 ) 
 
 
 0 . 8 . 3 
 diff - - git a / src / java / org / apache / cassandra / service / StorageProxy . java b / src / java / org / apache / cassandra / service / StorageProxy . java 
 index d295224 . . 17255ba 100644 
 - - - a / src / java / org / apache / cassandra / service / StorageProxy . java 
 + + + b / src / java / org / apache / cassandra / service / StorageProxy . java 
 @ @ - 96 , 7 + 96 , 7 @ @ public class StorageProxy implements StorageProxyMBean 
 public void apply ( IMutation mutation , Multimap < InetAddress , InetAddress > hintedEndpoints , IWriteResponseHandler responseHandler , String localDataCenter , ConsistencyLevel consistency _ level ) throws IOException 
 { 
 assert mutation instanceof RowMutation ; 
 - sendToHintedEndpoints ( ( RowMutation ) mutation , hintedEndpoints , responseHandler , localDataCenter , true , consistency _ level ) ; 
 + sendToHintedEndpoints ( ( RowMutation ) mutation , hintedEndpoints , responseHandler , localDataCenter , consistency _ level ) ; 
 } 
 } ; 
 
 @ @ - 110 , 7 + 110 , 11 @ @ public class StorageProxy implements StorageProxyMBean 
 { 
 public void apply ( IMutation mutation , Multimap < InetAddress , InetAddress > hintedEndpoints , IWriteResponseHandler responseHandler , String localDataCenter , ConsistencyLevel consistency _ level ) throws IOException 
 { 
 - applyCounterMutation ( mutation , hintedEndpoints , responseHandler , localDataCenter , consistency _ level , false ) ; 
 + if ( logger . isDebugEnabled ( ) ) 
 + logger . debug ( " insert writing local & replicate " + mutation . toString ( true ) ) ; 
 + 
 + Runnable runnable = counterWriteTask ( mutation , hintedEndpoints , responseHandler , localDataCenter , consistency _ level ) ; 
 + runnable . run ( ) ; 
 } 
 } ; 
 
 @ @ - 118 , 7 + 122 , 11 @ @ public class StorageProxy implements StorageProxyMBean 
 { 
 public void apply ( IMutation mutation , Multimap < InetAddress , InetAddress > hintedEndpoints , IWriteResponseHandler responseHandler , String localDataCenter , ConsistencyLevel consistency _ level ) throws IOException 
 { 
 - applyCounterMutation ( mutation , hintedEndpoints , responseHandler , localDataCenter , consistency _ level , true ) ; 
 + if ( logger . isDebugEnabled ( ) ) 
 + logger . debug ( " insert writing local & replicate " + mutation . toString ( true ) ) ; 
 + 
 + Runnable runnable = counterWriteTask ( mutation , hintedEndpoints , responseHandler , localDataCenter , consistency _ level ) ; 
 + StageManager . getStage ( Stage . MUTATION ) . execute ( runnable ) ; 
 } 
 } ; 
 } 
 @ @ - 218 , 7 + 226 , 7 @ @ public class StorageProxy implements StorageProxyMBean 
 return ss . getTokenMetadata ( ) . getWriteEndpoints ( StorageService . getPartitioner ( ) . getToken ( key ) , table , naturalEndpoints ) ; 
 } 
 
 - private static void sendToHintedEndpoints ( final RowMutation rm , Multimap < InetAddress , InetAddress > hintedEndpoints , IWriteResponseHandler responseHandler , String localDataCenter , boolean insertLocalMessages , ConsistencyLevel consistency _ level ) 
 + private static void sendToHintedEndpoints ( final RowMutation rm , Multimap < InetAddress , InetAddress > hintedEndpoints , IWriteResponseHandler responseHandler , String localDataCenter , ConsistencyLevel consistency _ level ) 
 throws IOException 
 { 
 / / Multimap that holds onto all the messages and addresses meant for a specific datacenter 
 @ @ - 237 , 8 + 245 , 7 @ @ public class StorageProxy implements StorageProxyMBean 
 / / unhinted writes 
 if ( destination . equals ( FBUtilities . getLocalAddress ( ) ) ) 
 { 
 - if ( insertLocalMessages ) 
 - insertLocal ( rm , responseHandler ) ; 
 + insertLocal ( rm , responseHandler ) ; 
 } 
 else 
 { 
 @ @ - 425 , 13 + 432 , 9 @ @ public class StorageProxy implements StorageProxyMBean 
 return performWrite ( cm , cm . consistency ( ) , localDataCenter , counterWriteOnCoordinatorPerformer ) ; 
 } 
 
 - private static void applyCounterMutation ( final IMutation mutation , final Multimap < InetAddress , InetAddress > hintedEndpoints , final IWriteResponseHandler responseHandler , final String localDataCenter , final ConsistencyLevel consistency _ level , boolean executeOnMutationStage ) 
 + private static Runnable counterWriteTask ( final IMutation mutation , final Multimap < InetAddress , InetAddress > hintedEndpoints , final IWriteResponseHandler responseHandler , final String localDataCenter , final ConsistencyLevel consistency _ level ) 
 { 
 - / / we apply locally first , then send it to other replica 
 - if ( logger . isDebugEnabled ( ) ) 
 - logger . debug ( " insert writing local & replicate " + mutation . toString ( true ) ) ; 
 - 
 - Runnable runnable = new DroppableRunnable ( StorageService . Verb . MUTATION ) 
 + return new DroppableRunnable ( StorageService . Verb . MUTATION ) 
 { 
 public void runMayThrow ( ) throws IOException 
 { 
 @ @ - 440 , 10 + 443 , 11 @ @ public class StorageProxy implements StorageProxyMBean 
 
 / / apply mutation 
 cm . apply ( ) ; 
 - 
 responseHandler . response ( null ) ; 
 
 - if ( cm . shouldReplicateOnWrite ( ) ) 
 + / / then send to replicas , if any 
 + hintedEndpoints . removeAll ( FBUtilities . getLocalAddress ( ) ) ; 
 + if ( cm . shouldReplicateOnWrite ( ) & & ! hintedEndpoints . isEmpty ( ) ) 
 { 
 / / We do the replication on another stage because it involves a read ( see CM . makeReplicationMutation ) 
 / / and we want to avoid blocking too much the MUTATION stage 
 @ @ - 452 , 16 + 456 , 12 @ @ public class StorageProxy implements StorageProxyMBean 
 public void runMayThrow ( ) throws IOException 
 { 
 / / send mutation to other replica 
 - sendToHintedEndpoints ( cm . makeReplicationMutation ( ) , hintedEndpoints , responseHandler , localDataCenter , false , consistency _ level ) ; 
 + sendToHintedEndpoints ( cm . makeReplicationMutation ( ) , hintedEndpoints , responseHandler , localDataCenter , consistency _ level ) ; 
 } 
 } ) ; 
 } 
 } 
 } ; 
 - if ( executeOnMutationStage ) 
 - StageManager . getStage ( Stage . MUTATION ) . execute ( runnable ) ; 
 - else 
 - runnable . run ( ) ; 
 } 
 
 / * *
