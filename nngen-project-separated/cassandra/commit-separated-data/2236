BLEU SCORE: 0.010697691669654345

TEST MSG: Fix minor potential leak in sstable2json
GENERATED MSG: merge from 0 . 7 using mine - conflict ( r1062896 was clobbered )

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index fdff490 . . 4392159 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 5 @ @ <nl> 2 . 0 . 10 <nl> + * Minor leak in sstable2jon ( CASSANDRA - 7709 ) <nl> * Add cassandra . auto _ bootstrap system property ( CASSANDRA - 7650 ) <nl> * Remove CqlPagingRecordReader / CqlPagingInputFormat ( CASSANDRA - 7570 ) <nl> * Fix IncompatibleClassChangeError from hadoop2 ( CASSANDRA - 7229 ) <nl> diff - - git a / src / java / org / apache / cassandra / tools / SSTableExport . java b / src / java / org / apache / cassandra / tools / SSTableExport . java <nl> index 197585b . . f8b85c3 100644 <nl> - - - a / src / java / org / apache / cassandra / tools / SSTableExport . java <nl> + + + b / src / java / org / apache / cassandra / tools / SSTableExport . java <nl> @ @ - 252 , 20 + 252 , 26 @ @ public class SSTableExport <nl> throws IOException <nl> { <nl> KeyIterator iter = new KeyIterator ( desc ) ; <nl> - DecoratedKey lastKey = null ; <nl> - while ( iter . hasNext ( ) ) <nl> + try <nl> { <nl> - DecoratedKey key = iter . next ( ) ; <nl> + DecoratedKey lastKey = null ; <nl> + while ( iter . hasNext ( ) ) <nl> + { <nl> + DecoratedKey key = iter . next ( ) ; <nl> <nl> - / / validate order of the keys in the sstable <nl> - if ( lastKey ! = null & & lastKey . compareTo ( key ) > 0 ) <nl> - throw new IOException ( " Key out of order ! " + lastKey + " > " + key ) ; <nl> - lastKey = key ; <nl> + / / validate order of the keys in the sstable <nl> + if ( lastKey ! = null & & lastKey . compareTo ( key ) > 0 ) <nl> + throw new IOException ( " Key out of order ! " + lastKey + " > " + key ) ; <nl> + lastKey = key ; <nl> <nl> - outs . println ( bytesToHex ( key . key ) ) ; <nl> - checkStream ( outs ) ; / / flushes <nl> + outs . println ( bytesToHex ( key . key ) ) ; <nl> + checkStream ( outs ) ; / / flushes <nl> + } <nl> + } <nl> + finally <nl> + { <nl> + iter . close ( ) ; <nl> } <nl> - iter . close ( ) ; <nl> } <nl> <nl> / * * <nl> @ @ - 281 , 51 + 287 , 59 @ @ public class SSTableExport <nl> { <nl> SSTableReader sstable = SSTableReader . open ( desc ) ; <nl> RandomAccessReader dfile = sstable . openDataReader ( ) ; <nl> + try <nl> + { <nl> + IPartitioner < ? > partitioner = sstable . partitioner ; <nl> <nl> - IPartitioner < ? > partitioner = sstable . partitioner ; <nl> + if ( excludes ! = null ) <nl> + toExport . removeAll ( Arrays . asList ( excludes ) ) ; <nl> <nl> - if ( excludes ! = null ) <nl> - toExport . removeAll ( Arrays . asList ( excludes ) ) ; <nl> + outs . println ( " [ " ) ; <nl> <nl> - outs . println ( " [ " ) ; <nl> + int i = 0 ; <nl> <nl> - int i = 0 ; <nl> + / / last key to compare order <nl> + DecoratedKey lastKey = null ; <nl> <nl> - / / last key to compare order <nl> - DecoratedKey lastKey = null ; <nl> + for ( String key : toExport ) <nl> + { <nl> + DecoratedKey decoratedKey = partitioner . decorateKey ( hexToBytes ( key ) ) ; <nl> <nl> - for ( String key : toExport ) <nl> - { <nl> - DecoratedKey decoratedKey = partitioner . decorateKey ( hexToBytes ( key ) ) ; <nl> + if ( lastKey ! = null & & lastKey . compareTo ( decoratedKey ) > 0 ) <nl> + throw new IOException ( " Key out of order ! " + lastKey + " > " + decoratedKey ) ; <nl> <nl> - if ( lastKey ! = null & & lastKey . compareTo ( decoratedKey ) > 0 ) <nl> - throw new IOException ( " Key out of order ! " + lastKey + " > " + decoratedKey ) ; <nl> + lastKey = decoratedKey ; <nl> <nl> - lastKey = decoratedKey ; <nl> + RowIndexEntry entry = sstable . getPosition ( decoratedKey , SSTableReader . Operator . EQ ) ; <nl> + if ( entry = = null ) <nl> + continue ; <nl> <nl> - RowIndexEntry entry = sstable . getPosition ( decoratedKey , SSTableReader . Operator . EQ ) ; <nl> - if ( entry = = null ) <nl> - continue ; <nl> + dfile . seek ( entry . position ) ; <nl> + ByteBufferUtil . readWithShortLength ( dfile ) ; / / row key <nl> + if ( sstable . descriptor . version . hasRowSizeAndColumnCount ) <nl> + dfile . readLong ( ) ; / / row size <nl> + DeletionInfo deletionInfo = new DeletionInfo ( DeletionTime . serializer . deserialize ( dfile ) ) ; <nl> + int columnCount = sstable . descriptor . version . hasRowSizeAndColumnCount ? dfile . readInt ( ) <nl> + : Integer . MAX _ VALUE ; <nl> <nl> - dfile . seek ( entry . position ) ; <nl> - ByteBufferUtil . readWithShortLength ( dfile ) ; / / row key <nl> - if ( sstable . descriptor . version . hasRowSizeAndColumnCount ) <nl> - dfile . readLong ( ) ; / / row size <nl> - DeletionInfo deletionInfo = new DeletionInfo ( DeletionTime . serializer . deserialize ( dfile ) ) ; <nl> - int columnCount = sstable . descriptor . version . hasRowSizeAndColumnCount ? dfile . readInt ( ) : Integer . MAX _ VALUE ; <nl> + Iterator < OnDiskAtom > atomIterator = sstable . metadata . getOnDiskIterator ( dfile , columnCount , <nl> + sstable . descriptor . version ) ; <nl> <nl> - Iterator < OnDiskAtom > atomIterator = sstable . metadata . getOnDiskIterator ( dfile , columnCount , sstable . descriptor . version ) ; <nl> + checkStream ( outs ) ; <nl> <nl> - checkStream ( outs ) ; <nl> + if ( i ! = 0 ) <nl> + outs . println ( " , " ) ; <nl> + i + + ; <nl> + serializeRow ( deletionInfo , atomIterator , sstable . metadata , decoratedKey , outs ) ; <nl> + } <nl> <nl> - if ( i ! = 0 ) <nl> - outs . println ( " , " ) ; <nl> - i + + ; <nl> - serializeRow ( deletionInfo , atomIterator , sstable . metadata , decoratedKey , outs ) ; <nl> + outs . println ( " \ n ] " ) ; <nl> + outs . flush ( ) ; <nl> + } <nl> + finally <nl> + { <nl> + dfile . close ( ) ; <nl> } <nl> - <nl> - outs . println ( " \ n ] " ) ; <nl> - outs . flush ( ) ; <nl> } <nl> <nl> / / This is necessary to accommodate the test suite since you cannot open a Reader more <nl> @ @ - 337 , 36 + 351 , 39 @ @ public class SSTableExport <nl> if ( excludes ! = null ) <nl> excludeSet = new HashSet < String > ( Arrays . asList ( excludes ) ) ; <nl> <nl> - <nl> SSTableIdentityIterator row ; <nl> SSTableScanner scanner = reader . getScanner ( ) ; <nl> + try <nl> + { <nl> + outs . println ( " [ " ) ; <nl> <nl> - outs . println ( " [ " ) ; <nl> + int i = 0 ; <nl> <nl> - int i = 0 ; <nl> + / / collecting keys to export <nl> + while ( scanner . hasNext ( ) ) <nl> + { <nl> + row = ( SSTableIdentityIterator ) scanner . next ( ) ; <nl> <nl> - / / collecting keys to export <nl> - while ( scanner . hasNext ( ) ) <nl> - { <nl> - row = ( SSTableIdentityIterator ) scanner . next ( ) ; <nl> + String currentKey = bytesToHex ( row . getKey ( ) . key ) ; <nl> <nl> - String currentKey = bytesToHex ( row . getKey ( ) . key ) ; <nl> + if ( excludeSet . contains ( currentKey ) ) <nl> + continue ; <nl> + else if ( i ! = 0 ) <nl> + outs . println ( " , " ) ; <nl> <nl> - if ( excludeSet . contains ( currentKey ) ) <nl> - continue ; <nl> - else if ( i ! = 0 ) <nl> - outs . println ( " , " ) ; <nl> + serializeRow ( row , row . getKey ( ) , outs ) ; <nl> + checkStream ( outs ) ; <nl> <nl> - serializeRow ( row , row . getKey ( ) , outs ) ; <nl> - checkStream ( outs ) ; <nl> + i + + ; <nl> + } <nl> <nl> - i + + ; <nl> + outs . println ( " \ n ] " ) ; <nl> + outs . flush ( ) ; <nl> + } <nl> + finally <nl> + { <nl> + scanner . close ( ) ; <nl> } <nl> - <nl> - outs . println ( " \ n ] " ) ; <nl> - outs . flush ( ) ; <nl> - <nl> - scanner . close ( ) ; <nl> } <nl> <nl> / * *
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 28bf806 . . 9739983 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 9 + 1 , 3 @ @ <nl> - 0 . 7 . 2 <nl> - * cache writing moved to CompactionManager to reduce i / o contention and <nl> - updated to use non - cache - polluting writes ( CASSANDRA - 2053 ) <nl> - * page through large rows when exporting to JSON ( CASSANDRA - 2041 ) <nl> - <nl> - <nl> 0 . 7 . 1 <nl> * buffer network stack to avoid inefficient small TCP messages while avoiding <nl> the nagle / delayed ack problem ( CASSANDRA - 1896 ) <nl> @ @ - 56 , 6 + 50 , 11 @ @ <nl> * avoid blocking gossip while deleting handoff hints ( CASSANDRA - 2073 ) <nl> * ignore messages from newer versions , keep track of nodes in gossip <nl> regardless of version ( CASSANDRA - 1970 ) <nl> + * cache writing moved to CompactionManager to reduce i / o contention and <nl> + updated to use non - cache - polluting writes ( CASSANDRA - 2053 ) <nl> + * page through large rows when exporting to JSON ( CASSANDRA - 2041 ) <nl> + * add flush _ largest _ memtables _ at and reduce _ cache _ sizes _ at options <nl> + ( CASSANDRA - 2142 ) <nl> <nl> <nl> 0 . 7 . 0 - final <nl> diff - - git a / NEWS . txt b / NEWS . txt <nl> index df58fde . . 798adcf 100644 <nl> - - - a / NEWS . txt <nl> + + + b / NEWS . txt <nl> @ @ - 1 , 17 + 1 , 33 @ @ <nl> 0 . 7 . 1 <nl> = = = = = <nl> <nl> - Uprading <nl> - - - - - - - - - <nl> + Upgrading <nl> + - - - - - - - - - <nl> - 0 . 7 . 1 is completely backwards compatible with 0 . 7 . 0 . Just restart <nl> each node with the new version , one at a time . ( The cluster does <nl> not all need to be upgraded simultaneously . ) <nl> <nl> Features <nl> - - - - - - - - <nl> - - Cassandra can perform writes efficiently across datacenters by <nl> + - added flush _ largest _ memtables _ at and reduce _ cache _ sizes _ at options <nl> + to cassandra . yaml as an escape valve for memory pressure <nl> + - added option to specify - Dcassandra . join _ ring = false on startup <nl> + to allow " warm spare " nodes or performing JMX maintenance before <nl> + joining the ring <nl> + <nl> + Performance <nl> + - - - - - - - - - - - <nl> + - Disk writes and sequential scans avoid polluting page cache <nl> + ( requires JNA to be enabled ) <nl> + - Cassandra performs writes efficiently across datacenters by <nl> sending a single copy of the mutation and having the recipient <nl> forward that to other replicas in its datacenter . <nl> + - Improved network buffering <nl> + - Reduced lock contention on memtable flush <nl> + - Optimized supercolumn deserialization <nl> + - Zero - copy reads from mmapped sstable files <nl> + - Explicitly set higher JVM new generation size <nl> + - Reduced i / o contention during saving of caches <nl> <nl> <nl> 0 . 7 . 0 <nl> diff - - git a / conf / cassandra . yaml b / conf / cassandra . yaml <nl> index 5f43843 . . 90193cf 100644 <nl> - - - a / conf / cassandra . yaml <nl> + + + b / conf / cassandra . yaml <nl> @ @ - 90 , 6 + 90 , 31 @ @ commitlog _ sync : periodic <nl> # milliseconds . <nl> commitlog _ sync _ period _ in _ ms : 10000 <nl> <nl> + # emergency pressure valve : each time heap usage after a full ( CMS ) <nl> + # garbage collection is above this fraction of the max , Cassandra will <nl> + # flush the largest memtables . <nl> + # <nl> + # Set to 1 . 0 to disable . Setting this lower than <nl> + # CMSInitiatingOccupancyFraction is not likely to be useful . <nl> + # <nl> + # RELYING ON THIS AS YOUR PRIMARY TUNING MECHANISM WILL WORK POORLY : <nl> + # it is most effective under light to moderate load , or read - heavy <nl> + # workloads ; under truly massive write load , it will often be too <nl> + # little , too late . <nl> + flush _ largest _ memtables _ at : 0 . 75 <nl> + <nl> + # emergency pressure valve # 2 : the first time heap usage after a full <nl> + # ( CMS ) garbage collection is above this fraction of the max , <nl> + # Cassandra will reduce cache maximum _ capacity _ to the given fraction <nl> + # of the current _ size _ . Should usually be set substantially above <nl> + # flush _ largest _ memtables _ at , since that will have less long - term <nl> + # impact on the system . <nl> + # <nl> + # Set to 1 . 0 to disable . Setting this lower than <nl> + # CMSInitiatingOccupancyFraction is not likely to be useful . <nl> + reduce _ cache _ sizes _ at : 0 . 85 <nl> + reduce _ cache _ capacity _ to : 0 . 6 <nl> + <nl> # Addresses of hosts that are deemed contact points . <nl> # Cassandra nodes use this list of hosts to find each other and learn <nl> # the topology of the ring . You must change this if you are running <nl> diff - - git a / src / java / org / apache / cassandra / config / Config . java b / src / java / org / apache / cassandra / config / Config . java <nl> index fd4bca0 . . 25e719b 100644 <nl> - - - a / src / java / org / apache / cassandra / config / Config . java <nl> + + + b / src / java / org / apache / cassandra / config / Config . java <nl> @ @ - 103 , 7 + 103 , 10 @ @ public class Config <nl> public Integer index _ interval = 128 ; <nl> <nl> public List < RawKeyspace > keyspaces ; <nl> - <nl> + public Double flush _ largest _ memtables _ at = 1 . 0 ; <nl> + public Double reduce _ cache _ sizes _ at = 1 . 0 ; <nl> + public double reduce _ cache _ capacity _ to = 0 . 6 ; <nl> + <nl> public static enum CommitLogSync { <nl> periodic , <nl> batch <nl> diff - - git a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> index be073cd . . 75997d6 100644 <nl> - - - a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> + + + b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> @ @ - 20 , 7 + 20 , 6 @ @ package org . apache . cassandra . config ; <nl> <nl> import java . io . * ; <nl> import java . net . InetAddress ; <nl> - import java . net . MalformedURLException ; <nl> import java . net . URL ; <nl> import java . net . UnknownHostException ; <nl> import java . nio . ByteBuffer ; <nl> @ @ - 1155 , 4 + 1154 , 19 @ @ public class DatabaseDescriptor <nl> { <nl> conf . dynamic _ snitch _ badness _ threshold = dynamicBadnessThreshold ; <nl> } <nl> + <nl> + public static double getFlushLargestMemtablesAt ( ) <nl> + { <nl> + return conf . flush _ largest _ memtables _ at ; <nl> + } <nl> + <nl> + public static double getReduceCacheSizesAt ( ) <nl> + { <nl> + return conf . reduce _ cache _ sizes _ at ; <nl> + } <nl> + <nl> + public static double getReduceCacheCapacityTo ( ) <nl> + { <nl> + return conf . reduce _ cache _ capacity _ to ; <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> index d9abe67 . . 6501a8a 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> @ @ - 1878 , 7 + 1878 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> public String toString ( ) <nl> { <nl> return " ColumnFamilyStore ( " + <nl> - " table = ' " + table + ' \ ' ' + <nl> + " table = ' " + table . name + ' \ ' ' + <nl> " , columnFamily = ' " + columnFamily + ' \ ' ' + <nl> ' ) ' ; <nl> } <nl> @ @ - 2019 , 4 + 2019 , 26 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> { <nl> return partitioner instanceof LocalPartitioner ; <nl> } <nl> + <nl> + / * * <nl> + * sets each cache ' s maximum capacity to 75 % of its current size <nl> + * / <nl> + public void reduceCacheSizes ( ) <nl> + { <nl> + if ( ssTables . getRowCache ( ) . getCapacity ( ) > 0 ) <nl> + { <nl> + int newCapacity = ( int ) ( DatabaseDescriptor . getReduceCacheCapacityTo ( ) * ssTables . getRowCache ( ) . getSize ( ) ) ; <nl> + logger . warn ( String . format ( " Reducing % s row cache capacity from % d to % s to reduce memory pressure " , <nl> + columnFamily , ssTables . getRowCache ( ) . getCapacity ( ) , newCapacity ) ) ; <nl> + ssTables . getRowCache ( ) . setCapacity ( newCapacity ) ; <nl> + } <nl> + <nl> + if ( ssTables . getKeyCache ( ) . getCapacity ( ) > 0 ) <nl> + { <nl> + int newCapacity = ( int ) ( DatabaseDescriptor . getReduceCacheCapacityTo ( ) * ssTables . getKeyCache ( ) . getSize ( ) ) ; <nl> + logger . warn ( String . format ( " Reducing % s key cache capacity from % d to % s to reduce memory pressure " , <nl> + columnFamily , ssTables . getKeyCache ( ) . getCapacity ( ) , newCapacity ) ) ; <nl> + ssTables . getKeyCache ( ) . setCapacity ( newCapacity ) ; <nl> + } <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / service / AbstractCassandraDaemon . java b / src / java / org / apache / cassandra / service / AbstractCassandraDaemon . java <nl> index 8375b34 . . 1d4b956 100644 <nl> - - - a / src / java / org / apache / cassandra / service / AbstractCassandraDaemon . java <nl> + + + b / src / java / org / apache / cassandra / service / AbstractCassandraDaemon . java <nl> @ @ - 162 , 6 + 162 , 15 @ @ public abstract class AbstractCassandraDaemon implements CassandraDaemon <nl> Table . open ( table ) ; <nl> } <nl> <nl> + try <nl> + { <nl> + GCInspector . instance . start ( ) ; <nl> + } <nl> + catch ( Throwable t ) <nl> + { <nl> + logger . warn ( " Unable to start GCInspector ( currently only supported on the Sun JVM ) " ) ; <nl> + } <nl> + <nl> / / replay the log if necessary and check for compaction candidates <nl> CommitLog . recover ( ) ; <nl> CompactionManager . instance . checkAllColumnFamilies ( ) ; <nl> diff - - git a / src / java / org / apache / cassandra / service / GCInspector . java b / src / java / org / apache / cassandra / service / GCInspector . java <nl> index 6a3747b . . 3ed2dfe 100644 <nl> - - - a / src / java / org / apache / cassandra / service / GCInspector . java <nl> + + + b / src / java / org / apache / cassandra / service / GCInspector . java <nl> @ @ - 32 , 6 + 32 , 7 @ @ import javax . management . ObjectName ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . utils . StatusLogger ; <nl> <nl> public class GCInspector <nl> @ @ - 46 , 6 + 47 , 7 @ @ public class GCInspector <nl> private HashMap < String , Long > gctimes = new HashMap < String , Long > ( ) ; <nl> <nl> List < Object > beans = new ArrayList < Object > ( ) ; / / these are instances of com . sun . management . GarbageCollectorMXBean <nl> + private volatile boolean cacheSizesReduced ; <nl> <nl> public GCInspector ( ) <nl> { <nl> @ @ - 87 , 13 + 89 , 13 @ @ public class GCInspector <nl> { <nl> public void run ( ) <nl> { <nl> - logIntervalGCStats ( ) ; <nl> + logGCResults ( ) ; <nl> } <nl> } ; <nl> StorageService . scheduledTasks . scheduleWithFixedDelay ( t , INTERVAL _ IN _ MS , INTERVAL _ IN _ MS , TimeUnit . MILLISECONDS ) ; <nl> } <nl> <nl> - private void logIntervalGCStats ( ) <nl> + private void logGCResults ( ) <nl> { <nl> for ( Object gc : beans ) <nl> { <nl> @ @ - 121 , 7 + 123 , 7 @ @ public class GCInspector <nl> } <nl> <nl> String st = String . format ( " GC for % s : % s ms , % s reclaimed leaving % s used ; max is % s " , <nl> - gcw . getName ( ) , gcw . getDuration ( ) , previousMemoryUsed - memoryUsed , memoryUsed , memoryMax ) ; <nl> + gcw . getName ( ) , gcw . getDuration ( ) , previousMemoryUsed - memoryUsed , memoryUsed , memoryMax ) ; <nl> if ( gcw . getDuration ( ) > MIN _ DURATION ) <nl> logger . info ( st ) ; <nl> else if ( logger . isDebugEnabled ( ) ) <nl> @ @ - 129 , 6 + 131 , 25 @ @ public class GCInspector <nl> <nl> if ( gcw . getDuration ( ) > MIN _ DURATION _ TPSTATS ) <nl> StatusLogger . log ( ) ; <nl> + <nl> + / / if we just finished a full collection and we ' re still using a lot of memory , try to reduce the pressure <nl> + if ( gcw . getName ( ) . equals ( " ConcurrentMarkSweep " ) ) <nl> + { <nl> + double usage = ( double ) memoryUsed / memoryMax ; <nl> + <nl> + if ( memoryUsed > DatabaseDescriptor . getReduceCacheSizesAt ( ) * memoryMax & & ! cacheSizesReduced ) <nl> + { <nl> + cacheSizesReduced = true ; <nl> + logger . warn ( " Heap is " + usage + " full . You may need to reduce memtable and / or cache sizes . Cassandra is now reducing cache sizes to free up memory . Adjust reduce _ cache _ sizes _ at threshold in cassandra . yaml if you don ' t want Cassandra to do this automatically " ) ; <nl> + StorageService . instance . reduceCacheSizes ( ) ; <nl> + } <nl> + <nl> + if ( memoryUsed > DatabaseDescriptor . getFlushLargestMemtablesAt ( ) * memoryMax ) <nl> + { <nl> + logger . warn ( " Heap is " + usage + " full . You may need to reduce memtable and / or cache sizes . Cassandra will now flush up to the two largest memtables to free up memory . Adjust flush _ largest _ memtables _ at threshold in cassandra . yaml if you don ' t want Cassandra to do this automatically " ) ; <nl> + StorageService . instance . flushLargestMemtables ( ) ; <nl> + } <nl> + } <nl> } <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / service / StorageService . java b / src / java / org / apache / cassandra / service / StorageService . java <nl> index 0f82712 . . 070ebed 100644 <nl> - - - a / src / java / org / apache / cassandra / service / StorageService . java <nl> + + + b / src / java / org / apache / cassandra / service / StorageService . java <nl> @ @ - 363 , 15 + 363 , 6 @ @ public class StorageService implements IEndpointStateChangeSubscriber , StorageSe <nl> initialized = true ; <nl> isClientMode = false ; <nl> <nl> - try <nl> - { <nl> - GCInspector . instance . start ( ) ; <nl> - } <nl> - catch ( Throwable t ) <nl> - { <nl> - logger _ . warn ( " Unable to start GCInspector ( currently only supported on the Sun JVM ) " ) ; <nl> - } <nl> - <nl> if ( Boolean . parseBoolean ( System . getProperty ( " cassandra . load _ ring _ state " , " true " ) ) ) <nl> { <nl> logger _ . info ( " Loading persisted ring state " ) ; <nl> @ @ - 2176 , 4 + 2167 , 39 @ @ public class StorageService implements IEndpointStateChangeSubscriber , StorageSe <nl> { <nl> return efficientCrossDCWrites ; <nl> } <nl> + <nl> + / * * <nl> + * Flushes the two largest memtables by ops and by throughput <nl> + * / <nl> + public void flushLargestMemtables ( ) <nl> + { <nl> + ColumnFamilyStore largestByOps = null ; <nl> + ColumnFamilyStore largestByThroughput = null ; <nl> + for ( ColumnFamilyStore cfs : ColumnFamilyStore . all ( ) ) <nl> + { <nl> + if ( largestByOps = = null | | cfs . getMemtableColumnsCount ( ) > largestByOps . getMemtableColumnsCount ( ) ) <nl> + largestByOps = cfs ; <nl> + if ( largestByThroughput = = null | | cfs . getMemtableThroughputInMB ( ) > largestByThroughput . getMemtableThroughputInMB ( ) ) <nl> + largestByThroughput = cfs ; <nl> + } <nl> + if ( largestByOps = = null ) <nl> + { <nl> + logger _ . error ( " Unable to reduce heap usage since there are no column families defined " ) ; <nl> + return ; <nl> + } <nl> + <nl> + logger _ . warn ( " Flushing " + largestByOps + " to relieve memory pressure " ) ; <nl> + largestByOps . forceFlush ( ) ; <nl> + if ( largestByThroughput ! = largestByOps ) <nl> + { <nl> + logger _ . warn ( " Flushing " + largestByThroughput + " to relieve memory pressure " ) ; <nl> + largestByThroughput . forceFlush ( ) ; <nl> + } <nl> + } <nl> + <nl> + public void reduceCacheSizes ( ) <nl> + { <nl> + for ( ColumnFamilyStore cfs : ColumnFamilyStore . all ( ) ) <nl> + cfs . reduceCacheSizes ( ) ; <nl> + } <nl> }

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index fdff490 . . 4392159 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 5 @ @ 
 2 . 0 . 10 
 + * Minor leak in sstable2jon ( CASSANDRA - 7709 ) 
 * Add cassandra . auto _ bootstrap system property ( CASSANDRA - 7650 ) 
 * Remove CqlPagingRecordReader / CqlPagingInputFormat ( CASSANDRA - 7570 ) 
 * Fix IncompatibleClassChangeError from hadoop2 ( CASSANDRA - 7229 ) 
 diff - - git a / src / java / org / apache / cassandra / tools / SSTableExport . java b / src / java / org / apache / cassandra / tools / SSTableExport . java 
 index 197585b . . f8b85c3 100644 
 - - - a / src / java / org / apache / cassandra / tools / SSTableExport . java 
 + + + b / src / java / org / apache / cassandra / tools / SSTableExport . java 
 @ @ - 252 , 20 + 252 , 26 @ @ public class SSTableExport 
 throws IOException 
 { 
 KeyIterator iter = new KeyIterator ( desc ) ; 
 - DecoratedKey lastKey = null ; 
 - while ( iter . hasNext ( ) ) 
 + try 
 { 
 - DecoratedKey key = iter . next ( ) ; 
 + DecoratedKey lastKey = null ; 
 + while ( iter . hasNext ( ) ) 
 + { 
 + DecoratedKey key = iter . next ( ) ; 
 
 - / / validate order of the keys in the sstable 
 - if ( lastKey ! = null & & lastKey . compareTo ( key ) > 0 ) 
 - throw new IOException ( " Key out of order ! " + lastKey + " > " + key ) ; 
 - lastKey = key ; 
 + / / validate order of the keys in the sstable 
 + if ( lastKey ! = null & & lastKey . compareTo ( key ) > 0 ) 
 + throw new IOException ( " Key out of order ! " + lastKey + " > " + key ) ; 
 + lastKey = key ; 
 
 - outs . println ( bytesToHex ( key . key ) ) ; 
 - checkStream ( outs ) ; / / flushes 
 + outs . println ( bytesToHex ( key . key ) ) ; 
 + checkStream ( outs ) ; / / flushes 
 + } 
 + } 
 + finally 
 + { 
 + iter . close ( ) ; 
 } 
 - iter . close ( ) ; 
 } 
 
 / * * 
 @ @ - 281 , 51 + 287 , 59 @ @ public class SSTableExport 
 { 
 SSTableReader sstable = SSTableReader . open ( desc ) ; 
 RandomAccessReader dfile = sstable . openDataReader ( ) ; 
 + try 
 + { 
 + IPartitioner < ? > partitioner = sstable . partitioner ; 
 
 - IPartitioner < ? > partitioner = sstable . partitioner ; 
 + if ( excludes ! = null ) 
 + toExport . removeAll ( Arrays . asList ( excludes ) ) ; 
 
 - if ( excludes ! = null ) 
 - toExport . removeAll ( Arrays . asList ( excludes ) ) ; 
 + outs . println ( " [ " ) ; 
 
 - outs . println ( " [ " ) ; 
 + int i = 0 ; 
 
 - int i = 0 ; 
 + / / last key to compare order 
 + DecoratedKey lastKey = null ; 
 
 - / / last key to compare order 
 - DecoratedKey lastKey = null ; 
 + for ( String key : toExport ) 
 + { 
 + DecoratedKey decoratedKey = partitioner . decorateKey ( hexToBytes ( key ) ) ; 
 
 - for ( String key : toExport ) 
 - { 
 - DecoratedKey decoratedKey = partitioner . decorateKey ( hexToBytes ( key ) ) ; 
 + if ( lastKey ! = null & & lastKey . compareTo ( decoratedKey ) > 0 ) 
 + throw new IOException ( " Key out of order ! " + lastKey + " > " + decoratedKey ) ; 
 
 - if ( lastKey ! = null & & lastKey . compareTo ( decoratedKey ) > 0 ) 
 - throw new IOException ( " Key out of order ! " + lastKey + " > " + decoratedKey ) ; 
 + lastKey = decoratedKey ; 
 
 - lastKey = decoratedKey ; 
 + RowIndexEntry entry = sstable . getPosition ( decoratedKey , SSTableReader . Operator . EQ ) ; 
 + if ( entry = = null ) 
 + continue ; 
 
 - RowIndexEntry entry = sstable . getPosition ( decoratedKey , SSTableReader . Operator . EQ ) ; 
 - if ( entry = = null ) 
 - continue ; 
 + dfile . seek ( entry . position ) ; 
 + ByteBufferUtil . readWithShortLength ( dfile ) ; / / row key 
 + if ( sstable . descriptor . version . hasRowSizeAndColumnCount ) 
 + dfile . readLong ( ) ; / / row size 
 + DeletionInfo deletionInfo = new DeletionInfo ( DeletionTime . serializer . deserialize ( dfile ) ) ; 
 + int columnCount = sstable . descriptor . version . hasRowSizeAndColumnCount ? dfile . readInt ( ) 
 + : Integer . MAX _ VALUE ; 
 
 - dfile . seek ( entry . position ) ; 
 - ByteBufferUtil . readWithShortLength ( dfile ) ; / / row key 
 - if ( sstable . descriptor . version . hasRowSizeAndColumnCount ) 
 - dfile . readLong ( ) ; / / row size 
 - DeletionInfo deletionInfo = new DeletionInfo ( DeletionTime . serializer . deserialize ( dfile ) ) ; 
 - int columnCount = sstable . descriptor . version . hasRowSizeAndColumnCount ? dfile . readInt ( ) : Integer . MAX _ VALUE ; 
 + Iterator < OnDiskAtom > atomIterator = sstable . metadata . getOnDiskIterator ( dfile , columnCount , 
 + sstable . descriptor . version ) ; 
 
 - Iterator < OnDiskAtom > atomIterator = sstable . metadata . getOnDiskIterator ( dfile , columnCount , sstable . descriptor . version ) ; 
 + checkStream ( outs ) ; 
 
 - checkStream ( outs ) ; 
 + if ( i ! = 0 ) 
 + outs . println ( " , " ) ; 
 + i + + ; 
 + serializeRow ( deletionInfo , atomIterator , sstable . metadata , decoratedKey , outs ) ; 
 + } 
 
 - if ( i ! = 0 ) 
 - outs . println ( " , " ) ; 
 - i + + ; 
 - serializeRow ( deletionInfo , atomIterator , sstable . metadata , decoratedKey , outs ) ; 
 + outs . println ( " \ n ] " ) ; 
 + outs . flush ( ) ; 
 + } 
 + finally 
 + { 
 + dfile . close ( ) ; 
 } 
 - 
 - outs . println ( " \ n ] " ) ; 
 - outs . flush ( ) ; 
 } 
 
 / / This is necessary to accommodate the test suite since you cannot open a Reader more 
 @ @ - 337 , 36 + 351 , 39 @ @ public class SSTableExport 
 if ( excludes ! = null ) 
 excludeSet = new HashSet < String > ( Arrays . asList ( excludes ) ) ; 
 
 - 
 SSTableIdentityIterator row ; 
 SSTableScanner scanner = reader . getScanner ( ) ; 
 + try 
 + { 
 + outs . println ( " [ " ) ; 
 
 - outs . println ( " [ " ) ; 
 + int i = 0 ; 
 
 - int i = 0 ; 
 + / / collecting keys to export 
 + while ( scanner . hasNext ( ) ) 
 + { 
 + row = ( SSTableIdentityIterator ) scanner . next ( ) ; 
 
 - / / collecting keys to export 
 - while ( scanner . hasNext ( ) ) 
 - { 
 - row = ( SSTableIdentityIterator ) scanner . next ( ) ; 
 + String currentKey = bytesToHex ( row . getKey ( ) . key ) ; 
 
 - String currentKey = bytesToHex ( row . getKey ( ) . key ) ; 
 + if ( excludeSet . contains ( currentKey ) ) 
 + continue ; 
 + else if ( i ! = 0 ) 
 + outs . println ( " , " ) ; 
 
 - if ( excludeSet . contains ( currentKey ) ) 
 - continue ; 
 - else if ( i ! = 0 ) 
 - outs . println ( " , " ) ; 
 + serializeRow ( row , row . getKey ( ) , outs ) ; 
 + checkStream ( outs ) ; 
 
 - serializeRow ( row , row . getKey ( ) , outs ) ; 
 - checkStream ( outs ) ; 
 + i + + ; 
 + } 
 
 - i + + ; 
 + outs . println ( " \ n ] " ) ; 
 + outs . flush ( ) ; 
 + } 
 + finally 
 + { 
 + scanner . close ( ) ; 
 } 
 - 
 - outs . println ( " \ n ] " ) ; 
 - outs . flush ( ) ; 
 - 
 - scanner . close ( ) ; 
 } 
 
 / * *

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 28bf806 . . 9739983 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 9 + 1 , 3 @ @ 
 - 0 . 7 . 2 
 - * cache writing moved to CompactionManager to reduce i / o contention and 
 - updated to use non - cache - polluting writes ( CASSANDRA - 2053 ) 
 - * page through large rows when exporting to JSON ( CASSANDRA - 2041 ) 
 - 
 - 
 0 . 7 . 1 
 * buffer network stack to avoid inefficient small TCP messages while avoiding 
 the nagle / delayed ack problem ( CASSANDRA - 1896 ) 
 @ @ - 56 , 6 + 50 , 11 @ @ 
 * avoid blocking gossip while deleting handoff hints ( CASSANDRA - 2073 ) 
 * ignore messages from newer versions , keep track of nodes in gossip 
 regardless of version ( CASSANDRA - 1970 ) 
 + * cache writing moved to CompactionManager to reduce i / o contention and 
 + updated to use non - cache - polluting writes ( CASSANDRA - 2053 ) 
 + * page through large rows when exporting to JSON ( CASSANDRA - 2041 ) 
 + * add flush _ largest _ memtables _ at and reduce _ cache _ sizes _ at options 
 + ( CASSANDRA - 2142 ) 
 
 
 0 . 7 . 0 - final 
 diff - - git a / NEWS . txt b / NEWS . txt 
 index df58fde . . 798adcf 100644 
 - - - a / NEWS . txt 
 + + + b / NEWS . txt 
 @ @ - 1 , 17 + 1 , 33 @ @ 
 0 . 7 . 1 
 = = = = = 
 
 - Uprading 
 - - - - - - - - - 
 + Upgrading 
 + - - - - - - - - - 
 - 0 . 7 . 1 is completely backwards compatible with 0 . 7 . 0 . Just restart 
 each node with the new version , one at a time . ( The cluster does 
 not all need to be upgraded simultaneously . ) 
 
 Features 
 - - - - - - - - 
 - - Cassandra can perform writes efficiently across datacenters by 
 + - added flush _ largest _ memtables _ at and reduce _ cache _ sizes _ at options 
 + to cassandra . yaml as an escape valve for memory pressure 
 + - added option to specify - Dcassandra . join _ ring = false on startup 
 + to allow " warm spare " nodes or performing JMX maintenance before 
 + joining the ring 
 + 
 + Performance 
 + - - - - - - - - - - - 
 + - Disk writes and sequential scans avoid polluting page cache 
 + ( requires JNA to be enabled ) 
 + - Cassandra performs writes efficiently across datacenters by 
 sending a single copy of the mutation and having the recipient 
 forward that to other replicas in its datacenter . 
 + - Improved network buffering 
 + - Reduced lock contention on memtable flush 
 + - Optimized supercolumn deserialization 
 + - Zero - copy reads from mmapped sstable files 
 + - Explicitly set higher JVM new generation size 
 + - Reduced i / o contention during saving of caches 
 
 
 0 . 7 . 0 
 diff - - git a / conf / cassandra . yaml b / conf / cassandra . yaml 
 index 5f43843 . . 90193cf 100644 
 - - - a / conf / cassandra . yaml 
 + + + b / conf / cassandra . yaml 
 @ @ - 90 , 6 + 90 , 31 @ @ commitlog _ sync : periodic 
 # milliseconds . 
 commitlog _ sync _ period _ in _ ms : 10000 
 
 + # emergency pressure valve : each time heap usage after a full ( CMS ) 
 + # garbage collection is above this fraction of the max , Cassandra will 
 + # flush the largest memtables . 
 + # 
 + # Set to 1 . 0 to disable . Setting this lower than 
 + # CMSInitiatingOccupancyFraction is not likely to be useful . 
 + # 
 + # RELYING ON THIS AS YOUR PRIMARY TUNING MECHANISM WILL WORK POORLY : 
 + # it is most effective under light to moderate load , or read - heavy 
 + # workloads ; under truly massive write load , it will often be too 
 + # little , too late . 
 + flush _ largest _ memtables _ at : 0 . 75 
 + 
 + # emergency pressure valve # 2 : the first time heap usage after a full 
 + # ( CMS ) garbage collection is above this fraction of the max , 
 + # Cassandra will reduce cache maximum _ capacity _ to the given fraction 
 + # of the current _ size _ . Should usually be set substantially above 
 + # flush _ largest _ memtables _ at , since that will have less long - term 
 + # impact on the system . 
 + # 
 + # Set to 1 . 0 to disable . Setting this lower than 
 + # CMSInitiatingOccupancyFraction is not likely to be useful . 
 + reduce _ cache _ sizes _ at : 0 . 85 
 + reduce _ cache _ capacity _ to : 0 . 6 
 + 
 # Addresses of hosts that are deemed contact points . 
 # Cassandra nodes use this list of hosts to find each other and learn 
 # the topology of the ring . You must change this if you are running 
 diff - - git a / src / java / org / apache / cassandra / config / Config . java b / src / java / org / apache / cassandra / config / Config . java 
 index fd4bca0 . . 25e719b 100644 
 - - - a / src / java / org / apache / cassandra / config / Config . java 
 + + + b / src / java / org / apache / cassandra / config / Config . java 
 @ @ - 103 , 7 + 103 , 10 @ @ public class Config 
 public Integer index _ interval = 128 ; 
 
 public List < RawKeyspace > keyspaces ; 
 - 
 + public Double flush _ largest _ memtables _ at = 1 . 0 ; 
 + public Double reduce _ cache _ sizes _ at = 1 . 0 ; 
 + public double reduce _ cache _ capacity _ to = 0 . 6 ; 
 + 
 public static enum CommitLogSync { 
 periodic , 
 batch 
 diff - - git a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 index be073cd . . 75997d6 100644 
 - - - a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 + + + b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 @ @ - 20 , 7 + 20 , 6 @ @ package org . apache . cassandra . config ; 
 
 import java . io . * ; 
 import java . net . InetAddress ; 
 - import java . net . MalformedURLException ; 
 import java . net . URL ; 
 import java . net . UnknownHostException ; 
 import java . nio . ByteBuffer ; 
 @ @ - 1155 , 4 + 1154 , 19 @ @ public class DatabaseDescriptor 
 { 
 conf . dynamic _ snitch _ badness _ threshold = dynamicBadnessThreshold ; 
 } 
 + 
 + public static double getFlushLargestMemtablesAt ( ) 
 + { 
 + return conf . flush _ largest _ memtables _ at ; 
 + } 
 + 
 + public static double getReduceCacheSizesAt ( ) 
 + { 
 + return conf . reduce _ cache _ sizes _ at ; 
 + } 
 + 
 + public static double getReduceCacheCapacityTo ( ) 
 + { 
 + return conf . reduce _ cache _ capacity _ to ; 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 index d9abe67 . . 6501a8a 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 @ @ - 1878 , 7 + 1878 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 public String toString ( ) 
 { 
 return " ColumnFamilyStore ( " + 
 - " table = ' " + table + ' \ ' ' + 
 + " table = ' " + table . name + ' \ ' ' + 
 " , columnFamily = ' " + columnFamily + ' \ ' ' + 
 ' ) ' ; 
 } 
 @ @ - 2019 , 4 + 2019 , 26 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 { 
 return partitioner instanceof LocalPartitioner ; 
 } 
 + 
 + / * * 
 + * sets each cache ' s maximum capacity to 75 % of its current size 
 + * / 
 + public void reduceCacheSizes ( ) 
 + { 
 + if ( ssTables . getRowCache ( ) . getCapacity ( ) > 0 ) 
 + { 
 + int newCapacity = ( int ) ( DatabaseDescriptor . getReduceCacheCapacityTo ( ) * ssTables . getRowCache ( ) . getSize ( ) ) ; 
 + logger . warn ( String . format ( " Reducing % s row cache capacity from % d to % s to reduce memory pressure " , 
 + columnFamily , ssTables . getRowCache ( ) . getCapacity ( ) , newCapacity ) ) ; 
 + ssTables . getRowCache ( ) . setCapacity ( newCapacity ) ; 
 + } 
 + 
 + if ( ssTables . getKeyCache ( ) . getCapacity ( ) > 0 ) 
 + { 
 + int newCapacity = ( int ) ( DatabaseDescriptor . getReduceCacheCapacityTo ( ) * ssTables . getKeyCache ( ) . getSize ( ) ) ; 
 + logger . warn ( String . format ( " Reducing % s key cache capacity from % d to % s to reduce memory pressure " , 
 + columnFamily , ssTables . getKeyCache ( ) . getCapacity ( ) , newCapacity ) ) ; 
 + ssTables . getKeyCache ( ) . setCapacity ( newCapacity ) ; 
 + } 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / service / AbstractCassandraDaemon . java b / src / java / org / apache / cassandra / service / AbstractCassandraDaemon . java 
 index 8375b34 . . 1d4b956 100644 
 - - - a / src / java / org / apache / cassandra / service / AbstractCassandraDaemon . java 
 + + + b / src / java / org / apache / cassandra / service / AbstractCassandraDaemon . java 
 @ @ - 162 , 6 + 162 , 15 @ @ public abstract class AbstractCassandraDaemon implements CassandraDaemon 
 Table . open ( table ) ; 
 } 
 
 + try 
 + { 
 + GCInspector . instance . start ( ) ; 
 + } 
 + catch ( Throwable t ) 
 + { 
 + logger . warn ( " Unable to start GCInspector ( currently only supported on the Sun JVM ) " ) ; 
 + } 
 + 
 / / replay the log if necessary and check for compaction candidates 
 CommitLog . recover ( ) ; 
 CompactionManager . instance . checkAllColumnFamilies ( ) ; 
 diff - - git a / src / java / org / apache / cassandra / service / GCInspector . java b / src / java / org / apache / cassandra / service / GCInspector . java 
 index 6a3747b . . 3ed2dfe 100644 
 - - - a / src / java / org / apache / cassandra / service / GCInspector . java 
 + + + b / src / java / org / apache / cassandra / service / GCInspector . java 
 @ @ - 32 , 6 + 32 , 7 @ @ import javax . management . ObjectName ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . utils . StatusLogger ; 
 
 public class GCInspector 
 @ @ - 46 , 6 + 47 , 7 @ @ public class GCInspector 
 private HashMap < String , Long > gctimes = new HashMap < String , Long > ( ) ; 
 
 List < Object > beans = new ArrayList < Object > ( ) ; / / these are instances of com . sun . management . GarbageCollectorMXBean 
 + private volatile boolean cacheSizesReduced ; 
 
 public GCInspector ( ) 
 { 
 @ @ - 87 , 13 + 89 , 13 @ @ public class GCInspector 
 { 
 public void run ( ) 
 { 
 - logIntervalGCStats ( ) ; 
 + logGCResults ( ) ; 
 } 
 } ; 
 StorageService . scheduledTasks . scheduleWithFixedDelay ( t , INTERVAL _ IN _ MS , INTERVAL _ IN _ MS , TimeUnit . MILLISECONDS ) ; 
 } 
 
 - private void logIntervalGCStats ( ) 
 + private void logGCResults ( ) 
 { 
 for ( Object gc : beans ) 
 { 
 @ @ - 121 , 7 + 123 , 7 @ @ public class GCInspector 
 } 
 
 String st = String . format ( " GC for % s : % s ms , % s reclaimed leaving % s used ; max is % s " , 
 - gcw . getName ( ) , gcw . getDuration ( ) , previousMemoryUsed - memoryUsed , memoryUsed , memoryMax ) ; 
 + gcw . getName ( ) , gcw . getDuration ( ) , previousMemoryUsed - memoryUsed , memoryUsed , memoryMax ) ; 
 if ( gcw . getDuration ( ) > MIN _ DURATION ) 
 logger . info ( st ) ; 
 else if ( logger . isDebugEnabled ( ) ) 
 @ @ - 129 , 6 + 131 , 25 @ @ public class GCInspector 
 
 if ( gcw . getDuration ( ) > MIN _ DURATION _ TPSTATS ) 
 StatusLogger . log ( ) ; 
 + 
 + / / if we just finished a full collection and we ' re still using a lot of memory , try to reduce the pressure 
 + if ( gcw . getName ( ) . equals ( " ConcurrentMarkSweep " ) ) 
 + { 
 + double usage = ( double ) memoryUsed / memoryMax ; 
 + 
 + if ( memoryUsed > DatabaseDescriptor . getReduceCacheSizesAt ( ) * memoryMax & & ! cacheSizesReduced ) 
 + { 
 + cacheSizesReduced = true ; 
 + logger . warn ( " Heap is " + usage + " full . You may need to reduce memtable and / or cache sizes . Cassandra is now reducing cache sizes to free up memory . Adjust reduce _ cache _ sizes _ at threshold in cassandra . yaml if you don ' t want Cassandra to do this automatically " ) ; 
 + StorageService . instance . reduceCacheSizes ( ) ; 
 + } 
 + 
 + if ( memoryUsed > DatabaseDescriptor . getFlushLargestMemtablesAt ( ) * memoryMax ) 
 + { 
 + logger . warn ( " Heap is " + usage + " full . You may need to reduce memtable and / or cache sizes . Cassandra will now flush up to the two largest memtables to free up memory . Adjust flush _ largest _ memtables _ at threshold in cassandra . yaml if you don ' t want Cassandra to do this automatically " ) ; 
 + StorageService . instance . flushLargestMemtables ( ) ; 
 + } 
 + } 
 } 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / service / StorageService . java b / src / java / org / apache / cassandra / service / StorageService . java 
 index 0f82712 . . 070ebed 100644 
 - - - a / src / java / org / apache / cassandra / service / StorageService . java 
 + + + b / src / java / org / apache / cassandra / service / StorageService . java 
 @ @ - 363 , 15 + 363 , 6 @ @ public class StorageService implements IEndpointStateChangeSubscriber , StorageSe 
 initialized = true ; 
 isClientMode = false ; 
 
 - try 
 - { 
 - GCInspector . instance . start ( ) ; 
 - } 
 - catch ( Throwable t ) 
 - { 
 - logger _ . warn ( " Unable to start GCInspector ( currently only supported on the Sun JVM ) " ) ; 
 - } 
 - 
 if ( Boolean . parseBoolean ( System . getProperty ( " cassandra . load _ ring _ state " , " true " ) ) ) 
 { 
 logger _ . info ( " Loading persisted ring state " ) ; 
 @ @ - 2176 , 4 + 2167 , 39 @ @ public class StorageService implements IEndpointStateChangeSubscriber , StorageSe 
 { 
 return efficientCrossDCWrites ; 
 } 
 + 
 + / * * 
 + * Flushes the two largest memtables by ops and by throughput 
 + * / 
 + public void flushLargestMemtables ( ) 
 + { 
 + ColumnFamilyStore largestByOps = null ; 
 + ColumnFamilyStore largestByThroughput = null ; 
 + for ( ColumnFamilyStore cfs : ColumnFamilyStore . all ( ) ) 
 + { 
 + if ( largestByOps = = null | | cfs . getMemtableColumnsCount ( ) > largestByOps . getMemtableColumnsCount ( ) ) 
 + largestByOps = cfs ; 
 + if ( largestByThroughput = = null | | cfs . getMemtableThroughputInMB ( ) > largestByThroughput . getMemtableThroughputInMB ( ) ) 
 + largestByThroughput = cfs ; 
 + } 
 + if ( largestByOps = = null ) 
 + { 
 + logger _ . error ( " Unable to reduce heap usage since there are no column families defined " ) ; 
 + return ; 
 + } 
 + 
 + logger _ . warn ( " Flushing " + largestByOps + " to relieve memory pressure " ) ; 
 + largestByOps . forceFlush ( ) ; 
 + if ( largestByThroughput ! = largestByOps ) 
 + { 
 + logger _ . warn ( " Flushing " + largestByThroughput + " to relieve memory pressure " ) ; 
 + largestByThroughput . forceFlush ( ) ; 
 + } 
 + } 
 + 
 + public void reduceCacheSizes ( ) 
 + { 
 + for ( ColumnFamilyStore cfs : ColumnFamilyStore . all ( ) ) 
 + cfs . reduceCacheSizes ( ) ; 
 + } 
 }
