BLEU SCORE: 0.02931251013275449

TEST MSG: Fix resetAndTruncate : ing CompressionMetadata
GENERATED MSG: enable skipping bad rows on LazilyCompacted path .

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 3e73f91 . . 6de11c5 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 28 , 6 + 28 , 7 @ @ <nl> * Disallow post - query re - ordering when paging ( CASSANDRA - 6722 ) <nl> * Fix potential paging bug with deleted columns ( CASSANDRA - 6748 ) <nl> * Fix NPE on BulkLoader caused by losing StreamEvent ( CASSANDRA - 6636 ) <nl> + * Fix truncating compression metadata ( CASSANDRA - 6791 ) <nl> Merged from 1 . 2 : <nl> * Add CMSClassUnloadingEnabled JVM option ( CASSANDRA - 6541 ) <nl> * Catch memtable flush exceptions during shutdown ( CASSANDRA - 6735 ) <nl> diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> index 54b990f . . eef5b17 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> @ @ - 231 , 7 + 231 , 7 @ @ public class CompressedSequentialWriter extends SequentialWriter <nl> <nl> / / truncate data and index file <nl> truncate ( chunkOffset ) ; <nl> - metadataWriter . resetAndTruncate ( realMark . nextChunkIndex ) ; <nl> + metadataWriter . resetAndTruncate ( realMark . nextChunkIndex - 1 ) ; <nl> } <nl> <nl> / * * <nl> diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> index ee32a0e . . 3c9dfe5 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> @ @ - 19 , 11 + 19 , 13 @ @ <nl> package org . apache . cassandra . io . compress ; <nl> <nl> import java . io . * ; <nl> + import java . util . Collections ; <nl> import java . util . Random ; <nl> <nl> import org . junit . Test ; <nl> <nl> import org . apache . cassandra . db . marshal . BytesType ; <nl> + import org . apache . cassandra . exceptions . ConfigurationException ; <nl> import org . apache . cassandra . io . sstable . CorruptSSTableException ; <nl> import org . apache . cassandra . io . sstable . SSTableMetadata ; <nl> import org . apache . cassandra . io . util . * ; <nl> @ @ - 48 , 6 + 50 , 46 @ @ public class CompressedRandomAccessReaderTest <nl> testResetAndTruncate ( File . createTempFile ( " compressed " , " 1 " ) , true , 10 ) ; <nl> testResetAndTruncate ( File . createTempFile ( " compressed " , " 2 " ) , true , CompressionParameters . DEFAULT _ CHUNK _ LENGTH ) ; <nl> } <nl> + @ Test <nl> + public void test6791 ( ) throws IOException , ConfigurationException <nl> + { <nl> + File f = File . createTempFile ( " compressed6791 _ " , " 3 " ) ; <nl> + String filename = f . getAbsolutePath ( ) ; <nl> + try <nl> + { <nl> + <nl> + SSTableMetadata . Collector sstableMetadataCollector = SSTableMetadata . createCollector ( BytesType . instance ) . replayPosition ( null ) ; <nl> + CompressedSequentialWriter writer = new CompressedSequentialWriter ( f , filename + " . metadata " , false , new CompressionParameters ( SnappyCompressor . instance , 32 , Collections . < String , String > emptyMap ( ) ) , sstableMetadataCollector ) ; <nl> + <nl> + for ( int i = 0 ; i < 20 ; i + + ) <nl> + writer . write ( " x " . getBytes ( ) ) ; <nl> + <nl> + FileMark mark = writer . mark ( ) ; <nl> + / / write enough garbage to create new chunks : <nl> + for ( int i = 0 ; i < 40 ; + + i ) <nl> + writer . write ( " y " . getBytes ( ) ) ; <nl> + <nl> + writer . resetAndTruncate ( mark ) ; <nl> + <nl> + for ( int i = 0 ; i < 20 ; i + + ) <nl> + writer . write ( " x " . getBytes ( ) ) ; <nl> + writer . close ( ) ; <nl> + <nl> + CompressedRandomAccessReader reader = CompressedRandomAccessReader . open ( filename , new CompressionMetadata ( filename + " . metadata " , f . length ( ) , true ) ) ; <nl> + String res = reader . readLine ( ) ; <nl> + assertEquals ( res , " xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx " ) ; <nl> + assertEquals ( 40 , res . length ( ) ) ; <nl> + } <nl> + finally <nl> + { <nl> + / / cleanup <nl> + if ( f . exists ( ) ) <nl> + f . delete ( ) ; <nl> + File metadata = new File ( filename + " . metadata " ) ; <nl> + if ( metadata . exists ( ) ) <nl> + metadata . delete ( ) ; <nl> + } <nl> + } <nl> <nl> private void testResetAndTruncate ( File f , boolean compressed , int junkSize ) throws IOException <nl> {
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index b993c48 . . 36050c5 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 15 , 6 + 15 , 7 @ @ dev <nl> * include jna dependency in RPM package ( CASSANDRA - 1690 ) <nl> * add - - skip - keys option to stress . py ( CASSANDRA - 1696 ) <nl> * improve cli handling of non - string column names ( CASSANDRA - 1701 ) <nl> + * enable skipping bad rows on LazilyCompacted path ( CASSANDRA - 1702 ) <nl> <nl> <nl> 0 . 7 . 0 - beta3 <nl> diff - - git a / src / java / org / apache / cassandra / db / ColumnSerializer . java b / src / java / org / apache / cassandra / db / ColumnSerializer . java <nl> index 34ac0ac . . 569289d 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnSerializer . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnSerializer . java <nl> @ @ - 64 , 6 + 64 , 9 @ @ public class ColumnSerializer implements ICompactSerializer2 < IColumn > <nl> public Column deserialize ( DataInput dis ) throws IOException <nl> { <nl> ByteBuffer name = FBUtilities . readShortByteArray ( dis ) ; <nl> + if ( name . remaining ( ) < = 0 ) <nl> + throw new CorruptColumnException ( " invalid column name length " + name . remaining ( ) ) ; <nl> + <nl> int b = dis . readUnsignedByte ( ) ; <nl> if ( ( b & EXPIRATION _ MASK ) ! = 0 ) <nl> { <nl> @ @ - 97 , 4 + 100 , 12 @ @ public class ColumnSerializer implements ICompactSerializer2 < IColumn > <nl> } <nl> } <nl> } <nl> + <nl> + private static class CorruptColumnException extends IOException <nl> + { <nl> + public CorruptColumnException ( String s ) <nl> + { <nl> + super ( s ) ; <nl> + } <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / CompactionManager . java b / src / java / org / apache / cassandra / db / CompactionManager . java <nl> index 057f4a2 . . 077914e 100644 <nl> - - - a / src / java / org / apache / cassandra / db / CompactionManager . java <nl> + + + b / src / java / org / apache / cassandra / db / CompactionManager . java <nl> @ @ - 272 , 16 + 272 , 21 @ @ public class CompactionManager implements CompactionManagerMBean <nl> writer = new SSTableWriter ( newFilename , expectedBloomFilterSize , cfs . metadata , cfs . partitioner ) ; <nl> while ( nni . hasNext ( ) ) <nl> { <nl> - AbstractCompactedRow row = nni . next ( ) ; <nl> + writer . mark ( ) ; <nl> try <nl> { <nl> + AbstractCompactedRow row = nni . next ( ) ; <nl> writer . append ( row ) ; <nl> } <nl> - catch ( IOException ex ) <nl> + catch ( Exception e ) <nl> + { <nl> + logger . error ( " non - fatal error during compaction " , e ) ; <nl> + writer . reset ( ) ; <nl> + } <nl> + catch ( IOError e ) <nl> { <nl> - writer . abort ( ) ; <nl> - / / rethrow the exception so that caller knows compaction failed . <nl> - throw ex ; <nl> + logger . error ( " non - fatal error during compaction " , e ) ; <nl> + writer . reset ( ) ; <nl> } <nl> totalkeysWritten + + ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> index 4b3b5f0 . . 523a2c4 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> @ @ - 38 , 6 + 38 , 7 @ @ import org . apache . cassandra . dht . IPartitioner ; <nl> import org . apache . cassandra . io . AbstractCompactedRow ; <nl> import org . apache . cassandra . io . ICompactionInfo ; <nl> import org . apache . cassandra . io . util . BufferedRandomAccessFile ; <nl> + import org . apache . cassandra . io . util . FileMark ; <nl> import org . apache . cassandra . io . util . FileUtils ; <nl> import org . apache . cassandra . io . util . SegmentedFile ; <nl> import org . apache . cassandra . service . StorageService ; <nl> @ @ - 55 , 6 + 56 , 7 @ @ public class SSTableWriter extends SSTable <nl> private SegmentedFile . Builder dbuilder ; <nl> private final BufferedRandomAccessFile dataFile ; <nl> private DecoratedKey lastWrittenKey ; <nl> + private FileMark dataMark ; <nl> <nl> public SSTableWriter ( String filename , long keyCount ) throws IOException <nl> { <nl> @ @ - 99 , 6 + 101 , 25 @ @ public class SSTableWriter extends SSTable <nl> } <nl> } <nl> <nl> + public void mark ( ) <nl> + { <nl> + dataMark = dataFile . mark ( ) ; <nl> + iwriter . mark ( ) ; <nl> + } <nl> + <nl> + public void reset ( ) <nl> + { <nl> + try <nl> + { <nl> + dataFile . reset ( dataMark ) ; <nl> + iwriter . reset ( ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + throw new IOError ( e ) ; <nl> + } <nl> + } <nl> + <nl> private long beforeAppend ( DecoratedKey decoratedKey ) throws IOException <nl> { <nl> if ( decoratedKey = = null ) <nl> @ @ - 121 , 8 + 142 , 8 @ @ public class SSTableWriter extends SSTable <nl> <nl> if ( logger . isTraceEnabled ( ) ) <nl> logger . trace ( " wrote " + decoratedKey + " at " + dataPosition ) ; <nl> - dbuilder . addPotentialBoundary ( dataPosition ) ; <nl> iwriter . afterAppend ( decoratedKey , dataPosition ) ; <nl> + dbuilder . addPotentialBoundary ( dataPosition ) ; <nl> } <nl> <nl> public void append ( AbstractCompactedRow row ) throws IOException <nl> @ @ - 176 , 7 + 197 , 9 @ @ public class SSTableWriter extends SSTable <nl> iwriter . close ( ) ; <nl> <nl> / / main data <nl> + long position = dataFile . getFilePointer ( ) ; <nl> dataFile . close ( ) ; / / calls force <nl> + FileUtils . truncate ( dataFile . getPath ( ) , position ) ; <nl> <nl> / / write sstable statistics <nl> writeStatistics ( descriptor , estimatedRowSize , estimatedColumnCount ) ; <nl> @ @ - 358 , 7 + 381 , 8 @ @ public class SSTableWriter extends SSTable <nl> public final SegmentedFile . Builder builder ; <nl> public final IndexSummary summary ; <nl> public final BloomFilter bf ; <nl> - <nl> + private FileMark mark ; <nl> + <nl> IndexWriter ( Descriptor desc , IPartitioner part , long keyCount ) throws IOException <nl> { <nl> this . desc = desc ; <nl> @ @ - 396 , 11 + 420 , 25 @ @ public class SSTableWriter extends SSTable <nl> stream . close ( ) ; <nl> <nl> / / index <nl> - indexFile . getChannel ( ) . force ( true ) ; <nl> - indexFile . close ( ) ; <nl> + long position = indexFile . getFilePointer ( ) ; <nl> + indexFile . close ( ) ; / / calls force <nl> + FileUtils . truncate ( indexFile . getPath ( ) , position ) ; <nl> <nl> / / finalize in - memory index state <nl> summary . complete ( ) ; <nl> } <nl> + <nl> + public void mark ( ) <nl> + { <nl> + mark = indexFile . mark ( ) ; <nl> + } <nl> + <nl> + public void reset ( ) throws IOException <nl> + { <nl> + / / we can ' t un - set the bloom filter addition , but extra keys in there are harmless . <nl> + / / we can ' t reset dbuilder either , but that is the last thing called in afterappend so <nl> + / / we assume that if that worked then we won ' t be trying to reset . <nl> + indexFile . reset ( mark ) ; <nl> + } <nl> } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / util / FileUtils . java b / src / java / org / apache / cassandra / io / util / FileUtils . java <nl> index e2ca78c . . 11c6b4b 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / FileUtils . java <nl> + + + b / src / java / org / apache / cassandra / io / util / FileUtils . java <nl> @ @ - 19 , 7 + 19 , 9 @ @ <nl> package org . apache . cassandra . io . util ; <nl> <nl> import java . io . File ; <nl> + import java . io . FileNotFoundException ; <nl> import java . io . IOException ; <nl> + import java . io . RandomAccessFile ; <nl> import java . text . DecimalFormat ; <nl> import java . util . Comparator ; <nl> import java . util . List ; <nl> @ @ - 27 , 9 + 29 , 6 @ @ import java . util . List ; <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> - import com . sun . jna . LastErrorException ; <nl> - import org . apache . cassandra . utils . CLibrary ; <nl> - <nl> <nl> public class FileUtils <nl> { <nl> @ @ - 65 , 6 + 64 , 20 @ @ public class FileUtils <nl> throw new IOException ( String . format ( " Failed to rename % s to % s " , from . getPath ( ) , to . getPath ( ) ) ) ; <nl> } <nl> <nl> + public static void truncate ( String path , long size ) throws IOException <nl> + { <nl> + RandomAccessFile file ; <nl> + try <nl> + { <nl> + file = new RandomAccessFile ( path , " rw " ) ; <nl> + } <nl> + catch ( FileNotFoundException e ) <nl> + { <nl> + throw new RuntimeException ( e ) ; <nl> + } <nl> + file . getChannel ( ) . truncate ( size ) ; <nl> + } <nl> + <nl> public static class FileComparator implements Comparator < File > <nl> { <nl> public int compare ( File f , File f2 )

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 3e73f91 . . 6de11c5 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 28 , 6 + 28 , 7 @ @ 
 * Disallow post - query re - ordering when paging ( CASSANDRA - 6722 ) 
 * Fix potential paging bug with deleted columns ( CASSANDRA - 6748 ) 
 * Fix NPE on BulkLoader caused by losing StreamEvent ( CASSANDRA - 6636 ) 
 + * Fix truncating compression metadata ( CASSANDRA - 6791 ) 
 Merged from 1 . 2 : 
 * Add CMSClassUnloadingEnabled JVM option ( CASSANDRA - 6541 ) 
 * Catch memtable flush exceptions during shutdown ( CASSANDRA - 6735 ) 
 diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 index 54b990f . . eef5b17 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 + + + b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 @ @ - 231 , 7 + 231 , 7 @ @ public class CompressedSequentialWriter extends SequentialWriter 
 
 / / truncate data and index file 
 truncate ( chunkOffset ) ; 
 - metadataWriter . resetAndTruncate ( realMark . nextChunkIndex ) ; 
 + metadataWriter . resetAndTruncate ( realMark . nextChunkIndex - 1 ) ; 
 } 
 
 / * * 
 diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 index ee32a0e . . 3c9dfe5 100644 
 - - - a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 + + + b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 @ @ - 19 , 11 + 19 , 13 @ @ 
 package org . apache . cassandra . io . compress ; 
 
 import java . io . * ; 
 + import java . util . Collections ; 
 import java . util . Random ; 
 
 import org . junit . Test ; 
 
 import org . apache . cassandra . db . marshal . BytesType ; 
 + import org . apache . cassandra . exceptions . ConfigurationException ; 
 import org . apache . cassandra . io . sstable . CorruptSSTableException ; 
 import org . apache . cassandra . io . sstable . SSTableMetadata ; 
 import org . apache . cassandra . io . util . * ; 
 @ @ - 48 , 6 + 50 , 46 @ @ public class CompressedRandomAccessReaderTest 
 testResetAndTruncate ( File . createTempFile ( " compressed " , " 1 " ) , true , 10 ) ; 
 testResetAndTruncate ( File . createTempFile ( " compressed " , " 2 " ) , true , CompressionParameters . DEFAULT _ CHUNK _ LENGTH ) ; 
 } 
 + @ Test 
 + public void test6791 ( ) throws IOException , ConfigurationException 
 + { 
 + File f = File . createTempFile ( " compressed6791 _ " , " 3 " ) ; 
 + String filename = f . getAbsolutePath ( ) ; 
 + try 
 + { 
 + 
 + SSTableMetadata . Collector sstableMetadataCollector = SSTableMetadata . createCollector ( BytesType . instance ) . replayPosition ( null ) ; 
 + CompressedSequentialWriter writer = new CompressedSequentialWriter ( f , filename + " . metadata " , false , new CompressionParameters ( SnappyCompressor . instance , 32 , Collections . < String , String > emptyMap ( ) ) , sstableMetadataCollector ) ; 
 + 
 + for ( int i = 0 ; i < 20 ; i + + ) 
 + writer . write ( " x " . getBytes ( ) ) ; 
 + 
 + FileMark mark = writer . mark ( ) ; 
 + / / write enough garbage to create new chunks : 
 + for ( int i = 0 ; i < 40 ; + + i ) 
 + writer . write ( " y " . getBytes ( ) ) ; 
 + 
 + writer . resetAndTruncate ( mark ) ; 
 + 
 + for ( int i = 0 ; i < 20 ; i + + ) 
 + writer . write ( " x " . getBytes ( ) ) ; 
 + writer . close ( ) ; 
 + 
 + CompressedRandomAccessReader reader = CompressedRandomAccessReader . open ( filename , new CompressionMetadata ( filename + " . metadata " , f . length ( ) , true ) ) ; 
 + String res = reader . readLine ( ) ; 
 + assertEquals ( res , " xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx " ) ; 
 + assertEquals ( 40 , res . length ( ) ) ; 
 + } 
 + finally 
 + { 
 + / / cleanup 
 + if ( f . exists ( ) ) 
 + f . delete ( ) ; 
 + File metadata = new File ( filename + " . metadata " ) ; 
 + if ( metadata . exists ( ) ) 
 + metadata . delete ( ) ; 
 + } 
 + } 
 
 private void testResetAndTruncate ( File f , boolean compressed , int junkSize ) throws IOException 
 {

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index b993c48 . . 36050c5 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 15 , 6 + 15 , 7 @ @ dev 
 * include jna dependency in RPM package ( CASSANDRA - 1690 ) 
 * add - - skip - keys option to stress . py ( CASSANDRA - 1696 ) 
 * improve cli handling of non - string column names ( CASSANDRA - 1701 ) 
 + * enable skipping bad rows on LazilyCompacted path ( CASSANDRA - 1702 ) 
 
 
 0 . 7 . 0 - beta3 
 diff - - git a / src / java / org / apache / cassandra / db / ColumnSerializer . java b / src / java / org / apache / cassandra / db / ColumnSerializer . java 
 index 34ac0ac . . 569289d 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnSerializer . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnSerializer . java 
 @ @ - 64 , 6 + 64 , 9 @ @ public class ColumnSerializer implements ICompactSerializer2 < IColumn > 
 public Column deserialize ( DataInput dis ) throws IOException 
 { 
 ByteBuffer name = FBUtilities . readShortByteArray ( dis ) ; 
 + if ( name . remaining ( ) < = 0 ) 
 + throw new CorruptColumnException ( " invalid column name length " + name . remaining ( ) ) ; 
 + 
 int b = dis . readUnsignedByte ( ) ; 
 if ( ( b & EXPIRATION _ MASK ) ! = 0 ) 
 { 
 @ @ - 97 , 4 + 100 , 12 @ @ public class ColumnSerializer implements ICompactSerializer2 < IColumn > 
 } 
 } 
 } 
 + 
 + private static class CorruptColumnException extends IOException 
 + { 
 + public CorruptColumnException ( String s ) 
 + { 
 + super ( s ) ; 
 + } 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / CompactionManager . java b / src / java / org / apache / cassandra / db / CompactionManager . java 
 index 057f4a2 . . 077914e 100644 
 - - - a / src / java / org / apache / cassandra / db / CompactionManager . java 
 + + + b / src / java / org / apache / cassandra / db / CompactionManager . java 
 @ @ - 272 , 16 + 272 , 21 @ @ public class CompactionManager implements CompactionManagerMBean 
 writer = new SSTableWriter ( newFilename , expectedBloomFilterSize , cfs . metadata , cfs . partitioner ) ; 
 while ( nni . hasNext ( ) ) 
 { 
 - AbstractCompactedRow row = nni . next ( ) ; 
 + writer . mark ( ) ; 
 try 
 { 
 + AbstractCompactedRow row = nni . next ( ) ; 
 writer . append ( row ) ; 
 } 
 - catch ( IOException ex ) 
 + catch ( Exception e ) 
 + { 
 + logger . error ( " non - fatal error during compaction " , e ) ; 
 + writer . reset ( ) ; 
 + } 
 + catch ( IOError e ) 
 { 
 - writer . abort ( ) ; 
 - / / rethrow the exception so that caller knows compaction failed . 
 - throw ex ; 
 + logger . error ( " non - fatal error during compaction " , e ) ; 
 + writer . reset ( ) ; 
 } 
 totalkeysWritten + + ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 index 4b3b5f0 . . 523a2c4 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 @ @ - 38 , 6 + 38 , 7 @ @ import org . apache . cassandra . dht . IPartitioner ; 
 import org . apache . cassandra . io . AbstractCompactedRow ; 
 import org . apache . cassandra . io . ICompactionInfo ; 
 import org . apache . cassandra . io . util . BufferedRandomAccessFile ; 
 + import org . apache . cassandra . io . util . FileMark ; 
 import org . apache . cassandra . io . util . FileUtils ; 
 import org . apache . cassandra . io . util . SegmentedFile ; 
 import org . apache . cassandra . service . StorageService ; 
 @ @ - 55 , 6 + 56 , 7 @ @ public class SSTableWriter extends SSTable 
 private SegmentedFile . Builder dbuilder ; 
 private final BufferedRandomAccessFile dataFile ; 
 private DecoratedKey lastWrittenKey ; 
 + private FileMark dataMark ; 
 
 public SSTableWriter ( String filename , long keyCount ) throws IOException 
 { 
 @ @ - 99 , 6 + 101 , 25 @ @ public class SSTableWriter extends SSTable 
 } 
 } 
 
 + public void mark ( ) 
 + { 
 + dataMark = dataFile . mark ( ) ; 
 + iwriter . mark ( ) ; 
 + } 
 + 
 + public void reset ( ) 
 + { 
 + try 
 + { 
 + dataFile . reset ( dataMark ) ; 
 + iwriter . reset ( ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + throw new IOError ( e ) ; 
 + } 
 + } 
 + 
 private long beforeAppend ( DecoratedKey decoratedKey ) throws IOException 
 { 
 if ( decoratedKey = = null ) 
 @ @ - 121 , 8 + 142 , 8 @ @ public class SSTableWriter extends SSTable 
 
 if ( logger . isTraceEnabled ( ) ) 
 logger . trace ( " wrote " + decoratedKey + " at " + dataPosition ) ; 
 - dbuilder . addPotentialBoundary ( dataPosition ) ; 
 iwriter . afterAppend ( decoratedKey , dataPosition ) ; 
 + dbuilder . addPotentialBoundary ( dataPosition ) ; 
 } 
 
 public void append ( AbstractCompactedRow row ) throws IOException 
 @ @ - 176 , 7 + 197 , 9 @ @ public class SSTableWriter extends SSTable 
 iwriter . close ( ) ; 
 
 / / main data 
 + long position = dataFile . getFilePointer ( ) ; 
 dataFile . close ( ) ; / / calls force 
 + FileUtils . truncate ( dataFile . getPath ( ) , position ) ; 
 
 / / write sstable statistics 
 writeStatistics ( descriptor , estimatedRowSize , estimatedColumnCount ) ; 
 @ @ - 358 , 7 + 381 , 8 @ @ public class SSTableWriter extends SSTable 
 public final SegmentedFile . Builder builder ; 
 public final IndexSummary summary ; 
 public final BloomFilter bf ; 
 - 
 + private FileMark mark ; 
 + 
 IndexWriter ( Descriptor desc , IPartitioner part , long keyCount ) throws IOException 
 { 
 this . desc = desc ; 
 @ @ - 396 , 11 + 420 , 25 @ @ public class SSTableWriter extends SSTable 
 stream . close ( ) ; 
 
 / / index 
 - indexFile . getChannel ( ) . force ( true ) ; 
 - indexFile . close ( ) ; 
 + long position = indexFile . getFilePointer ( ) ; 
 + indexFile . close ( ) ; / / calls force 
 + FileUtils . truncate ( indexFile . getPath ( ) , position ) ; 
 
 / / finalize in - memory index state 
 summary . complete ( ) ; 
 } 
 + 
 + public void mark ( ) 
 + { 
 + mark = indexFile . mark ( ) ; 
 + } 
 + 
 + public void reset ( ) throws IOException 
 + { 
 + / / we can ' t un - set the bloom filter addition , but extra keys in there are harmless . 
 + / / we can ' t reset dbuilder either , but that is the last thing called in afterappend so 
 + / / we assume that if that worked then we won ' t be trying to reset . 
 + indexFile . reset ( mark ) ; 
 + } 
 } 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / util / FileUtils . java b / src / java / org / apache / cassandra / io / util / FileUtils . java 
 index e2ca78c . . 11c6b4b 100644 
 - - - a / src / java / org / apache / cassandra / io / util / FileUtils . java 
 + + + b / src / java / org / apache / cassandra / io / util / FileUtils . java 
 @ @ - 19 , 7 + 19 , 9 @ @ 
 package org . apache . cassandra . io . util ; 
 
 import java . io . File ; 
 + import java . io . FileNotFoundException ; 
 import java . io . IOException ; 
 + import java . io . RandomAccessFile ; 
 import java . text . DecimalFormat ; 
 import java . util . Comparator ; 
 import java . util . List ; 
 @ @ - 27 , 9 + 29 , 6 @ @ import java . util . List ; 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 - import com . sun . jna . LastErrorException ; 
 - import org . apache . cassandra . utils . CLibrary ; 
 - 
 
 public class FileUtils 
 { 
 @ @ - 65 , 6 + 64 , 20 @ @ public class FileUtils 
 throw new IOException ( String . format ( " Failed to rename % s to % s " , from . getPath ( ) , to . getPath ( ) ) ) ; 
 } 
 
 + public static void truncate ( String path , long size ) throws IOException 
 + { 
 + RandomAccessFile file ; 
 + try 
 + { 
 + file = new RandomAccessFile ( path , " rw " ) ; 
 + } 
 + catch ( FileNotFoundException e ) 
 + { 
 + throw new RuntimeException ( e ) ; 
 + } 
 + file . getChannel ( ) . truncate ( size ) ; 
 + } 
 + 
 public static class FileComparator implements Comparator < File > 
 { 
 public int compare ( File f , File f2 )
