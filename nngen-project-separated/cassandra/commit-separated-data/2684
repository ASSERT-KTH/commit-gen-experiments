BLEU SCORE: 0.01317095981362614

TEST MSG: Fix resetAndTruncate : ing CompressionMetadata
GENERATED MSG: avoid writing index for rows that fit within a single index block

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 780b528 . . b3c0a35 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 19 , 7 + 19 , 7 @ @ <nl> * Support negative timestamps for CQL3 dates in query string ( CASSANDRA - 6718 ) <nl> * Avoid NPEs when receiving table changes for an unknown keyspace ( CASSANDRA - 5631 ) <nl> * Fix bootstrapping when there is no schema ( CASSANDRA - 6685 ) <nl> - <nl> + * Fix truncating compression metadata ( CASSANDRA - 6791 ) <nl> <nl> 1 . 2 . 15 <nl> * Move handling of migration event source to solve bootstrap race ( CASSANDRA - 6648 ) <nl> diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> index 00eb5a7 . . da55e83 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java <nl> @ @ - 231 , 7 + 231 , 7 @ @ public class CompressedSequentialWriter extends SequentialWriter <nl> <nl> / / truncate data and index file <nl> truncate ( chunkOffset ) ; <nl> - metadataWriter . resetAndTruncate ( realMark . nextChunkIndex ) ; <nl> + metadataWriter . resetAndTruncate ( realMark . nextChunkIndex - 1 ) ; <nl> } <nl> <nl> / * * <nl> diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> index 830c3e1 . . 678a650 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> @ @ - 19 , 10 + 19 , 12 @ @ <nl> package org . apache . cassandra . io . compress ; <nl> <nl> import java . io . * ; <nl> + import java . util . Collections ; <nl> import java . util . Random ; <nl> <nl> import org . junit . Test ; <nl> <nl> + import org . apache . cassandra . exceptions . ConfigurationException ; <nl> import org . apache . cassandra . io . sstable . CorruptSSTableException ; <nl> import org . apache . cassandra . io . sstable . SSTableMetadata ; <nl> import org . apache . cassandra . io . util . * ; <nl> @ @ - 95 , 6 + 97 , 47 @ @ public class CompressedRandomAccessReaderTest <nl> } <nl> <nl> @ Test <nl> + public void test6791 ( ) throws IOException , ConfigurationException <nl> + { <nl> + File f = File . createTempFile ( " compressed6791 _ " , " 3 " ) ; <nl> + String filename = f . getAbsolutePath ( ) ; <nl> + try <nl> + { <nl> + <nl> + SSTableMetadata . Collector sstableMetadataCollector = SSTableMetadata . createCollector ( ) . replayPosition ( null ) ; <nl> + CompressedSequentialWriter writer = new CompressedSequentialWriter ( f , filename + " . metadata " , false , new CompressionParameters ( SnappyCompressor . instance , 32 , Collections . < String , String > emptyMap ( ) ) , sstableMetadataCollector ) ; <nl> + <nl> + for ( int i = 0 ; i < 20 ; i + + ) <nl> + writer . write ( " x " . getBytes ( ) ) ; <nl> + <nl> + FileMark mark = writer . mark ( ) ; <nl> + / / write enough garbage to create new chunks : <nl> + for ( int i = 0 ; i < 40 ; i + + ) <nl> + writer . write ( " y " . getBytes ( ) ) ; <nl> + <nl> + writer . resetAndTruncate ( mark ) ; <nl> + <nl> + for ( int i = 0 ; i < 20 ; i + + ) <nl> + writer . write ( " x " . getBytes ( ) ) ; <nl> + writer . close ( ) ; <nl> + <nl> + CompressedRandomAccessReader reader = CompressedRandomAccessReader . open ( filename , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) , false ) ; <nl> + String res = reader . readLine ( ) ; <nl> + assertEquals ( res , " xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx " ) ; <nl> + assertEquals ( 40 , res . length ( ) ) ; <nl> + } <nl> + finally <nl> + { <nl> + / / cleanup <nl> + if ( f . exists ( ) ) <nl> + f . delete ( ) ; <nl> + File metadata = new File ( filename + " . metadata " ) ; <nl> + if ( metadata . exists ( ) ) <nl> + metadata . delete ( ) ; <nl> + } <nl> + } <nl> + <nl> + @ Test <nl> public void testDataCorruptionDetection ( ) throws IOException <nl> { <nl> String CONTENT = " Lorem ipsum dolor sit amet , consectetur adipiscing elit . Etiam vitae . " ;
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / ColumnIndexer . java b / src / java / org / apache / cassandra / db / ColumnIndexer . java <nl> index 3e4a6f8 . . ff67982 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnIndexer . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnIndexer . java <nl> @ @ - 113 , 12 + 113 , 19 @ @ public class ColumnIndexer <nl> / * Write out the bloom filter . * / <nl> writeBloomFilter ( dos , bf ) ; <nl> <nl> - / / write the index <nl> + / / write the index . we should always have at least one computed index block , but we only write it out if there is more than that . <nl> assert indexSizeInBytes > 0 ; <nl> - dos . writeInt ( indexSizeInBytes ) ; <nl> - for ( IndexHelper . IndexInfo cIndexInfo : indexList ) <nl> + if ( indexList . size ( ) > 1 ) <nl> { <nl> - cIndexInfo . serialize ( dos ) ; <nl> + dos . writeInt ( indexSizeInBytes ) ; <nl> + for ( IndexHelper . IndexInfo cIndexInfo : indexList ) <nl> + { <nl> + cIndexInfo . serialize ( dos ) ; <nl> + } <nl> + } <nl> + else <nl> + { <nl> + dos . writeInt ( 0 ) ; <nl> } <nl> 	 } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / db / columniterator / IndexedSliceReader . java b / src / java / org / apache / cassandra / db / columniterator / IndexedSliceReader . java <nl> index 134683f . . 152ca55 100644 <nl> - - - a / src / java / org / apache / cassandra / db / columniterator / IndexedSliceReader . java <nl> + + + b / src / java / org / apache / cassandra / db / columniterator / IndexedSliceReader . java <nl> @ @ - 32 , 9 + 32 , 8 @ @ class IndexedSliceReader extends AbstractIterator < IColumn > implements IColumnIte <nl> private final byte [ ] finishColumn ; <nl> private final boolean reversed ; <nl> <nl> - private int curRangeIndex ; <nl> + private BlockFetcher fetcher ; <nl> private Deque < IColumn > blockColumns = new ArrayDeque < IColumn > ( ) ; <nl> - private final FileMark mark ; <nl> private AbstractType comparator ; <nl> <nl> public IndexedSliceReader ( SSTableReader sstable , FileDataInput input , byte [ ] startColumn , byte [ ] finishColumn , boolean reversed ) <nl> @ @ - 50 , 16 + 49 , 12 @ @ class IndexedSliceReader extends AbstractIterator < IColumn > implements IColumnIte <nl> indexes = IndexHelper . deserializeIndex ( file ) ; <nl> <nl> emptyColumnFamily = ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( sstable . makeColumnFamily ( ) , file ) ; <nl> - file . readInt ( ) ; / / column count <nl> + fetcher = indexes = = null ? new SimpleBlockFetcher ( ) : new IndexedBlockFetcher ( ) ; <nl> } <nl> catch ( IOException e ) <nl> { <nl> throw new IOError ( e ) ; <nl> } <nl> - this . mark = file . mark ( ) ; <nl> - curRangeIndex = IndexHelper . indexFor ( startColumn , indexes , comparator , reversed ) ; <nl> - if ( reversed & & curRangeIndex = = indexes . size ( ) ) <nl> - curRangeIndex - - ; <nl> } <nl> <nl> public ColumnFamily getColumnFamily ( ) <nl> @ @ - 99 , 7 + 94 , 7 @ @ class IndexedSliceReader extends AbstractIterator < IColumn > implements IColumnIte <nl> return column ; <nl> try <nl> { <nl> - if ( column = = null & & ! getNextBlock ( ) ) <nl> + if ( column = = null & & ! fetcher . getNextBlock ( ) ) <nl> return endOfData ( ) ; <nl> } <nl> catch ( IOException e ) <nl> @ @ - 109 , 59 + 104 , 105 @ @ class IndexedSliceReader extends AbstractIterator < IColumn > implements IColumnIte <nl> } <nl> } <nl> <nl> - public boolean getNextBlock ( ) throws IOException <nl> + public void close ( ) <nl> { <nl> - if ( curRangeIndex < 0 | | curRangeIndex > = indexes . size ( ) ) <nl> - return false ; <nl> + } <nl> + <nl> + interface BlockFetcher <nl> + { <nl> + public boolean getNextBlock ( ) throws IOException ; <nl> + } <nl> <nl> - / * seek to the correct offset to the data , and calculate the data size * / <nl> - IndexHelper . IndexInfo curColPosition = indexes . get ( curRangeIndex ) ; <nl> + private class IndexedBlockFetcher implements BlockFetcher <nl> + { <nl> + private final FileMark mark ; <nl> + private int curRangeIndex ; <nl> <nl> - / * see if this read is really necessary . * / <nl> - if ( reversed ) <nl> + IndexedBlockFetcher ( ) throws IOException <nl> { <nl> - if ( ( finishColumn . length > 0 & & comparator . compare ( finishColumn , curColPosition . lastName ) > 0 ) | | <nl> - ( startColumn . length > 0 & & comparator . compare ( startColumn , curColPosition . firstName ) < 0 ) ) <nl> - return false ; <nl> + file . readInt ( ) ; / / column count <nl> + this . mark = file . mark ( ) ; <nl> + curRangeIndex = IndexHelper . indexFor ( startColumn , indexes , comparator , reversed ) ; <nl> + if ( reversed & & curRangeIndex = = indexes . size ( ) ) <nl> + curRangeIndex - - ; <nl> } <nl> - else <nl> + <nl> + public boolean getNextBlock ( ) throws IOException <nl> { <nl> - if ( ( startColumn . length > 0 & & comparator . compare ( startColumn , curColPosition . lastName ) > 0 ) | | <nl> - ( finishColumn . length > 0 & & comparator . compare ( finishColumn , curColPosition . firstName ) < 0 ) ) <nl> + if ( curRangeIndex < 0 | | curRangeIndex > = indexes . size ( ) ) <nl> return false ; <nl> - } <nl> <nl> - boolean outOfBounds = false ; <nl> + / * seek to the correct offset to the data , and calculate the data size * / <nl> + IndexHelper . IndexInfo curColPosition = indexes . get ( curRangeIndex ) ; <nl> <nl> - file . reset ( mark ) ; <nl> - long curOffset = file . skipBytes ( ( int ) curColPosition . offset ) ; <nl> - assert curOffset = = curColPosition . offset ; <nl> - while ( file . bytesPastMark ( mark ) < curColPosition . offset + curColPosition . width & & ! outOfBounds ) <nl> - { <nl> - IColumn column = emptyColumnFamily . getColumnSerializer ( ) . deserialize ( file ) ; <nl> + / * see if this read is really necessary . * / <nl> if ( reversed ) <nl> - blockColumns . addFirst ( column ) ; <nl> + { <nl> + if ( ( finishColumn . length > 0 & & comparator . compare ( finishColumn , curColPosition . lastName ) > 0 ) | | <nl> + ( startColumn . length > 0 & & comparator . compare ( startColumn , curColPosition . firstName ) < 0 ) ) <nl> + return false ; <nl> + } <nl> else <nl> - blockColumns . addLast ( column ) ; <nl> + { <nl> + if ( ( startColumn . length > 0 & & comparator . compare ( startColumn , curColPosition . lastName ) > 0 ) | | <nl> + ( finishColumn . length > 0 & & comparator . compare ( finishColumn , curColPosition . firstName ) < 0 ) ) <nl> + return false ; <nl> + } <nl> <nl> - / * see if we can stop seeking . * / <nl> - if ( ! reversed & & finishColumn . length > 0 ) <nl> - outOfBounds = comparator . compare ( column . name ( ) , finishColumn ) > = 0 ; <nl> - else if ( reversed & & startColumn . length > 0 ) <nl> - outOfBounds = comparator . compare ( column . name ( ) , startColumn ) > = 0 ; <nl> + boolean outOfBounds = false ; <nl> + file . reset ( mark ) ; <nl> + long curOffset = file . skipBytes ( ( int ) curColPosition . offset ) ; <nl> + assert curOffset = = curColPosition . offset ; <nl> + while ( file . bytesPastMark ( mark ) < curColPosition . offset + curColPosition . width & & ! outOfBounds ) <nl> + { <nl> + IColumn column = emptyColumnFamily . getColumnSerializer ( ) . deserialize ( file ) ; <nl> + if ( reversed ) <nl> + blockColumns . addFirst ( column ) ; <nl> + else <nl> + blockColumns . addLast ( column ) ; <nl> + <nl> + / * see if we can stop seeking . * / <nl> + if ( ! reversed & & finishColumn . length > 0 ) <nl> + outOfBounds = comparator . compare ( column . name ( ) , finishColumn ) > = 0 ; <nl> + else if ( reversed & & startColumn . length > 0 ) <nl> + outOfBounds = comparator . compare ( column . name ( ) , startColumn ) > = 0 ; <nl> + } <nl> <nl> - if ( outOfBounds ) <nl> - break ; <nl> + if ( reversed ) <nl> + curRangeIndex - - ; <nl> + else <nl> + curRangeIndex + + ; <nl> + return true ; <nl> } <nl> - <nl> - if ( reversed ) <nl> - curRangeIndex - - ; <nl> - else <nl> - curRangeIndex + + ; <nl> - return true ; <nl> } <nl> <nl> - public void close ( ) throws IOException <nl> + private class SimpleBlockFetcher implements BlockFetcher <nl> { <nl> + private SimpleBlockFetcher ( ) throws IOException <nl> + { <nl> + int columns = file . readInt ( ) ; <nl> + for ( int i = 0 ; i < columns ; i + + ) <nl> + { <nl> + IColumn column = emptyColumnFamily . getColumnSerializer ( ) . deserialize ( file ) ; <nl> + if ( reversed ) <nl> + blockColumns . addFirst ( column ) ; <nl> + else <nl> + blockColumns . addLast ( column ) ; <nl> + <nl> + / * see if we can stop seeking . * / <nl> + boolean outOfBounds = false ; <nl> + if ( ! reversed & & finishColumn . length > 0 ) <nl> + outOfBounds = comparator . compare ( column . name ( ) , finishColumn ) > = 0 ; <nl> + else if ( reversed & & startColumn . length > 0 ) <nl> + outOfBounds = comparator . compare ( column . name ( ) , startColumn ) > = 0 ; <nl> + if ( outOfBounds ) <nl> + break ; <nl> + } <nl> + } <nl> + <nl> + public boolean getNextBlock ( ) throws IOException <nl> + { <nl> + return false ; <nl> + } <nl> } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java b / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java <nl> index 8bbf98f . . 3b686cb 100644 <nl> - - - a / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java <nl> + + + b / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java <nl> @ @ - 53 , 7 + 53 , 7 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement <nl> this ( ssTable , null , key , columnNames ) ; <nl> } <nl> <nl> - public SSTableNamesIterator ( SSTableReader ssTable , FileDataInput file , DecoratedKey key , SortedSet < byte [ ] > columnNames ) <nl> + public SSTableNamesIterator ( SSTableReader sstable , FileDataInput file , DecoratedKey key , SortedSet < byte [ ] > columnNames ) <nl> { <nl> boolean closeFileWhenDone = file = = null ; <nl> <nl> @ @ - 67 , 15 + 67 , 15 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement <nl> / / open the sstable file , if we don ' t have one passed to use from range scan <nl> if ( file = = null ) <nl> { <nl> - file = ssTable . getFileDataInput ( decoratedKey , DatabaseDescriptor . getIndexedReadBufferSizeInKB ( ) * 1024 ) ; <nl> + file = sstable . getFileDataInput ( decoratedKey , DatabaseDescriptor . getIndexedReadBufferSizeInKB ( ) * 1024 ) ; <nl> if ( file = = null ) <nl> return ; <nl> - DecoratedKey keyInDisk = SSTableReader . decodeKey ( ssTable . getPartitioner ( ) , <nl> - ssTable . getDescriptor ( ) , <nl> + DecoratedKey keyInDisk = SSTableReader . decodeKey ( sstable . getPartitioner ( ) , <nl> + sstable . getDescriptor ( ) , <nl> FBUtilities . readShortByteArray ( file ) ) ; <nl> assert keyInDisk . equals ( decoratedKey ) <nl> : String . format ( " % s ! = % s in % s " , keyInDisk , decoratedKey , file . getPath ( ) ) ; <nl> - SSTableReader . readRowSize ( file , ssTable . getDescriptor ( ) ) ; <nl> + SSTableReader . readRowSize ( file , sstable . getDescriptor ( ) ) ; <nl> } <nl> <nl> / / read the requested columns into ` cf ` <nl> @ @ - 85 , 7 + 85 , 7 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement <nl> <nl> / / we can stop early if bloom filter says none of the columns actually exist - - but , <nl> / / we can ' t stop before initializing the cf above , in case there ' s a relevant tombstone <nl> - cf = ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( ssTable . makeColumnFamily ( ) , file ) ; <nl> + cf = ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( sstable . makeColumnFamily ( ) , file ) ; <nl> <nl> List < byte [ ] > filteredColumnNames1 = new ArrayList < byte [ ] > ( columnNames . size ( ) ) ; <nl> for ( byte [ ] name : columnNames ) <nl> @ @ - 99 , 39 + 99 , 10 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement <nl> if ( filteredColumnNames . isEmpty ( ) ) <nl> return ; <nl> <nl> - file . readInt ( ) ; / / column count <nl> - <nl> - / * get the various column ranges we have to read * / <nl> - AbstractType comparator = ssTable . getColumnComparator ( ) ; <nl> - SortedSet < IndexHelper . IndexInfo > ranges = new TreeSet < IndexHelper . IndexInfo > ( IndexHelper . getComparator ( comparator ) ) ; <nl> - for ( byte [ ] name : filteredColumnNames ) <nl> - { <nl> - int index = IndexHelper . indexFor ( name , indexList , comparator , false ) ; <nl> - if ( index = = indexList . size ( ) ) <nl> - continue ; <nl> - IndexHelper . IndexInfo indexInfo = indexList . get ( index ) ; <nl> - if ( comparator . compare ( name , indexInfo . firstName ) < 0 ) <nl> - continue ; <nl> - ranges . add ( indexInfo ) ; <nl> - } <nl> - <nl> - FileMark mark = file . mark ( ) ; <nl> - for ( IndexHelper . IndexInfo indexInfo : ranges ) <nl> - { <nl> - file . reset ( mark ) ; <nl> - long curOffsert = file . skipBytes ( ( int ) indexInfo . offset ) ; <nl> - assert curOffsert = = indexInfo . offset ; <nl> - / / TODO only completely deserialize columns we are interested in <nl> - while ( file . bytesPastMark ( mark ) < indexInfo . offset + indexInfo . width ) <nl> - { <nl> - final IColumn column = cf . getColumnSerializer ( ) . deserialize ( file ) ; <nl> - / / we check vs the original Set , not the filtered List , for efficiency <nl> - if ( columnNames . contains ( column . name ( ) ) ) <nl> - { <nl> - cf . addColumn ( column ) ; <nl> - } <nl> - } <nl> - } <nl> + if ( indexList = = null ) <nl> + readSimpleColumns ( file , columnNames , filteredColumnNames ) ; <nl> + else <nl> + readIndexedColumns ( sstable , file , columnNames , filteredColumnNames , indexList ) ; <nl> <nl> / / create an iterator view of the columns we read <nl> iter = cf . getSortedColumns ( ) . iterator ( ) ; <nl> @ @ - 156 , 6 + 127 , 60 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement <nl> } <nl> } <nl> <nl> + private void readSimpleColumns ( FileDataInput file , SortedSet < byte [ ] > columnNames , List < byte [ ] > filteredColumnNames ) throws IOException <nl> + { <nl> + int columns = file . readInt ( ) ; <nl> + int n = 0 ; <nl> + for ( int i = 0 ; i < columns ; i + + ) <nl> + { <nl> + IColumn column = cf . getColumnSerializer ( ) . deserialize ( file ) ; <nl> + if ( columnNames . contains ( column . name ( ) ) ) <nl> + { <nl> + cf . addColumn ( column ) ; <nl> + if ( n + + > filteredColumnNames . size ( ) ) <nl> + break ; <nl> + } <nl> + } <nl> + } <nl> + <nl> + private void readIndexedColumns ( SSTableReader sstable , FileDataInput file , SortedSet < byte [ ] > columnNames , List < byte [ ] > filteredColumnNames , List < IndexHelper . IndexInfo > indexList ) <nl> + throws IOException <nl> + { <nl> + file . readInt ( ) ; / / column count <nl> + <nl> + / * get the various column ranges we have to read * / <nl> + AbstractType comparator = sstable . getColumnComparator ( ) ; <nl> + SortedSet < IndexHelper . IndexInfo > ranges = new TreeSet < IndexHelper . IndexInfo > ( IndexHelper . getComparator ( comparator ) ) ; <nl> + for ( byte [ ] name : filteredColumnNames ) <nl> + { <nl> + int index = IndexHelper . indexFor ( name , indexList , comparator , false ) ; <nl> + if ( index = = indexList . size ( ) ) <nl> + continue ; <nl> + IndexHelper . IndexInfo indexInfo = indexList . get ( index ) ; <nl> + if ( comparator . compare ( name , indexInfo . firstName ) < 0 ) <nl> + continue ; <nl> + ranges . add ( indexInfo ) ; <nl> + } <nl> + <nl> + FileMark mark = file . mark ( ) ; <nl> + for ( IndexHelper . IndexInfo indexInfo : ranges ) <nl> + { <nl> + file . reset ( mark ) ; <nl> + long curOffsert = file . skipBytes ( ( int ) indexInfo . offset ) ; <nl> + assert curOffsert = = indexInfo . offset ; <nl> + / / TODO only completely deserialize columns we are interested in <nl> + while ( file . bytesPastMark ( mark ) < indexInfo . offset + indexInfo . width ) <nl> + { <nl> + IColumn column = cf . getColumnSerializer ( ) . deserialize ( file ) ; <nl> + / / we check vs the original Set , not the filtered List , for efficiency <nl> + if ( columnNames . contains ( column . name ( ) ) ) <nl> + { <nl> + cf . addColumn ( column ) ; <nl> + } <nl> + } <nl> + } <nl> + } <nl> + <nl> public DecoratedKey getKey ( ) <nl> { <nl> return decoratedKey ; <nl> diff - - git a / src / java / org / apache / cassandra / db / columniterator / SSTableSliceIterator . java b / src / java / org / apache / cassandra / db / columniterator / SSTableSliceIterator . java <nl> index e7ec535 . . 7f25222 100644 <nl> - - - a / src / java / org / apache / cassandra / db / columniterator / SSTableSliceIterator . java <nl> + + + b / src / java / org / apache / cassandra / db / columniterator / SSTableSliceIterator . java <nl> @ @ - 118 , 7 + 118 , 7 @ @ public class SSTableSliceIterator implements IColumnIterator <nl> <nl> public void close ( ) throws IOException <nl> { <nl> - if ( closeFileWhenDone ) <nl> + if ( closeFileWhenDone & & file ! = null ) <nl> file . close ( ) ; <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java <nl> index ae1bada . . 1db7441 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java <nl> @ @ - 68 , 9 + 68 , 10 @ @ public class IndexHelper <nl> * / <nl> 	 public static ArrayList < IndexInfo > deserializeIndex ( FileDataInput in ) throws IOException <nl> 	 { <nl> - ArrayList < IndexInfo > indexList = new ArrayList < IndexInfo > ( ) ; <nl> - <nl> 	 	 int columnIndexSize = in . readInt ( ) ; <nl> + if ( columnIndexSize = = 0 ) <nl> + return null ; <nl> + ArrayList < IndexInfo > indexList = new ArrayList < IndexInfo > ( ) ; <nl> FileMark mark = in . mark ( ) ; <nl> while ( in . bytesPastMark ( mark ) < columnIndexSize ) <nl> {

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 780b528 . . b3c0a35 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 19 , 7 + 19 , 7 @ @ 
 * Support negative timestamps for CQL3 dates in query string ( CASSANDRA - 6718 ) 
 * Avoid NPEs when receiving table changes for an unknown keyspace ( CASSANDRA - 5631 ) 
 * Fix bootstrapping when there is no schema ( CASSANDRA - 6685 ) 
 - 
 + * Fix truncating compression metadata ( CASSANDRA - 6791 ) 
 
 1 . 2 . 15 
 * Move handling of migration event source to solve bootstrap race ( CASSANDRA - 6648 ) 
 diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 index 00eb5a7 . . da55e83 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 + + + b / src / java / org / apache / cassandra / io / compress / CompressedSequentialWriter . java 
 @ @ - 231 , 7 + 231 , 7 @ @ public class CompressedSequentialWriter extends SequentialWriter 
 
 / / truncate data and index file 
 truncate ( chunkOffset ) ; 
 - metadataWriter . resetAndTruncate ( realMark . nextChunkIndex ) ; 
 + metadataWriter . resetAndTruncate ( realMark . nextChunkIndex - 1 ) ; 
 } 
 
 / * * 
 diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 index 830c3e1 . . 678a650 100644 
 - - - a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 + + + b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 @ @ - 19 , 10 + 19 , 12 @ @ 
 package org . apache . cassandra . io . compress ; 
 
 import java . io . * ; 
 + import java . util . Collections ; 
 import java . util . Random ; 
 
 import org . junit . Test ; 
 
 + import org . apache . cassandra . exceptions . ConfigurationException ; 
 import org . apache . cassandra . io . sstable . CorruptSSTableException ; 
 import org . apache . cassandra . io . sstable . SSTableMetadata ; 
 import org . apache . cassandra . io . util . * ; 
 @ @ - 95 , 6 + 97 , 47 @ @ public class CompressedRandomAccessReaderTest 
 } 
 
 @ Test 
 + public void test6791 ( ) throws IOException , ConfigurationException 
 + { 
 + File f = File . createTempFile ( " compressed6791 _ " , " 3 " ) ; 
 + String filename = f . getAbsolutePath ( ) ; 
 + try 
 + { 
 + 
 + SSTableMetadata . Collector sstableMetadataCollector = SSTableMetadata . createCollector ( ) . replayPosition ( null ) ; 
 + CompressedSequentialWriter writer = new CompressedSequentialWriter ( f , filename + " . metadata " , false , new CompressionParameters ( SnappyCompressor . instance , 32 , Collections . < String , String > emptyMap ( ) ) , sstableMetadataCollector ) ; 
 + 
 + for ( int i = 0 ; i < 20 ; i + + ) 
 + writer . write ( " x " . getBytes ( ) ) ; 
 + 
 + FileMark mark = writer . mark ( ) ; 
 + / / write enough garbage to create new chunks : 
 + for ( int i = 0 ; i < 40 ; i + + ) 
 + writer . write ( " y " . getBytes ( ) ) ; 
 + 
 + writer . resetAndTruncate ( mark ) ; 
 + 
 + for ( int i = 0 ; i < 20 ; i + + ) 
 + writer . write ( " x " . getBytes ( ) ) ; 
 + writer . close ( ) ; 
 + 
 + CompressedRandomAccessReader reader = CompressedRandomAccessReader . open ( filename , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) , false ) ; 
 + String res = reader . readLine ( ) ; 
 + assertEquals ( res , " xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx " ) ; 
 + assertEquals ( 40 , res . length ( ) ) ; 
 + } 
 + finally 
 + { 
 + / / cleanup 
 + if ( f . exists ( ) ) 
 + f . delete ( ) ; 
 + File metadata = new File ( filename + " . metadata " ) ; 
 + if ( metadata . exists ( ) ) 
 + metadata . delete ( ) ; 
 + } 
 + } 
 + 
 + @ Test 
 public void testDataCorruptionDetection ( ) throws IOException 
 { 
 String CONTENT = " Lorem ipsum dolor sit amet , consectetur adipiscing elit . Etiam vitae . " ;

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / ColumnIndexer . java b / src / java / org / apache / cassandra / db / ColumnIndexer . java 
 index 3e4a6f8 . . ff67982 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnIndexer . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnIndexer . java 
 @ @ - 113 , 12 + 113 , 19 @ @ public class ColumnIndexer 
 / * Write out the bloom filter . * / 
 writeBloomFilter ( dos , bf ) ; 
 
 - / / write the index 
 + / / write the index . we should always have at least one computed index block , but we only write it out if there is more than that . 
 assert indexSizeInBytes > 0 ; 
 - dos . writeInt ( indexSizeInBytes ) ; 
 - for ( IndexHelper . IndexInfo cIndexInfo : indexList ) 
 + if ( indexList . size ( ) > 1 ) 
 { 
 - cIndexInfo . serialize ( dos ) ; 
 + dos . writeInt ( indexSizeInBytes ) ; 
 + for ( IndexHelper . IndexInfo cIndexInfo : indexList ) 
 + { 
 + cIndexInfo . serialize ( dos ) ; 
 + } 
 + } 
 + else 
 + { 
 + dos . writeInt ( 0 ) ; 
 } 
 	 } 
 
 diff - - git a / src / java / org / apache / cassandra / db / columniterator / IndexedSliceReader . java b / src / java / org / apache / cassandra / db / columniterator / IndexedSliceReader . java 
 index 134683f . . 152ca55 100644 
 - - - a / src / java / org / apache / cassandra / db / columniterator / IndexedSliceReader . java 
 + + + b / src / java / org / apache / cassandra / db / columniterator / IndexedSliceReader . java 
 @ @ - 32 , 9 + 32 , 8 @ @ class IndexedSliceReader extends AbstractIterator < IColumn > implements IColumnIte 
 private final byte [ ] finishColumn ; 
 private final boolean reversed ; 
 
 - private int curRangeIndex ; 
 + private BlockFetcher fetcher ; 
 private Deque < IColumn > blockColumns = new ArrayDeque < IColumn > ( ) ; 
 - private final FileMark mark ; 
 private AbstractType comparator ; 
 
 public IndexedSliceReader ( SSTableReader sstable , FileDataInput input , byte [ ] startColumn , byte [ ] finishColumn , boolean reversed ) 
 @ @ - 50 , 16 + 49 , 12 @ @ class IndexedSliceReader extends AbstractIterator < IColumn > implements IColumnIte 
 indexes = IndexHelper . deserializeIndex ( file ) ; 
 
 emptyColumnFamily = ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( sstable . makeColumnFamily ( ) , file ) ; 
 - file . readInt ( ) ; / / column count 
 + fetcher = indexes = = null ? new SimpleBlockFetcher ( ) : new IndexedBlockFetcher ( ) ; 
 } 
 catch ( IOException e ) 
 { 
 throw new IOError ( e ) ; 
 } 
 - this . mark = file . mark ( ) ; 
 - curRangeIndex = IndexHelper . indexFor ( startColumn , indexes , comparator , reversed ) ; 
 - if ( reversed & & curRangeIndex = = indexes . size ( ) ) 
 - curRangeIndex - - ; 
 } 
 
 public ColumnFamily getColumnFamily ( ) 
 @ @ - 99 , 7 + 94 , 7 @ @ class IndexedSliceReader extends AbstractIterator < IColumn > implements IColumnIte 
 return column ; 
 try 
 { 
 - if ( column = = null & & ! getNextBlock ( ) ) 
 + if ( column = = null & & ! fetcher . getNextBlock ( ) ) 
 return endOfData ( ) ; 
 } 
 catch ( IOException e ) 
 @ @ - 109 , 59 + 104 , 105 @ @ class IndexedSliceReader extends AbstractIterator < IColumn > implements IColumnIte 
 } 
 } 
 
 - public boolean getNextBlock ( ) throws IOException 
 + public void close ( ) 
 { 
 - if ( curRangeIndex < 0 | | curRangeIndex > = indexes . size ( ) ) 
 - return false ; 
 + } 
 + 
 + interface BlockFetcher 
 + { 
 + public boolean getNextBlock ( ) throws IOException ; 
 + } 
 
 - / * seek to the correct offset to the data , and calculate the data size * / 
 - IndexHelper . IndexInfo curColPosition = indexes . get ( curRangeIndex ) ; 
 + private class IndexedBlockFetcher implements BlockFetcher 
 + { 
 + private final FileMark mark ; 
 + private int curRangeIndex ; 
 
 - / * see if this read is really necessary . * / 
 - if ( reversed ) 
 + IndexedBlockFetcher ( ) throws IOException 
 { 
 - if ( ( finishColumn . length > 0 & & comparator . compare ( finishColumn , curColPosition . lastName ) > 0 ) | | 
 - ( startColumn . length > 0 & & comparator . compare ( startColumn , curColPosition . firstName ) < 0 ) ) 
 - return false ; 
 + file . readInt ( ) ; / / column count 
 + this . mark = file . mark ( ) ; 
 + curRangeIndex = IndexHelper . indexFor ( startColumn , indexes , comparator , reversed ) ; 
 + if ( reversed & & curRangeIndex = = indexes . size ( ) ) 
 + curRangeIndex - - ; 
 } 
 - else 
 + 
 + public boolean getNextBlock ( ) throws IOException 
 { 
 - if ( ( startColumn . length > 0 & & comparator . compare ( startColumn , curColPosition . lastName ) > 0 ) | | 
 - ( finishColumn . length > 0 & & comparator . compare ( finishColumn , curColPosition . firstName ) < 0 ) ) 
 + if ( curRangeIndex < 0 | | curRangeIndex > = indexes . size ( ) ) 
 return false ; 
 - } 
 
 - boolean outOfBounds = false ; 
 + / * seek to the correct offset to the data , and calculate the data size * / 
 + IndexHelper . IndexInfo curColPosition = indexes . get ( curRangeIndex ) ; 
 
 - file . reset ( mark ) ; 
 - long curOffset = file . skipBytes ( ( int ) curColPosition . offset ) ; 
 - assert curOffset = = curColPosition . offset ; 
 - while ( file . bytesPastMark ( mark ) < curColPosition . offset + curColPosition . width & & ! outOfBounds ) 
 - { 
 - IColumn column = emptyColumnFamily . getColumnSerializer ( ) . deserialize ( file ) ; 
 + / * see if this read is really necessary . * / 
 if ( reversed ) 
 - blockColumns . addFirst ( column ) ; 
 + { 
 + if ( ( finishColumn . length > 0 & & comparator . compare ( finishColumn , curColPosition . lastName ) > 0 ) | | 
 + ( startColumn . length > 0 & & comparator . compare ( startColumn , curColPosition . firstName ) < 0 ) ) 
 + return false ; 
 + } 
 else 
 - blockColumns . addLast ( column ) ; 
 + { 
 + if ( ( startColumn . length > 0 & & comparator . compare ( startColumn , curColPosition . lastName ) > 0 ) | | 
 + ( finishColumn . length > 0 & & comparator . compare ( finishColumn , curColPosition . firstName ) < 0 ) ) 
 + return false ; 
 + } 
 
 - / * see if we can stop seeking . * / 
 - if ( ! reversed & & finishColumn . length > 0 ) 
 - outOfBounds = comparator . compare ( column . name ( ) , finishColumn ) > = 0 ; 
 - else if ( reversed & & startColumn . length > 0 ) 
 - outOfBounds = comparator . compare ( column . name ( ) , startColumn ) > = 0 ; 
 + boolean outOfBounds = false ; 
 + file . reset ( mark ) ; 
 + long curOffset = file . skipBytes ( ( int ) curColPosition . offset ) ; 
 + assert curOffset = = curColPosition . offset ; 
 + while ( file . bytesPastMark ( mark ) < curColPosition . offset + curColPosition . width & & ! outOfBounds ) 
 + { 
 + IColumn column = emptyColumnFamily . getColumnSerializer ( ) . deserialize ( file ) ; 
 + if ( reversed ) 
 + blockColumns . addFirst ( column ) ; 
 + else 
 + blockColumns . addLast ( column ) ; 
 + 
 + / * see if we can stop seeking . * / 
 + if ( ! reversed & & finishColumn . length > 0 ) 
 + outOfBounds = comparator . compare ( column . name ( ) , finishColumn ) > = 0 ; 
 + else if ( reversed & & startColumn . length > 0 ) 
 + outOfBounds = comparator . compare ( column . name ( ) , startColumn ) > = 0 ; 
 + } 
 
 - if ( outOfBounds ) 
 - break ; 
 + if ( reversed ) 
 + curRangeIndex - - ; 
 + else 
 + curRangeIndex + + ; 
 + return true ; 
 } 
 - 
 - if ( reversed ) 
 - curRangeIndex - - ; 
 - else 
 - curRangeIndex + + ; 
 - return true ; 
 } 
 
 - public void close ( ) throws IOException 
 + private class SimpleBlockFetcher implements BlockFetcher 
 { 
 + private SimpleBlockFetcher ( ) throws IOException 
 + { 
 + int columns = file . readInt ( ) ; 
 + for ( int i = 0 ; i < columns ; i + + ) 
 + { 
 + IColumn column = emptyColumnFamily . getColumnSerializer ( ) . deserialize ( file ) ; 
 + if ( reversed ) 
 + blockColumns . addFirst ( column ) ; 
 + else 
 + blockColumns . addLast ( column ) ; 
 + 
 + / * see if we can stop seeking . * / 
 + boolean outOfBounds = false ; 
 + if ( ! reversed & & finishColumn . length > 0 ) 
 + outOfBounds = comparator . compare ( column . name ( ) , finishColumn ) > = 0 ; 
 + else if ( reversed & & startColumn . length > 0 ) 
 + outOfBounds = comparator . compare ( column . name ( ) , startColumn ) > = 0 ; 
 + if ( outOfBounds ) 
 + break ; 
 + } 
 + } 
 + 
 + public boolean getNextBlock ( ) throws IOException 
 + { 
 + return false ; 
 + } 
 } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java b / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java 
 index 8bbf98f . . 3b686cb 100644 
 - - - a / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java 
 + + + b / src / java / org / apache / cassandra / db / columniterator / SSTableNamesIterator . java 
 @ @ - 53 , 7 + 53 , 7 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement 
 this ( ssTable , null , key , columnNames ) ; 
 } 
 
 - public SSTableNamesIterator ( SSTableReader ssTable , FileDataInput file , DecoratedKey key , SortedSet < byte [ ] > columnNames ) 
 + public SSTableNamesIterator ( SSTableReader sstable , FileDataInput file , DecoratedKey key , SortedSet < byte [ ] > columnNames ) 
 { 
 boolean closeFileWhenDone = file = = null ; 
 
 @ @ - 67 , 15 + 67 , 15 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement 
 / / open the sstable file , if we don ' t have one passed to use from range scan 
 if ( file = = null ) 
 { 
 - file = ssTable . getFileDataInput ( decoratedKey , DatabaseDescriptor . getIndexedReadBufferSizeInKB ( ) * 1024 ) ; 
 + file = sstable . getFileDataInput ( decoratedKey , DatabaseDescriptor . getIndexedReadBufferSizeInKB ( ) * 1024 ) ; 
 if ( file = = null ) 
 return ; 
 - DecoratedKey keyInDisk = SSTableReader . decodeKey ( ssTable . getPartitioner ( ) , 
 - ssTable . getDescriptor ( ) , 
 + DecoratedKey keyInDisk = SSTableReader . decodeKey ( sstable . getPartitioner ( ) , 
 + sstable . getDescriptor ( ) , 
 FBUtilities . readShortByteArray ( file ) ) ; 
 assert keyInDisk . equals ( decoratedKey ) 
 : String . format ( " % s ! = % s in % s " , keyInDisk , decoratedKey , file . getPath ( ) ) ; 
 - SSTableReader . readRowSize ( file , ssTable . getDescriptor ( ) ) ; 
 + SSTableReader . readRowSize ( file , sstable . getDescriptor ( ) ) ; 
 } 
 
 / / read the requested columns into ` cf ` 
 @ @ - 85 , 7 + 85 , 7 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement 
 
 / / we can stop early if bloom filter says none of the columns actually exist - - but , 
 / / we can ' t stop before initializing the cf above , in case there ' s a relevant tombstone 
 - cf = ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( ssTable . makeColumnFamily ( ) , file ) ; 
 + cf = ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( sstable . makeColumnFamily ( ) , file ) ; 
 
 List < byte [ ] > filteredColumnNames1 = new ArrayList < byte [ ] > ( columnNames . size ( ) ) ; 
 for ( byte [ ] name : columnNames ) 
 @ @ - 99 , 39 + 99 , 10 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement 
 if ( filteredColumnNames . isEmpty ( ) ) 
 return ; 
 
 - file . readInt ( ) ; / / column count 
 - 
 - / * get the various column ranges we have to read * / 
 - AbstractType comparator = ssTable . getColumnComparator ( ) ; 
 - SortedSet < IndexHelper . IndexInfo > ranges = new TreeSet < IndexHelper . IndexInfo > ( IndexHelper . getComparator ( comparator ) ) ; 
 - for ( byte [ ] name : filteredColumnNames ) 
 - { 
 - int index = IndexHelper . indexFor ( name , indexList , comparator , false ) ; 
 - if ( index = = indexList . size ( ) ) 
 - continue ; 
 - IndexHelper . IndexInfo indexInfo = indexList . get ( index ) ; 
 - if ( comparator . compare ( name , indexInfo . firstName ) < 0 ) 
 - continue ; 
 - ranges . add ( indexInfo ) ; 
 - } 
 - 
 - FileMark mark = file . mark ( ) ; 
 - for ( IndexHelper . IndexInfo indexInfo : ranges ) 
 - { 
 - file . reset ( mark ) ; 
 - long curOffsert = file . skipBytes ( ( int ) indexInfo . offset ) ; 
 - assert curOffsert = = indexInfo . offset ; 
 - / / TODO only completely deserialize columns we are interested in 
 - while ( file . bytesPastMark ( mark ) < indexInfo . offset + indexInfo . width ) 
 - { 
 - final IColumn column = cf . getColumnSerializer ( ) . deserialize ( file ) ; 
 - / / we check vs the original Set , not the filtered List , for efficiency 
 - if ( columnNames . contains ( column . name ( ) ) ) 
 - { 
 - cf . addColumn ( column ) ; 
 - } 
 - } 
 - } 
 + if ( indexList = = null ) 
 + readSimpleColumns ( file , columnNames , filteredColumnNames ) ; 
 + else 
 + readIndexedColumns ( sstable , file , columnNames , filteredColumnNames , indexList ) ; 
 
 / / create an iterator view of the columns we read 
 iter = cf . getSortedColumns ( ) . iterator ( ) ; 
 @ @ - 156 , 6 + 127 , 60 @ @ public class SSTableNamesIterator extends SimpleAbstractColumnIterator implement 
 } 
 } 
 
 + private void readSimpleColumns ( FileDataInput file , SortedSet < byte [ ] > columnNames , List < byte [ ] > filteredColumnNames ) throws IOException 
 + { 
 + int columns = file . readInt ( ) ; 
 + int n = 0 ; 
 + for ( int i = 0 ; i < columns ; i + + ) 
 + { 
 + IColumn column = cf . getColumnSerializer ( ) . deserialize ( file ) ; 
 + if ( columnNames . contains ( column . name ( ) ) ) 
 + { 
 + cf . addColumn ( column ) ; 
 + if ( n + + > filteredColumnNames . size ( ) ) 
 + break ; 
 + } 
 + } 
 + } 
 + 
 + private void readIndexedColumns ( SSTableReader sstable , FileDataInput file , SortedSet < byte [ ] > columnNames , List < byte [ ] > filteredColumnNames , List < IndexHelper . IndexInfo > indexList ) 
 + throws IOException 
 + { 
 + file . readInt ( ) ; / / column count 
 + 
 + / * get the various column ranges we have to read * / 
 + AbstractType comparator = sstable . getColumnComparator ( ) ; 
 + SortedSet < IndexHelper . IndexInfo > ranges = new TreeSet < IndexHelper . IndexInfo > ( IndexHelper . getComparator ( comparator ) ) ; 
 + for ( byte [ ] name : filteredColumnNames ) 
 + { 
 + int index = IndexHelper . indexFor ( name , indexList , comparator , false ) ; 
 + if ( index = = indexList . size ( ) ) 
 + continue ; 
 + IndexHelper . IndexInfo indexInfo = indexList . get ( index ) ; 
 + if ( comparator . compare ( name , indexInfo . firstName ) < 0 ) 
 + continue ; 
 + ranges . add ( indexInfo ) ; 
 + } 
 + 
 + FileMark mark = file . mark ( ) ; 
 + for ( IndexHelper . IndexInfo indexInfo : ranges ) 
 + { 
 + file . reset ( mark ) ; 
 + long curOffsert = file . skipBytes ( ( int ) indexInfo . offset ) ; 
 + assert curOffsert = = indexInfo . offset ; 
 + / / TODO only completely deserialize columns we are interested in 
 + while ( file . bytesPastMark ( mark ) < indexInfo . offset + indexInfo . width ) 
 + { 
 + IColumn column = cf . getColumnSerializer ( ) . deserialize ( file ) ; 
 + / / we check vs the original Set , not the filtered List , for efficiency 
 + if ( columnNames . contains ( column . name ( ) ) ) 
 + { 
 + cf . addColumn ( column ) ; 
 + } 
 + } 
 + } 
 + } 
 + 
 public DecoratedKey getKey ( ) 
 { 
 return decoratedKey ; 
 diff - - git a / src / java / org / apache / cassandra / db / columniterator / SSTableSliceIterator . java b / src / java / org / apache / cassandra / db / columniterator / SSTableSliceIterator . java 
 index e7ec535 . . 7f25222 100644 
 - - - a / src / java / org / apache / cassandra / db / columniterator / SSTableSliceIterator . java 
 + + + b / src / java / org / apache / cassandra / db / columniterator / SSTableSliceIterator . java 
 @ @ - 118 , 7 + 118 , 7 @ @ public class SSTableSliceIterator implements IColumnIterator 
 
 public void close ( ) throws IOException 
 { 
 - if ( closeFileWhenDone ) 
 + if ( closeFileWhenDone & & file ! = null ) 
 file . close ( ) ; 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java 
 index ae1bada . . 1db7441 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / IndexHelper . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / IndexHelper . java 
 @ @ - 68 , 9 + 68 , 10 @ @ public class IndexHelper 
 * / 
 	 public static ArrayList < IndexInfo > deserializeIndex ( FileDataInput in ) throws IOException 
 	 { 
 - ArrayList < IndexInfo > indexList = new ArrayList < IndexInfo > ( ) ; 
 - 
 	 	 int columnIndexSize = in . readInt ( ) ; 
 + if ( columnIndexSize = = 0 ) 
 + return null ; 
 + ArrayList < IndexInfo > indexList = new ArrayList < IndexInfo > ( ) ; 
 FileMark mark = in . mark ( ) ; 
 while ( in . bytesPastMark ( mark ) < columnIndexSize ) 
 {
