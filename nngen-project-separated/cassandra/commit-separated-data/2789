BLEU SCORE: 0.026279617104084448

TEST MSG: sstables from stalled repair sessions become live after a reboot and can resurrect deleted data
GENERATED MSG: merge from 0 . 6

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 110bf50 . . d85d3a4 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 21 , 6 + 21 , 7 @ @ <nl> * Fix preparing with batch and delete from collection ( CASSANDRA - 6607 ) <nl> * Fix ABSC reverse iterator ' s remove ( ) method ( CASSANDRA - 6629 ) <nl> * Handle host ID conflicts properly ( CASSANDRA - 6615 ) <nl> + * sstables from stalled repair sessions can resurrect deleted data ( CASSANDRA - 6503 ) <nl> <nl> <nl> 1 . 2 . 13 <nl> diff - - git a / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java b / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java <nl> index 0b058fc . . 940f8de 100644 <nl> - - - a / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java <nl> + + + b / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java <nl> @ @ - 119 , 8 + 119 , 8 @ @ public class IncomingStreamReader <nl> DataInput dis = new DataInputStream ( underliningStream ) ; <nl> try <nl> { <nl> - SSTableReader reader = streamIn ( dis , localFile , remoteFile ) ; <nl> - session . finished ( remoteFile , reader ) ; <nl> + SSTableWriter writer = streamIn ( dis , localFile , remoteFile ) ; <nl> + session . finished ( remoteFile , writer ) ; <nl> } <nl> catch ( IOException ex ) <nl> { <nl> @ @ - 141 , 7 + 141 , 7 @ @ public class IncomingStreamReader <nl> / * * <nl> * @ throws IOException if reading the remote sstable fails . Will throw an RTE if local write fails . <nl> * / <nl> - private SSTableReader streamIn ( DataInput input , PendingFile localFile , PendingFile remoteFile ) throws IOException <nl> + private SSTableWriter streamIn ( DataInput input , PendingFile localFile , PendingFile remoteFile ) throws IOException <nl> { <nl> ColumnFamilyStore cfs = Table . open ( localFile . desc . ksname ) . getColumnFamilyStore ( localFile . desc . cfname ) ; <nl> DecoratedKey key ; <nl> @ @ - 197 , 7 + 197 , 7 @ @ public class IncomingStreamReader <nl> } <nl> StreamingMetrics . totalIncomingBytes . inc ( totalBytesRead ) ; <nl> metrics . incomingBytes . inc ( totalBytesRead ) ; <nl> - return writer . closeAndOpenReader ( ) ; <nl> + return writer ; <nl> } <nl> catch ( Throwable e ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / streaming / StreamInSession . java b / src / java / org / apache / cassandra / streaming / StreamInSession . java <nl> index 96c31da . . e83a5b6 100644 <nl> - - - a / src / java / org / apache / cassandra / streaming / StreamInSession . java <nl> + + + b / src / java / org / apache / cassandra / streaming / StreamInSession . java <nl> @ @ - 24 , 6 + 24 , 7 @ @ import java . net . Socket ; <nl> import java . util . * ; <nl> import java . util . concurrent . ConcurrentMap ; <nl> <nl> + import org . apache . cassandra . io . sstable . SSTableWriter ; <nl> import org . cliffc . high _ scale _ lib . NonBlockingHashMap ; <nl> import org . cliffc . high _ scale _ lib . NonBlockingHashSet ; <nl> import org . slf4j . Logger ; <nl> @ @ - 47 , 7 + 48 , 7 @ @ public class StreamInSession extends AbstractStreamSession <nl> private static final ConcurrentMap < UUID , StreamInSession > sessions = new NonBlockingHashMap < UUID , StreamInSession > ( ) ; <nl> <nl> private final Set < PendingFile > files = new NonBlockingHashSet < PendingFile > ( ) ; <nl> - private final List < SSTableReader > readers = new ArrayList < SSTableReader > ( ) ; <nl> + private final List < SSTableWriter > writers = new ArrayList < SSTableWriter > ( ) ; <nl> private PendingFile current ; <nl> private Socket socket ; <nl> private volatile int retries ; <nl> @ @ - 106 , 13 + 107 , 13 @ @ public class StreamInSession extends AbstractStreamSession <nl> } <nl> } <nl> <nl> - public void finished ( PendingFile remoteFile , SSTableReader reader ) throws IOException <nl> + public void finished ( PendingFile remoteFile , SSTableWriter writer ) throws IOException <nl> { <nl> if ( logger . isDebugEnabled ( ) ) <nl> logger . debug ( " Finished { } ( from { } ) . Sending ack to { } " , new Object [ ] { remoteFile , getHost ( ) , this } ) ; <nl> <nl> - assert reader ! = null ; <nl> - readers . add ( reader ) ; <nl> + assert writer ! = null ; <nl> + writers . add ( writer ) ; <nl> files . remove ( remoteFile ) ; <nl> if ( remoteFile . equals ( current ) ) <nl> current = null ; <nl> @ @ - 163 , 6 + 164 , 10 @ @ public class StreamInSession extends AbstractStreamSession <nl> HashMap < ColumnFamilyStore , List < SSTableReader > > cfstores = new HashMap < ColumnFamilyStore , List < SSTableReader > > ( ) ; <nl> try <nl> { <nl> + List < SSTableReader > readers = new ArrayList < SSTableReader > ( ) ; <nl> + for ( SSTableWriter writer : writers ) <nl> + readers . add ( writer . closeAndOpenReader ( ) ) ; <nl> + <nl> for ( SSTableReader sstable : readers ) <nl> { <nl> assert sstable . getTableName ( ) . equals ( table ) ;
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 8d9e2ea . . c97b17f 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 7 + 1 , 10 @ @ <nl> dev <nl> * sstable versioning ( CASSANDRA - 389 ) <nl> <nl> - 0 . 6 . 0 - dev <nl> + 0 . 6 . 0 - RC1 <nl> + * fix compaction bucketing bug ( CASSANDRA - 814 ) <nl> + <nl> + 0 . 6 . 0 - beta1 / beta2 <nl> * add batch _ mutate thrift command , deprecating batch _ insert ( CASSANDRA - 336 ) <nl> * remove get _ key _ range Thrift API , deprecated in 0 . 5 ( CASSANDRA - 710 ) <nl> * add optional login ( ) Thrift call for authentication ( CASSANDRA - 547 ) <nl> @ @ - 42 , 7 + 45 , 9 @ @ dev <nl> * allow larger numbers of keys ( > 140M ) in a sstable bloom filter <nl> ( CASSANDRA - 790 ) <nl> * include jvm argument improvements from CASSANDRA - 504 in debian package <nl> - * change streaming chunk size to 32MB ( was 64MB ) ( CASSANDRA - 795 ) <nl> + * change streaming chunk size to 32MB to accomodate Windows XP limitations <nl> + ( was 64MB ) ( CASSANDRA - 795 ) <nl> + * fix get _ range _ slice returning results in the wrong order ( CASSANDRA - 781 ) <nl> <nl> <nl> 0 . 5 . 0 final <nl> diff - - git a / build . xml b / build . xml <nl> index ea79876 . . 54ef12d 100644 <nl> - - - a / build . xml <nl> + + + b / build . xml <nl> @ @ - 314 , 6 + 314 , 7 @ @ <nl> < include name = " * * " / > <nl> < exclude name = " build / * * " / > <nl> < exclude name = " src / gen - java / * * " / > <nl> + < exclude name = " interface / avro / * * " / > <nl> < / tarfileset > <nl> < / tar > <nl> < / target > <nl> diff - - git a / src / java / org / apache / cassandra / db / CompactionManager . java b / src / java / org / apache / cassandra / db / CompactionManager . java <nl> index fbd5ebb . . 1b5b6b1 100644 <nl> - - - a / src / java / org / apache / cassandra / db / CompactionManager . java <nl> + + + b / src / java / org / apache / cassandra / db / CompactionManager . java <nl> @ @ - 89 , 7 + 89 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> return 0 ; <nl> } <nl> logger . debug ( " Checking to see if compaction of " + cfs . columnFamily _ + " would be useful " ) ; <nl> - Set < List < SSTableReader > > buckets = getCompactionBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; <nl> + Set < List < SSTableReader > > buckets = getBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; <nl> updateEstimateFor ( cfs , buckets ) ; <nl> <nl> for ( List < SSTableReader > sstables : buckets ) <nl> @ @ - 441 , 7 + 441 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> / * <nl> * Group files of similar size into buckets . <nl> * / <nl> - static Set < List < SSTableReader > > getCompactionBuckets ( Iterable < SSTableReader > files , long min ) <nl> + static Set < List < SSTableReader > > getBuckets ( Iterable < SSTableReader > files , long min ) <nl> { <nl> Map < List < SSTableReader > , Long > buckets = new HashMap < List < SSTableReader > , Long > ( ) ; <nl> for ( SSTableReader sstable : files ) <nl> @ @ - 461 , 7 + 461 , 8 @ @ public class CompactionManager implements CompactionManagerMBean <nl> { <nl> / / remove and re - add because adding changes the hash <nl> buckets . remove ( bucket ) ; <nl> - averageSize = ( averageSize + size ) / 2 ; <nl> + long totalSize = bucket . size ( ) * averageSize ; <nl> + averageSize = ( totalSize + size ) / ( bucket . size ( ) + 1 ) ; <nl> bucket . add ( sstable ) ; <nl> buckets . put ( bucket , averageSize ) ; <nl> bFound = true ; <nl> @ @ - 538 , 7 + 539 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> public void run ( ) <nl> { <nl> logger . debug ( " Estimating compactions for " + cfs . columnFamily _ ) ; <nl> - final Set < List < SSTableReader > > buckets = getCompactionBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; <nl> + final Set < List < SSTableReader > > buckets = getBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; <nl> updateEstimateFor ( cfs , buckets ) ; <nl> } <nl> } ;

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 110bf50 . . d85d3a4 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 21 , 6 + 21 , 7 @ @ 
 * Fix preparing with batch and delete from collection ( CASSANDRA - 6607 ) 
 * Fix ABSC reverse iterator ' s remove ( ) method ( CASSANDRA - 6629 ) 
 * Handle host ID conflicts properly ( CASSANDRA - 6615 ) 
 + * sstables from stalled repair sessions can resurrect deleted data ( CASSANDRA - 6503 ) 
 
 
 1 . 2 . 13 
 diff - - git a / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java b / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java 
 index 0b058fc . . 940f8de 100644 
 - - - a / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java 
 + + + b / src / java / org / apache / cassandra / streaming / IncomingStreamReader . java 
 @ @ - 119 , 8 + 119 , 8 @ @ public class IncomingStreamReader 
 DataInput dis = new DataInputStream ( underliningStream ) ; 
 try 
 { 
 - SSTableReader reader = streamIn ( dis , localFile , remoteFile ) ; 
 - session . finished ( remoteFile , reader ) ; 
 + SSTableWriter writer = streamIn ( dis , localFile , remoteFile ) ; 
 + session . finished ( remoteFile , writer ) ; 
 } 
 catch ( IOException ex ) 
 { 
 @ @ - 141 , 7 + 141 , 7 @ @ public class IncomingStreamReader 
 / * * 
 * @ throws IOException if reading the remote sstable fails . Will throw an RTE if local write fails . 
 * / 
 - private SSTableReader streamIn ( DataInput input , PendingFile localFile , PendingFile remoteFile ) throws IOException 
 + private SSTableWriter streamIn ( DataInput input , PendingFile localFile , PendingFile remoteFile ) throws IOException 
 { 
 ColumnFamilyStore cfs = Table . open ( localFile . desc . ksname ) . getColumnFamilyStore ( localFile . desc . cfname ) ; 
 DecoratedKey key ; 
 @ @ - 197 , 7 + 197 , 7 @ @ public class IncomingStreamReader 
 } 
 StreamingMetrics . totalIncomingBytes . inc ( totalBytesRead ) ; 
 metrics . incomingBytes . inc ( totalBytesRead ) ; 
 - return writer . closeAndOpenReader ( ) ; 
 + return writer ; 
 } 
 catch ( Throwable e ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / streaming / StreamInSession . java b / src / java / org / apache / cassandra / streaming / StreamInSession . java 
 index 96c31da . . e83a5b6 100644 
 - - - a / src / java / org / apache / cassandra / streaming / StreamInSession . java 
 + + + b / src / java / org / apache / cassandra / streaming / StreamInSession . java 
 @ @ - 24 , 6 + 24 , 7 @ @ import java . net . Socket ; 
 import java . util . * ; 
 import java . util . concurrent . ConcurrentMap ; 
 
 + import org . apache . cassandra . io . sstable . SSTableWriter ; 
 import org . cliffc . high _ scale _ lib . NonBlockingHashMap ; 
 import org . cliffc . high _ scale _ lib . NonBlockingHashSet ; 
 import org . slf4j . Logger ; 
 @ @ - 47 , 7 + 48 , 7 @ @ public class StreamInSession extends AbstractStreamSession 
 private static final ConcurrentMap < UUID , StreamInSession > sessions = new NonBlockingHashMap < UUID , StreamInSession > ( ) ; 
 
 private final Set < PendingFile > files = new NonBlockingHashSet < PendingFile > ( ) ; 
 - private final List < SSTableReader > readers = new ArrayList < SSTableReader > ( ) ; 
 + private final List < SSTableWriter > writers = new ArrayList < SSTableWriter > ( ) ; 
 private PendingFile current ; 
 private Socket socket ; 
 private volatile int retries ; 
 @ @ - 106 , 13 + 107 , 13 @ @ public class StreamInSession extends AbstractStreamSession 
 } 
 } 
 
 - public void finished ( PendingFile remoteFile , SSTableReader reader ) throws IOException 
 + public void finished ( PendingFile remoteFile , SSTableWriter writer ) throws IOException 
 { 
 if ( logger . isDebugEnabled ( ) ) 
 logger . debug ( " Finished { } ( from { } ) . Sending ack to { } " , new Object [ ] { remoteFile , getHost ( ) , this } ) ; 
 
 - assert reader ! = null ; 
 - readers . add ( reader ) ; 
 + assert writer ! = null ; 
 + writers . add ( writer ) ; 
 files . remove ( remoteFile ) ; 
 if ( remoteFile . equals ( current ) ) 
 current = null ; 
 @ @ - 163 , 6 + 164 , 10 @ @ public class StreamInSession extends AbstractStreamSession 
 HashMap < ColumnFamilyStore , List < SSTableReader > > cfstores = new HashMap < ColumnFamilyStore , List < SSTableReader > > ( ) ; 
 try 
 { 
 + List < SSTableReader > readers = new ArrayList < SSTableReader > ( ) ; 
 + for ( SSTableWriter writer : writers ) 
 + readers . add ( writer . closeAndOpenReader ( ) ) ; 
 + 
 for ( SSTableReader sstable : readers ) 
 { 
 assert sstable . getTableName ( ) . equals ( table ) ;

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 8d9e2ea . . c97b17f 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 7 + 1 , 10 @ @ 
 dev 
 * sstable versioning ( CASSANDRA - 389 ) 
 
 - 0 . 6 . 0 - dev 
 + 0 . 6 . 0 - RC1 
 + * fix compaction bucketing bug ( CASSANDRA - 814 ) 
 + 
 + 0 . 6 . 0 - beta1 / beta2 
 * add batch _ mutate thrift command , deprecating batch _ insert ( CASSANDRA - 336 ) 
 * remove get _ key _ range Thrift API , deprecated in 0 . 5 ( CASSANDRA - 710 ) 
 * add optional login ( ) Thrift call for authentication ( CASSANDRA - 547 ) 
 @ @ - 42 , 7 + 45 , 9 @ @ dev 
 * allow larger numbers of keys ( > 140M ) in a sstable bloom filter 
 ( CASSANDRA - 790 ) 
 * include jvm argument improvements from CASSANDRA - 504 in debian package 
 - * change streaming chunk size to 32MB ( was 64MB ) ( CASSANDRA - 795 ) 
 + * change streaming chunk size to 32MB to accomodate Windows XP limitations 
 + ( was 64MB ) ( CASSANDRA - 795 ) 
 + * fix get _ range _ slice returning results in the wrong order ( CASSANDRA - 781 ) 
 
 
 0 . 5 . 0 final 
 diff - - git a / build . xml b / build . xml 
 index ea79876 . . 54ef12d 100644 
 - - - a / build . xml 
 + + + b / build . xml 
 @ @ - 314 , 6 + 314 , 7 @ @ 
 < include name = " * * " / > 
 < exclude name = " build / * * " / > 
 < exclude name = " src / gen - java / * * " / > 
 + < exclude name = " interface / avro / * * " / > 
 < / tarfileset > 
 < / tar > 
 < / target > 
 diff - - git a / src / java / org / apache / cassandra / db / CompactionManager . java b / src / java / org / apache / cassandra / db / CompactionManager . java 
 index fbd5ebb . . 1b5b6b1 100644 
 - - - a / src / java / org / apache / cassandra / db / CompactionManager . java 
 + + + b / src / java / org / apache / cassandra / db / CompactionManager . java 
 @ @ - 89 , 7 + 89 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 return 0 ; 
 } 
 logger . debug ( " Checking to see if compaction of " + cfs . columnFamily _ + " would be useful " ) ; 
 - Set < List < SSTableReader > > buckets = getCompactionBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; 
 + Set < List < SSTableReader > > buckets = getBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; 
 updateEstimateFor ( cfs , buckets ) ; 
 
 for ( List < SSTableReader > sstables : buckets ) 
 @ @ - 441 , 7 + 441 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 / * 
 * Group files of similar size into buckets . 
 * / 
 - static Set < List < SSTableReader > > getCompactionBuckets ( Iterable < SSTableReader > files , long min ) 
 + static Set < List < SSTableReader > > getBuckets ( Iterable < SSTableReader > files , long min ) 
 { 
 Map < List < SSTableReader > , Long > buckets = new HashMap < List < SSTableReader > , Long > ( ) ; 
 for ( SSTableReader sstable : files ) 
 @ @ - 461 , 7 + 461 , 8 @ @ public class CompactionManager implements CompactionManagerMBean 
 { 
 / / remove and re - add because adding changes the hash 
 buckets . remove ( bucket ) ; 
 - averageSize = ( averageSize + size ) / 2 ; 
 + long totalSize = bucket . size ( ) * averageSize ; 
 + averageSize = ( totalSize + size ) / ( bucket . size ( ) + 1 ) ; 
 bucket . add ( sstable ) ; 
 buckets . put ( bucket , averageSize ) ; 
 bFound = true ; 
 @ @ - 538 , 7 + 539 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 public void run ( ) 
 { 
 logger . debug ( " Estimating compactions for " + cfs . columnFamily _ ) ; 
 - final Set < List < SSTableReader > > buckets = getCompactionBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; 
 + final Set < List < SSTableReader > > buckets = getBuckets ( cfs . getSSTables ( ) , 50L * 1024L * 1024L ) ; 
 updateEstimateFor ( cfs , buckets ) ; 
 } 
 } ;
