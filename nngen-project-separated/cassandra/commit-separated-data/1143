BLEU SCORE: 2.4209788076720657E-5

TEST MSG: Fix bug with range tombstone in reverse queries and Partition test coverage
GENERATED MSG: Replace PriorityQueue mess with a CompactionIterator that efficiently yields compacted Rows from a set of sstables by feeding CollationIterator into a ReducingIterator transform . ( " Efficiently " means we never deserialize data until it is needed , so the number of sstables that can be compacted at once is virtually unlimited , and if only one sstable contains a given key that row data will be copied over without an intermediate de / serialize step . ) This is a very natural fit for the compaction algorithm and almost entirely gets rid of duplicated code between doFileCompaction and doAntiCompaction .

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index f6aed18 . . 2527c43 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 6 @ @ <nl> 3 . 0 . 1 <nl> + * Fix bug with range tombstones on reverse queries and test coverage for <nl> + AbstractBTreePartition ( CASSANDRA - 10059 ) <nl> * Remove 64k limit on collection elements ( CASSANDRA - 10374 ) <nl> * Remove unclear Indexer . indexes ( ) method ( CASSANDRA - 10690 ) <nl> * Fix NPE on stream read error ( CASSANDRA - 10771 ) <nl> diff - - git a / src / java / org / apache / cassandra / db / RangeTombstoneList . java b / src / java / org / apache / cassandra / db / RangeTombstoneList . java <nl> index c92a296 . . c67ea33 100644 <nl> - - - a / src / java / org / apache / cassandra / db / RangeTombstoneList . java <nl> + + + b / src / java / org / apache / cassandra / db / RangeTombstoneList . java <nl> @ @ - 17 , 21 + 17 , 16 @ @ <nl> * / <nl> package org . apache . cassandra . db ; <nl> <nl> - import java . io . IOException ; <nl> import java . nio . ByteBuffer ; <nl> import java . util . Arrays ; <nl> + import java . util . Collections ; <nl> import java . util . Iterator ; <nl> <nl> import org . apache . cassandra . utils . AbstractIterator ; <nl> import com . google . common . collect . Iterators ; <nl> <nl> - import org . slf4j . Logger ; <nl> - import org . slf4j . LoggerFactory ; <nl> import org . apache . cassandra . cache . IMeasurableMemory ; <nl> import org . apache . cassandra . db . rows . * ; <nl> - import org . apache . cassandra . io . IVersionedSerializer ; <nl> - import org . apache . cassandra . io . util . DataInputPlus ; <nl> - import org . apache . cassandra . io . util . DataOutputPlus ; <nl> import org . apache . cassandra . utils . ObjectSizes ; <nl> import org . apache . cassandra . utils . memory . AbstractAllocator ; <nl> <nl> @ @ - 53 , 8 + 48 , 6 @ @ import org . apache . cassandra . utils . memory . AbstractAllocator ; <nl> * / <nl> public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurableMemory <nl> { <nl> - private static final Logger logger = LoggerFactory . getLogger ( RangeTombstoneList . class ) ; <nl> - <nl> private static long EMPTY _ SIZE = ObjectSizes . measure ( new RangeTombstoneList ( null , 0 ) ) ; <nl> <nl> private final ClusteringComparator comparator ; <nl> @ @ - 265 , 6 + 258 , 8 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable <nl> / * <nl> * Return is the index of the range covering name if name is covered . If the return idx is negative , <nl> * no range cover name and - idx - 1 is the index of the first range whose start is greater than name . <nl> + * <nl> + * Note that bounds are not in the range if they fall on its boundary . <nl> * / <nl> private int searchInternal ( ClusteringPrefix name , int startIdx , int endIdx ) <nl> { <nl> @ @ - 274 , 7 + 269 , 9 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable <nl> int pos = Arrays . binarySearch ( starts , startIdx , endIdx , name , comparator ) ; <nl> if ( pos > = 0 ) <nl> { <nl> - return pos ; <nl> + / / Equality only happens for bounds ( as used by forward / reverseIterator ) , and bounds are equal only if they <nl> + / / are the same or complementary , in either case the bound itself is not part of the range . <nl> + return - pos - 1 ; <nl> } <nl> else <nl> { <nl> @ @ - 283 , 7 + 280 , 7 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable <nl> if ( idx < 0 ) <nl> return - 1 ; <nl> <nl> - return comparator . compare ( name , ends [ idx ] ) < = 0 ? idx : - idx - 2 ; <nl> + return comparator . compare ( name , ends [ idx ] ) < 0 ? idx : - idx - 2 ; <nl> } <nl> } <nl> <nl> @ @ - 387 , 14 + 384 , 14 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable <nl> final int start = startIdx < 0 ? - startIdx - 1 : startIdx ; <nl> <nl> if ( start > = size ) <nl> - return Iterators . < RangeTombstone > emptyIterator ( ) ; <nl> + return Collections . emptyIterator ( ) ; <nl> <nl> int finishIdx = slice . end ( ) = = Slice . Bound . TOP ? size - 1 : searchInternal ( slice . end ( ) , start , size ) ; <nl> / / if stopIdx is the first range after ' slice . end ( ) ' we care only until the previous range <nl> final int finish = finishIdx < 0 ? - finishIdx - 2 : finishIdx ; <nl> <nl> if ( start > finish ) <nl> - return Iterators . < RangeTombstone > emptyIterator ( ) ; <nl> + return Collections . emptyIterator ( ) ; <nl> <nl> if ( start = = finish ) <nl> { <nl> @ @ - 428 , 19 + 425 , 19 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable <nl> <nl> private Iterator < RangeTombstone > reverseIterator ( final Slice slice ) <nl> { <nl> - int startIdx = slice . end ( ) = = Slice . Bound . TOP ? 0 : searchInternal ( slice . end ( ) , 0 , size ) ; <nl> + int startIdx = slice . end ( ) = = Slice . Bound . TOP ? size - 1 : searchInternal ( slice . end ( ) , 0 , size ) ; <nl> / / if startIdx is the first range after ' slice . end ( ) ' we care only until the previous range <nl> final int start = startIdx < 0 ? - startIdx - 2 : startIdx ; <nl> <nl> - if ( start > = size ) <nl> - return Iterators . < RangeTombstone > emptyIterator ( ) ; <nl> + if ( start < 0 ) <nl> + return Collections . emptyIterator ( ) ; <nl> <nl> - int finishIdx = slice . start ( ) = = Slice . Bound . BOTTOM ? 0 : searchInternal ( slice . start ( ) , 0 , start ) ; <nl> + int finishIdx = slice . start ( ) = = Slice . Bound . BOTTOM ? 0 : searchInternal ( slice . start ( ) , 0 , start + 1 ) ; / / include same as finish <nl> / / if stopIdx is the first range after ' slice . end ( ) ' we care only until the previous range <nl> final int finish = finishIdx < 0 ? - finishIdx - 1 : finishIdx ; <nl> <nl> if ( start < finish ) <nl> - return Iterators . < RangeTombstone > emptyIterator ( ) ; <nl> + return Collections . emptyIterator ( ) ; <nl> <nl> if ( start = = finish ) <nl> { <nl> @ @ - 467 , 7 + 464 , 7 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable <nl> return rangeTombstoneWithNewEnd ( idx - - , slice . end ( ) ) ; <nl> if ( idx = = finish & & comparator . compare ( starts [ idx ] , slice . start ( ) ) < 0 ) <nl> return rangeTombstoneWithNewStart ( idx - - , slice . start ( ) ) ; <nl> - return rangeTombstone ( idx + + ) ; <nl> + return rangeTombstone ( idx - - ) ; <nl> } <nl> } ; <nl> } <nl> @ @ - 665 , 20 + 662 , 6 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable <nl> size + + ; <nl> } <nl> <nl> - private void removeInternal ( int i ) <nl> - { <nl> - assert i > = 0 ; <nl> - <nl> - System . arraycopy ( starts , i + 1 , starts , i , size - i - 1 ) ; <nl> - System . arraycopy ( ends , i + 1 , ends , i , size - i - 1 ) ; <nl> - System . arraycopy ( markedAts , i + 1 , markedAts , i , size - i - 1 ) ; <nl> - System . arraycopy ( delTimes , i + 1 , delTimes , i , size - i - 1 ) ; <nl> - <nl> - - - size ; <nl> - starts [ size ] = null ; <nl> - ends [ size ] = null ; <nl> - } <nl> - <nl> / * <nl> * Grow the arrays , leaving index i " free " in the process . <nl> * / <nl> diff - - git a / src / java / org / apache / cassandra / utils / SearchIterator . java b / src / java / org / apache / cassandra / utils / SearchIterator . java <nl> index ca7b2fa . . 5309f4a 100644 <nl> - - - a / src / java / org / apache / cassandra / utils / SearchIterator . java <nl> + + + b / src / java / org / apache / cassandra / utils / SearchIterator . java <nl> @ @ - 23 , 7 + 23 , 8 @ @ public interface SearchIterator < K , V > <nl> <nl> / * * <nl> * Searches " forwards " ( in direction of travel ) in the iterator for the required key ; <nl> - * if this or any key greater has already been returned by the iterator , null will be returned . <nl> + * if this or any key greater has already been returned by the iterator , the method may <nl> + * choose to return null , the correct or incorrect output , or fail an assertion . <nl> * <nl> * it is permitted to search past the end of the iterator , i . e . ! hasNext ( ) = > next ( ? ) = = null <nl> * <nl> diff - - git a / test / unit / org / apache / cassandra / Util . java b / test / unit / org / apache / cassandra / Util . java <nl> index ea0bd9b . . 91ec6b6 100644 <nl> - - - a / test / unit / org / apache / cassandra / Util . java <nl> + + + b / test / unit / org / apache / cassandra / Util . java <nl> @ @ - 538 , 4 + 538 , 27 @ @ public class Util <nl> { <nl> return ( ) - > new AssertionError ( message ) ; <nl> } <nl> + <nl> + public static class UnfilteredSource extends AbstractUnfilteredRowIterator implements UnfilteredRowIterator <nl> + { <nl> + Iterator < Unfiltered > content ; <nl> + <nl> + public UnfilteredSource ( CFMetaData cfm , DecoratedKey partitionKey , Row staticRow , Iterator < Unfiltered > content ) <nl> + { <nl> + super ( cfm , <nl> + partitionKey , <nl> + DeletionTime . LIVE , <nl> + cfm . partitionColumns ( ) , <nl> + staticRow ! = null ? staticRow : Rows . EMPTY _ STATIC _ ROW , <nl> + false , <nl> + EncodingStats . NO _ STATS ) ; <nl> + this . content = content ; <nl> + } <nl> + <nl> + @ Override <nl> + protected Unfiltered computeNext ( ) <nl> + { <nl> + return content . hasNext ( ) ? content . next ( ) : endOfData ( ) ; <nl> + } <nl> + } <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / db / partition / PartitionImplementationTest . java b / test / unit / org / apache / cassandra / db / partition / PartitionImplementationTest . java <nl> new file mode 100644 <nl> index 0000000 . . f215331 <nl> - - - / dev / null <nl> + + + b / test / unit / org / apache / cassandra / db / partition / PartitionImplementationTest . java <nl> @ @ - 0 , 0 + 1 , 524 @ @ <nl> + / * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , software <nl> + * distributed under the License is distributed on an " AS IS " BASIS , <nl> + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> + * See the License for the specific language governing permissions and <nl> + * limitations under the License . <nl> + * / <nl> + package org . apache . cassandra . db . partition ; <nl> + <nl> + import static org . junit . Assert . * ; <nl> + <nl> + import java . util . * ; <nl> + import java . util . function . Function ; <nl> + import java . util . function . Predicate ; <nl> + import java . util . function . Supplier ; <nl> + import java . util . stream . Collectors ; <nl> + import java . util . stream . Stream ; <nl> + import java . util . stream . StreamSupport ; <nl> + <nl> + import com . google . common . collect . Iterables ; <nl> + import com . google . common . collect . Iterators ; <nl> + <nl> + import org . junit . BeforeClass ; <nl> + import org . junit . Test ; <nl> + <nl> + import org . apache . cassandra . SchemaLoader ; <nl> + import org . apache . cassandra . Util ; <nl> + import org . apache . cassandra . config . CFMetaData ; <nl> + import org . apache . cassandra . config . ColumnDefinition ; <nl> + import org . apache . cassandra . cql3 . ColumnIdentifier ; <nl> + import org . apache . cassandra . db . * ; <nl> + import org . apache . cassandra . db . Slice . Bound ; <nl> + import org . apache . cassandra . db . filter . ColumnFilter ; <nl> + import org . apache . cassandra . db . marshal . AsciiType ; <nl> + import org . apache . cassandra . db . partitions . AbstractBTreePartition ; <nl> + import org . apache . cassandra . db . partitions . ImmutableBTreePartition ; <nl> + import org . apache . cassandra . db . partitions . Partition ; <nl> + import org . apache . cassandra . db . rows . * ; <nl> + import org . apache . cassandra . db . rows . Row . Deletion ; <nl> + import org . apache . cassandra . exceptions . ConfigurationException ; <nl> + import org . apache . cassandra . schema . KeyspaceParams ; <nl> + import org . apache . cassandra . utils . ByteBufferUtil ; <nl> + import org . apache . cassandra . utils . SearchIterator ; <nl> + <nl> + public class PartitionImplementationTest <nl> + { <nl> + private static final String KEYSPACE = " PartitionImplementationTest " ; <nl> + private static final String CF = " Standard " ; <nl> + <nl> + private static final int ENTRIES = 250 ; <nl> + private static final int TESTS = 1000 ; <nl> + private static final int KEY _ RANGE = ENTRIES * 5 ; <nl> + <nl> + private static final int TIMESTAMP = KEY _ RANGE + 1 ; <nl> + <nl> + private static CFMetaData cfm ; <nl> + private Random rand = new Random ( 2 ) ; <nl> + <nl> + @ BeforeClass <nl> + public static void defineSchema ( ) throws ConfigurationException <nl> + { <nl> + SchemaLoader . prepareServer ( ) ; <nl> + <nl> + cfm = CFMetaData . Builder . create ( KEYSPACE , CF ) <nl> + . addPartitionKey ( " pk " , AsciiType . instance ) <nl> + . addClusteringColumn ( " ck " , AsciiType . instance ) <nl> + . addRegularColumn ( " col " , AsciiType . instance ) <nl> + . addStaticColumn ( " static _ col " , AsciiType . instance ) <nl> + . build ( ) ; <nl> + SchemaLoader . createKeyspace ( KEYSPACE , <nl> + KeyspaceParams . simple ( 1 ) , <nl> + cfm ) ; <nl> + } <nl> + <nl> + private List < Row > generateRows ( ) <nl> + { <nl> + List < Row > content = new ArrayList < > ( ) ; <nl> + Set < Integer > keysUsed = new HashSet < > ( ) ; <nl> + for ( int i = 0 ; i < ENTRIES ; + + i ) <nl> + { <nl> + int rk ; <nl> + do <nl> + { <nl> + rk = rand . nextInt ( KEY _ RANGE ) ; <nl> + } <nl> + while ( ! keysUsed . add ( rk ) ) ; <nl> + content . add ( makeRow ( clustering ( rk ) , " Col " + rk ) ) ; <nl> + } <nl> + return content ; / / not sorted <nl> + } <nl> + <nl> + Row makeRow ( Clustering clustering , String colValue ) <nl> + { <nl> + ColumnDefinition defCol = cfm . getColumnDefinition ( new ColumnIdentifier ( " col " , true ) ) ; <nl> + Row . Builder row = BTreeRow . unsortedBuilder ( TIMESTAMP ) ; <nl> + row . newRow ( clustering ) ; <nl> + row . addCell ( BufferCell . live ( cfm , defCol , TIMESTAMP , ByteBufferUtil . bytes ( colValue ) ) ) ; <nl> + return row . build ( ) ; <nl> + } <nl> + <nl> + Row makeStaticRow ( ) <nl> + { <nl> + ColumnDefinition defCol = cfm . getColumnDefinition ( new ColumnIdentifier ( " static _ col " , true ) ) ; <nl> + Row . Builder row = BTreeRow . unsortedBuilder ( TIMESTAMP ) ; <nl> + row . newRow ( Clustering . STATIC _ CLUSTERING ) ; <nl> + row . addCell ( BufferCell . live ( cfm , defCol , TIMESTAMP , ByteBufferUtil . bytes ( " static value " ) ) ) ; <nl> + return row . build ( ) ; <nl> + } <nl> + <nl> + private List < Unfiltered > generateMarkersOnly ( ) <nl> + { <nl> + return addMarkers ( new ArrayList < > ( ) ) ; <nl> + } <nl> + <nl> + private List < Unfiltered > generateUnfiltereds ( ) <nl> + { <nl> + List < Unfiltered > content = new ArrayList < > ( generateRows ( ) ) ; <nl> + return addMarkers ( content ) ; <nl> + } <nl> + <nl> + List < Unfiltered > addMarkers ( List < Unfiltered > content ) <nl> + { <nl> + List < RangeTombstoneMarker > markers = new ArrayList < > ( ) ; <nl> + Set < Integer > delTimes = new HashSet < > ( ) ; <nl> + for ( int i = 0 ; i < ENTRIES / 10 ; + + i ) <nl> + { <nl> + int delTime ; <nl> + do <nl> + { <nl> + delTime = rand . nextInt ( KEY _ RANGE ) ; <nl> + } <nl> + while ( ! delTimes . add ( delTime ) ) ; <nl> + <nl> + int start = rand . nextInt ( KEY _ RANGE ) ; <nl> + DeletionTime dt = new DeletionTime ( delTime , delTime ) ; <nl> + RangeTombstoneMarker open = RangeTombstoneBoundMarker . inclusiveOpen ( false , clustering ( start ) . getRawValues ( ) , dt ) ; <nl> + int end = start + rand . nextInt ( ( KEY _ RANGE - start ) / 4 + 1 ) ; <nl> + RangeTombstoneMarker close = RangeTombstoneBoundMarker . inclusiveClose ( false , clustering ( end ) . getRawValues ( ) , dt ) ; <nl> + markers . add ( open ) ; <nl> + markers . add ( close ) ; <nl> + } <nl> + markers . sort ( cfm . comparator ) ; <nl> + <nl> + RangeTombstoneMarker toAdd = null ; <nl> + Set < DeletionTime > open = new HashSet < > ( ) ; <nl> + DeletionTime current = DeletionTime . LIVE ; <nl> + for ( RangeTombstoneMarker marker : markers ) <nl> + { <nl> + if ( marker . isOpen ( false ) ) <nl> + { <nl> + DeletionTime delTime = marker . openDeletionTime ( false ) ; <nl> + open . add ( delTime ) ; <nl> + if ( delTime . supersedes ( current ) ) <nl> + { <nl> + if ( toAdd ! = null ) <nl> + { <nl> + if ( cfm . comparator . compare ( toAdd , marker ) ! = 0 ) <nl> + content . add ( toAdd ) ; <nl> + else <nl> + { <nl> + / / gotta join <nl> + current = toAdd . isClose ( false ) ? toAdd . closeDeletionTime ( false ) : DeletionTime . LIVE ; <nl> + } <nl> + } <nl> + if ( current ! = DeletionTime . LIVE ) <nl> + marker = RangeTombstoneBoundaryMarker . makeBoundary ( false , marker . openBound ( false ) . invert ( ) , marker . openBound ( false ) , current , delTime ) ; <nl> + toAdd = marker ; <nl> + current = delTime ; <nl> + } <nl> + } <nl> + else <nl> + { <nl> + assert marker . isClose ( false ) ; <nl> + DeletionTime delTime = marker . closeDeletionTime ( false ) ; <nl> + boolean removed = open . remove ( delTime ) ; <nl> + assert removed ; <nl> + if ( current . equals ( delTime ) ) <nl> + { <nl> + if ( toAdd ! = null ) <nl> + { <nl> + if ( cfm . comparator . compare ( toAdd , marker ) ! = 0 ) <nl> + content . add ( toAdd ) ; <nl> + else <nl> + { <nl> + / / gotta join <nl> + current = toAdd . closeDeletionTime ( false ) ; <nl> + marker = new RangeTombstoneBoundMarker ( marker . closeBound ( false ) , current ) ; <nl> + } <nl> + } <nl> + DeletionTime best = open . stream ( ) . max ( DeletionTime : : compareTo ) . orElse ( DeletionTime . LIVE ) ; <nl> + if ( best ! = DeletionTime . LIVE ) <nl> + marker = RangeTombstoneBoundaryMarker . makeBoundary ( false , marker . closeBound ( false ) , marker . closeBound ( false ) . invert ( ) , current , best ) ; <nl> + toAdd = marker ; <nl> + current = best ; <nl> + } <nl> + } <nl> + } <nl> + content . add ( toAdd ) ; <nl> + assert current = = DeletionTime . LIVE ; <nl> + assert open . isEmpty ( ) ; <nl> + return content ; <nl> + } <nl> + <nl> + private Clustering clustering ( int i ) <nl> + { <nl> + return cfm . comparator . make ( String . format ( " Row % 06d " , i ) ) ; <nl> + } <nl> + <nl> + private void test ( Supplier < Collection < ? extends Unfiltered > > content , Row staticRow ) <nl> + { <nl> + for ( int i = 0 ; i < TESTS ; + + i ) <nl> + { <nl> + try <nl> + { <nl> + rand = new Random ( i ) ; <nl> + testIter ( content , staticRow ) ; <nl> + } <nl> + catch ( Throwable t ) <nl> + { <nl> + throw new AssertionError ( " Test failed with seed " + i , t ) ; <nl> + } <nl> + } <nl> + } <nl> + <nl> + private void testIter ( Supplier < Collection < ? extends Unfiltered > > contentSupplier , Row staticRow ) <nl> + { <nl> + NavigableSet < Clusterable > sortedContent = new TreeSet < Clusterable > ( cfm . comparator ) ; <nl> + sortedContent . addAll ( contentSupplier . get ( ) ) ; <nl> + AbstractBTreePartition partition ; <nl> + try ( UnfilteredRowIterator iter = new Util . UnfilteredSource ( cfm , Util . dk ( " pk " ) , staticRow , sortedContent . stream ( ) . map ( x - > ( Unfiltered ) x ) . iterator ( ) ) ) <nl> + { <nl> + partition = ImmutableBTreePartition . create ( iter ) ; <nl> + } <nl> + <nl> + ColumnDefinition defCol = cfm . getColumnDefinition ( new ColumnIdentifier ( " col " , true ) ) ; <nl> + ColumnFilter cf = ColumnFilter . selectionBuilder ( ) . add ( defCol ) . build ( ) ; <nl> + Function < ? super Clusterable , ? extends Clusterable > colFilter = x - > x instanceof Row ? ( ( Row ) x ) . filter ( cf , cfm ) : x ; <nl> + Slices slices = Slices . with ( cfm . comparator , Slice . make ( clustering ( KEY _ RANGE / 4 ) , clustering ( KEY _ RANGE * 3 / 4 ) ) ) ; <nl> + Slices multiSlices = makeSlices ( ) ; <nl> + <nl> + / / lastRow <nl> + assertRowsEqual ( ( Row ) get ( sortedContent . descendingSet ( ) , x - > x instanceof Row ) , <nl> + partition . lastRow ( ) ) ; <nl> + / / get ( static ) <nl> + assertRowsEqual ( staticRow , <nl> + partition . getRow ( Clustering . STATIC _ CLUSTERING ) ) ; <nl> + <nl> + / / get <nl> + for ( int i = 0 ; i < KEY _ RANGE ; + + i ) <nl> + { <nl> + Clustering cl = clustering ( i ) ; <nl> + assertRowsEqual ( getRow ( sortedContent , cl ) , <nl> + partition . getRow ( cl ) ) ; <nl> + } <nl> + / / isEmpty <nl> + assertEquals ( sortedContent . isEmpty ( ) & & staticRow = = null , <nl> + partition . isEmpty ( ) ) ; <nl> + / / hasRows <nl> + assertEquals ( sortedContent . stream ( ) . anyMatch ( x - > x instanceof Row ) , <nl> + partition . hasRows ( ) ) ; <nl> + <nl> + / / iterator <nl> + assertIteratorsEqual ( sortedContent . stream ( ) . filter ( x - > x instanceof Row ) . iterator ( ) , <nl> + partition . iterator ( ) ) ; <nl> + <nl> + / / unfiltered iterator <nl> + assertIteratorsEqual ( sortedContent . iterator ( ) , <nl> + partition . unfilteredIterator ( ) ) ; <nl> + <nl> + / / unfiltered iterator <nl> + assertIteratorsEqual ( sortedContent . iterator ( ) , <nl> + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , Slices . ALL , false ) ) ; <nl> + / / column - filtered <nl> + assertIteratorsEqual ( sortedContent . stream ( ) . map ( colFilter ) . iterator ( ) , <nl> + partition . unfilteredIterator ( cf , Slices . ALL , false ) ) ; <nl> + / / sliced <nl> + assertIteratorsEqual ( slice ( sortedContent , slices . get ( 0 ) ) , <nl> + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , slices , false ) ) ; <nl> + assertIteratorsEqual ( streamOf ( slice ( sortedContent , slices . get ( 0 ) ) ) . map ( colFilter ) . iterator ( ) , <nl> + partition . unfilteredIterator ( cf , slices , false ) ) ; <nl> + / / randomly multi - sliced <nl> + assertIteratorsEqual ( slice ( sortedContent , multiSlices ) , <nl> + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , multiSlices , false ) ) ; <nl> + assertIteratorsEqual ( streamOf ( slice ( sortedContent , multiSlices ) ) . map ( colFilter ) . iterator ( ) , <nl> + partition . unfilteredIterator ( cf , multiSlices , false ) ) ; <nl> + / / reversed <nl> + assertIteratorsEqual ( sortedContent . descendingIterator ( ) , <nl> + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , Slices . ALL , true ) ) ; <nl> + assertIteratorsEqual ( sortedContent . descendingSet ( ) . stream ( ) . map ( colFilter ) . iterator ( ) , <nl> + partition . unfilteredIterator ( cf , Slices . ALL , true ) ) ; <nl> + assertIteratorsEqual ( invert ( slice ( sortedContent , slices . get ( 0 ) ) ) , <nl> + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , slices , true ) ) ; <nl> + assertIteratorsEqual ( streamOf ( invert ( slice ( sortedContent , slices . get ( 0 ) ) ) ) . map ( colFilter ) . iterator ( ) , <nl> + partition . unfilteredIterator ( cf , slices , true ) ) ; <nl> + assertIteratorsEqual ( invert ( slice ( sortedContent , multiSlices ) ) , <nl> + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , multiSlices , true ) ) ; <nl> + assertIteratorsEqual ( streamOf ( invert ( slice ( sortedContent , multiSlices ) ) ) . map ( colFilter ) . iterator ( ) , <nl> + partition . unfilteredIterator ( cf , multiSlices , true ) ) ; <nl> + <nl> + / / search iterator <nl> + testSearchIterator ( sortedContent , partition , ColumnFilter . all ( cfm ) , false ) ; <nl> + testSearchIterator ( sortedContent , partition , cf , false ) ; <nl> + testSearchIterator ( sortedContent , partition , ColumnFilter . all ( cfm ) , true ) ; <nl> + testSearchIterator ( sortedContent , partition , cf , true ) ; <nl> + <nl> + / / sliceable iter <nl> + testSliceableIterator ( sortedContent , partition , ColumnFilter . all ( cfm ) , false ) ; <nl> + testSliceableIterator ( sortedContent , partition , cf , false ) ; <nl> + testSliceableIterator ( sortedContent , partition , ColumnFilter . all ( cfm ) , true ) ; <nl> + testSliceableIterator ( sortedContent , partition , cf , true ) ; <nl> + } <nl> + <nl> + void testSearchIterator ( NavigableSet < Clusterable > sortedContent , Partition partition , ColumnFilter cf , boolean reversed ) <nl> + { <nl> + SearchIterator < Clustering , Row > searchIter = partition . searchIterator ( cf , reversed ) ; <nl> + int pos = reversed ? KEY _ RANGE : 0 ; <nl> + int mul = reversed ? - 1 : 1 ; <nl> + boolean started = false ; <nl> + while ( searchIter . hasNext ( ) ) <nl> + { <nl> + int skip = rand . nextInt ( KEY _ RANGE / 10 ) ; <nl> + pos + = skip * mul ; <nl> + Clustering cl = clustering ( pos ) ; <nl> + Row row = searchIter . next ( cl ) ; / / returns row with deletion , incl . empty row with deletion <nl> + if ( row = = null & & skip = = 0 & & started ) / / allowed to return null if already reported row <nl> + continue ; <nl> + started = true ; <nl> + Row expected = getRow ( sortedContent , cl ) ; <nl> + assertEquals ( expected = = null , row = = null ) ; <nl> + if ( row = = null ) <nl> + continue ; <nl> + assertRowsEqual ( expected . filter ( cf , cfm ) , row ) ; <nl> + } <nl> + } <nl> + <nl> + Slices makeSlices ( ) <nl> + { <nl> + int pos = 0 ; <nl> + Slices . Builder builder = new Slices . Builder ( cfm . comparator ) ; <nl> + while ( pos < = KEY _ RANGE ) <nl> + { <nl> + int skip = rand . nextInt ( KEY _ RANGE / 10 ) * ( rand . nextInt ( 3 ) + 2 / 3 ) ; / / increased chance of getting 0 <nl> + pos + = skip ; <nl> + int sz = rand . nextInt ( KEY _ RANGE / 10 ) + ( skip = = 0 ? 1 : 0 ) ; / / if start is exclusive need at least sz 1 <nl> + Clustering start = clustering ( pos ) ; <nl> + pos + = sz ; <nl> + Clustering end = clustering ( pos ) ; <nl> + Slice slice = Slice . make ( skip = = 0 ? Bound . exclusiveStartOf ( start ) : Bound . inclusiveStartOf ( start ) , Bound . inclusiveEndOf ( end ) ) ; <nl> + builder . add ( slice ) ; <nl> + } <nl> + return builder . build ( ) ; <nl> + } <nl> + <nl> + void testSliceableIterator ( NavigableSet < Clusterable > sortedContent , AbstractBTreePartition partition , ColumnFilter cf , boolean reversed ) <nl> + { <nl> + Function < ? super Clusterable , ? extends Clusterable > colFilter = x - > x instanceof Row ? ( ( Row ) x ) . filter ( cf , cfm ) : x ; <nl> + Slices slices = makeSlices ( ) ; <nl> + try ( SliceableUnfilteredRowIterator sliceableIter = partition . sliceableUnfilteredIterator ( cf , reversed ) ) <nl> + { <nl> + for ( Slice slice : ( Iterable < Slice > ) ( ) - > directed ( slices , reversed ) ) <nl> + assertIteratorsEqual ( streamOf ( directed ( slice ( sortedContent , slice ) , reversed ) ) . map ( colFilter ) . iterator ( ) , <nl> + sliceableIter . slice ( slice ) ) ; <nl> + } <nl> + <nl> + / / Try using sliceable as unfiltered iterator <nl> + try ( SliceableUnfilteredRowIterator sliceableIter = partition . sliceableUnfilteredIterator ( cf , reversed ) ) <nl> + { <nl> + assertIteratorsEqual ( ( reversed ? sortedContent . descendingSet ( ) : sortedContent ) . <nl> + stream ( ) . map ( colFilter ) . iterator ( ) , <nl> + sliceableIter ) ; <nl> + } <nl> + } <nl> + <nl> + private < T > Iterator < T > invert ( Iterator < T > slice ) <nl> + { <nl> + Deque < T > dest = new LinkedList < > ( ) ; <nl> + Iterators . addAll ( dest , slice ) ; <nl> + return dest . descendingIterator ( ) ; <nl> + } <nl> + <nl> + private Iterator < Clusterable > slice ( NavigableSet < Clusterable > sortedContent , Slices slices ) <nl> + { <nl> + return Iterators . concat ( streamOf ( slices ) . map ( slice - > slice ( sortedContent , slice ) ) . iterator ( ) ) ; <nl> + } <nl> + <nl> + private Iterator < Clusterable > slice ( NavigableSet < Clusterable > sortedContent , Slice slice ) <nl> + { <nl> + / / Slice bounds are inclusive bounds , equal only to markers . Matched markers should be returned as one - sided boundaries . <nl> + RangeTombstoneMarker prev = ( RangeTombstoneMarker ) sortedContent . headSet ( slice . start ( ) , true ) . descendingSet ( ) . stream ( ) . filter ( x - > x instanceof RangeTombstoneMarker ) . findFirst ( ) . orElse ( null ) ; <nl> + RangeTombstoneMarker next = ( RangeTombstoneMarker ) sortedContent . tailSet ( slice . end ( ) , true ) . stream ( ) . filter ( x - > x instanceof RangeTombstoneMarker ) . findFirst ( ) . orElse ( null ) ; <nl> + Iterator < Clusterable > result = sortedContent . subSet ( slice . start ( ) , false , slice . end ( ) , false ) . iterator ( ) ; <nl> + if ( prev ! = null & & prev . isOpen ( false ) ) <nl> + result = Iterators . concat ( Iterators . singletonIterator ( new RangeTombstoneBoundMarker ( slice . start ( ) , prev . openDeletionTime ( false ) ) ) , result ) ; <nl> + if ( next ! = null & & next . isClose ( false ) ) <nl> + result = Iterators . concat ( result , Iterators . singletonIterator ( new RangeTombstoneBoundMarker ( slice . end ( ) , next . closeDeletionTime ( false ) ) ) ) ; <nl> + return result ; <nl> + } <nl> + <nl> + private Iterator < Slice > directed ( Slices slices , boolean reversed ) <nl> + { <nl> + return directed ( slices . iterator ( ) , reversed ) ; <nl> + } <nl> + <nl> + private < T > Iterator < T > directed ( Iterator < T > iter , boolean reversed ) <nl> + { <nl> + if ( ! reversed ) <nl> + return iter ; <nl> + return invert ( iter ) ; <nl> + } <nl> + <nl> + private < T > Stream < T > streamOf ( Iterator < T > iterator ) <nl> + { <nl> + Iterable < T > iterable = ( ) - > iterator ; <nl> + return streamOf ( iterable ) ; <nl> + } <nl> + <nl> + < T > Stream < T > streamOf ( Iterable < T > iterable ) <nl> + { <nl> + return StreamSupport . stream ( iterable . spliterator ( ) , false ) ; <nl> + } <nl> + <nl> + private void assertIteratorsEqual ( Iterator < ? extends Clusterable > it1 , Iterator < ? extends Clusterable > it2 ) <nl> + { <nl> + Clusterable [ ] a1 = ( Clusterable [ ] ) Iterators . toArray ( it1 , Clusterable . class ) ; <nl> + Clusterable [ ] a2 = ( Clusterable [ ] ) Iterators . toArray ( it2 , Clusterable . class ) ; <nl> + if ( Arrays . equals ( a1 , a2 ) ) <nl> + return ; <nl> + String a1s = Stream . of ( a1 ) . map ( x - > " \ n " + ( x instanceof Unfiltered ? ( ( Unfiltered ) x ) . toString ( cfm ) : x . toString ( ) ) ) . collect ( Collectors . toList ( ) ) . toString ( ) ; <nl> + String a2s = Stream . of ( a2 ) . map ( x - > " \ n " + ( x instanceof Unfiltered ? ( ( Unfiltered ) x ) . toString ( cfm ) : x . toString ( ) ) ) . collect ( Collectors . toList ( ) ) . toString ( ) ; <nl> + assertArrayEquals ( " Arrays differ . Expected " + a1s + " was " + a2s , a1 , a2 ) ; <nl> + } <nl> + <nl> + private Row getRow ( NavigableSet < Clusterable > sortedContent , Clustering cl ) <nl> + { <nl> + NavigableSet < Clusterable > nexts = sortedContent . tailSet ( cl , true ) ; <nl> + if ( nexts . isEmpty ( ) ) <nl> + return null ; <nl> + Row row = nexts . first ( ) instanceof Row & & cfm . comparator . compare ( cl , nexts . first ( ) ) = = 0 ? ( Row ) nexts . first ( ) : null ; <nl> + for ( Clusterable next : nexts ) <nl> + if ( next instanceof RangeTombstoneMarker ) <nl> + { <nl> + RangeTombstoneMarker rt = ( RangeTombstoneMarker ) next ; <nl> + if ( ! rt . isClose ( false ) ) <nl> + return row ; <nl> + DeletionTime delTime = rt . closeDeletionTime ( false ) ; <nl> + return row = = null ? BTreeRow . emptyDeletedRow ( cl , Deletion . regular ( delTime ) ) : row . filter ( ColumnFilter . all ( cfm ) , delTime , true , cfm ) ; <nl> + } <nl> + return row ; <nl> + } <nl> + <nl> + private void assertRowsEqual ( Row expected , Row actual ) <nl> + { <nl> + try <nl> + { <nl> + assertEquals ( expected = = null , actual = = null ) ; <nl> + if ( expected = = null ) <nl> + return ; <nl> + assertEquals ( expected . clustering ( ) , actual . clustering ( ) ) ; <nl> + assertEquals ( expected . deletion ( ) , actual . deletion ( ) ) ; <nl> + assertArrayEquals ( Iterables . toArray ( expected . cells ( ) , Cell . class ) , Iterables . toArray ( expected . cells ( ) , Cell . class ) ) ; <nl> + } catch ( Throwable t ) <nl> + { <nl> + throw new AssertionError ( String . format ( " Row comparison failed , expected % s got % s " , expected , actual ) , t ) ; <nl> + } <nl> + } <nl> + <nl> + private static < T > T get ( NavigableSet < T > sortedContent , Predicate < T > test ) <nl> + { <nl> + return sortedContent . stream ( ) . filter ( test ) . findFirst ( ) . orElse ( null ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testEmpty ( ) <nl> + { <nl> + test ( ( ) - > Collections . < Row > emptyList ( ) , null ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testStaticOnly ( ) <nl> + { <nl> + test ( ( ) - > Collections . < Row > emptyList ( ) , makeStaticRow ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testRows ( ) <nl> + { <nl> + test ( this : : generateRows , null ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testRowsWithStatic ( ) <nl> + { <nl> + test ( this : : generateRows , makeStaticRow ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testMarkersOnly ( ) <nl> + { <nl> + test ( this : : generateMarkersOnly , null ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testMarkersWithStatic ( ) <nl> + { <nl> + test ( this : : generateMarkersOnly , makeStaticRow ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testUnfiltereds ( ) <nl> + { <nl> + test ( this : : generateUnfiltereds , makeStaticRow ( ) ) ; <nl> + } <nl> + <nl> + }
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / ColumnFamily . java b / src / java / org / apache / cassandra / db / ColumnFamily . java <nl> index c3bddd4 . . cb87833 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnFamily . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnFamily . java <nl> @ @ - 35 , 7 + 35 , 6 @ @ import org . apache . cassandra . utils . FBUtilities ; <nl> import org . apache . cassandra . io . ICompactSerializer2 ; <nl> import org . apache . cassandra . db . filter . QueryPath ; <nl> import org . apache . cassandra . db . marshal . AbstractType ; <nl> - import org . apache . cassandra . db . marshal . MarshalException ; <nl> <nl> <nl> public final class ColumnFamily implements IColumnContainer <nl> @ @ - 121 , 12 + 120 , 13 @ @ public final class ColumnFamily implements IColumnContainer <nl> * We need to go through each column <nl> * in the column family and resolve it before adding <nl> * / <nl> - void addColumns ( ColumnFamily cf ) <nl> + public void addAll ( ColumnFamily cf ) <nl> { <nl> for ( IColumn column : cf . getSortedColumns ( ) ) <nl> { <nl> addColumn ( column ) ; <nl> } <nl> + delete ( cf ) ; <nl> } <nl> <nl> public ICompactSerializer2 < IColumn > getColumnSerializer ( ) <nl> @ @ - 415 , 8 + 415 , 7 @ @ public final class ColumnFamily implements IColumnContainer <nl> for ( ColumnFamily cf2 : columnFamilies ) <nl> { <nl> assert cf . name ( ) . equals ( cf2 . name ( ) ) ; <nl> - cf . addColumns ( cf2 ) ; <nl> - cf . delete ( cf2 ) ; <nl> + cf . addAll ( cf2 ) ; <nl> } <nl> return cf ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> index 001c644 . . 96bb18b 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> @ @ - 560 , 26 + 560 , 6 @ @ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> CompactionManager . instance ( ) . submit ( this ) ; <nl> } <nl> <nl> - private PriorityQueue < FileStruct > initializePriorityQueue ( Collection < SSTableReader > sstables , List < Range > ranges ) throws IOException <nl> - { <nl> - PriorityQueue < FileStruct > pq = new PriorityQueue < FileStruct > ( ) ; <nl> - if ( sstables . size ( ) > 1 | | ( ranges ! = null & & sstables . size ( ) > 0 ) ) <nl> - { <nl> - FileStruct fs = null ; <nl> - for ( SSTableReader sstable : sstables ) <nl> - { <nl> - fs = sstable . getFileStruct ( ) ; <nl> - fs . advance ( true ) ; <nl> - if ( fs . isExhausted ( ) ) <nl> - { <nl> - continue ; <nl> - } <nl> - pq . add ( fs ) ; <nl> - } <nl> - } <nl> - return pq ; <nl> - } <nl> - <nl> / * <nl> * Group files of similar size into buckets . <nl> * / <nl> @ @ - 766 , 150 + 746 , 67 @ @ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> * / <nl> List < SSTableReader > doFileAntiCompaction ( Collection < SSTableReader > sstables , List < Range > ranges , EndPoint target ) throws IOException <nl> { <nl> - List < SSTableReader > results = new ArrayList < SSTableReader > ( ) ; <nl> - long startTime = System . currentTimeMillis ( ) ; <nl> - long totalBytesRead = 0 ; <nl> - long totalBytesWritten = 0 ; <nl> - long totalkeysRead = 0 ; <nl> - long totalkeysWritten = 0 ; <nl> - String rangeFileLocation ; <nl> - String mergedFileName ; <nl> + logger _ . info ( " AntiCompacting [ " + StringUtils . join ( sstables , " , " ) + " ] " ) ; <nl> / / Calculate the expected compacted filesize <nl> - long expectedRangeFileSize = getExpectedCompactedFileSize ( sstables ) ; <nl> - / * in the worst case a node will be giving out half of its data so we take a chance * / <nl> - expectedRangeFileSize = expectedRangeFileSize / 2 ; <nl> - rangeFileLocation = DatabaseDescriptor . getDataFileLocationForTable ( table _ , expectedRangeFileSize ) ; <nl> - / / If the compaction file path is null that means we have no space left for this compaction . <nl> - if ( rangeFileLocation = = null ) <nl> - { <nl> - logger _ . error ( " Total bytes to be written for range compaction . . . " <nl> - + expectedRangeFileSize + " is greater than the safe limit of the disk space available . " ) ; <nl> - return results ; <nl> - } <nl> - PriorityQueue < FileStruct > pq = initializePriorityQueue ( sstables , ranges ) ; <nl> - if ( pq . isEmpty ( ) ) <nl> + long expectedRangeFileSize = getExpectedCompactedFileSize ( sstables ) / 2 ; <nl> + String compactionFileLocation = DatabaseDescriptor . getDataFileLocationForTable ( table _ , expectedRangeFileSize ) ; <nl> + if ( compactionFileLocation = = null ) <nl> { <nl> - return results ; <nl> + throw new UnsupportedOperationException ( " disk full " ) ; <nl> } <nl> + List < SSTableReader > results = new ArrayList < SSTableReader > ( ) ; <nl> <nl> - mergedFileName = getTempSSTableFileName ( ) ; <nl> - SSTableWriter rangeWriter = null ; <nl> - String lastkey = null ; <nl> - List < FileStruct > lfs = new ArrayList < FileStruct > ( ) ; <nl> - DataOutputBuffer bufOut = new DataOutputBuffer ( ) ; <nl> - int expectedBloomFilterSize = SSTableReader . getApproximateKeyCount ( sstables ) ; <nl> - expectedBloomFilterSize = ( expectedBloomFilterSize > 0 ) ? expectedBloomFilterSize : SSTableReader . indexInterval ( ) ; <nl> + long startTime = System . currentTimeMillis ( ) ; <nl> + long totalkeysWritten = 0 ; <nl> + <nl> + int expectedBloomFilterSize = Math . max ( SSTableReader . indexInterval ( ) , SSTableReader . getApproximateKeyCount ( sstables ) / 2 ) ; <nl> if ( logger _ . isDebugEnabled ( ) ) <nl> logger _ . debug ( " Expected bloom filter size : " + expectedBloomFilterSize ) ; <nl> - List < ColumnFamily > columnFamilies = new ArrayList < ColumnFamily > ( ) ; <nl> <nl> - while ( pq . size ( ) > 0 | | lfs . size ( ) > 0 ) <nl> + SSTableWriter writer = null ; <nl> + CompactionIterator ci = new CompactionIterator ( sstables ) ; <nl> + <nl> + try <nl> { <nl> - FileStruct fs = null ; <nl> - if ( pq . size ( ) > 0 ) <nl> + if ( ! ci . hasNext ( ) ) <nl> { <nl> - fs = pq . poll ( ) ; <nl> + logger _ . warn ( " Nothing to compact ( all files empty or corrupt ) . This should not happen . " ) ; <nl> + return results ; <nl> } <nl> - if ( fs ! = null <nl> - & & ( lastkey = = null | | lastkey . equals ( fs . getKey ( ) ) ) ) <nl> - { <nl> - / / The keys are the same so we need to add this to the <nl> - / / ldfs list <nl> - lastkey = fs . getKey ( ) ; <nl> - lfs . add ( fs ) ; <nl> - } <nl> - else <nl> + <nl> + while ( ci . hasNext ( ) ) <nl> { <nl> - Collections . sort ( lfs , new FileStructComparator ( ) ) ; <nl> - ColumnFamily columnFamily ; <nl> - bufOut . reset ( ) ; <nl> - if ( lfs . size ( ) > 1 ) <nl> + CompactionIterator . CompactedRow row = ci . next ( ) ; <nl> + if ( Range . isTokenInRanges ( StorageService . getPartitioner ( ) . getToken ( row . key ) , ranges ) ) <nl> { <nl> - for ( FileStruct filestruct : lfs ) <nl> - { <nl> - / / We want to add only 2 and resolve them right there in order to save on memory footprint <nl> - if ( columnFamilies . size ( ) > 1 ) <nl> - { <nl> - / / Now merge the 2 column families <nl> - merge ( columnFamilies ) ; <nl> - } <nl> - / / deserialize into column families <nl> - columnFamilies . add ( filestruct . getColumnFamily ( ) ) ; <nl> - } <nl> - / / Now after merging all crap append to the sstable <nl> - columnFamily = resolveAndRemoveDeleted ( columnFamilies ) ; <nl> - columnFamilies . clear ( ) ; <nl> - if ( columnFamily ! = null ) <nl> - { <nl> - ColumnFamily . serializer ( ) . serializeWithIndexes ( columnFamily , bufOut ) ; <nl> - } <nl> - } <nl> - else <nl> - { <nl> - / / TODO deserializing only to reserialize is dumb <nl> - FileStruct filestruct = lfs . get ( 0 ) ; <nl> - ColumnFamily . serializer ( ) . serializeWithIndexes ( filestruct . getColumnFamily ( ) , bufOut ) ; <nl> - } <nl> - if ( Range . isTokenInRanges ( StorageService . getPartitioner ( ) . getToken ( lastkey ) , ranges ) ) <nl> - { <nl> - if ( rangeWriter = = null ) <nl> + if ( writer = = null ) <nl> { <nl> if ( target ! = null ) <nl> { <nl> - rangeFileLocation = rangeFileLocation + File . separator + " bootstrap " ; <nl> + compactionFileLocation = compactionFileLocation + File . separator + " bootstrap " ; <nl> } <nl> - FileUtils . createDirectory ( rangeFileLocation ) ; <nl> - String fname = new File ( rangeFileLocation , mergedFileName ) . getAbsolutePath ( ) ; <nl> - rangeWriter = new SSTableWriter ( fname , expectedBloomFilterSize , StorageService . getPartitioner ( ) ) ; <nl> + FileUtils . createDirectory ( compactionFileLocation ) ; <nl> + String newFilename = new File ( compactionFileLocation , getTempSSTableFileName ( ) ) . getAbsolutePath ( ) ; <nl> + writer = new SSTableWriter ( newFilename , expectedBloomFilterSize , StorageService . getPartitioner ( ) ) ; <nl> } <nl> - rangeWriter . append ( lastkey , bufOut ) ; <nl> - } <nl> - totalkeysWritten + + ; <nl> - for ( FileStruct filestruct : lfs ) <nl> - { <nl> - filestruct . advance ( true ) ; <nl> - if ( filestruct . isExhausted ( ) ) <nl> - { <nl> - continue ; <nl> - } <nl> - / * keep on looping until we find a key in the range * / <nl> - while ( ! Range . isTokenInRanges ( StorageService . getPartitioner ( ) . getToken ( filestruct . getKey ( ) ) , ranges ) ) <nl> - { <nl> - filestruct . advance ( true ) ; <nl> - if ( filestruct . isExhausted ( ) ) <nl> - { <nl> - break ; <nl> - } <nl> - } <nl> - if ( ! filestruct . isExhausted ( ) ) <nl> - { <nl> - pq . add ( filestruct ) ; <nl> - } <nl> - totalkeysRead + + ; <nl> - } <nl> - lfs . clear ( ) ; <nl> - lastkey = null ; <nl> - if ( fs ! = null ) <nl> - { <nl> - / / Add back the fs since we processed the rest of <nl> - / / filestructs <nl> - pq . add ( fs ) ; <nl> + writer . append ( row . key , row . buffer ) ; <nl> + totalkeysWritten + + ; <nl> } <nl> } <nl> } <nl> - <nl> - if ( rangeWriter ! = null ) <nl> + finally <nl> { <nl> - results . add ( rangeWriter . closeAndOpenReader ( ) ) ; <nl> + ci . close ( ) ; <nl> } <nl> <nl> - if ( logger _ . isDebugEnabled ( ) ) <nl> + if ( writer ! = null ) <nl> { <nl> - logger _ . debug ( " Total time taken for range split . . . " + ( System . currentTimeMillis ( ) - startTime ) ) ; <nl> - logger _ . debug ( " Total bytes Read for range split . . . " + totalBytesRead ) ; <nl> - logger _ . debug ( " Total bytes written for range split . . . " <nl> - + totalBytesWritten + " Total keys read . . . " + totalkeysRead ) ; <nl> + results . add ( writer . closeAndOpenReader ( ) ) ; <nl> + String format = " AntiCompacted to % s . % d / % d bytes for % d keys . Time : % dms . " ; <nl> + long dTime = System . currentTimeMillis ( ) - startTime ; <nl> + logger _ . info ( String . format ( format , writer . getFilename ( ) , getTotalBytes ( sstables ) , results . get ( 0 ) . length ( ) , totalkeysWritten , dTime ) ) ; <nl> } <nl> + <nl> return results ; <nl> } <nl> <nl> @ @ - 938 , 111 + 835 , 59 @ @ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> } <nl> <nl> long startTime = System . currentTimeMillis ( ) ; <nl> - long totalBytesRead = 0 ; <nl> - long totalkeysRead = 0 ; <nl> long totalkeysWritten = 0 ; <nl> - PriorityQueue < FileStruct > pq = initializePriorityQueue ( sstables , null ) ; <nl> - <nl> - if ( pq . isEmpty ( ) ) <nl> - { <nl> - logger _ . warn ( " Nothing to compact ( all files empty or corrupt ) . This should not happen . " ) ; <nl> - / / TODO clean out bad files , if any <nl> - return 0 ; <nl> - } <nl> <nl> - int expectedBloomFilterSize = SSTableReader . getApproximateKeyCount ( sstables ) ; <nl> - if ( expectedBloomFilterSize < 0 ) <nl> - expectedBloomFilterSize = SSTableReader . indexInterval ( ) ; <nl> - String newFilename = new File ( compactionFileLocation , getTempSSTableFileName ( ) ) . getAbsolutePath ( ) ; <nl> - SSTableWriter writer = new SSTableWriter ( newFilename , expectedBloomFilterSize , StorageService . getPartitioner ( ) ) ; <nl> - SSTableReader ssTable = null ; <nl> - String lastkey = null ; <nl> - List < FileStruct > lfs = new ArrayList < FileStruct > ( ) ; <nl> - DataOutputBuffer bufOut = new DataOutputBuffer ( ) ; <nl> + int expectedBloomFilterSize = Math . max ( SSTableReader . indexInterval ( ) , SSTableReader . getApproximateKeyCount ( sstables ) ) ; <nl> if ( logger _ . isDebugEnabled ( ) ) <nl> logger _ . debug ( " Expected bloom filter size : " + expectedBloomFilterSize ) ; <nl> - List < ColumnFamily > columnFamilies = new ArrayList < ColumnFamily > ( ) ; <nl> <nl> - while ( pq . size ( ) > 0 | | lfs . size ( ) > 0 ) <nl> + SSTableWriter writer ; <nl> + CompactionIterator ci = new CompactionIterator ( sstables ) ; <nl> + <nl> + try <nl> { <nl> - FileStruct fs = null ; <nl> - if ( pq . size ( ) > 0 ) <nl> + if ( ! ci . hasNext ( ) ) <nl> { <nl> - fs = pq . poll ( ) ; <nl> + logger _ . warn ( " Nothing to compact ( all files empty or corrupt ) . This should not happen . " ) ; <nl> + return 0 ; <nl> } <nl> - if ( fs ! = null <nl> - & & ( lastkey = = null | | lastkey . equals ( fs . getKey ( ) ) ) ) <nl> - { <nl> - / / The keys are the same so we need to add this to the <nl> - / / ldfs list <nl> - lastkey = fs . getKey ( ) ; <nl> - lfs . add ( fs ) ; <nl> - } <nl> - else <nl> - { <nl> - Collections . sort ( lfs , new FileStructComparator ( ) ) ; <nl> - ColumnFamily columnFamily ; <nl> - bufOut . reset ( ) ; <nl> - if ( lfs . size ( ) > 1 ) <nl> - { <nl> - for ( FileStruct filestruct : lfs ) <nl> - { <nl> - / / We want to add only 2 and resolve them right there in order to save on memory footprint <nl> - if ( columnFamilies . size ( ) > 1 ) <nl> - { <nl> - merge ( columnFamilies ) ; <nl> - } <nl> - / / deserialize into column families <nl> - columnFamilies . add ( filestruct . getColumnFamily ( ) ) ; <nl> - } <nl> - / / Now after merging all crap append to the sstable <nl> - columnFamily = resolveAndRemoveDeleted ( columnFamilies ) ; <nl> - columnFamilies . clear ( ) ; <nl> - if ( columnFamily ! = null ) <nl> - { <nl> - ColumnFamily . serializer ( ) . serializeWithIndexes ( columnFamily , bufOut ) ; <nl> - } <nl> - } <nl> - else <nl> - { <nl> - / / TODO deserializing only to reserialize is dumb <nl> - FileStruct filestruct = lfs . get ( 0 ) ; <nl> - ColumnFamily . serializer ( ) . serializeWithIndexes ( filestruct . getColumnFamily ( ) , bufOut ) ; <nl> - } <nl> <nl> - writer . append ( lastkey , bufOut ) ; <nl> - totalkeysWritten + + ; <nl> + String newFilename = new File ( compactionFileLocation , getTempSSTableFileName ( ) ) . getAbsolutePath ( ) ; <nl> + writer = new SSTableWriter ( newFilename , expectedBloomFilterSize , StorageService . getPartitioner ( ) ) ; <nl> <nl> - for ( FileStruct filestruct : lfs ) <nl> - { <nl> - filestruct . advance ( true ) ; <nl> - if ( filestruct . isExhausted ( ) ) <nl> - { <nl> - continue ; <nl> - } <nl> - pq . add ( filestruct ) ; <nl> - totalkeysRead + + ; <nl> - } <nl> - lfs . clear ( ) ; <nl> - lastkey = null ; <nl> - if ( fs ! = null ) <nl> - { <nl> - / * Add back the fs since we processed the rest of filestructs * / <nl> - pq . add ( fs ) ; <nl> - } <nl> + while ( ci . hasNext ( ) ) <nl> + { <nl> + CompactionIterator . CompactedRow row = ci . next ( ) ; <nl> + writer . append ( row . key , row . buffer ) ; <nl> + totalkeysWritten + + ; <nl> } <nl> } <nl> - ssTable = writer . closeAndOpenReader ( ) ; <nl> + finally <nl> + { <nl> + ci . close ( ) ; <nl> + } <nl> + <nl> + SSTableReader ssTable = writer . closeAndOpenReader ( ) ; <nl> ssTables _ . add ( ssTable ) ; <nl> ssTables _ . markCompacted ( sstables ) ; <nl> CompactionManager . instance ( ) . submit ( ColumnFamilyStore . this ) ; <nl> <nl> - String format = " Compacted to % s . % d / % d bytes for % d / % d keys read / written . Time : % dms . " ; <nl> + String format = " Compacted to % s . % d / % d bytes for % d keys . Time : % dms . " ; <nl> long dTime = System . currentTimeMillis ( ) - startTime ; <nl> - logger _ . info ( String . format ( format , writer . getFilename ( ) , totalBytesRead , ssTable . length ( ) , totalkeysRead , totalkeysWritten , dTime ) ) ; <nl> + logger _ . info ( String . format ( format , writer . getFilename ( ) , getTotalBytes ( sstables ) , ssTable . length ( ) , totalkeysWritten , dTime ) ) ; <nl> return sstables . size ( ) ; <nl> } <nl> <nl> + private long getTotalBytes ( Iterable < SSTableReader > sstables ) <nl> + { <nl> + long sum = 0 ; <nl> + for ( SSTableReader sstable : sstables ) <nl> + { <nl> + sum + = sstable . length ( ) ; <nl> + } <nl> + return sum ; <nl> + } <nl> + <nl> public static List < Memtable > getUnflushedMemtables ( String cfName ) <nl> { <nl> return new ArrayList < Memtable > ( getMemtablesPendingFlushNotNull ( cfName ) ) ; <nl> @ @ - 1341 , 23 + 1186 , 24 @ @ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> / / sstables <nl> for ( SSTableReader sstable : ssTables _ ) <nl> { <nl> - final SSTableScanner fs = sstable . getScanner ( ) ; <nl> - fs . seekTo ( startWith ) ; <nl> - iterators . add ( new Iterator < String > ( ) <nl> + final SSTableScanner scanner = sstable . getScanner ( ) ; <nl> + scanner . seekTo ( startWith ) ; <nl> + Iterator < String > iter = new Iterator < String > ( ) <nl> { <nl> public boolean hasNext ( ) <nl> { <nl> - return fs . hasNext ( ) ; <nl> + return scanner . hasNext ( ) ; <nl> } <nl> public String next ( ) <nl> { <nl> - return fs . next ( ) . getKey ( ) ; <nl> + return scanner . next ( ) . getKey ( ) ; <nl> } <nl> public void remove ( ) <nl> { <nl> throw new UnsupportedOperationException ( ) ; <nl> } <nl> - } ) ; <nl> + } ; <nl> + iterators . add ( iter ) ; <nl> } <nl> <nl> Iterator < String > collated = IteratorUtils . collatedIterator ( comparator , iterators ) ; <nl> diff - - git a / src / java / org / apache / cassandra / db / FileStructComparator . java b / src / java / org / apache / cassandra / db / FileStructComparator . java <nl> deleted file mode 100644 <nl> index e81a992 . . 0000000 <nl> - - - a / src / java / org / apache / cassandra / db / FileStructComparator . java <nl> + + + / dev / null <nl> @ @ - 1 , 31 + 0 , 0 @ @ <nl> - / * <nl> - * Licensed to the Apache Software Foundation ( ASF ) under one <nl> - * or more contributor license agreements . See the NOTICE file <nl> - * distributed with this work for additional information <nl> - * regarding copyright ownership . The ASF licenses this file <nl> - * to you under the Apache License , Version 2 . 0 ( the <nl> - * " License " ) ; you may not use this file except in compliance <nl> - * with the License . You may obtain a copy of the License at <nl> - * <nl> - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> - * <nl> - * Unless required by applicable law or agreed to in writing , <nl> - * software distributed under the License is distributed on an <nl> - * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY <nl> - * KIND , either express or implied . See the License for the <nl> - * specific language governing permissions and limitations <nl> - * under the License . <nl> - * / <nl> - package org . apache . cassandra . db ; <nl> - <nl> - import java . util . Comparator ; <nl> - <nl> - import org . apache . cassandra . io . FileStruct ; <nl> - <nl> - class FileStructComparator implements Comparator < FileStruct > <nl> - { <nl> - public int compare ( FileStruct f , FileStruct f2 ) <nl> - { <nl> - return f . getFileName ( ) . compareTo ( f2 . getFileName ( ) ) ; <nl> - } <nl> - } <nl> \ No newline at end of file <nl> diff - - git a / src / java / org / apache / cassandra / db / Memtable . java b / src / java / org / apache / cassandra / db / Memtable . java <nl> index d88e004 . . 696ae5a 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Memtable . java <nl> + + + b / src / java / org / apache / cassandra / db / Memtable . java <nl> @ @ - 153 , 7 + 153 , 7 @ @ public class Memtable implements Comparable < Memtable > <nl> { <nl> int oldSize = oldCf . size ( ) ; <nl> int oldObjectCount = oldCf . getColumnCount ( ) ; <nl> - oldCf . addColumns ( columnFamily ) ; <nl> + oldCf . addAll ( columnFamily ) ; <nl> int newSize = oldCf . size ( ) ; <nl> int newObjectCount = oldCf . getColumnCount ( ) ; <nl> resolveSize ( oldSize , newSize ) ; <nl> diff - - git a / src / java / org / apache / cassandra / io / CompactionIterator . java b / src / java / org / apache / cassandra / io / CompactionIterator . java <nl> new file mode 100644 <nl> index 0000000 . . b65e132 <nl> - - - / dev / null <nl> + + + b / src / java / org / apache / cassandra / io / CompactionIterator . java <nl> @ @ - 0 , 0 + 1 , 113 @ @ <nl> + package org . apache . cassandra . io ; <nl> + <nl> + import java . io . Closeable ; <nl> + import java . io . IOException ; <nl> + import java . util . List ; <nl> + import java . util . ArrayList ; <nl> + import java . util . Comparator ; <nl> + <nl> + import org . apache . commons . collections . iterators . CollatingIterator ; <nl> + <nl> + import org . apache . cassandra . utils . ReducingIterator ; <nl> + import org . apache . cassandra . db . ColumnFamily ; <nl> + <nl> + public class CompactionIterator extends ReducingIterator < IteratingRow , CompactionIterator . CompactedRow > implements Closeable <nl> + { <nl> + private final List < IteratingRow > rows = new ArrayList < IteratingRow > ( ) ; <nl> + <nl> + @ SuppressWarnings ( " unchecked " ) <nl> + public CompactionIterator ( Iterable < SSTableReader > sstables ) throws IOException <nl> + { <nl> + super ( getCollatingIterator ( sstables ) ) ; <nl> + } <nl> + <nl> + @ SuppressWarnings ( " unchecked " ) <nl> + private static CollatingIterator getCollatingIterator ( Iterable < SSTableReader > sstables ) throws IOException <nl> + { <nl> + / / CollatingIterator has a bug that causes NPE when you try to use default comparator . : ( <nl> + CollatingIterator iter = new CollatingIterator ( new Comparator ( ) <nl> + { <nl> + public int compare ( Object o1 , Object o2 ) <nl> + { <nl> + return ( ( Comparable ) o1 ) . compareTo ( o2 ) ; <nl> + } <nl> + } ) ; <nl> + for ( SSTableReader sstable : sstables ) <nl> + { <nl> + iter . addIterator ( sstable . getScanner ( ) ) ; <nl> + } <nl> + return iter ; <nl> + } <nl> + <nl> + @ Override <nl> + protected boolean isEqual ( IteratingRow o1 , IteratingRow o2 ) <nl> + { <nl> + return o1 . getKey ( ) . equals ( o2 . getKey ( ) ) ; <nl> + } <nl> + <nl> + public void reduce ( IteratingRow current ) <nl> + { <nl> + rows . add ( current ) ; <nl> + } <nl> + <nl> + protected CompactedRow getReduced ( ) <nl> + { <nl> + try <nl> + { <nl> + return getReducedRaw ( ) ; <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + throw new RuntimeException ( e ) ; <nl> + } <nl> + } <nl> + <nl> + protected CompactedRow getReducedRaw ( ) throws IOException <nl> + { <nl> + DataOutputBuffer buffer = new DataOutputBuffer ( ) ; <nl> + String key = rows . get ( 0 ) . getKey ( ) ; <nl> + if ( rows . size ( ) > 1 ) <nl> + { <nl> + ColumnFamily cf = null ; <nl> + for ( IteratingRow row : rows ) <nl> + { <nl> + if ( cf = = null ) <nl> + { <nl> + cf = row . getColumnFamily ( ) ; <nl> + } <nl> + else <nl> + { <nl> + cf . addAll ( row . getColumnFamily ( ) ) ; <nl> + } <nl> + } <nl> + ColumnFamily . serializer ( ) . serializeWithIndexes ( cf , buffer ) ; <nl> + } <nl> + else <nl> + { <nl> + assert rows . size ( ) = = 1 ; <nl> + rows . get ( 0 ) . echoData ( buffer ) ; <nl> + } <nl> + rows . clear ( ) ; <nl> + return new CompactedRow ( key , buffer ) ; <nl> + } <nl> + <nl> + public void close ( ) throws IOException <nl> + { <nl> + for ( Object o : ( ( CollatingIterator ) source ) . getIterators ( ) ) <nl> + { <nl> + ( ( SSTableScanner ) o ) . close ( ) ; <nl> + } <nl> + } <nl> + <nl> + public static class CompactedRow <nl> + { <nl> + public final String key ; <nl> + public final DataOutputBuffer buffer ; <nl> + <nl> + public CompactedRow ( String key , DataOutputBuffer buffer ) <nl> + { <nl> + this . key = key ; <nl> + this . buffer = buffer ; <nl> + } <nl> + } <nl> + } <nl> diff - - git a / src / java / org / apache / cassandra / io / FileStruct . java b / src / java / org / apache / cassandra / io / FileStruct . java <nl> deleted file mode 100644 <nl> index b561239 . . 0000000 <nl> - - - a / src / java / org / apache / cassandra / io / FileStruct . java <nl> + + + / dev / null <nl> @ @ - 1 , 195 + 0 , 0 @ @ <nl> - / * * <nl> - * Licensed to the Apache Software Foundation ( ASF ) under one <nl> - * or more contributor license agreements . See the NOTICE file <nl> - * distributed with this work for additional information <nl> - * regarding copyright ownership . The ASF licenses this file <nl> - * to you under the Apache License , Version 2 . 0 ( the <nl> - * " License " ) ; you may not use this file except in compliance <nl> - * with the License . You may obtain a copy of the License at <nl> - * <nl> - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> - * <nl> - * Unless required by applicable law or agreed to in writing , software <nl> - * distributed under the License is distributed on an " AS IS " BASIS , <nl> - * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> - * See the License for the specific language governing permissions and <nl> - * limitations under the License . <nl> - * / <nl> - <nl> - package org . apache . cassandra . io ; <nl> - <nl> - import java . io . IOException ; <nl> - import java . io . File ; <nl> - import java . util . Iterator ; <nl> - <nl> - import org . apache . cassandra . db . IColumn ; <nl> - import org . apache . cassandra . db . ColumnFamily ; <nl> - import org . apache . cassandra . config . DatabaseDescriptor ; <nl> - <nl> - import org . apache . log4j . Logger ; <nl> - import com . google . common . collect . AbstractIterator ; <nl> - <nl> - <nl> - public class FileStruct implements Comparable < FileStruct > , Iterator < String > <nl> - { <nl> - private static Logger logger = Logger . getLogger ( FileStruct . class ) ; <nl> - <nl> - private IteratingRow row ; <nl> - private boolean exhausted = false ; <nl> - private BufferedRandomAccessFile file ; <nl> - private SSTableReader sstable ; <nl> - private FileStructIterator iterator ; <nl> - <nl> - FileStruct ( SSTableReader sstable ) throws IOException <nl> - { <nl> - / / TODO this is used for both compactions and key ranges . the buffer sizes we want <nl> - / / to use for these ops are very different . here we are leaning towards the key - range <nl> - / / use case since that is more common . What we really want is to split those <nl> - / / two uses of this class up . <nl> - this . file = new BufferedRandomAccessFile ( sstable . getFilename ( ) , " r " , 256 * 1024 ) ; <nl> - this . sstable = sstable ; <nl> - } <nl> - <nl> - public String getFileName ( ) <nl> - { <nl> - return file . getPath ( ) ; <nl> - } <nl> - <nl> - public void close ( ) throws IOException <nl> - { <nl> - file . close ( ) ; <nl> - } <nl> - <nl> - public boolean isExhausted ( ) <nl> - { <nl> - return exhausted ; <nl> - } <nl> - <nl> - public String getKey ( ) <nl> - { <nl> - return row . getKey ( ) ; <nl> - } <nl> - <nl> - public ColumnFamily getColumnFamily ( ) <nl> - { <nl> - return row . getEmptyColumnFamily ( ) ; <nl> - } <nl> - <nl> - public int compareTo ( FileStruct f ) <nl> - { <nl> - return sstable . getPartitioner ( ) . getDecoratedKeyComparator ( ) . compare ( getKey ( ) , f . getKey ( ) ) ; <nl> - } <nl> - <nl> - public void seekTo ( String seekKey ) <nl> - { <nl> - try <nl> - { <nl> - long position = sstable . getNearestPosition ( seekKey ) ; <nl> - if ( position < 0 ) <nl> - { <nl> - exhausted = true ; <nl> - return ; <nl> - } <nl> - file . seek ( position ) ; <nl> - advance ( false ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - throw new RuntimeException ( " corrupt sstable " , e ) ; <nl> - } <nl> - } <nl> - <nl> - / * <nl> - * Read the next key from the data file . <nl> - * Caller must check isExhausted after each call to see if further <nl> - * reads are valid . <nl> - * Do not mix with calls to the iterator interface ( next / hasnext ) . <nl> - * @ deprecated - - prefer the iterator interface . <nl> - * / <nl> - public void advance ( boolean materialize ) throws IOException <nl> - { <nl> - / / TODO r / m materialize option - - use iterableness ! <nl> - if ( exhausted ) <nl> - { <nl> - throw new IndexOutOfBoundsException ( ) ; <nl> - } <nl> - <nl> - if ( file . isEOF ( ) ) <nl> - { <nl> - file . close ( ) ; <nl> - exhausted = true ; <nl> - return ; <nl> - } <nl> - <nl> - row = new IteratingRow ( file , sstable ) ; <nl> - if ( materialize ) <nl> - { <nl> - while ( row . hasNext ( ) ) <nl> - { <nl> - IColumn column = row . next ( ) ; <nl> - row . getEmptyColumnFamily ( ) . addColumn ( column ) ; <nl> - } <nl> - } <nl> - else <nl> - { <nl> - row . skipRemaining ( ) ; <nl> - } <nl> - } <nl> - <nl> - public boolean hasNext ( ) <nl> - { <nl> - if ( iterator = = null ) <nl> - iterator = new FileStructIterator ( ) ; <nl> - return iterator . hasNext ( ) ; <nl> - } <nl> - <nl> - / * * do not mix with manual calls to advance ( ) . * / <nl> - public String next ( ) <nl> - { <nl> - if ( iterator = = null ) <nl> - iterator = new FileStructIterator ( ) ; <nl> - return iterator . next ( ) ; <nl> - } <nl> - <nl> - public void remove ( ) <nl> - { <nl> - throw new UnsupportedOperationException ( ) ; <nl> - } <nl> - <nl> - private class FileStructIterator extends AbstractIterator < String > <nl> - { <nl> - public FileStructIterator ( ) <nl> - { <nl> - if ( row = = null ) <nl> - { <nl> - if ( ! isExhausted ( ) ) <nl> - { <nl> - forward ( ) ; <nl> - } <nl> - } <nl> - } <nl> - <nl> - private void forward ( ) <nl> - { <nl> - try <nl> - { <nl> - advance ( false ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - } <nl> - <nl> - protected String computeNext ( ) <nl> - { <nl> - if ( isExhausted ( ) ) <nl> - { <nl> - return endOfData ( ) ; <nl> - } <nl> - String oldKey = getKey ( ) ; <nl> - forward ( ) ; <nl> - return oldKey ; <nl> - } <nl> - } <nl> - } <nl> diff - - git a / src / java / org / apache / cassandra / io / IteratingRow . java b / src / java / org / apache / cassandra / io / IteratingRow . java <nl> index 5ace95f . . 628fe50 100644 <nl> - - - a / src / java / org / apache / cassandra / io / IteratingRow . java <nl> + + + b / src / java / org / apache / cassandra / io / IteratingRow . java <nl> @ @ - 37 , 7 + 37 , 6 @ @ public class IteratingRow extends AbstractIterator < IColumn > implements Comparabl <nl> { <nl> private final String key ; <nl> private final long finishedAt ; <nl> - private final ColumnFamily emptyColumnFamily ; <nl> private final BufferedRandomAccessFile file ; <nl> private SSTableReader sstable ; <nl> private long dataStart ; <nl> @ @ - 51 , 10 + 50 , 6 @ @ public class IteratingRow extends AbstractIterator < IColumn > implements Comparabl <nl> int dataSize = file . readInt ( ) ; <nl> dataStart = file . getFilePointer ( ) ; <nl> finishedAt = dataStart + dataSize ; <nl> - / / legacy stuff to support FileStruct : <nl> - IndexHelper . skipBloomFilterAndIndex ( file ) ; <nl> - emptyColumnFamily = ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( sstable . makeColumnFamily ( ) , file ) ; <nl> - file . readInt ( ) ; <nl> } <nl> <nl> public String getKey ( ) <nl> @ @ - 62 , 11 + 57 , 6 @ @ public class IteratingRow extends AbstractIterator < IColumn > implements Comparabl <nl> return key ; <nl> } <nl> <nl> - public ColumnFamily getEmptyColumnFamily ( ) <nl> - { <nl> - return emptyColumnFamily ; <nl> - } <nl> - <nl> public void echoData ( DataOutput out ) throws IOException <nl> { <nl> file . seek ( dataStart ) ; <nl> diff - - git a / src / java / org / apache / cassandra / io / SSTableReader . java b / src / java / org / apache / cassandra / io / SSTableReader . java <nl> index fc94fca . . 81a71fd 100644 <nl> - - - a / src / java / org / apache / cassandra / io / SSTableReader . java <nl> + + + b / src / java / org / apache / cassandra / io / SSTableReader . java <nl> @ @ - 333 , 11 + 333 , 6 @ @ public class SSTableReader extends SSTable implements Comparable < SSTableReader > <nl> return partitioner ; <nl> } <nl> <nl> - public FileStruct getFileStruct ( ) throws IOException <nl> - { <nl> - return new FileStruct ( this ) ; <nl> - } <nl> - <nl> public SSTableScanner getScanner ( ) throws IOException <nl> { <nl> return new SSTableScanner ( this ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / db / ColumnFamilyTest . java b / test / unit / org / apache / cassandra / db / ColumnFamilyTest . java <nl> index 40e5889 . . 4219ff9 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / ColumnFamilyTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / ColumnFamilyTest . java <nl> @ @ - 20 , 8 + 20 , 6 @ @ package org . apache . cassandra . db ; <nl> <nl> import java . io . IOException ; <nl> import java . util . Arrays ; <nl> - import java . util . HashSet ; <nl> - import java . util . Random ; <nl> import java . util . TreeMap ; <nl> <nl> import org . junit . Test ; <nl> @ @ - 125 , 8 + 123 , 8 @ @ public class ColumnFamilyTest <nl> cf _ old . addColumn ( QueryPath . column ( " col2 " . getBytes ( ) ) , val2 , 1 ) ; <nl> cf _ old . addColumn ( QueryPath . column ( " col3 " . getBytes ( ) ) , val2 , 2 ) ; <nl> <nl> - cf _ result . addColumns ( cf _ new ) ; <nl> - cf _ result . addColumns ( cf _ old ) ; <nl> + cf _ result . addAll ( cf _ new ) ; <nl> + cf _ result . addAll ( cf _ old ) ; <nl> <nl> assert 3 = = cf _ result . getColumnCount ( ) : " Count is " + cf _ new . getColumnCount ( ) ; <nl> / / addcolumns will only add if timestamp > = old timestamp

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index f6aed18 . . 2527c43 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 6 @ @ 
 3 . 0 . 1 
 + * Fix bug with range tombstones on reverse queries and test coverage for 
 + AbstractBTreePartition ( CASSANDRA - 10059 ) 
 * Remove 64k limit on collection elements ( CASSANDRA - 10374 ) 
 * Remove unclear Indexer . indexes ( ) method ( CASSANDRA - 10690 ) 
 * Fix NPE on stream read error ( CASSANDRA - 10771 ) 
 diff - - git a / src / java / org / apache / cassandra / db / RangeTombstoneList . java b / src / java / org / apache / cassandra / db / RangeTombstoneList . java 
 index c92a296 . . c67ea33 100644 
 - - - a / src / java / org / apache / cassandra / db / RangeTombstoneList . java 
 + + + b / src / java / org / apache / cassandra / db / RangeTombstoneList . java 
 @ @ - 17 , 21 + 17 , 16 @ @ 
 * / 
 package org . apache . cassandra . db ; 
 
 - import java . io . IOException ; 
 import java . nio . ByteBuffer ; 
 import java . util . Arrays ; 
 + import java . util . Collections ; 
 import java . util . Iterator ; 
 
 import org . apache . cassandra . utils . AbstractIterator ; 
 import com . google . common . collect . Iterators ; 
 
 - import org . slf4j . Logger ; 
 - import org . slf4j . LoggerFactory ; 
 import org . apache . cassandra . cache . IMeasurableMemory ; 
 import org . apache . cassandra . db . rows . * ; 
 - import org . apache . cassandra . io . IVersionedSerializer ; 
 - import org . apache . cassandra . io . util . DataInputPlus ; 
 - import org . apache . cassandra . io . util . DataOutputPlus ; 
 import org . apache . cassandra . utils . ObjectSizes ; 
 import org . apache . cassandra . utils . memory . AbstractAllocator ; 
 
 @ @ - 53 , 8 + 48 , 6 @ @ import org . apache . cassandra . utils . memory . AbstractAllocator ; 
 * / 
 public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurableMemory 
 { 
 - private static final Logger logger = LoggerFactory . getLogger ( RangeTombstoneList . class ) ; 
 - 
 private static long EMPTY _ SIZE = ObjectSizes . measure ( new RangeTombstoneList ( null , 0 ) ) ; 
 
 private final ClusteringComparator comparator ; 
 @ @ - 265 , 6 + 258 , 8 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable 
 / * 
 * Return is the index of the range covering name if name is covered . If the return idx is negative , 
 * no range cover name and - idx - 1 is the index of the first range whose start is greater than name . 
 + * 
 + * Note that bounds are not in the range if they fall on its boundary . 
 * / 
 private int searchInternal ( ClusteringPrefix name , int startIdx , int endIdx ) 
 { 
 @ @ - 274 , 7 + 269 , 9 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable 
 int pos = Arrays . binarySearch ( starts , startIdx , endIdx , name , comparator ) ; 
 if ( pos > = 0 ) 
 { 
 - return pos ; 
 + / / Equality only happens for bounds ( as used by forward / reverseIterator ) , and bounds are equal only if they 
 + / / are the same or complementary , in either case the bound itself is not part of the range . 
 + return - pos - 1 ; 
 } 
 else 
 { 
 @ @ - 283 , 7 + 280 , 7 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable 
 if ( idx < 0 ) 
 return - 1 ; 
 
 - return comparator . compare ( name , ends [ idx ] ) < = 0 ? idx : - idx - 2 ; 
 + return comparator . compare ( name , ends [ idx ] ) < 0 ? idx : - idx - 2 ; 
 } 
 } 
 
 @ @ - 387 , 14 + 384 , 14 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable 
 final int start = startIdx < 0 ? - startIdx - 1 : startIdx ; 
 
 if ( start > = size ) 
 - return Iterators . < RangeTombstone > emptyIterator ( ) ; 
 + return Collections . emptyIterator ( ) ; 
 
 int finishIdx = slice . end ( ) = = Slice . Bound . TOP ? size - 1 : searchInternal ( slice . end ( ) , start , size ) ; 
 / / if stopIdx is the first range after ' slice . end ( ) ' we care only until the previous range 
 final int finish = finishIdx < 0 ? - finishIdx - 2 : finishIdx ; 
 
 if ( start > finish ) 
 - return Iterators . < RangeTombstone > emptyIterator ( ) ; 
 + return Collections . emptyIterator ( ) ; 
 
 if ( start = = finish ) 
 { 
 @ @ - 428 , 19 + 425 , 19 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable 
 
 private Iterator < RangeTombstone > reverseIterator ( final Slice slice ) 
 { 
 - int startIdx = slice . end ( ) = = Slice . Bound . TOP ? 0 : searchInternal ( slice . end ( ) , 0 , size ) ; 
 + int startIdx = slice . end ( ) = = Slice . Bound . TOP ? size - 1 : searchInternal ( slice . end ( ) , 0 , size ) ; 
 / / if startIdx is the first range after ' slice . end ( ) ' we care only until the previous range 
 final int start = startIdx < 0 ? - startIdx - 2 : startIdx ; 
 
 - if ( start > = size ) 
 - return Iterators . < RangeTombstone > emptyIterator ( ) ; 
 + if ( start < 0 ) 
 + return Collections . emptyIterator ( ) ; 
 
 - int finishIdx = slice . start ( ) = = Slice . Bound . BOTTOM ? 0 : searchInternal ( slice . start ( ) , 0 , start ) ; 
 + int finishIdx = slice . start ( ) = = Slice . Bound . BOTTOM ? 0 : searchInternal ( slice . start ( ) , 0 , start + 1 ) ; / / include same as finish 
 / / if stopIdx is the first range after ' slice . end ( ) ' we care only until the previous range 
 final int finish = finishIdx < 0 ? - finishIdx - 1 : finishIdx ; 
 
 if ( start < finish ) 
 - return Iterators . < RangeTombstone > emptyIterator ( ) ; 
 + return Collections . emptyIterator ( ) ; 
 
 if ( start = = finish ) 
 { 
 @ @ - 467 , 7 + 464 , 7 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable 
 return rangeTombstoneWithNewEnd ( idx - - , slice . end ( ) ) ; 
 if ( idx = = finish & & comparator . compare ( starts [ idx ] , slice . start ( ) ) < 0 ) 
 return rangeTombstoneWithNewStart ( idx - - , slice . start ( ) ) ; 
 - return rangeTombstone ( idx + + ) ; 
 + return rangeTombstone ( idx - - ) ; 
 } 
 } ; 
 } 
 @ @ - 665 , 20 + 662 , 6 @ @ public class RangeTombstoneList implements Iterable < RangeTombstone > , IMeasurable 
 size + + ; 
 } 
 
 - private void removeInternal ( int i ) 
 - { 
 - assert i > = 0 ; 
 - 
 - System . arraycopy ( starts , i + 1 , starts , i , size - i - 1 ) ; 
 - System . arraycopy ( ends , i + 1 , ends , i , size - i - 1 ) ; 
 - System . arraycopy ( markedAts , i + 1 , markedAts , i , size - i - 1 ) ; 
 - System . arraycopy ( delTimes , i + 1 , delTimes , i , size - i - 1 ) ; 
 - 
 - - - size ; 
 - starts [ size ] = null ; 
 - ends [ size ] = null ; 
 - } 
 - 
 / * 
 * Grow the arrays , leaving index i " free " in the process . 
 * / 
 diff - - git a / src / java / org / apache / cassandra / utils / SearchIterator . java b / src / java / org / apache / cassandra / utils / SearchIterator . java 
 index ca7b2fa . . 5309f4a 100644 
 - - - a / src / java / org / apache / cassandra / utils / SearchIterator . java 
 + + + b / src / java / org / apache / cassandra / utils / SearchIterator . java 
 @ @ - 23 , 7 + 23 , 8 @ @ public interface SearchIterator < K , V > 
 
 / * * 
 * Searches " forwards " ( in direction of travel ) in the iterator for the required key ; 
 - * if this or any key greater has already been returned by the iterator , null will be returned . 
 + * if this or any key greater has already been returned by the iterator , the method may 
 + * choose to return null , the correct or incorrect output , or fail an assertion . 
 * 
 * it is permitted to search past the end of the iterator , i . e . ! hasNext ( ) = > next ( ? ) = = null 
 * 
 diff - - git a / test / unit / org / apache / cassandra / Util . java b / test / unit / org / apache / cassandra / Util . java 
 index ea0bd9b . . 91ec6b6 100644 
 - - - a / test / unit / org / apache / cassandra / Util . java 
 + + + b / test / unit / org / apache / cassandra / Util . java 
 @ @ - 538 , 4 + 538 , 27 @ @ public class Util 
 { 
 return ( ) - > new AssertionError ( message ) ; 
 } 
 + 
 + public static class UnfilteredSource extends AbstractUnfilteredRowIterator implements UnfilteredRowIterator 
 + { 
 + Iterator < Unfiltered > content ; 
 + 
 + public UnfilteredSource ( CFMetaData cfm , DecoratedKey partitionKey , Row staticRow , Iterator < Unfiltered > content ) 
 + { 
 + super ( cfm , 
 + partitionKey , 
 + DeletionTime . LIVE , 
 + cfm . partitionColumns ( ) , 
 + staticRow ! = null ? staticRow : Rows . EMPTY _ STATIC _ ROW , 
 + false , 
 + EncodingStats . NO _ STATS ) ; 
 + this . content = content ; 
 + } 
 + 
 + @ Override 
 + protected Unfiltered computeNext ( ) 
 + { 
 + return content . hasNext ( ) ? content . next ( ) : endOfData ( ) ; 
 + } 
 + } 
 } 
 diff - - git a / test / unit / org / apache / cassandra / db / partition / PartitionImplementationTest . java b / test / unit / org / apache / cassandra / db / partition / PartitionImplementationTest . java 
 new file mode 100644 
 index 0000000 . . f215331 
 - - - / dev / null 
 + + + b / test / unit / org / apache / cassandra / db / partition / PartitionImplementationTest . java 
 @ @ - 0 , 0 + 1 , 524 @ @ 
 + / * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , software 
 + * distributed under the License is distributed on an " AS IS " BASIS , 
 + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 + * See the License for the specific language governing permissions and 
 + * limitations under the License . 
 + * / 
 + package org . apache . cassandra . db . partition ; 
 + 
 + import static org . junit . Assert . * ; 
 + 
 + import java . util . * ; 
 + import java . util . function . Function ; 
 + import java . util . function . Predicate ; 
 + import java . util . function . Supplier ; 
 + import java . util . stream . Collectors ; 
 + import java . util . stream . Stream ; 
 + import java . util . stream . StreamSupport ; 
 + 
 + import com . google . common . collect . Iterables ; 
 + import com . google . common . collect . Iterators ; 
 + 
 + import org . junit . BeforeClass ; 
 + import org . junit . Test ; 
 + 
 + import org . apache . cassandra . SchemaLoader ; 
 + import org . apache . cassandra . Util ; 
 + import org . apache . cassandra . config . CFMetaData ; 
 + import org . apache . cassandra . config . ColumnDefinition ; 
 + import org . apache . cassandra . cql3 . ColumnIdentifier ; 
 + import org . apache . cassandra . db . * ; 
 + import org . apache . cassandra . db . Slice . Bound ; 
 + import org . apache . cassandra . db . filter . ColumnFilter ; 
 + import org . apache . cassandra . db . marshal . AsciiType ; 
 + import org . apache . cassandra . db . partitions . AbstractBTreePartition ; 
 + import org . apache . cassandra . db . partitions . ImmutableBTreePartition ; 
 + import org . apache . cassandra . db . partitions . Partition ; 
 + import org . apache . cassandra . db . rows . * ; 
 + import org . apache . cassandra . db . rows . Row . Deletion ; 
 + import org . apache . cassandra . exceptions . ConfigurationException ; 
 + import org . apache . cassandra . schema . KeyspaceParams ; 
 + import org . apache . cassandra . utils . ByteBufferUtil ; 
 + import org . apache . cassandra . utils . SearchIterator ; 
 + 
 + public class PartitionImplementationTest 
 + { 
 + private static final String KEYSPACE = " PartitionImplementationTest " ; 
 + private static final String CF = " Standard " ; 
 + 
 + private static final int ENTRIES = 250 ; 
 + private static final int TESTS = 1000 ; 
 + private static final int KEY _ RANGE = ENTRIES * 5 ; 
 + 
 + private static final int TIMESTAMP = KEY _ RANGE + 1 ; 
 + 
 + private static CFMetaData cfm ; 
 + private Random rand = new Random ( 2 ) ; 
 + 
 + @ BeforeClass 
 + public static void defineSchema ( ) throws ConfigurationException 
 + { 
 + SchemaLoader . prepareServer ( ) ; 
 + 
 + cfm = CFMetaData . Builder . create ( KEYSPACE , CF ) 
 + . addPartitionKey ( " pk " , AsciiType . instance ) 
 + . addClusteringColumn ( " ck " , AsciiType . instance ) 
 + . addRegularColumn ( " col " , AsciiType . instance ) 
 + . addStaticColumn ( " static _ col " , AsciiType . instance ) 
 + . build ( ) ; 
 + SchemaLoader . createKeyspace ( KEYSPACE , 
 + KeyspaceParams . simple ( 1 ) , 
 + cfm ) ; 
 + } 
 + 
 + private List < Row > generateRows ( ) 
 + { 
 + List < Row > content = new ArrayList < > ( ) ; 
 + Set < Integer > keysUsed = new HashSet < > ( ) ; 
 + for ( int i = 0 ; i < ENTRIES ; + + i ) 
 + { 
 + int rk ; 
 + do 
 + { 
 + rk = rand . nextInt ( KEY _ RANGE ) ; 
 + } 
 + while ( ! keysUsed . add ( rk ) ) ; 
 + content . add ( makeRow ( clustering ( rk ) , " Col " + rk ) ) ; 
 + } 
 + return content ; / / not sorted 
 + } 
 + 
 + Row makeRow ( Clustering clustering , String colValue ) 
 + { 
 + ColumnDefinition defCol = cfm . getColumnDefinition ( new ColumnIdentifier ( " col " , true ) ) ; 
 + Row . Builder row = BTreeRow . unsortedBuilder ( TIMESTAMP ) ; 
 + row . newRow ( clustering ) ; 
 + row . addCell ( BufferCell . live ( cfm , defCol , TIMESTAMP , ByteBufferUtil . bytes ( colValue ) ) ) ; 
 + return row . build ( ) ; 
 + } 
 + 
 + Row makeStaticRow ( ) 
 + { 
 + ColumnDefinition defCol = cfm . getColumnDefinition ( new ColumnIdentifier ( " static _ col " , true ) ) ; 
 + Row . Builder row = BTreeRow . unsortedBuilder ( TIMESTAMP ) ; 
 + row . newRow ( Clustering . STATIC _ CLUSTERING ) ; 
 + row . addCell ( BufferCell . live ( cfm , defCol , TIMESTAMP , ByteBufferUtil . bytes ( " static value " ) ) ) ; 
 + return row . build ( ) ; 
 + } 
 + 
 + private List < Unfiltered > generateMarkersOnly ( ) 
 + { 
 + return addMarkers ( new ArrayList < > ( ) ) ; 
 + } 
 + 
 + private List < Unfiltered > generateUnfiltereds ( ) 
 + { 
 + List < Unfiltered > content = new ArrayList < > ( generateRows ( ) ) ; 
 + return addMarkers ( content ) ; 
 + } 
 + 
 + List < Unfiltered > addMarkers ( List < Unfiltered > content ) 
 + { 
 + List < RangeTombstoneMarker > markers = new ArrayList < > ( ) ; 
 + Set < Integer > delTimes = new HashSet < > ( ) ; 
 + for ( int i = 0 ; i < ENTRIES / 10 ; + + i ) 
 + { 
 + int delTime ; 
 + do 
 + { 
 + delTime = rand . nextInt ( KEY _ RANGE ) ; 
 + } 
 + while ( ! delTimes . add ( delTime ) ) ; 
 + 
 + int start = rand . nextInt ( KEY _ RANGE ) ; 
 + DeletionTime dt = new DeletionTime ( delTime , delTime ) ; 
 + RangeTombstoneMarker open = RangeTombstoneBoundMarker . inclusiveOpen ( false , clustering ( start ) . getRawValues ( ) , dt ) ; 
 + int end = start + rand . nextInt ( ( KEY _ RANGE - start ) / 4 + 1 ) ; 
 + RangeTombstoneMarker close = RangeTombstoneBoundMarker . inclusiveClose ( false , clustering ( end ) . getRawValues ( ) , dt ) ; 
 + markers . add ( open ) ; 
 + markers . add ( close ) ; 
 + } 
 + markers . sort ( cfm . comparator ) ; 
 + 
 + RangeTombstoneMarker toAdd = null ; 
 + Set < DeletionTime > open = new HashSet < > ( ) ; 
 + DeletionTime current = DeletionTime . LIVE ; 
 + for ( RangeTombstoneMarker marker : markers ) 
 + { 
 + if ( marker . isOpen ( false ) ) 
 + { 
 + DeletionTime delTime = marker . openDeletionTime ( false ) ; 
 + open . add ( delTime ) ; 
 + if ( delTime . supersedes ( current ) ) 
 + { 
 + if ( toAdd ! = null ) 
 + { 
 + if ( cfm . comparator . compare ( toAdd , marker ) ! = 0 ) 
 + content . add ( toAdd ) ; 
 + else 
 + { 
 + / / gotta join 
 + current = toAdd . isClose ( false ) ? toAdd . closeDeletionTime ( false ) : DeletionTime . LIVE ; 
 + } 
 + } 
 + if ( current ! = DeletionTime . LIVE ) 
 + marker = RangeTombstoneBoundaryMarker . makeBoundary ( false , marker . openBound ( false ) . invert ( ) , marker . openBound ( false ) , current , delTime ) ; 
 + toAdd = marker ; 
 + current = delTime ; 
 + } 
 + } 
 + else 
 + { 
 + assert marker . isClose ( false ) ; 
 + DeletionTime delTime = marker . closeDeletionTime ( false ) ; 
 + boolean removed = open . remove ( delTime ) ; 
 + assert removed ; 
 + if ( current . equals ( delTime ) ) 
 + { 
 + if ( toAdd ! = null ) 
 + { 
 + if ( cfm . comparator . compare ( toAdd , marker ) ! = 0 ) 
 + content . add ( toAdd ) ; 
 + else 
 + { 
 + / / gotta join 
 + current = toAdd . closeDeletionTime ( false ) ; 
 + marker = new RangeTombstoneBoundMarker ( marker . closeBound ( false ) , current ) ; 
 + } 
 + } 
 + DeletionTime best = open . stream ( ) . max ( DeletionTime : : compareTo ) . orElse ( DeletionTime . LIVE ) ; 
 + if ( best ! = DeletionTime . LIVE ) 
 + marker = RangeTombstoneBoundaryMarker . makeBoundary ( false , marker . closeBound ( false ) , marker . closeBound ( false ) . invert ( ) , current , best ) ; 
 + toAdd = marker ; 
 + current = best ; 
 + } 
 + } 
 + } 
 + content . add ( toAdd ) ; 
 + assert current = = DeletionTime . LIVE ; 
 + assert open . isEmpty ( ) ; 
 + return content ; 
 + } 
 + 
 + private Clustering clustering ( int i ) 
 + { 
 + return cfm . comparator . make ( String . format ( " Row % 06d " , i ) ) ; 
 + } 
 + 
 + private void test ( Supplier < Collection < ? extends Unfiltered > > content , Row staticRow ) 
 + { 
 + for ( int i = 0 ; i < TESTS ; + + i ) 
 + { 
 + try 
 + { 
 + rand = new Random ( i ) ; 
 + testIter ( content , staticRow ) ; 
 + } 
 + catch ( Throwable t ) 
 + { 
 + throw new AssertionError ( " Test failed with seed " + i , t ) ; 
 + } 
 + } 
 + } 
 + 
 + private void testIter ( Supplier < Collection < ? extends Unfiltered > > contentSupplier , Row staticRow ) 
 + { 
 + NavigableSet < Clusterable > sortedContent = new TreeSet < Clusterable > ( cfm . comparator ) ; 
 + sortedContent . addAll ( contentSupplier . get ( ) ) ; 
 + AbstractBTreePartition partition ; 
 + try ( UnfilteredRowIterator iter = new Util . UnfilteredSource ( cfm , Util . dk ( " pk " ) , staticRow , sortedContent . stream ( ) . map ( x - > ( Unfiltered ) x ) . iterator ( ) ) ) 
 + { 
 + partition = ImmutableBTreePartition . create ( iter ) ; 
 + } 
 + 
 + ColumnDefinition defCol = cfm . getColumnDefinition ( new ColumnIdentifier ( " col " , true ) ) ; 
 + ColumnFilter cf = ColumnFilter . selectionBuilder ( ) . add ( defCol ) . build ( ) ; 
 + Function < ? super Clusterable , ? extends Clusterable > colFilter = x - > x instanceof Row ? ( ( Row ) x ) . filter ( cf , cfm ) : x ; 
 + Slices slices = Slices . with ( cfm . comparator , Slice . make ( clustering ( KEY _ RANGE / 4 ) , clustering ( KEY _ RANGE * 3 / 4 ) ) ) ; 
 + Slices multiSlices = makeSlices ( ) ; 
 + 
 + / / lastRow 
 + assertRowsEqual ( ( Row ) get ( sortedContent . descendingSet ( ) , x - > x instanceof Row ) , 
 + partition . lastRow ( ) ) ; 
 + / / get ( static ) 
 + assertRowsEqual ( staticRow , 
 + partition . getRow ( Clustering . STATIC _ CLUSTERING ) ) ; 
 + 
 + / / get 
 + for ( int i = 0 ; i < KEY _ RANGE ; + + i ) 
 + { 
 + Clustering cl = clustering ( i ) ; 
 + assertRowsEqual ( getRow ( sortedContent , cl ) , 
 + partition . getRow ( cl ) ) ; 
 + } 
 + / / isEmpty 
 + assertEquals ( sortedContent . isEmpty ( ) & & staticRow = = null , 
 + partition . isEmpty ( ) ) ; 
 + / / hasRows 
 + assertEquals ( sortedContent . stream ( ) . anyMatch ( x - > x instanceof Row ) , 
 + partition . hasRows ( ) ) ; 
 + 
 + / / iterator 
 + assertIteratorsEqual ( sortedContent . stream ( ) . filter ( x - > x instanceof Row ) . iterator ( ) , 
 + partition . iterator ( ) ) ; 
 + 
 + / / unfiltered iterator 
 + assertIteratorsEqual ( sortedContent . iterator ( ) , 
 + partition . unfilteredIterator ( ) ) ; 
 + 
 + / / unfiltered iterator 
 + assertIteratorsEqual ( sortedContent . iterator ( ) , 
 + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , Slices . ALL , false ) ) ; 
 + / / column - filtered 
 + assertIteratorsEqual ( sortedContent . stream ( ) . map ( colFilter ) . iterator ( ) , 
 + partition . unfilteredIterator ( cf , Slices . ALL , false ) ) ; 
 + / / sliced 
 + assertIteratorsEqual ( slice ( sortedContent , slices . get ( 0 ) ) , 
 + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , slices , false ) ) ; 
 + assertIteratorsEqual ( streamOf ( slice ( sortedContent , slices . get ( 0 ) ) ) . map ( colFilter ) . iterator ( ) , 
 + partition . unfilteredIterator ( cf , slices , false ) ) ; 
 + / / randomly multi - sliced 
 + assertIteratorsEqual ( slice ( sortedContent , multiSlices ) , 
 + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , multiSlices , false ) ) ; 
 + assertIteratorsEqual ( streamOf ( slice ( sortedContent , multiSlices ) ) . map ( colFilter ) . iterator ( ) , 
 + partition . unfilteredIterator ( cf , multiSlices , false ) ) ; 
 + / / reversed 
 + assertIteratorsEqual ( sortedContent . descendingIterator ( ) , 
 + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , Slices . ALL , true ) ) ; 
 + assertIteratorsEqual ( sortedContent . descendingSet ( ) . stream ( ) . map ( colFilter ) . iterator ( ) , 
 + partition . unfilteredIterator ( cf , Slices . ALL , true ) ) ; 
 + assertIteratorsEqual ( invert ( slice ( sortedContent , slices . get ( 0 ) ) ) , 
 + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , slices , true ) ) ; 
 + assertIteratorsEqual ( streamOf ( invert ( slice ( sortedContent , slices . get ( 0 ) ) ) ) . map ( colFilter ) . iterator ( ) , 
 + partition . unfilteredIterator ( cf , slices , true ) ) ; 
 + assertIteratorsEqual ( invert ( slice ( sortedContent , multiSlices ) ) , 
 + partition . unfilteredIterator ( ColumnFilter . all ( cfm ) , multiSlices , true ) ) ; 
 + assertIteratorsEqual ( streamOf ( invert ( slice ( sortedContent , multiSlices ) ) ) . map ( colFilter ) . iterator ( ) , 
 + partition . unfilteredIterator ( cf , multiSlices , true ) ) ; 
 + 
 + / / search iterator 
 + testSearchIterator ( sortedContent , partition , ColumnFilter . all ( cfm ) , false ) ; 
 + testSearchIterator ( sortedContent , partition , cf , false ) ; 
 + testSearchIterator ( sortedContent , partition , ColumnFilter . all ( cfm ) , true ) ; 
 + testSearchIterator ( sortedContent , partition , cf , true ) ; 
 + 
 + / / sliceable iter 
 + testSliceableIterator ( sortedContent , partition , ColumnFilter . all ( cfm ) , false ) ; 
 + testSliceableIterator ( sortedContent , partition , cf , false ) ; 
 + testSliceableIterator ( sortedContent , partition , ColumnFilter . all ( cfm ) , true ) ; 
 + testSliceableIterator ( sortedContent , partition , cf , true ) ; 
 + } 
 + 
 + void testSearchIterator ( NavigableSet < Clusterable > sortedContent , Partition partition , ColumnFilter cf , boolean reversed ) 
 + { 
 + SearchIterator < Clustering , Row > searchIter = partition . searchIterator ( cf , reversed ) ; 
 + int pos = reversed ? KEY _ RANGE : 0 ; 
 + int mul = reversed ? - 1 : 1 ; 
 + boolean started = false ; 
 + while ( searchIter . hasNext ( ) ) 
 + { 
 + int skip = rand . nextInt ( KEY _ RANGE / 10 ) ; 
 + pos + = skip * mul ; 
 + Clustering cl = clustering ( pos ) ; 
 + Row row = searchIter . next ( cl ) ; / / returns row with deletion , incl . empty row with deletion 
 + if ( row = = null & & skip = = 0 & & started ) / / allowed to return null if already reported row 
 + continue ; 
 + started = true ; 
 + Row expected = getRow ( sortedContent , cl ) ; 
 + assertEquals ( expected = = null , row = = null ) ; 
 + if ( row = = null ) 
 + continue ; 
 + assertRowsEqual ( expected . filter ( cf , cfm ) , row ) ; 
 + } 
 + } 
 + 
 + Slices makeSlices ( ) 
 + { 
 + int pos = 0 ; 
 + Slices . Builder builder = new Slices . Builder ( cfm . comparator ) ; 
 + while ( pos < = KEY _ RANGE ) 
 + { 
 + int skip = rand . nextInt ( KEY _ RANGE / 10 ) * ( rand . nextInt ( 3 ) + 2 / 3 ) ; / / increased chance of getting 0 
 + pos + = skip ; 
 + int sz = rand . nextInt ( KEY _ RANGE / 10 ) + ( skip = = 0 ? 1 : 0 ) ; / / if start is exclusive need at least sz 1 
 + Clustering start = clustering ( pos ) ; 
 + pos + = sz ; 
 + Clustering end = clustering ( pos ) ; 
 + Slice slice = Slice . make ( skip = = 0 ? Bound . exclusiveStartOf ( start ) : Bound . inclusiveStartOf ( start ) , Bound . inclusiveEndOf ( end ) ) ; 
 + builder . add ( slice ) ; 
 + } 
 + return builder . build ( ) ; 
 + } 
 + 
 + void testSliceableIterator ( NavigableSet < Clusterable > sortedContent , AbstractBTreePartition partition , ColumnFilter cf , boolean reversed ) 
 + { 
 + Function < ? super Clusterable , ? extends Clusterable > colFilter = x - > x instanceof Row ? ( ( Row ) x ) . filter ( cf , cfm ) : x ; 
 + Slices slices = makeSlices ( ) ; 
 + try ( SliceableUnfilteredRowIterator sliceableIter = partition . sliceableUnfilteredIterator ( cf , reversed ) ) 
 + { 
 + for ( Slice slice : ( Iterable < Slice > ) ( ) - > directed ( slices , reversed ) ) 
 + assertIteratorsEqual ( streamOf ( directed ( slice ( sortedContent , slice ) , reversed ) ) . map ( colFilter ) . iterator ( ) , 
 + sliceableIter . slice ( slice ) ) ; 
 + } 
 + 
 + / / Try using sliceable as unfiltered iterator 
 + try ( SliceableUnfilteredRowIterator sliceableIter = partition . sliceableUnfilteredIterator ( cf , reversed ) ) 
 + { 
 + assertIteratorsEqual ( ( reversed ? sortedContent . descendingSet ( ) : sortedContent ) . 
 + stream ( ) . map ( colFilter ) . iterator ( ) , 
 + sliceableIter ) ; 
 + } 
 + } 
 + 
 + private < T > Iterator < T > invert ( Iterator < T > slice ) 
 + { 
 + Deque < T > dest = new LinkedList < > ( ) ; 
 + Iterators . addAll ( dest , slice ) ; 
 + return dest . descendingIterator ( ) ; 
 + } 
 + 
 + private Iterator < Clusterable > slice ( NavigableSet < Clusterable > sortedContent , Slices slices ) 
 + { 
 + return Iterators . concat ( streamOf ( slices ) . map ( slice - > slice ( sortedContent , slice ) ) . iterator ( ) ) ; 
 + } 
 + 
 + private Iterator < Clusterable > slice ( NavigableSet < Clusterable > sortedContent , Slice slice ) 
 + { 
 + / / Slice bounds are inclusive bounds , equal only to markers . Matched markers should be returned as one - sided boundaries . 
 + RangeTombstoneMarker prev = ( RangeTombstoneMarker ) sortedContent . headSet ( slice . start ( ) , true ) . descendingSet ( ) . stream ( ) . filter ( x - > x instanceof RangeTombstoneMarker ) . findFirst ( ) . orElse ( null ) ; 
 + RangeTombstoneMarker next = ( RangeTombstoneMarker ) sortedContent . tailSet ( slice . end ( ) , true ) . stream ( ) . filter ( x - > x instanceof RangeTombstoneMarker ) . findFirst ( ) . orElse ( null ) ; 
 + Iterator < Clusterable > result = sortedContent . subSet ( slice . start ( ) , false , slice . end ( ) , false ) . iterator ( ) ; 
 + if ( prev ! = null & & prev . isOpen ( false ) ) 
 + result = Iterators . concat ( Iterators . singletonIterator ( new RangeTombstoneBoundMarker ( slice . start ( ) , prev . openDeletionTime ( false ) ) ) , result ) ; 
 + if ( next ! = null & & next . isClose ( false ) ) 
 + result = Iterators . concat ( result , Iterators . singletonIterator ( new RangeTombstoneBoundMarker ( slice . end ( ) , next . closeDeletionTime ( false ) ) ) ) ; 
 + return result ; 
 + } 
 + 
 + private Iterator < Slice > directed ( Slices slices , boolean reversed ) 
 + { 
 + return directed ( slices . iterator ( ) , reversed ) ; 
 + } 
 + 
 + private < T > Iterator < T > directed ( Iterator < T > iter , boolean reversed ) 
 + { 
 + if ( ! reversed ) 
 + return iter ; 
 + return invert ( iter ) ; 
 + } 
 + 
 + private < T > Stream < T > streamOf ( Iterator < T > iterator ) 
 + { 
 + Iterable < T > iterable = ( ) - > iterator ; 
 + return streamOf ( iterable ) ; 
 + } 
 + 
 + < T > Stream < T > streamOf ( Iterable < T > iterable ) 
 + { 
 + return StreamSupport . stream ( iterable . spliterator ( ) , false ) ; 
 + } 
 + 
 + private void assertIteratorsEqual ( Iterator < ? extends Clusterable > it1 , Iterator < ? extends Clusterable > it2 ) 
 + { 
 + Clusterable [ ] a1 = ( Clusterable [ ] ) Iterators . toArray ( it1 , Clusterable . class ) ; 
 + Clusterable [ ] a2 = ( Clusterable [ ] ) Iterators . toArray ( it2 , Clusterable . class ) ; 
 + if ( Arrays . equals ( a1 , a2 ) ) 
 + return ; 
 + String a1s = Stream . of ( a1 ) . map ( x - > " \ n " + ( x instanceof Unfiltered ? ( ( Unfiltered ) x ) . toString ( cfm ) : x . toString ( ) ) ) . collect ( Collectors . toList ( ) ) . toString ( ) ; 
 + String a2s = Stream . of ( a2 ) . map ( x - > " \ n " + ( x instanceof Unfiltered ? ( ( Unfiltered ) x ) . toString ( cfm ) : x . toString ( ) ) ) . collect ( Collectors . toList ( ) ) . toString ( ) ; 
 + assertArrayEquals ( " Arrays differ . Expected " + a1s + " was " + a2s , a1 , a2 ) ; 
 + } 
 + 
 + private Row getRow ( NavigableSet < Clusterable > sortedContent , Clustering cl ) 
 + { 
 + NavigableSet < Clusterable > nexts = sortedContent . tailSet ( cl , true ) ; 
 + if ( nexts . isEmpty ( ) ) 
 + return null ; 
 + Row row = nexts . first ( ) instanceof Row & & cfm . comparator . compare ( cl , nexts . first ( ) ) = = 0 ? ( Row ) nexts . first ( ) : null ; 
 + for ( Clusterable next : nexts ) 
 + if ( next instanceof RangeTombstoneMarker ) 
 + { 
 + RangeTombstoneMarker rt = ( RangeTombstoneMarker ) next ; 
 + if ( ! rt . isClose ( false ) ) 
 + return row ; 
 + DeletionTime delTime = rt . closeDeletionTime ( false ) ; 
 + return row = = null ? BTreeRow . emptyDeletedRow ( cl , Deletion . regular ( delTime ) ) : row . filter ( ColumnFilter . all ( cfm ) , delTime , true , cfm ) ; 
 + } 
 + return row ; 
 + } 
 + 
 + private void assertRowsEqual ( Row expected , Row actual ) 
 + { 
 + try 
 + { 
 + assertEquals ( expected = = null , actual = = null ) ; 
 + if ( expected = = null ) 
 + return ; 
 + assertEquals ( expected . clustering ( ) , actual . clustering ( ) ) ; 
 + assertEquals ( expected . deletion ( ) , actual . deletion ( ) ) ; 
 + assertArrayEquals ( Iterables . toArray ( expected . cells ( ) , Cell . class ) , Iterables . toArray ( expected . cells ( ) , Cell . class ) ) ; 
 + } catch ( Throwable t ) 
 + { 
 + throw new AssertionError ( String . format ( " Row comparison failed , expected % s got % s " , expected , actual ) , t ) ; 
 + } 
 + } 
 + 
 + private static < T > T get ( NavigableSet < T > sortedContent , Predicate < T > test ) 
 + { 
 + return sortedContent . stream ( ) . filter ( test ) . findFirst ( ) . orElse ( null ) ; 
 + } 
 + 
 + @ Test 
 + public void testEmpty ( ) 
 + { 
 + test ( ( ) - > Collections . < Row > emptyList ( ) , null ) ; 
 + } 
 + 
 + @ Test 
 + public void testStaticOnly ( ) 
 + { 
 + test ( ( ) - > Collections . < Row > emptyList ( ) , makeStaticRow ( ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testRows ( ) 
 + { 
 + test ( this : : generateRows , null ) ; 
 + } 
 + 
 + @ Test 
 + public void testRowsWithStatic ( ) 
 + { 
 + test ( this : : generateRows , makeStaticRow ( ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testMarkersOnly ( ) 
 + { 
 + test ( this : : generateMarkersOnly , null ) ; 
 + } 
 + 
 + @ Test 
 + public void testMarkersWithStatic ( ) 
 + { 
 + test ( this : : generateMarkersOnly , makeStaticRow ( ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testUnfiltereds ( ) 
 + { 
 + test ( this : : generateUnfiltereds , makeStaticRow ( ) ) ; 
 + } 
 + 
 + }

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / ColumnFamily . java b / src / java / org / apache / cassandra / db / ColumnFamily . java 
 index c3bddd4 . . cb87833 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnFamily . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnFamily . java 
 @ @ - 35 , 7 + 35 , 6 @ @ import org . apache . cassandra . utils . FBUtilities ; 
 import org . apache . cassandra . io . ICompactSerializer2 ; 
 import org . apache . cassandra . db . filter . QueryPath ; 
 import org . apache . cassandra . db . marshal . AbstractType ; 
 - import org . apache . cassandra . db . marshal . MarshalException ; 
 
 
 public final class ColumnFamily implements IColumnContainer 
 @ @ - 121 , 12 + 120 , 13 @ @ public final class ColumnFamily implements IColumnContainer 
 * We need to go through each column 
 * in the column family and resolve it before adding 
 * / 
 - void addColumns ( ColumnFamily cf ) 
 + public void addAll ( ColumnFamily cf ) 
 { 
 for ( IColumn column : cf . getSortedColumns ( ) ) 
 { 
 addColumn ( column ) ; 
 } 
 + delete ( cf ) ; 
 } 
 
 public ICompactSerializer2 < IColumn > getColumnSerializer ( ) 
 @ @ - 415 , 8 + 415 , 7 @ @ public final class ColumnFamily implements IColumnContainer 
 for ( ColumnFamily cf2 : columnFamilies ) 
 { 
 assert cf . name ( ) . equals ( cf2 . name ( ) ) ; 
 - cf . addColumns ( cf2 ) ; 
 - cf . delete ( cf2 ) ; 
 + cf . addAll ( cf2 ) ; 
 } 
 return cf ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 index 001c644 . . 96bb18b 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 @ @ - 560 , 26 + 560 , 6 @ @ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 CompactionManager . instance ( ) . submit ( this ) ; 
 } 
 
 - private PriorityQueue < FileStruct > initializePriorityQueue ( Collection < SSTableReader > sstables , List < Range > ranges ) throws IOException 
 - { 
 - PriorityQueue < FileStruct > pq = new PriorityQueue < FileStruct > ( ) ; 
 - if ( sstables . size ( ) > 1 | | ( ranges ! = null & & sstables . size ( ) > 0 ) ) 
 - { 
 - FileStruct fs = null ; 
 - for ( SSTableReader sstable : sstables ) 
 - { 
 - fs = sstable . getFileStruct ( ) ; 
 - fs . advance ( true ) ; 
 - if ( fs . isExhausted ( ) ) 
 - { 
 - continue ; 
 - } 
 - pq . add ( fs ) ; 
 - } 
 - } 
 - return pq ; 
 - } 
 - 
 / * 
 * Group files of similar size into buckets . 
 * / 
 @ @ - 766 , 150 + 746 , 67 @ @ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 * / 
 List < SSTableReader > doFileAntiCompaction ( Collection < SSTableReader > sstables , List < Range > ranges , EndPoint target ) throws IOException 
 { 
 - List < SSTableReader > results = new ArrayList < SSTableReader > ( ) ; 
 - long startTime = System . currentTimeMillis ( ) ; 
 - long totalBytesRead = 0 ; 
 - long totalBytesWritten = 0 ; 
 - long totalkeysRead = 0 ; 
 - long totalkeysWritten = 0 ; 
 - String rangeFileLocation ; 
 - String mergedFileName ; 
 + logger _ . info ( " AntiCompacting [ " + StringUtils . join ( sstables , " , " ) + " ] " ) ; 
 / / Calculate the expected compacted filesize 
 - long expectedRangeFileSize = getExpectedCompactedFileSize ( sstables ) ; 
 - / * in the worst case a node will be giving out half of its data so we take a chance * / 
 - expectedRangeFileSize = expectedRangeFileSize / 2 ; 
 - rangeFileLocation = DatabaseDescriptor . getDataFileLocationForTable ( table _ , expectedRangeFileSize ) ; 
 - / / If the compaction file path is null that means we have no space left for this compaction . 
 - if ( rangeFileLocation = = null ) 
 - { 
 - logger _ . error ( " Total bytes to be written for range compaction . . . " 
 - + expectedRangeFileSize + " is greater than the safe limit of the disk space available . " ) ; 
 - return results ; 
 - } 
 - PriorityQueue < FileStruct > pq = initializePriorityQueue ( sstables , ranges ) ; 
 - if ( pq . isEmpty ( ) ) 
 + long expectedRangeFileSize = getExpectedCompactedFileSize ( sstables ) / 2 ; 
 + String compactionFileLocation = DatabaseDescriptor . getDataFileLocationForTable ( table _ , expectedRangeFileSize ) ; 
 + if ( compactionFileLocation = = null ) 
 { 
 - return results ; 
 + throw new UnsupportedOperationException ( " disk full " ) ; 
 } 
 + List < SSTableReader > results = new ArrayList < SSTableReader > ( ) ; 
 
 - mergedFileName = getTempSSTableFileName ( ) ; 
 - SSTableWriter rangeWriter = null ; 
 - String lastkey = null ; 
 - List < FileStruct > lfs = new ArrayList < FileStruct > ( ) ; 
 - DataOutputBuffer bufOut = new DataOutputBuffer ( ) ; 
 - int expectedBloomFilterSize = SSTableReader . getApproximateKeyCount ( sstables ) ; 
 - expectedBloomFilterSize = ( expectedBloomFilterSize > 0 ) ? expectedBloomFilterSize : SSTableReader . indexInterval ( ) ; 
 + long startTime = System . currentTimeMillis ( ) ; 
 + long totalkeysWritten = 0 ; 
 + 
 + int expectedBloomFilterSize = Math . max ( SSTableReader . indexInterval ( ) , SSTableReader . getApproximateKeyCount ( sstables ) / 2 ) ; 
 if ( logger _ . isDebugEnabled ( ) ) 
 logger _ . debug ( " Expected bloom filter size : " + expectedBloomFilterSize ) ; 
 - List < ColumnFamily > columnFamilies = new ArrayList < ColumnFamily > ( ) ; 
 
 - while ( pq . size ( ) > 0 | | lfs . size ( ) > 0 ) 
 + SSTableWriter writer = null ; 
 + CompactionIterator ci = new CompactionIterator ( sstables ) ; 
 + 
 + try 
 { 
 - FileStruct fs = null ; 
 - if ( pq . size ( ) > 0 ) 
 + if ( ! ci . hasNext ( ) ) 
 { 
 - fs = pq . poll ( ) ; 
 + logger _ . warn ( " Nothing to compact ( all files empty or corrupt ) . This should not happen . " ) ; 
 + return results ; 
 } 
 - if ( fs ! = null 
 - & & ( lastkey = = null | | lastkey . equals ( fs . getKey ( ) ) ) ) 
 - { 
 - / / The keys are the same so we need to add this to the 
 - / / ldfs list 
 - lastkey = fs . getKey ( ) ; 
 - lfs . add ( fs ) ; 
 - } 
 - else 
 + 
 + while ( ci . hasNext ( ) ) 
 { 
 - Collections . sort ( lfs , new FileStructComparator ( ) ) ; 
 - ColumnFamily columnFamily ; 
 - bufOut . reset ( ) ; 
 - if ( lfs . size ( ) > 1 ) 
 + CompactionIterator . CompactedRow row = ci . next ( ) ; 
 + if ( Range . isTokenInRanges ( StorageService . getPartitioner ( ) . getToken ( row . key ) , ranges ) ) 
 { 
 - for ( FileStruct filestruct : lfs ) 
 - { 
 - / / We want to add only 2 and resolve them right there in order to save on memory footprint 
 - if ( columnFamilies . size ( ) > 1 ) 
 - { 
 - / / Now merge the 2 column families 
 - merge ( columnFamilies ) ; 
 - } 
 - / / deserialize into column families 
 - columnFamilies . add ( filestruct . getColumnFamily ( ) ) ; 
 - } 
 - / / Now after merging all crap append to the sstable 
 - columnFamily = resolveAndRemoveDeleted ( columnFamilies ) ; 
 - columnFamilies . clear ( ) ; 
 - if ( columnFamily ! = null ) 
 - { 
 - ColumnFamily . serializer ( ) . serializeWithIndexes ( columnFamily , bufOut ) ; 
 - } 
 - } 
 - else 
 - { 
 - / / TODO deserializing only to reserialize is dumb 
 - FileStruct filestruct = lfs . get ( 0 ) ; 
 - ColumnFamily . serializer ( ) . serializeWithIndexes ( filestruct . getColumnFamily ( ) , bufOut ) ; 
 - } 
 - if ( Range . isTokenInRanges ( StorageService . getPartitioner ( ) . getToken ( lastkey ) , ranges ) ) 
 - { 
 - if ( rangeWriter = = null ) 
 + if ( writer = = null ) 
 { 
 if ( target ! = null ) 
 { 
 - rangeFileLocation = rangeFileLocation + File . separator + " bootstrap " ; 
 + compactionFileLocation = compactionFileLocation + File . separator + " bootstrap " ; 
 } 
 - FileUtils . createDirectory ( rangeFileLocation ) ; 
 - String fname = new File ( rangeFileLocation , mergedFileName ) . getAbsolutePath ( ) ; 
 - rangeWriter = new SSTableWriter ( fname , expectedBloomFilterSize , StorageService . getPartitioner ( ) ) ; 
 + FileUtils . createDirectory ( compactionFileLocation ) ; 
 + String newFilename = new File ( compactionFileLocation , getTempSSTableFileName ( ) ) . getAbsolutePath ( ) ; 
 + writer = new SSTableWriter ( newFilename , expectedBloomFilterSize , StorageService . getPartitioner ( ) ) ; 
 } 
 - rangeWriter . append ( lastkey , bufOut ) ; 
 - } 
 - totalkeysWritten + + ; 
 - for ( FileStruct filestruct : lfs ) 
 - { 
 - filestruct . advance ( true ) ; 
 - if ( filestruct . isExhausted ( ) ) 
 - { 
 - continue ; 
 - } 
 - / * keep on looping until we find a key in the range * / 
 - while ( ! Range . isTokenInRanges ( StorageService . getPartitioner ( ) . getToken ( filestruct . getKey ( ) ) , ranges ) ) 
 - { 
 - filestruct . advance ( true ) ; 
 - if ( filestruct . isExhausted ( ) ) 
 - { 
 - break ; 
 - } 
 - } 
 - if ( ! filestruct . isExhausted ( ) ) 
 - { 
 - pq . add ( filestruct ) ; 
 - } 
 - totalkeysRead + + ; 
 - } 
 - lfs . clear ( ) ; 
 - lastkey = null ; 
 - if ( fs ! = null ) 
 - { 
 - / / Add back the fs since we processed the rest of 
 - / / filestructs 
 - pq . add ( fs ) ; 
 + writer . append ( row . key , row . buffer ) ; 
 + totalkeysWritten + + ; 
 } 
 } 
 } 
 - 
 - if ( rangeWriter ! = null ) 
 + finally 
 { 
 - results . add ( rangeWriter . closeAndOpenReader ( ) ) ; 
 + ci . close ( ) ; 
 } 
 
 - if ( logger _ . isDebugEnabled ( ) ) 
 + if ( writer ! = null ) 
 { 
 - logger _ . debug ( " Total time taken for range split . . . " + ( System . currentTimeMillis ( ) - startTime ) ) ; 
 - logger _ . debug ( " Total bytes Read for range split . . . " + totalBytesRead ) ; 
 - logger _ . debug ( " Total bytes written for range split . . . " 
 - + totalBytesWritten + " Total keys read . . . " + totalkeysRead ) ; 
 + results . add ( writer . closeAndOpenReader ( ) ) ; 
 + String format = " AntiCompacted to % s . % d / % d bytes for % d keys . Time : % dms . " ; 
 + long dTime = System . currentTimeMillis ( ) - startTime ; 
 + logger _ . info ( String . format ( format , writer . getFilename ( ) , getTotalBytes ( sstables ) , results . get ( 0 ) . length ( ) , totalkeysWritten , dTime ) ) ; 
 } 
 + 
 return results ; 
 } 
 
 @ @ - 938 , 111 + 835 , 59 @ @ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 } 
 
 long startTime = System . currentTimeMillis ( ) ; 
 - long totalBytesRead = 0 ; 
 - long totalkeysRead = 0 ; 
 long totalkeysWritten = 0 ; 
 - PriorityQueue < FileStruct > pq = initializePriorityQueue ( sstables , null ) ; 
 - 
 - if ( pq . isEmpty ( ) ) 
 - { 
 - logger _ . warn ( " Nothing to compact ( all files empty or corrupt ) . This should not happen . " ) ; 
 - / / TODO clean out bad files , if any 
 - return 0 ; 
 - } 
 
 - int expectedBloomFilterSize = SSTableReader . getApproximateKeyCount ( sstables ) ; 
 - if ( expectedBloomFilterSize < 0 ) 
 - expectedBloomFilterSize = SSTableReader . indexInterval ( ) ; 
 - String newFilename = new File ( compactionFileLocation , getTempSSTableFileName ( ) ) . getAbsolutePath ( ) ; 
 - SSTableWriter writer = new SSTableWriter ( newFilename , expectedBloomFilterSize , StorageService . getPartitioner ( ) ) ; 
 - SSTableReader ssTable = null ; 
 - String lastkey = null ; 
 - List < FileStruct > lfs = new ArrayList < FileStruct > ( ) ; 
 - DataOutputBuffer bufOut = new DataOutputBuffer ( ) ; 
 + int expectedBloomFilterSize = Math . max ( SSTableReader . indexInterval ( ) , SSTableReader . getApproximateKeyCount ( sstables ) ) ; 
 if ( logger _ . isDebugEnabled ( ) ) 
 logger _ . debug ( " Expected bloom filter size : " + expectedBloomFilterSize ) ; 
 - List < ColumnFamily > columnFamilies = new ArrayList < ColumnFamily > ( ) ; 
 
 - while ( pq . size ( ) > 0 | | lfs . size ( ) > 0 ) 
 + SSTableWriter writer ; 
 + CompactionIterator ci = new CompactionIterator ( sstables ) ; 
 + 
 + try 
 { 
 - FileStruct fs = null ; 
 - if ( pq . size ( ) > 0 ) 
 + if ( ! ci . hasNext ( ) ) 
 { 
 - fs = pq . poll ( ) ; 
 + logger _ . warn ( " Nothing to compact ( all files empty or corrupt ) . This should not happen . " ) ; 
 + return 0 ; 
 } 
 - if ( fs ! = null 
 - & & ( lastkey = = null | | lastkey . equals ( fs . getKey ( ) ) ) ) 
 - { 
 - / / The keys are the same so we need to add this to the 
 - / / ldfs list 
 - lastkey = fs . getKey ( ) ; 
 - lfs . add ( fs ) ; 
 - } 
 - else 
 - { 
 - Collections . sort ( lfs , new FileStructComparator ( ) ) ; 
 - ColumnFamily columnFamily ; 
 - bufOut . reset ( ) ; 
 - if ( lfs . size ( ) > 1 ) 
 - { 
 - for ( FileStruct filestruct : lfs ) 
 - { 
 - / / We want to add only 2 and resolve them right there in order to save on memory footprint 
 - if ( columnFamilies . size ( ) > 1 ) 
 - { 
 - merge ( columnFamilies ) ; 
 - } 
 - / / deserialize into column families 
 - columnFamilies . add ( filestruct . getColumnFamily ( ) ) ; 
 - } 
 - / / Now after merging all crap append to the sstable 
 - columnFamily = resolveAndRemoveDeleted ( columnFamilies ) ; 
 - columnFamilies . clear ( ) ; 
 - if ( columnFamily ! = null ) 
 - { 
 - ColumnFamily . serializer ( ) . serializeWithIndexes ( columnFamily , bufOut ) ; 
 - } 
 - } 
 - else 
 - { 
 - / / TODO deserializing only to reserialize is dumb 
 - FileStruct filestruct = lfs . get ( 0 ) ; 
 - ColumnFamily . serializer ( ) . serializeWithIndexes ( filestruct . getColumnFamily ( ) , bufOut ) ; 
 - } 
 
 - writer . append ( lastkey , bufOut ) ; 
 - totalkeysWritten + + ; 
 + String newFilename = new File ( compactionFileLocation , getTempSSTableFileName ( ) ) . getAbsolutePath ( ) ; 
 + writer = new SSTableWriter ( newFilename , expectedBloomFilterSize , StorageService . getPartitioner ( ) ) ; 
 
 - for ( FileStruct filestruct : lfs ) 
 - { 
 - filestruct . advance ( true ) ; 
 - if ( filestruct . isExhausted ( ) ) 
 - { 
 - continue ; 
 - } 
 - pq . add ( filestruct ) ; 
 - totalkeysRead + + ; 
 - } 
 - lfs . clear ( ) ; 
 - lastkey = null ; 
 - if ( fs ! = null ) 
 - { 
 - / * Add back the fs since we processed the rest of filestructs * / 
 - pq . add ( fs ) ; 
 - } 
 + while ( ci . hasNext ( ) ) 
 + { 
 + CompactionIterator . CompactedRow row = ci . next ( ) ; 
 + writer . append ( row . key , row . buffer ) ; 
 + totalkeysWritten + + ; 
 } 
 } 
 - ssTable = writer . closeAndOpenReader ( ) ; 
 + finally 
 + { 
 + ci . close ( ) ; 
 + } 
 + 
 + SSTableReader ssTable = writer . closeAndOpenReader ( ) ; 
 ssTables _ . add ( ssTable ) ; 
 ssTables _ . markCompacted ( sstables ) ; 
 CompactionManager . instance ( ) . submit ( ColumnFamilyStore . this ) ; 
 
 - String format = " Compacted to % s . % d / % d bytes for % d / % d keys read / written . Time : % dms . " ; 
 + String format = " Compacted to % s . % d / % d bytes for % d keys . Time : % dms . " ; 
 long dTime = System . currentTimeMillis ( ) - startTime ; 
 - logger _ . info ( String . format ( format , writer . getFilename ( ) , totalBytesRead , ssTable . length ( ) , totalkeysRead , totalkeysWritten , dTime ) ) ; 
 + logger _ . info ( String . format ( format , writer . getFilename ( ) , getTotalBytes ( sstables ) , ssTable . length ( ) , totalkeysWritten , dTime ) ) ; 
 return sstables . size ( ) ; 
 } 
 
 + private long getTotalBytes ( Iterable < SSTableReader > sstables ) 
 + { 
 + long sum = 0 ; 
 + for ( SSTableReader sstable : sstables ) 
 + { 
 + sum + = sstable . length ( ) ; 
 + } 
 + return sum ; 
 + } 
 + 
 public static List < Memtable > getUnflushedMemtables ( String cfName ) 
 { 
 return new ArrayList < Memtable > ( getMemtablesPendingFlushNotNull ( cfName ) ) ; 
 @ @ - 1341 , 23 + 1186 , 24 @ @ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 / / sstables 
 for ( SSTableReader sstable : ssTables _ ) 
 { 
 - final SSTableScanner fs = sstable . getScanner ( ) ; 
 - fs . seekTo ( startWith ) ; 
 - iterators . add ( new Iterator < String > ( ) 
 + final SSTableScanner scanner = sstable . getScanner ( ) ; 
 + scanner . seekTo ( startWith ) ; 
 + Iterator < String > iter = new Iterator < String > ( ) 
 { 
 public boolean hasNext ( ) 
 { 
 - return fs . hasNext ( ) ; 
 + return scanner . hasNext ( ) ; 
 } 
 public String next ( ) 
 { 
 - return fs . next ( ) . getKey ( ) ; 
 + return scanner . next ( ) . getKey ( ) ; 
 } 
 public void remove ( ) 
 { 
 throw new UnsupportedOperationException ( ) ; 
 } 
 - } ) ; 
 + } ; 
 + iterators . add ( iter ) ; 
 } 
 
 Iterator < String > collated = IteratorUtils . collatedIterator ( comparator , iterators ) ; 
 diff - - git a / src / java / org / apache / cassandra / db / FileStructComparator . java b / src / java / org / apache / cassandra / db / FileStructComparator . java 
 deleted file mode 100644 
 index e81a992 . . 0000000 
 - - - a / src / java / org / apache / cassandra / db / FileStructComparator . java 
 + + + / dev / null 
 @ @ - 1 , 31 + 0 , 0 @ @ 
 - / * 
 - * Licensed to the Apache Software Foundation ( ASF ) under one 
 - * or more contributor license agreements . See the NOTICE file 
 - * distributed with this work for additional information 
 - * regarding copyright ownership . The ASF licenses this file 
 - * to you under the Apache License , Version 2 . 0 ( the 
 - * " License " ) ; you may not use this file except in compliance 
 - * with the License . You may obtain a copy of the License at 
 - * 
 - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 - * 
 - * Unless required by applicable law or agreed to in writing , 
 - * software distributed under the License is distributed on an 
 - * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY 
 - * KIND , either express or implied . See the License for the 
 - * specific language governing permissions and limitations 
 - * under the License . 
 - * / 
 - package org . apache . cassandra . db ; 
 - 
 - import java . util . Comparator ; 
 - 
 - import org . apache . cassandra . io . FileStruct ; 
 - 
 - class FileStructComparator implements Comparator < FileStruct > 
 - { 
 - public int compare ( FileStruct f , FileStruct f2 ) 
 - { 
 - return f . getFileName ( ) . compareTo ( f2 . getFileName ( ) ) ; 
 - } 
 - } 
 \ No newline at end of file 
 diff - - git a / src / java / org / apache / cassandra / db / Memtable . java b / src / java / org / apache / cassandra / db / Memtable . java 
 index d88e004 . . 696ae5a 100644 
 - - - a / src / java / org / apache / cassandra / db / Memtable . java 
 + + + b / src / java / org / apache / cassandra / db / Memtable . java 
 @ @ - 153 , 7 + 153 , 7 @ @ public class Memtable implements Comparable < Memtable > 
 { 
 int oldSize = oldCf . size ( ) ; 
 int oldObjectCount = oldCf . getColumnCount ( ) ; 
 - oldCf . addColumns ( columnFamily ) ; 
 + oldCf . addAll ( columnFamily ) ; 
 int newSize = oldCf . size ( ) ; 
 int newObjectCount = oldCf . getColumnCount ( ) ; 
 resolveSize ( oldSize , newSize ) ; 
 diff - - git a / src / java / org / apache / cassandra / io / CompactionIterator . java b / src / java / org / apache / cassandra / io / CompactionIterator . java 
 new file mode 100644 
 index 0000000 . . b65e132 
 - - - / dev / null 
 + + + b / src / java / org / apache / cassandra / io / CompactionIterator . java 
 @ @ - 0 , 0 + 1 , 113 @ @ 
 + package org . apache . cassandra . io ; 
 + 
 + import java . io . Closeable ; 
 + import java . io . IOException ; 
 + import java . util . List ; 
 + import java . util . ArrayList ; 
 + import java . util . Comparator ; 
 + 
 + import org . apache . commons . collections . iterators . CollatingIterator ; 
 + 
 + import org . apache . cassandra . utils . ReducingIterator ; 
 + import org . apache . cassandra . db . ColumnFamily ; 
 + 
 + public class CompactionIterator extends ReducingIterator < IteratingRow , CompactionIterator . CompactedRow > implements Closeable 
 + { 
 + private final List < IteratingRow > rows = new ArrayList < IteratingRow > ( ) ; 
 + 
 + @ SuppressWarnings ( " unchecked " ) 
 + public CompactionIterator ( Iterable < SSTableReader > sstables ) throws IOException 
 + { 
 + super ( getCollatingIterator ( sstables ) ) ; 
 + } 
 + 
 + @ SuppressWarnings ( " unchecked " ) 
 + private static CollatingIterator getCollatingIterator ( Iterable < SSTableReader > sstables ) throws IOException 
 + { 
 + / / CollatingIterator has a bug that causes NPE when you try to use default comparator . : ( 
 + CollatingIterator iter = new CollatingIterator ( new Comparator ( ) 
 + { 
 + public int compare ( Object o1 , Object o2 ) 
 + { 
 + return ( ( Comparable ) o1 ) . compareTo ( o2 ) ; 
 + } 
 + } ) ; 
 + for ( SSTableReader sstable : sstables ) 
 + { 
 + iter . addIterator ( sstable . getScanner ( ) ) ; 
 + } 
 + return iter ; 
 + } 
 + 
 + @ Override 
 + protected boolean isEqual ( IteratingRow o1 , IteratingRow o2 ) 
 + { 
 + return o1 . getKey ( ) . equals ( o2 . getKey ( ) ) ; 
 + } 
 + 
 + public void reduce ( IteratingRow current ) 
 + { 
 + rows . add ( current ) ; 
 + } 
 + 
 + protected CompactedRow getReduced ( ) 
 + { 
 + try 
 + { 
 + return getReducedRaw ( ) ; 
 + } 
 + catch ( IOException e ) 
 + { 
 + throw new RuntimeException ( e ) ; 
 + } 
 + } 
 + 
 + protected CompactedRow getReducedRaw ( ) throws IOException 
 + { 
 + DataOutputBuffer buffer = new DataOutputBuffer ( ) ; 
 + String key = rows . get ( 0 ) . getKey ( ) ; 
 + if ( rows . size ( ) > 1 ) 
 + { 
 + ColumnFamily cf = null ; 
 + for ( IteratingRow row : rows ) 
 + { 
 + if ( cf = = null ) 
 + { 
 + cf = row . getColumnFamily ( ) ; 
 + } 
 + else 
 + { 
 + cf . addAll ( row . getColumnFamily ( ) ) ; 
 + } 
 + } 
 + ColumnFamily . serializer ( ) . serializeWithIndexes ( cf , buffer ) ; 
 + } 
 + else 
 + { 
 + assert rows . size ( ) = = 1 ; 
 + rows . get ( 0 ) . echoData ( buffer ) ; 
 + } 
 + rows . clear ( ) ; 
 + return new CompactedRow ( key , buffer ) ; 
 + } 
 + 
 + public void close ( ) throws IOException 
 + { 
 + for ( Object o : ( ( CollatingIterator ) source ) . getIterators ( ) ) 
 + { 
 + ( ( SSTableScanner ) o ) . close ( ) ; 
 + } 
 + } 
 + 
 + public static class CompactedRow 
 + { 
 + public final String key ; 
 + public final DataOutputBuffer buffer ; 
 + 
 + public CompactedRow ( String key , DataOutputBuffer buffer ) 
 + { 
 + this . key = key ; 
 + this . buffer = buffer ; 
 + } 
 + } 
 + } 
 diff - - git a / src / java / org / apache / cassandra / io / FileStruct . java b / src / java / org / apache / cassandra / io / FileStruct . java 
 deleted file mode 100644 
 index b561239 . . 0000000 
 - - - a / src / java / org / apache / cassandra / io / FileStruct . java 
 + + + / dev / null 
 @ @ - 1 , 195 + 0 , 0 @ @ 
 - / * * 
 - * Licensed to the Apache Software Foundation ( ASF ) under one 
 - * or more contributor license agreements . See the NOTICE file 
 - * distributed with this work for additional information 
 - * regarding copyright ownership . The ASF licenses this file 
 - * to you under the Apache License , Version 2 . 0 ( the 
 - * " License " ) ; you may not use this file except in compliance 
 - * with the License . You may obtain a copy of the License at 
 - * 
 - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 - * 
 - * Unless required by applicable law or agreed to in writing , software 
 - * distributed under the License is distributed on an " AS IS " BASIS , 
 - * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 - * See the License for the specific language governing permissions and 
 - * limitations under the License . 
 - * / 
 - 
 - package org . apache . cassandra . io ; 
 - 
 - import java . io . IOException ; 
 - import java . io . File ; 
 - import java . util . Iterator ; 
 - 
 - import org . apache . cassandra . db . IColumn ; 
 - import org . apache . cassandra . db . ColumnFamily ; 
 - import org . apache . cassandra . config . DatabaseDescriptor ; 
 - 
 - import org . apache . log4j . Logger ; 
 - import com . google . common . collect . AbstractIterator ; 
 - 
 - 
 - public class FileStruct implements Comparable < FileStruct > , Iterator < String > 
 - { 
 - private static Logger logger = Logger . getLogger ( FileStruct . class ) ; 
 - 
 - private IteratingRow row ; 
 - private boolean exhausted = false ; 
 - private BufferedRandomAccessFile file ; 
 - private SSTableReader sstable ; 
 - private FileStructIterator iterator ; 
 - 
 - FileStruct ( SSTableReader sstable ) throws IOException 
 - { 
 - / / TODO this is used for both compactions and key ranges . the buffer sizes we want 
 - / / to use for these ops are very different . here we are leaning towards the key - range 
 - / / use case since that is more common . What we really want is to split those 
 - / / two uses of this class up . 
 - this . file = new BufferedRandomAccessFile ( sstable . getFilename ( ) , " r " , 256 * 1024 ) ; 
 - this . sstable = sstable ; 
 - } 
 - 
 - public String getFileName ( ) 
 - { 
 - return file . getPath ( ) ; 
 - } 
 - 
 - public void close ( ) throws IOException 
 - { 
 - file . close ( ) ; 
 - } 
 - 
 - public boolean isExhausted ( ) 
 - { 
 - return exhausted ; 
 - } 
 - 
 - public String getKey ( ) 
 - { 
 - return row . getKey ( ) ; 
 - } 
 - 
 - public ColumnFamily getColumnFamily ( ) 
 - { 
 - return row . getEmptyColumnFamily ( ) ; 
 - } 
 - 
 - public int compareTo ( FileStruct f ) 
 - { 
 - return sstable . getPartitioner ( ) . getDecoratedKeyComparator ( ) . compare ( getKey ( ) , f . getKey ( ) ) ; 
 - } 
 - 
 - public void seekTo ( String seekKey ) 
 - { 
 - try 
 - { 
 - long position = sstable . getNearestPosition ( seekKey ) ; 
 - if ( position < 0 ) 
 - { 
 - exhausted = true ; 
 - return ; 
 - } 
 - file . seek ( position ) ; 
 - advance ( false ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - throw new RuntimeException ( " corrupt sstable " , e ) ; 
 - } 
 - } 
 - 
 - / * 
 - * Read the next key from the data file . 
 - * Caller must check isExhausted after each call to see if further 
 - * reads are valid . 
 - * Do not mix with calls to the iterator interface ( next / hasnext ) . 
 - * @ deprecated - - prefer the iterator interface . 
 - * / 
 - public void advance ( boolean materialize ) throws IOException 
 - { 
 - / / TODO r / m materialize option - - use iterableness ! 
 - if ( exhausted ) 
 - { 
 - throw new IndexOutOfBoundsException ( ) ; 
 - } 
 - 
 - if ( file . isEOF ( ) ) 
 - { 
 - file . close ( ) ; 
 - exhausted = true ; 
 - return ; 
 - } 
 - 
 - row = new IteratingRow ( file , sstable ) ; 
 - if ( materialize ) 
 - { 
 - while ( row . hasNext ( ) ) 
 - { 
 - IColumn column = row . next ( ) ; 
 - row . getEmptyColumnFamily ( ) . addColumn ( column ) ; 
 - } 
 - } 
 - else 
 - { 
 - row . skipRemaining ( ) ; 
 - } 
 - } 
 - 
 - public boolean hasNext ( ) 
 - { 
 - if ( iterator = = null ) 
 - iterator = new FileStructIterator ( ) ; 
 - return iterator . hasNext ( ) ; 
 - } 
 - 
 - / * * do not mix with manual calls to advance ( ) . * / 
 - public String next ( ) 
 - { 
 - if ( iterator = = null ) 
 - iterator = new FileStructIterator ( ) ; 
 - return iterator . next ( ) ; 
 - } 
 - 
 - public void remove ( ) 
 - { 
 - throw new UnsupportedOperationException ( ) ; 
 - } 
 - 
 - private class FileStructIterator extends AbstractIterator < String > 
 - { 
 - public FileStructIterator ( ) 
 - { 
 - if ( row = = null ) 
 - { 
 - if ( ! isExhausted ( ) ) 
 - { 
 - forward ( ) ; 
 - } 
 - } 
 - } 
 - 
 - private void forward ( ) 
 - { 
 - try 
 - { 
 - advance ( false ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - } 
 - 
 - protected String computeNext ( ) 
 - { 
 - if ( isExhausted ( ) ) 
 - { 
 - return endOfData ( ) ; 
 - } 
 - String oldKey = getKey ( ) ; 
 - forward ( ) ; 
 - return oldKey ; 
 - } 
 - } 
 - } 
 diff - - git a / src / java / org / apache / cassandra / io / IteratingRow . java b / src / java / org / apache / cassandra / io / IteratingRow . java 
 index 5ace95f . . 628fe50 100644 
 - - - a / src / java / org / apache / cassandra / io / IteratingRow . java 
 + + + b / src / java / org / apache / cassandra / io / IteratingRow . java 
 @ @ - 37 , 7 + 37 , 6 @ @ public class IteratingRow extends AbstractIterator < IColumn > implements Comparabl 
 { 
 private final String key ; 
 private final long finishedAt ; 
 - private final ColumnFamily emptyColumnFamily ; 
 private final BufferedRandomAccessFile file ; 
 private SSTableReader sstable ; 
 private long dataStart ; 
 @ @ - 51 , 10 + 50 , 6 @ @ public class IteratingRow extends AbstractIterator < IColumn > implements Comparabl 
 int dataSize = file . readInt ( ) ; 
 dataStart = file . getFilePointer ( ) ; 
 finishedAt = dataStart + dataSize ; 
 - / / legacy stuff to support FileStruct : 
 - IndexHelper . skipBloomFilterAndIndex ( file ) ; 
 - emptyColumnFamily = ColumnFamily . serializer ( ) . deserializeFromSSTableNoColumns ( sstable . makeColumnFamily ( ) , file ) ; 
 - file . readInt ( ) ; 
 } 
 
 public String getKey ( ) 
 @ @ - 62 , 11 + 57 , 6 @ @ public class IteratingRow extends AbstractIterator < IColumn > implements Comparabl 
 return key ; 
 } 
 
 - public ColumnFamily getEmptyColumnFamily ( ) 
 - { 
 - return emptyColumnFamily ; 
 - } 
 - 
 public void echoData ( DataOutput out ) throws IOException 
 { 
 file . seek ( dataStart ) ; 
 diff - - git a / src / java / org / apache / cassandra / io / SSTableReader . java b / src / java / org / apache / cassandra / io / SSTableReader . java 
 index fc94fca . . 81a71fd 100644 
 - - - a / src / java / org / apache / cassandra / io / SSTableReader . java 
 + + + b / src / java / org / apache / cassandra / io / SSTableReader . java 
 @ @ - 333 , 11 + 333 , 6 @ @ public class SSTableReader extends SSTable implements Comparable < SSTableReader > 
 return partitioner ; 
 } 
 
 - public FileStruct getFileStruct ( ) throws IOException 
 - { 
 - return new FileStruct ( this ) ; 
 - } 
 - 
 public SSTableScanner getScanner ( ) throws IOException 
 { 
 return new SSTableScanner ( this ) ; 
 diff - - git a / test / unit / org / apache / cassandra / db / ColumnFamilyTest . java b / test / unit / org / apache / cassandra / db / ColumnFamilyTest . java 
 index 40e5889 . . 4219ff9 100644 
 - - - a / test / unit / org / apache / cassandra / db / ColumnFamilyTest . java 
 + + + b / test / unit / org / apache / cassandra / db / ColumnFamilyTest . java 
 @ @ - 20 , 8 + 20 , 6 @ @ package org . apache . cassandra . db ; 
 
 import java . io . IOException ; 
 import java . util . Arrays ; 
 - import java . util . HashSet ; 
 - import java . util . Random ; 
 import java . util . TreeMap ; 
 
 import org . junit . Test ; 
 @ @ - 125 , 8 + 123 , 8 @ @ public class ColumnFamilyTest 
 cf _ old . addColumn ( QueryPath . column ( " col2 " . getBytes ( ) ) , val2 , 1 ) ; 
 cf _ old . addColumn ( QueryPath . column ( " col3 " . getBytes ( ) ) , val2 , 2 ) ; 
 
 - cf _ result . addColumns ( cf _ new ) ; 
 - cf _ result . addColumns ( cf _ old ) ; 
 + cf _ result . addAll ( cf _ new ) ; 
 + cf _ result . addAll ( cf _ old ) ; 
 
 assert 3 = = cf _ result . getColumnCount ( ) : " Count is " + cf _ new . getColumnCount ( ) ; 
 / / addcolumns will only add if timestamp > = old timestamp
