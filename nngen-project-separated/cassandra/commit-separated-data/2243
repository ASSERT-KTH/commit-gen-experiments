BLEU SCORE: 0.026341919627252283

TEST MSG: Remove CPRR / CPIF .
GENERATED MSG: Revert incomplete move of sstable [ ex / im ] port code

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 5033722 . . b91711d 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 5 + 1 , 12 @ @ <nl> 2 . 1 . 0 - final <nl> + * cqlsh DESC CLUSTER fails retrieving ring information ( CASSANDRA - 7687 ) <nl> + * Fix binding null values inside UDT ( CASSANDRA - 7685 ) <nl> + * Fix UDT field selection with empty fields ( CASSANDRA - 7670 ) <nl> + * Bogus deserialization of static cells from sstable ( CASSANDRA - 7684 ) <nl> Merged from 2 . 0 : <nl> + * Remove CqlPagingRecordReader / CqlPagingInputFormat ( CASSANDRA - 7570 ) <nl> + * Add stop method to EmbeddedCassandraService ( CASSANDRA - 7595 ) <nl> + * Update java driver ( for hadoop ) ( CASSANDRA - 7618 ) <nl> * Support connecting to ipv6 jmx with nodetool ( CASSANDRA - 7669 ) <nl> <nl> <nl> @ @ - 114 , 61 + 121 , 6 @ @ Merged from 2 . 0 : <nl> * Fix NPE when listing saved caches dir ( CASSANDRA - 7632 ) <nl> <nl> <nl> - 2 . 0 . 9 <nl> - * Fix CC # collectTimeOrderedData ( ) tombstone optimisations ( CASSANDRA - 7394 ) <nl> - * Fix assertion error in CL . ANY timeout handling ( CASSANDRA - 7364 ) <nl> - * Handle empty CFs in Memtable # maybeUpdateLiveRatio ( ) ( CASSANDRA - 7401 ) <nl> - * Fix native protocol CAS batches ( CASSANDRA - 7337 ) <nl> - * Reduce likelihood of contention on local paxos locking ( CASSANDRA - 7359 ) <nl> - * Upgrade to Pig 0 . 12 . 1 ( CASSANDRA - 6556 ) <nl> - * Make sure we clear out repair sessions from netstats ( CASSANDRA - 7329 ) <nl> - * Don ' t fail streams on failure detector downs ( CASSANDRA - 3569 ) <nl> - * Add optional keyspace to DROP INDEX statement ( CASSANDRA - 7314 ) <nl> - * Reduce run time for CQL tests ( CASSANDRA - 7327 ) <nl> - * Fix heap size calculation on Windows ( CASSANDRA - 7352 , 7353 ) <nl> - * RefCount native frames from netty ( CASSANDRA - 7245 ) <nl> - * Use tarball dir instead of / var for default paths ( CASSANDRA - 7136 ) <nl> - * Remove rows _ per _ partition _ to _ cache keyword ( CASSANDRA - 7193 ) <nl> - * Fix schema change response in native protocol v3 ( CASSANDRA - 7413 ) <nl> - Merged from 2 . 0 : <nl> - * Fix assertion error in CL . ANY timeout handling ( CASSANDRA - 7364 ) <nl> - * Add per - CF range read request latency metrics ( CASSANDRA - 7338 ) <nl> - * Fix NPE in StreamTransferTask . createMessageForRetry ( ) ( CASSANDRA - 7323 ) <nl> - * Make StreamSession # closeSession ( ) idempotent ( CASSANDRA - 7262 ) <nl> - * Fix infinite loop on exception while streaming ( CASSANDRA - 7330 ) <nl> - * Account for range tombstones in min / max column names ( CASSANDRA - 7235 ) <nl> - * Improve sub range repair validation ( CASSANDRA - 7317 ) <nl> - * Accept subtypes for function results , type casts ( CASSANDRA - 6766 ) <nl> - * Support DISTINCT for static columns and fix behaviour when DISTINC is <nl> - not use ( CASSANDRA - 7305 ) . <nl> - * Refuse range queries with strict bounds on compact tables since they <nl> - are broken ( CASSANDRA - 7059 ) <nl> - Merged from 1 . 2 : <nl> - * Expose global ColumnFamily metrics ( CASSANDRA - 7273 ) <nl> - | | | | | | | merged common ancestors <nl> - 1 . 2 . 17 <nl> - = = = = = = = <nl> - <nl> - 1 . 2 . 17 <nl> - > > > > > > > cassandra - 1 . 2 <nl> - * cqlsh : Fix CompositeType columns in DESCRIBE TABLE output ( CASSANDRA - 7399 ) <nl> - * Expose global ColumnFamily metrics ( CASSANDRA - 7273 ) <nl> - * Handle possible integer overflow in FastByteArrayOutputStream ( CASSANDRA - 7373 ) <nl> - * cqlsh : ' ascii ' values weren ' t formatted as text ( CASSANDRA - 7407 ) <nl> - * cqlsh : ignore . cassandra permission errors ( CASSANDRA - 7266 ) <nl> - * reduce failure detector initial value to 2s ( CASSANDRA - 7307 ) <nl> - * Fix problem truncating on a node that was previously in a dead state ( CASSANDRA - 7318 ) <nl> - * Don ' t insert tombstones that hide indexed values into 2i ( CASSANDRA - 7268 ) <nl> - * Track metrics at a keyspace level ( CASSANDRA - 6539 ) <nl> - * Add replace _ address _ first _ boot flag to only replace if not bootstrapped <nl> - ( CASSANDRA - 7356 ) <nl> - * Enable keepalive for native protocol ( CASSANDRA - 7380 ) <nl> - * Check internal addresses for seeds ( CASSANDRA - 6523 ) <nl> - * Fix potential / by 0 in HHOM page size calculation ( CASSANDRA - 7354 ) <nl> - * Use LOCAL _ ONE for non - superuser auth queries ( CASSANDRA - 7328 ) <nl> - * Fix RangeTombstone copy bug ( CASSANDRA - 7371 ) <nl> - <nl> - <nl> 2 . 1 . 0 - rc1 <nl> * Revert flush directory ( CASSANDRA - 6357 ) <nl> * More efficient executor service for fast operations ( CASSANDRA - 4718 ) <nl> diff - - git a / NEWS . txt b / NEWS . txt <nl> index b42db60 . . 79212f8 100644 <nl> - - - a / NEWS . txt <nl> + + + b / NEWS . txt <nl> @ @ - 75 , 7 + 75 , 10 @ @ Upgrading <nl> = = = = <nl> New features <nl> - - - - - - - - - - - - <nl> - - If you are using Leveled Compaction , you can now disable doing size - tiered <nl> + - CqlPaginRecordReader and CqlPagingInputFormat have both been removed . <nl> + Use CqlInputFormat instead . <nl> + - If you are using Leveled Compaction , you can now disable doing <nl> + size - tiered <nl> compaction in L0 by starting Cassandra with - Dcassandra . disable _ stcs _ in _ l0 <nl> ( see CASSANDRA - 6621 for details ) . <nl> - Shuffle and taketoken have been removed . For clusters that choose to <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java <nl> deleted file mode 100644 <nl> index 96f2f94 . . 0000000 <nl> - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java <nl> + + + / dev / null <nl> @ @ - 1 , 85 + 0 , 0 @ @ <nl> - / * <nl> - * Licensed to the Apache Software Foundation ( ASF ) under one <nl> - * or more contributor license agreements . See the NOTICE file <nl> - * distributed with this work for additional information <nl> - * regarding copyright ownership . The ASF licenses this file <nl> - * to you under the Apache License , Version 2 . 0 ( the <nl> - * " License " ) ; you may not use this file except in compliance <nl> - * with the License . You may obtain a copy of the License at <nl> - * <nl> - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> - * <nl> - * Unless required by applicable law or agreed to in writing , software <nl> - * distributed under the License is distributed on an " AS IS " BASIS , <nl> - * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> - * See the License for the specific language governing permissions and <nl> - * limitations under the License . <nl> - * / <nl> - package org . apache . cassandra . hadoop . cql3 ; <nl> - <nl> - import java . io . IOException ; <nl> - import java . nio . ByteBuffer ; <nl> - import java . util . Map ; <nl> - <nl> - import org . apache . cassandra . hadoop . HadoopCompat ; <nl> - import org . apache . cassandra . hadoop . AbstractColumnFamilyInputFormat ; <nl> - import org . apache . cassandra . hadoop . ReporterWrapper ; <nl> - import org . apache . hadoop . mapred . InputSplit ; <nl> - import org . apache . hadoop . mapred . JobConf ; <nl> - import org . apache . hadoop . mapred . RecordReader ; <nl> - import org . apache . hadoop . mapred . Reporter ; <nl> - import org . apache . hadoop . mapreduce . TaskAttemptContext ; <nl> - import org . apache . hadoop . mapreduce . TaskAttemptID ; <nl> - <nl> - / * * <nl> - * Hadoop InputFormat allowing map / reduce against Cassandra rows within one ColumnFamily . <nl> - * <nl> - * At minimum , you need to set the KS and CF in your Hadoop job Configuration . <nl> - * The ConfigHelper class is provided to make this <nl> - * simple : <nl> - * ConfigHelper . setInputColumnFamily <nl> - * <nl> - * You can also configure the number of rows per InputSplit with <nl> - * ConfigHelper . setInputSplitSize . The default split size is 64k rows . <nl> - * the number of CQL rows per page <nl> - * <nl> - * the number of CQL rows per page <nl> - * CQLConfigHelper . setInputCQLPageRowSize . The default page row size is 1000 . You <nl> - * should set it to " as big as possible , but no bigger . " It set the LIMIT for the CQL <nl> - * query , so you need set it big enough to minimize the network overhead , and also <nl> - * not too big to avoid out of memory issue . <nl> - * <nl> - * the column names of the select CQL query . The default is all columns <nl> - * CQLConfigHelper . setInputColumns <nl> - * <nl> - * the user defined the where clause <nl> - * CQLConfigHelper . setInputWhereClauses . The default is no user defined where clause <nl> - * / <nl> - public class CqlPagingInputFormat extends AbstractColumnFamilyInputFormat < Map < String , ByteBuffer > , Map < String , ByteBuffer > > <nl> - { <nl> - public RecordReader < Map < String , ByteBuffer > , Map < String , ByteBuffer > > getRecordReader ( InputSplit split , JobConf jobConf , final Reporter reporter ) <nl> - throws IOException <nl> - { <nl> - TaskAttemptContext tac = HadoopCompat . newMapContext ( <nl> - jobConf , <nl> - TaskAttemptID . forName ( jobConf . get ( MAPRED _ TASK _ ID ) ) , <nl> - null , <nl> - null , <nl> - null , <nl> - new ReporterWrapper ( reporter ) , <nl> - null ) ; <nl> - <nl> - CqlPagingRecordReader recordReader = new CqlPagingRecordReader ( ) ; <nl> - recordReader . initialize ( ( org . apache . hadoop . mapreduce . InputSplit ) split , tac ) ; <nl> - return recordReader ; <nl> - } <nl> - <nl> - @ Override <nl> - public org . apache . hadoop . mapreduce . RecordReader < Map < String , ByteBuffer > , Map < String , ByteBuffer > > createRecordReader ( <nl> - org . apache . hadoop . mapreduce . InputSplit arg0 , TaskAttemptContext arg1 ) throws IOException , <nl> - InterruptedException <nl> - { <nl> - return new CqlPagingRecordReader ( ) ; <nl> - } <nl> - <nl> - } <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java <nl> deleted file mode 100644 <nl> index 2427e9c . . 0000000 <nl> - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java <nl> + + + / dev / null <nl> @ @ - 1 , 800 + 0 , 0 @ @ <nl> - / * <nl> - * Licensed to the Apache Software Foundation ( ASF ) under one <nl> - * or more contributor license agreements . See the NOTICE file <nl> - * distributed with this work for additional information <nl> - * regarding copyright ownership . The ASF licenses this file <nl> - * to you under the Apache License , Version 2 . 0 ( the <nl> - * " License " ) ; you may not use this file except in compliance <nl> - * with the License . You may obtain a copy of the License at <nl> - * <nl> - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> - * <nl> - * Unless required by applicable law or agreed to in writing , software <nl> - * distributed under the License is distributed on an " AS IS " BASIS , <nl> - * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> - * See the License for the specific language governing permissions and <nl> - * limitations under the License . <nl> - * / <nl> - package org . apache . cassandra . hadoop . cql3 ; <nl> - <nl> - import java . io . IOException ; <nl> - import java . net . InetAddress ; <nl> - import java . net . UnknownHostException ; <nl> - import java . nio . ByteBuffer ; <nl> - import java . nio . charset . CharacterCodingException ; <nl> - import java . util . * ; <nl> - <nl> - import com . google . common . base . Optional ; <nl> - import com . google . common . collect . AbstractIterator ; <nl> - import com . google . common . collect . Iterables ; <nl> - <nl> - import org . apache . cassandra . hadoop . HadoopCompat ; <nl> - import org . slf4j . Logger ; <nl> - import org . slf4j . LoggerFactory ; <nl> - import org . apache . cassandra . db . marshal . AbstractType ; <nl> - import org . apache . cassandra . db . marshal . CompositeType ; <nl> - import org . apache . cassandra . db . marshal . LongType ; <nl> - import org . apache . cassandra . db . marshal . ReversedType ; <nl> - import org . apache . cassandra . db . marshal . TypeParser ; <nl> - import org . apache . cassandra . dht . IPartitioner ; <nl> - import org . apache . cassandra . exceptions . ConfigurationException ; <nl> - import org . apache . cassandra . exceptions . SyntaxException ; <nl> - import org . apache . cassandra . hadoop . ColumnFamilySplit ; <nl> - import org . apache . cassandra . hadoop . ConfigHelper ; <nl> - import org . apache . cassandra . thrift . * ; <nl> - import org . apache . cassandra . utils . ByteBufferUtil ; <nl> - import org . apache . cassandra . utils . FBUtilities ; <nl> - import org . apache . cassandra . utils . Pair ; <nl> - import org . apache . hadoop . conf . Configuration ; <nl> - import org . apache . hadoop . mapreduce . InputSplit ; <nl> - import org . apache . hadoop . mapreduce . RecordReader ; <nl> - import org . apache . hadoop . mapreduce . TaskAttemptContext ; <nl> - import org . apache . thrift . TException ; <nl> - import org . apache . thrift . transport . TTransport ; <nl> - <nl> - / * * <nl> - * Hadoop RecordReader read the values return from the CQL query <nl> - * It use CQL key range query to page through the wide rows . <nl> - * < p / > <nl> - * Return List < IColumn > as keys columns <nl> - * < p / > <nl> - * Map < ByteBuffer , IColumn > as column name to columns mappings <nl> - * / <nl> - public class CqlPagingRecordReader extends RecordReader < Map < String , ByteBuffer > , Map < String , ByteBuffer > > <nl> - implements org . apache . hadoop . mapred . RecordReader < Map < String , ByteBuffer > , Map < String , ByteBuffer > > <nl> - { <nl> - private static final Logger logger = LoggerFactory . getLogger ( CqlPagingRecordReader . class ) ; <nl> - <nl> - public static final int DEFAULT _ CQL _ PAGE _ LIMIT = 1000 ; / / TODO : find the number large enough but not OOM <nl> - <nl> - private ColumnFamilySplit split ; <nl> - protected RowIterator rowIterator ; <nl> - <nl> - private Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > currentRow ; <nl> - private int totalRowCount ; / / total number of rows to fetch <nl> - private String keyspace ; <nl> - private String cfName ; <nl> - private Cassandra . Client client ; <nl> - private ConsistencyLevel consistencyLevel ; <nl> - <nl> - / / partition keys - - key aliases <nl> - private List < BoundColumn > partitionBoundColumns = new ArrayList < BoundColumn > ( ) ; <nl> - <nl> - / / cluster keys - - column aliases <nl> - private List < BoundColumn > clusterColumns = new ArrayList < BoundColumn > ( ) ; <nl> - <nl> - / / map prepared query type to item id <nl> - private Map < Integer , Integer > preparedQueryIds = new HashMap < Integer , Integer > ( ) ; <nl> - <nl> - / / cql query select columns <nl> - private String columns ; <nl> - <nl> - / / the number of cql rows per page <nl> - private int pageRowSize ; <nl> - <nl> - / / user defined where clauses <nl> - private String userDefinedWhereClauses ; <nl> - <nl> - private IPartitioner partitioner ; <nl> - <nl> - private AbstractType < ? > keyValidator ; <nl> - <nl> - public CqlPagingRecordReader ( ) <nl> - { <nl> - super ( ) ; <nl> - } <nl> - <nl> - public void initialize ( InputSplit split , TaskAttemptContext context ) throws IOException <nl> - { <nl> - this . split = ( ColumnFamilySplit ) split ; <nl> - Configuration conf = HadoopCompat . getConfiguration ( context ) ; <nl> - totalRowCount = ( this . split . getLength ( ) < Long . MAX _ VALUE ) <nl> - ? ( int ) this . split . getLength ( ) <nl> - : ConfigHelper . getInputSplitSize ( conf ) ; <nl> - cfName = ConfigHelper . getInputColumnFamily ( conf ) ; <nl> - consistencyLevel = ConsistencyLevel . valueOf ( ConfigHelper . getReadConsistencyLevel ( conf ) ) ; <nl> - keyspace = ConfigHelper . getInputKeyspace ( conf ) ; <nl> - columns = CqlConfigHelper . getInputcolumns ( conf ) ; <nl> - userDefinedWhereClauses = CqlConfigHelper . getInputWhereClauses ( conf ) ; <nl> - <nl> - Optional < Integer > pageRowSizeOptional = CqlConfigHelper . getInputPageRowSize ( conf ) ; <nl> - try <nl> - { <nl> - 	 pageRowSize = pageRowSizeOptional . isPresent ( ) ? pageRowSizeOptional . get ( ) : DEFAULT _ CQL _ PAGE _ LIMIT ; <nl> - } <nl> - catch ( NumberFormatException e ) <nl> - { <nl> - 	 pageRowSize = DEFAULT _ CQL _ PAGE _ LIMIT ; <nl> - } <nl> - <nl> - partitioner = ConfigHelper . getInputPartitioner ( HadoopCompat . getConfiguration ( context ) ) ; <nl> - <nl> - try <nl> - { <nl> - if ( client ! = null ) <nl> - return ; <nl> - <nl> - / / create connection using thrift <nl> - String [ ] locations = split . getLocations ( ) ; <nl> - Exception lastException = null ; <nl> - for ( String location : locations ) <nl> - { <nl> - int port = ConfigHelper . getInputRpcPort ( conf ) ; <nl> - try <nl> - { <nl> - client = CqlPagingInputFormat . createAuthenticatedClient ( location , port , conf ) ; <nl> - break ; <nl> - } <nl> - catch ( Exception e ) <nl> - { <nl> - lastException = e ; <nl> - logger . warn ( " Failed to create authenticated client to { } : { } " , location , port ) ; <nl> - } <nl> - } <nl> - if ( client = = null & & lastException ! = null ) <nl> - throw lastException ; <nl> - <nl> - / / retrieve partition keys and cluster keys from system . schema _ columnfamilies table <nl> - retrieveKeys ( ) ; <nl> - <nl> - client . set _ keyspace ( keyspace ) ; <nl> - } <nl> - catch ( Exception e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - <nl> - rowIterator = new RowIterator ( ) ; <nl> - <nl> - logger . debug ( " created { } " , rowIterator ) ; <nl> - } <nl> - <nl> - public void close ( ) <nl> - { <nl> - if ( client ! = null ) <nl> - { <nl> - TTransport transport = client . getOutputProtocol ( ) . getTransport ( ) ; <nl> - if ( transport . isOpen ( ) ) <nl> - transport . close ( ) ; <nl> - client = null ; <nl> - } <nl> - } <nl> - <nl> - public Map < String , ByteBuffer > getCurrentKey ( ) <nl> - { <nl> - return currentRow . left ; <nl> - } <nl> - <nl> - public Map < String , ByteBuffer > getCurrentValue ( ) <nl> - { <nl> - return currentRow . right ; <nl> - } <nl> - <nl> - public float getProgress ( ) <nl> - { <nl> - if ( ! rowIterator . hasNext ( ) ) <nl> - return 1 . 0F ; <nl> - <nl> - / / the progress is likely to be reported slightly off the actual but close enough <nl> - float progress = ( ( float ) rowIterator . totalRead / totalRowCount ) ; <nl> - return progress > 1 . 0F ? 1 . 0F : progress ; <nl> - } <nl> - <nl> - public boolean nextKeyValue ( ) throws IOException <nl> - { <nl> - if ( ! rowIterator . hasNext ( ) ) <nl> - { <nl> - logger . debug ( " Finished scanning { } rows ( estimate was : { } ) " , rowIterator . totalRead , totalRowCount ) ; <nl> - return false ; <nl> - } <nl> - <nl> - try <nl> - { <nl> - currentRow = rowIterator . next ( ) ; <nl> - } <nl> - catch ( Exception e ) <nl> - { <nl> - / / throw it as IOException , so client can catch it and handle it at client side <nl> - IOException ioe = new IOException ( e . getMessage ( ) ) ; <nl> - ioe . initCause ( ioe . getCause ( ) ) ; <nl> - throw ioe ; <nl> - } <nl> - return true ; <nl> - } <nl> - <nl> - / / we don ' t use endpointsnitch since we are trying to support hadoop nodes that are <nl> - / / not necessarily on Cassandra machines , too . This should be adequate for single - DC clusters , at least . <nl> - private String [ ] getLocations ( ) <nl> - { <nl> - Collection < InetAddress > localAddresses = FBUtilities . getAllLocalAddresses ( ) ; <nl> - <nl> - for ( InetAddress address : localAddresses ) <nl> - { <nl> - for ( String location : split . getLocations ( ) ) <nl> - { <nl> - InetAddress locationAddress ; <nl> - try <nl> - { <nl> - locationAddress = InetAddress . getByName ( location ) ; <nl> - } <nl> - catch ( UnknownHostException e ) <nl> - { <nl> - throw new AssertionError ( e ) ; <nl> - } <nl> - if ( address . equals ( locationAddress ) ) <nl> - { <nl> - return new String [ ] { location } ; <nl> - } <nl> - } <nl> - } <nl> - return split . getLocations ( ) ; <nl> - } <nl> - <nl> - / / Because the old Hadoop API wants us to write to the key and value <nl> - / / and the new asks for them , we need to copy the output of the new API <nl> - / / to the old . Thus , expect a small performance hit . <nl> - / / And obviously this wouldn ' t work for wide rows . But since ColumnFamilyInputFormat <nl> - / / and ColumnFamilyRecordReader don ' t support them , it should be fine for now . <nl> - public boolean next ( Map < String , ByteBuffer > keys , Map < String , ByteBuffer > value ) throws IOException <nl> - { <nl> - if ( nextKeyValue ( ) ) <nl> - { <nl> - value . clear ( ) ; <nl> - value . putAll ( getCurrentValue ( ) ) ; <nl> - <nl> - keys . clear ( ) ; <nl> - keys . putAll ( getCurrentKey ( ) ) ; <nl> - <nl> - return true ; <nl> - } <nl> - return false ; <nl> - } <nl> - <nl> - public long getPos ( ) throws IOException <nl> - { <nl> - return ( long ) rowIterator . totalRead ; <nl> - } <nl> - <nl> - public Map < String , ByteBuffer > createKey ( ) <nl> - { <nl> - return new LinkedHashMap < String , ByteBuffer > ( ) ; <nl> - } <nl> - <nl> - public Map < String , ByteBuffer > createValue ( ) <nl> - { <nl> - return new LinkedHashMap < String , ByteBuffer > ( ) ; <nl> - } <nl> - <nl> - / * * CQL row iterator * / <nl> - protected class RowIterator extends AbstractIterator < Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > > <nl> - { <nl> - protected int totalRead = 0 ; / / total number of cf rows read <nl> - protected Iterator < CqlRow > rows ; <nl> - private int pageRows = 0 ; / / the number of cql rows read of this page <nl> - private String previousRowKey = null ; / / previous CF row key <nl> - private String partitionKeyString ; / / keys in < key1 > , < key2 > , < key3 > string format <nl> - private String partitionKeyMarkers ; / / question marks in ? , ? , ? format which matches the number of keys <nl> - <nl> - public RowIterator ( ) <nl> - { <nl> - / / initial page <nl> - executeQuery ( ) ; <nl> - } <nl> - <nl> - protected Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > computeNext ( ) <nl> - { <nl> - if ( rows = = null ) <nl> - return endOfData ( ) ; <nl> - <nl> - int index = - 2 ; <nl> - / / check there are more page to read <nl> - while ( ! rows . hasNext ( ) ) <nl> - { <nl> - / / no more data <nl> - if ( index = = - 1 | | emptyPartitionKeyValues ( ) ) <nl> - { <nl> - logger . debug ( " no more data " ) ; <nl> - return endOfData ( ) ; <nl> - } <nl> - <nl> - index = setTailNull ( clusterColumns ) ; <nl> - logger . debug ( " set tail to null , index : { } " , index ) ; <nl> - executeQuery ( ) ; <nl> - pageRows = 0 ; <nl> - <nl> - if ( rows = = null | | ! rows . hasNext ( ) & & index < 0 ) <nl> - { <nl> - logger . debug ( " no more data " ) ; <nl> - return endOfData ( ) ; <nl> - } <nl> - } <nl> - <nl> - Map < String , ByteBuffer > valueColumns = createValue ( ) ; <nl> - Map < String , ByteBuffer > keyColumns = createKey ( ) ; <nl> - int i = 0 ; <nl> - CqlRow row = rows . next ( ) ; <nl> - for ( Column column : row . columns ) <nl> - { <nl> - String columnName = stringValue ( ByteBuffer . wrap ( column . getName ( ) ) ) ; <nl> - logger . debug ( " column : { } " , columnName ) ; <nl> - <nl> - if ( i < partitionBoundColumns . size ( ) + clusterColumns . size ( ) ) <nl> - keyColumns . put ( stringValue ( column . name ) , column . value ) ; <nl> - else <nl> - valueColumns . put ( stringValue ( column . name ) , column . value ) ; <nl> - <nl> - i + + ; <nl> - } <nl> - <nl> - / / increase total CQL row read for this page <nl> - pageRows + + ; <nl> - <nl> - / / increase total CF row read <nl> - if ( newRow ( keyColumns , previousRowKey ) ) <nl> - totalRead + + ; <nl> - <nl> - / / read full page <nl> - if ( pageRows > = pageRowSize | | ! rows . hasNext ( ) ) <nl> - { <nl> - Iterator < String > newKeys = keyColumns . keySet ( ) . iterator ( ) ; <nl> - for ( BoundColumn column : partitionBoundColumns ) <nl> - column . value = keyColumns . get ( newKeys . next ( ) ) ; <nl> - <nl> - for ( BoundColumn column : clusterColumns ) <nl> - column . value = keyColumns . get ( newKeys . next ( ) ) ; <nl> - <nl> - executeQuery ( ) ; <nl> - pageRows = 0 ; <nl> - } <nl> - <nl> - return Pair . create ( keyColumns , valueColumns ) ; <nl> - } <nl> - <nl> - / * * check whether start to read a new CF row by comparing the partition keys * / <nl> - private boolean newRow ( Map < String , ByteBuffer > keyColumns , String previousRowKey ) <nl> - { <nl> - if ( keyColumns . isEmpty ( ) ) <nl> - return false ; <nl> - <nl> - String rowKey = " " ; <nl> - if ( keyColumns . size ( ) = = 1 ) <nl> - { <nl> - rowKey = partitionBoundColumns . get ( 0 ) . validator . getString ( keyColumns . get ( partitionBoundColumns . get ( 0 ) . name ) ) ; <nl> - } <nl> - else <nl> - { <nl> - Iterator < ByteBuffer > iter = keyColumns . values ( ) . iterator ( ) ; <nl> - for ( BoundColumn column : partitionBoundColumns ) <nl> - rowKey = rowKey + column . validator . getString ( ByteBufferUtil . clone ( iter . next ( ) ) ) + " : " ; <nl> - } <nl> - <nl> - logger . debug ( " previous RowKey : { } , new row key : { } " , previousRowKey , rowKey ) ; <nl> - if ( previousRowKey = = null ) <nl> - { <nl> - this . previousRowKey = rowKey ; <nl> - return true ; <nl> - } <nl> - <nl> - if ( rowKey . equals ( previousRowKey ) ) <nl> - return false ; <nl> - <nl> - this . previousRowKey = rowKey ; <nl> - return true ; <nl> - } <nl> - <nl> - / * * set the last non - null key value to null , and return the previous index * / <nl> - private int setTailNull ( List < BoundColumn > values ) <nl> - { <nl> - if ( values . isEmpty ( ) ) <nl> - return - 1 ; <nl> - <nl> - Iterator < BoundColumn > iterator = values . iterator ( ) ; <nl> - int previousIndex = - 1 ; <nl> - BoundColumn current ; <nl> - while ( iterator . hasNext ( ) ) <nl> - { <nl> - current = iterator . next ( ) ; <nl> - if ( current . value = = null ) <nl> - { <nl> - int index = previousIndex > 0 ? previousIndex : 0 ; <nl> - BoundColumn column = values . get ( index ) ; <nl> - logger . debug ( " set key { } value to null " , column . name ) ; <nl> - column . value = null ; <nl> - return previousIndex - 1 ; <nl> - } <nl> - <nl> - previousIndex + + ; <nl> - } <nl> - <nl> - BoundColumn column = values . get ( previousIndex ) ; <nl> - logger . debug ( " set key { } value to null " , column . name ) ; <nl> - column . value = null ; <nl> - return previousIndex - 1 ; <nl> - } <nl> - <nl> - / * * serialize the prepared query , pair . left is query id , pair . right is query * / <nl> - private Pair < Integer , String > composeQuery ( String columns ) <nl> - { <nl> - Pair < Integer , String > clause = whereClause ( ) ; <nl> - if ( columns = = null ) <nl> - { <nl> - columns = " * " ; <nl> - } <nl> - else <nl> - { <nl> - / / add keys in the front in order <nl> - String partitionKey = keyString ( partitionBoundColumns ) ; <nl> - String clusterKey = keyString ( clusterColumns ) ; <nl> - <nl> - columns = withoutKeyColumns ( columns ) ; <nl> - columns = ( clusterKey = = null | | " " . equals ( clusterKey ) ) <nl> - ? partitionKey + ( columns ! = null ? ( " , " + columns ) : " " ) <nl> - : partitionKey + " , " + clusterKey + ( columns ! = null ? ( " , " + columns ) : " " ) ; <nl> - } <nl> - <nl> - String whereStr = userDefinedWhereClauses = = null ? " " : " AND " + userDefinedWhereClauses ; <nl> - return Pair . create ( clause . left , <nl> - String . format ( " SELECT % s FROM % s % s % s LIMIT % d ALLOW FILTERING " , <nl> - columns , quote ( cfName ) , clause . right , whereStr , pageRowSize ) ) ; <nl> - } <nl> - <nl> - <nl> - / * * remove key columns from the column string * / <nl> - private String withoutKeyColumns ( String columnString ) <nl> - { <nl> - Set < String > keyNames = new HashSet < String > ( ) ; <nl> - for ( BoundColumn column : Iterables . concat ( partitionBoundColumns , clusterColumns ) ) <nl> - keyNames . add ( column . name ) ; <nl> - <nl> - String [ ] columns = columnString . split ( " , " ) ; <nl> - String result = null ; <nl> - for ( String column : columns ) <nl> - { <nl> - String trimmed = column . trim ( ) ; <nl> - if ( keyNames . contains ( trimmed ) ) <nl> - continue ; <nl> - <nl> - String quoted = quote ( trimmed ) ; <nl> - result = result = = null ? quoted : result + " , " + quoted ; <nl> - } <nl> - return result ; <nl> - } <nl> - <nl> - / * * serialize the where clause * / <nl> - private Pair < Integer , String > whereClause ( ) <nl> - { <nl> - if ( partitionKeyString = = null ) <nl> - partitionKeyString = keyString ( partitionBoundColumns ) ; <nl> - <nl> - if ( partitionKeyMarkers = = null ) <nl> - partitionKeyMarkers = partitionKeyMarkers ( ) ; <nl> - / / initial query token ( k ) > = start _ token and token ( k ) < = end _ token <nl> - if ( emptyPartitionKeyValues ( ) ) <nl> - return Pair . create ( 0 , String . format ( " WHERE token ( % s ) > ? AND token ( % s ) < = ? " , partitionKeyString , partitionKeyString ) ) ; <nl> - <nl> - / / query token ( k ) > token ( pre _ partition _ key ) and token ( k ) < = end _ token <nl> - if ( clusterColumns . size ( ) = = 0 | | clusterColumns . get ( 0 ) . value = = null ) <nl> - return Pair . create ( 1 , <nl> - String . format ( " WHERE token ( % s ) > token ( % s ) AND token ( % s ) < = ? " , <nl> - partitionKeyString , partitionKeyMarkers , partitionKeyString ) ) ; <nl> - <nl> - / / query token ( k ) = token ( pre _ partition _ key ) and m = pre _ cluster _ key _ m and n > pre _ cluster _ key _ n <nl> - Pair < Integer , String > clause = whereClause ( clusterColumns , 0 ) ; <nl> - return Pair . create ( clause . left , <nl> - String . format ( " WHERE token ( % s ) = token ( % s ) % s " , partitionKeyString , partitionKeyMarkers , clause . right ) ) ; <nl> - } <nl> - <nl> - / * * recursively serialize the where clause * / <nl> - private Pair < Integer , String > whereClause ( List < BoundColumn > column , int position ) <nl> - { <nl> - if ( position = = column . size ( ) - 1 | | column . get ( position + 1 ) . value = = null ) <nl> - return Pair . create ( position + 2 , String . format ( " AND % s % s ? " , quote ( column . get ( position ) . name ) , column . get ( position ) . reversed ? " < " : " > " ) ) ; <nl> - <nl> - Pair < Integer , String > clause = whereClause ( column , position + 1 ) ; <nl> - return Pair . create ( clause . left , String . format ( " AND % s = ? % s " , quote ( column . get ( position ) . name ) , clause . right ) ) ; <nl> - } <nl> - <nl> - / * * check whether all key values are null * / <nl> - private boolean emptyPartitionKeyValues ( ) <nl> - { <nl> - for ( BoundColumn column : partitionBoundColumns ) <nl> - { <nl> - if ( column . value ! = null ) <nl> - return false ; <nl> - } <nl> - return true ; <nl> - } <nl> - <nl> - / * * serialize the partition key string in format of < key1 > , < key2 > , < key3 > * / <nl> - private String keyString ( List < BoundColumn > columns ) <nl> - { <nl> - String result = null ; <nl> - for ( BoundColumn column : columns ) <nl> - result = result = = null ? quote ( column . name ) : result + " , " + quote ( column . name ) ; <nl> - <nl> - return result = = null ? " " : result ; <nl> - } <nl> - <nl> - / * * serialize the question marks for partition key string in format of ? , ? , ? * / <nl> - private String partitionKeyMarkers ( ) <nl> - { <nl> - String result = null ; <nl> - for ( BoundColumn column : partitionBoundColumns ) <nl> - result = result = = null ? " ? " : result + " , ? " ; <nl> - <nl> - return result ; <nl> - } <nl> - <nl> - / * * serialize the query binding variables , pair . left is query id , pair . right is the binding variables * / <nl> - private Pair < Integer , List < ByteBuffer > > preparedQueryBindValues ( ) <nl> - { <nl> - List < ByteBuffer > values = new LinkedList < ByteBuffer > ( ) ; <nl> - <nl> - / / initial query token ( k ) > = start _ token and token ( k ) < = end _ token <nl> - if ( emptyPartitionKeyValues ( ) ) <nl> - { <nl> - values . add ( partitioner . getTokenValidator ( ) . fromString ( split . getStartToken ( ) ) ) ; <nl> - values . add ( partitioner . getTokenValidator ( ) . fromString ( split . getEndToken ( ) ) ) ; <nl> - return Pair . create ( 0 , values ) ; <nl> - } <nl> - else <nl> - { <nl> - for ( BoundColumn partitionBoundColumn1 : partitionBoundColumns ) <nl> - values . add ( partitionBoundColumn1 . value ) ; <nl> - <nl> - if ( clusterColumns . size ( ) = = 0 | | clusterColumns . get ( 0 ) . value = = null ) <nl> - { <nl> - / / query token ( k ) > token ( pre _ partition _ key ) and token ( k ) < = end _ token <nl> - values . add ( partitioner . getTokenValidator ( ) . fromString ( split . getEndToken ( ) ) ) ; <nl> - return Pair . create ( 1 , values ) ; <nl> - } <nl> - else <nl> - { <nl> - / / query token ( k ) = token ( pre _ partition _ key ) and m = pre _ cluster _ key _ m and n > pre _ cluster _ key _ n <nl> - int type = preparedQueryBindValues ( clusterColumns , 0 , values ) ; <nl> - return Pair . create ( type , values ) ; <nl> - } <nl> - } <nl> - } <nl> - <nl> - / * * recursively serialize the query binding variables * / <nl> - private int preparedQueryBindValues ( List < BoundColumn > column , int position , List < ByteBuffer > bindValues ) <nl> - { <nl> - if ( position = = column . size ( ) - 1 | | column . get ( position + 1 ) . value = = null ) <nl> - { <nl> - bindValues . add ( column . get ( position ) . value ) ; <nl> - return position + 2 ; <nl> - } <nl> - else <nl> - { <nl> - bindValues . add ( column . get ( position ) . value ) ; <nl> - return preparedQueryBindValues ( column , position + 1 , bindValues ) ; <nl> - } <nl> - } <nl> - <nl> - / * * get the prepared query item Id * / <nl> - private int prepareQuery ( int type ) throws InvalidRequestException , TException <nl> - { <nl> - Integer itemId = preparedQueryIds . get ( type ) ; <nl> - if ( itemId ! = null ) <nl> - return itemId ; <nl> - <nl> - Pair < Integer , String > query = null ; <nl> - query = composeQuery ( columns ) ; <nl> - logger . debug ( " type : { } , query : { } " , query . left , query . right ) ; <nl> - CqlPreparedResult cqlPreparedResult = client . prepare _ cql3 _ query ( ByteBufferUtil . bytes ( query . right ) , Compression . NONE ) ; <nl> - preparedQueryIds . put ( query . left , cqlPreparedResult . itemId ) ; <nl> - return cqlPreparedResult . itemId ; <nl> - } <nl> - <nl> - / * * Quoting for working with uppercase * / <nl> - private String quote ( String identifier ) <nl> - { <nl> - return " \ " " + identifier . replaceAll ( " \ " " , " \ " \ " " ) + " \ " " ; <nl> - } <nl> - <nl> - / * * execute the prepared query * / <nl> - private void executeQuery ( ) <nl> - { <nl> - Pair < Integer , List < ByteBuffer > > bindValues = preparedQueryBindValues ( ) ; <nl> - logger . debug ( " query type : { } " , bindValues . left ) ; <nl> - <nl> - / / check whether it reach end of range for type 1 query CASSANDRA - 5573 <nl> - if ( bindValues . left = = 1 & & reachEndRange ( ) ) <nl> - { <nl> - rows = null ; <nl> - return ; <nl> - } <nl> - <nl> - int retries = 0 ; <nl> - / / only try three times for TimedOutException and UnavailableException <nl> - while ( retries < 3 ) <nl> - { <nl> - try <nl> - { <nl> - CqlResult cqlResult = client . execute _ prepared _ cql3 _ query ( prepareQuery ( bindValues . left ) , bindValues . right , consistencyLevel ) ; <nl> - if ( cqlResult ! = null & & cqlResult . rows ! = null ) <nl> - rows = cqlResult . rows . iterator ( ) ; <nl> - return ; <nl> - } <nl> - catch ( TimedOutException e ) <nl> - { <nl> - retries + + ; <nl> - if ( retries > = 3 ) <nl> - { <nl> - rows = null ; <nl> - RuntimeException rte = new RuntimeException ( e . getMessage ( ) ) ; <nl> - rte . initCause ( e ) ; <nl> - throw rte ; <nl> - } <nl> - } <nl> - catch ( UnavailableException e ) <nl> - { <nl> - retries + + ; <nl> - if ( retries > = 3 ) <nl> - { <nl> - rows = null ; <nl> - RuntimeException rte = new RuntimeException ( e . getMessage ( ) ) ; <nl> - rte . initCause ( e ) ; <nl> - throw rte ; <nl> - } <nl> - } <nl> - catch ( Exception e ) <nl> - { <nl> - rows = null ; <nl> - RuntimeException rte = new RuntimeException ( e . getMessage ( ) ) ; <nl> - rte . initCause ( e ) ; <nl> - throw rte ; <nl> - } <nl> - } <nl> - } <nl> - } <nl> - <nl> - / * * retrieve the partition keys and cluster keys from system . schema _ columnfamilies table * / <nl> - private void retrieveKeys ( ) throws Exception <nl> - { <nl> - String query = " select key _ aliases , " + <nl> - " column _ aliases , " + <nl> - " key _ validator , " + <nl> - " comparator " + <nl> - " from system . schema _ columnfamilies " + <nl> - " where keyspace _ name = ' % s ' and columnfamily _ name = ' % s ' " ; <nl> - String formatted = String . format ( query , keyspace , cfName ) ; <nl> - CqlResult result = client . execute _ cql3 _ query ( ByteBufferUtil . bytes ( formatted ) , Compression . NONE , ConsistencyLevel . ONE ) ; <nl> - <nl> - CqlRow cqlRow = result . rows . get ( 0 ) ; <nl> - String keyString = ByteBufferUtil . string ( ByteBuffer . wrap ( cqlRow . columns . get ( 0 ) . getValue ( ) ) ) ; <nl> - logger . debug ( " partition keys : { } " , keyString ) ; <nl> - List < String > keys = FBUtilities . fromJsonList ( keyString ) ; <nl> - <nl> - for ( String key : keys ) <nl> - partitionBoundColumns . add ( new BoundColumn ( key ) ) ; <nl> - <nl> - keyString = ByteBufferUtil . string ( ByteBuffer . wrap ( cqlRow . columns . get ( 1 ) . getValue ( ) ) ) ; <nl> - logger . debug ( " cluster columns : { } " , keyString ) ; <nl> - keys = FBUtilities . fromJsonList ( keyString ) ; <nl> - <nl> - for ( String key : keys ) <nl> - clusterColumns . add ( new BoundColumn ( key ) ) ; <nl> - <nl> - Column rawKeyValidator = cqlRow . columns . get ( 2 ) ; <nl> - String validator = ByteBufferUtil . string ( ByteBuffer . wrap ( rawKeyValidator . getValue ( ) ) ) ; <nl> - logger . debug ( " row key validator : { } " , validator ) ; <nl> - keyValidator = parseType ( validator ) ; <nl> - <nl> - if ( keyValidator instanceof CompositeType ) <nl> - { <nl> - List < AbstractType < ? > > types = ( ( CompositeType ) keyValidator ) . types ; <nl> - for ( int i = 0 ; i < partitionBoundColumns . size ( ) ; i + + ) <nl> - partitionBoundColumns . get ( i ) . validator = types . get ( i ) ; <nl> - } <nl> - else <nl> - { <nl> - partitionBoundColumns . get ( 0 ) . validator = keyValidator ; <nl> - } <nl> - <nl> - Column rawComparator = cqlRow . columns . get ( 3 ) ; <nl> - String comparator = ByteBufferUtil . string ( ByteBuffer . wrap ( rawComparator . getValue ( ) ) ) ; <nl> - logger . debug ( " comparator : { } " , comparator ) ; <nl> - AbstractType comparatorValidator = parseType ( comparator ) ; <nl> - if ( comparatorValidator instanceof CompositeType ) <nl> - { <nl> - for ( int i = 0 ; i < clusterColumns . size ( ) ; i + + ) <nl> - clusterColumns . get ( i ) . reversed = ( ( ( CompositeType ) comparatorValidator ) . types . get ( i ) instanceof ReversedType ) ; <nl> - } <nl> - else if ( comparatorValidator instanceof ReversedType ) <nl> - { <nl> - clusterColumns . get ( 0 ) . reversed = true ; <nl> - } <nl> - } <nl> - <nl> - / * * check whether current row is at the end of range * / <nl> - private boolean reachEndRange ( ) <nl> - { <nl> - / / current row key <nl> - ByteBuffer rowKey ; <nl> - if ( keyValidator instanceof CompositeType ) <nl> - { <nl> - ByteBuffer [ ] keys = new ByteBuffer [ partitionBoundColumns . size ( ) ] ; <nl> - for ( int i = 0 ; i < partitionBoundColumns . size ( ) ; i + + ) <nl> - keys [ i ] = partitionBoundColumns . get ( i ) . value . duplicate ( ) ; <nl> - <nl> - rowKey = CompositeType . build ( keys ) ; <nl> - } <nl> - else <nl> - { <nl> - rowKey = partitionBoundColumns . get ( 0 ) . value ; <nl> - } <nl> - <nl> - String endToken = split . getEndToken ( ) ; <nl> - String currentToken = partitioner . getToken ( rowKey ) . toString ( ) ; <nl> - logger . debug ( " End token : { } , current token : { } " , endToken , currentToken ) ; <nl> - <nl> - return endToken . equals ( currentToken ) ; <nl> - } <nl> - <nl> - private static AbstractType < ? > parseType ( String type ) throws IOException <nl> - { <nl> - try <nl> - { <nl> - / / always treat counters like longs , specifically CCT . serialize is not what we need <nl> - if ( type ! = null & & type . equals ( " org . apache . cassandra . db . marshal . CounterColumnType " ) ) <nl> - return LongType . instance ; <nl> - return TypeParser . parse ( type ) ; <nl> - } <nl> - catch ( ConfigurationException e ) <nl> - { <nl> - throw new IOException ( e ) ; <nl> - } <nl> - catch ( SyntaxException e ) <nl> - { <nl> - throw new IOException ( e ) ; <nl> - } <nl> - } <nl> - <nl> - private static class BoundColumn <nl> - { <nl> - final String name ; <nl> - ByteBuffer value ; <nl> - AbstractType < ? > validator ; <nl> - boolean reversed = false ; <nl> - <nl> - public BoundColumn ( String name ) <nl> - { <nl> - this . name = name ; <nl> - } <nl> - } <nl> - <nl> - / * * get string from a ByteBuffer , catch the exception and throw it as runtime exception * / <nl> - private static String stringValue ( ByteBuffer value ) <nl> - { <nl> - try <nl> - { <nl> - return ByteBufferUtil . string ( value ) ; <nl> - } <nl> - catch ( CharacterCodingException e ) <nl> - { <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> - } <nl> - } <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / IterableCqlPagingRecordReader . java b / src / java / org / apache / cassandra / hadoop / cql3 / IterableCqlPagingRecordReader . java <nl> deleted file mode 100644 <nl> index d7348b0 . . 0000000 <nl> - - - a / src / java / org / apache / cassandra / hadoop / cql3 / IterableCqlPagingRecordReader . java <nl> + + + / dev / null <nl> @ @ - 1 , 73 + 0 , 0 @ @ <nl> - / * <nl> - * Licensed to the Apache Software Foundation ( ASF ) under one <nl> - * or more contributor license agreements . See the NOTICE file <nl> - * distributed with this work for additional information <nl> - * regarding copyright ownership . The ASF licenses this file <nl> - * to you under the Apache License , Version 2 . 0 ( the <nl> - * " License " ) ; you may not use this file except in compliance <nl> - * with the License . You may obtain a copy of the License at <nl> - * <nl> - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> - * <nl> - * Unless required by applicable law or agreed to in writing , software <nl> - * distributed under the License is distributed on an " AS IS " BASIS , <nl> - * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> - * See the License for the specific language governing permissions and <nl> - * limitations under the License . <nl> - * / <nl> - package org . apache . cassandra . hadoop . cql3 ; <nl> - <nl> - import java . io . Closeable ; <nl> - import java . io . IOException ; <nl> - import java . nio . ByteBuffer ; <nl> - import java . util . Iterator ; <nl> - import java . util . Map ; <nl> - <nl> - import org . apache . cassandra . utils . Pair ; <nl> - <nl> - / * * <nl> - * Implements an iterable - friendly { @ link CqlPagingRecordReader } . <nl> - * / <nl> - public class IterableCqlPagingRecordReader extends CqlPagingRecordReader <nl> - implements Iterable < Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > > , Closeable <nl> - { <nl> - public Iterator < Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > > iterator ( ) <nl> - { <nl> - return new Iterator < Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > > ( ) <nl> - { <nl> - public boolean hasNext ( ) <nl> - { <nl> - return rowIterator . hasNext ( ) ; <nl> - } <nl> - <nl> - public Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > next ( ) <nl> - { <nl> - return rowIterator . next ( ) ; <nl> - } <nl> - <nl> - public void remove ( ) <nl> - { <nl> - throw new UnsupportedOperationException ( " Cannot remove an element on this iterator ! " ) ; <nl> - <nl> - } <nl> - } ; <nl> - } <nl> - <nl> - / * * <nl> - * @ throws NotImplementedException Always throws this exception , this operation does not make sense in this implementation . <nl> - * / <nl> - @ Override <nl> - public boolean nextKeyValue ( ) throws IOException <nl> - { <nl> - throw new UnsupportedOperationException ( " Calling method nextKeyValue ( ) does not make sense in this implementation " ) ; <nl> - } <nl> - <nl> - / * * <nl> - * @ throws NotImplementedException Always throws this exception , this operation does not make sense in this implementation . <nl> - * / <nl> - @ Override <nl> - public boolean next ( Map < String , ByteBuffer > keys , Map < String , ByteBuffer > value ) throws IOException <nl> - { <nl> - throw new UnsupportedOperationException ( " Calling method next ( ) does not make sense in this implementation " ) ; <nl> - } <nl> - }
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / tools / SSTableExport . java b / src / java / org / apache / cassandra / tools / SSTableExport . java <nl> new file mode 100644 <nl> index 0000000 . . e5aca7d <nl> - - - / dev / null <nl> + + + b / src / java / org / apache / cassandra / tools / SSTableExport . java <nl> @ @ - 0 , 0 + 1 , 420 @ @ <nl> + / * * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , software <nl> + * distributed under the License is distributed on an " AS IS " BASIS , <nl> + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> + * See the License for the specific language governing permissions and <nl> + * limitations under the License . <nl> + * / <nl> + <nl> + package org . apache . cassandra . tools ; <nl> + <nl> + import java . io . File ; <nl> + import java . io . IOException ; <nl> + import java . io . PrintStream ; <nl> + import java . nio . ByteBuffer ; <nl> + import java . util . * ; <nl> + <nl> + import org . apache . cassandra . config . CFMetaData ; <nl> + import org . apache . cassandra . db . * ; <nl> + import org . apache . cassandra . db . marshal . AbstractType ; <nl> + import org . apache . cassandra . io . util . BufferedRandomAccessFile ; <nl> + import org . apache . cassandra . service . StorageService ; <nl> + <nl> + import org . apache . commons . cli . * ; <nl> + <nl> + import org . apache . cassandra . config . ConfigurationException ; <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + import org . apache . cassandra . dht . IPartitioner ; <nl> + import org . apache . cassandra . io . sstable . * ; <nl> + import org . apache . cassandra . utils . ByteBufferUtil ; <nl> + import org . apache . cassandra . utils . Pair ; <nl> + <nl> + import static org . apache . cassandra . utils . ByteBufferUtil . bytesToHex ; <nl> + import static org . apache . cassandra . utils . ByteBufferUtil . hexToBytes ; <nl> + <nl> + / * * <nl> + * Export SSTables to JSON format . <nl> + * / <nl> + public class SSTableExport <nl> + { <nl> + / / size of the columns page <nl> + private static final int PAGE _ SIZE = 1000 ; <nl> + <nl> + private static final String KEY _ OPTION = " k " ; <nl> + private static final String EXCLUDEKEY _ OPTION = " x " ; <nl> + private static final String ENUMERATEKEYS _ OPTION = " e " ; <nl> + private static Options options ; <nl> + private static CommandLine cmd ; <nl> + <nl> + static <nl> + { <nl> + options = new Options ( ) ; <nl> + <nl> + Option optKey = new Option ( KEY _ OPTION , true , " Row key " ) ; <nl> + / / Number of times - k < key > can be passed on the command line . <nl> + optKey . setArgs ( 500 ) ; <nl> + options . addOption ( optKey ) ; <nl> + <nl> + Option excludeKey = new Option ( EXCLUDEKEY _ OPTION , true , " Excluded row key " ) ; <nl> + / / Number of times - x < key > can be passed on the command line . <nl> + excludeKey . setArgs ( 500 ) ; <nl> + options . addOption ( excludeKey ) ; <nl> + <nl> + Option optEnumerate = new Option ( ENUMERATEKEYS _ OPTION , false , " enumerate keys only " ) ; <nl> + options . addOption ( optEnumerate ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Wraps given string into quotes <nl> + * @ param val string to quote <nl> + * @ return quoted string <nl> + * / <nl> + private static String quote ( String val ) <nl> + { <nl> + return String . format ( " \ " % s \ " " , val ) ; <nl> + } <nl> + <nl> + / * * <nl> + * JSON Hash Key serializer <nl> + * @ param val value to set as a key <nl> + * @ return JSON Hash key <nl> + * / <nl> + private static String asKey ( String val ) <nl> + { <nl> + return String . format ( " % s : " , quote ( val ) ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Serialize columns using given column iterator <nl> + * @ param columns column iterator <nl> + * @ param out output stream <nl> + * @ param comparator columns comparator <nl> + * @ param cfMetaData Column Family metadata ( to get validator ) <nl> + * @ return pair of ( number of columns serialized , last column serialized ) <nl> + * / <nl> + private static void serializeColumns ( Iterator < IColumn > columns , PrintStream out , AbstractType comparator , CFMetaData cfMetaData ) <nl> + { <nl> + while ( columns . hasNext ( ) ) <nl> + { <nl> + IColumn column = columns . next ( ) ; <nl> + serializeColumn ( column , out , comparator , cfMetaData ) ; <nl> + <nl> + if ( columns . hasNext ( ) ) <nl> + out . print ( " , " ) ; <nl> + } <nl> + } <nl> + <nl> + / * * <nl> + * Serialize a given column to the JSON format <nl> + * @ param column column presentation <nl> + * @ param out output stream <nl> + * @ param comparator columns comparator <nl> + * @ param cfMetaData Column Family metadata ( to get validator ) <nl> + * / <nl> + private static void serializeColumn ( IColumn column , PrintStream out , AbstractType comparator , CFMetaData cfMetaData ) <nl> + { <nl> + ByteBuffer name = ByteBufferUtil . clone ( column . name ( ) ) ; <nl> + ByteBuffer value = ByteBufferUtil . clone ( column . value ( ) ) ; <nl> + AbstractType validator = cfMetaData . getValueValidator ( name ) ; <nl> + <nl> + out . print ( " [ " ) ; <nl> + out . print ( quote ( comparator . getString ( name ) ) ) ; <nl> + out . print ( " , " ) ; <nl> + out . print ( quote ( validator . getString ( value ) ) ) ; <nl> + out . print ( " , " ) ; <nl> + out . print ( column . timestamp ( ) ) ; <nl> + <nl> + if ( column instanceof DeletedColumn ) <nl> + { <nl> + out . print ( " , " ) ; <nl> + out . print ( " \ " d \ " " ) ; <nl> + } <nl> + else if ( column instanceof ExpiringColumn ) <nl> + { <nl> + out . print ( " , " ) ; <nl> + out . print ( " \ " e \ " " ) ; <nl> + out . print ( " , " ) ; <nl> + out . print ( ( ( ExpiringColumn ) column ) . getTimeToLive ( ) ) ; <nl> + out . print ( " , " ) ; <nl> + out . print ( column . getLocalDeletionTime ( ) ) ; <nl> + } <nl> + else if ( column instanceof CounterColumn ) <nl> + { <nl> + out . print ( " , " ) ; <nl> + out . print ( " \ " c \ " " ) ; <nl> + out . print ( " , " ) ; <nl> + out . print ( ( ( CounterColumn ) column ) . timestampOfLastDelete ( ) ) ; <nl> + } <nl> + <nl> + out . print ( " ] " ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Get portion of the columns and serialize in loop while not more columns left in the row <nl> + * @ param row SSTableIdentityIterator row representation with Column Family <nl> + * @ param key Decorated Key for the required row <nl> + * @ param out output stream <nl> + * / <nl> + private static void serializeRow ( SSTableIdentityIterator row , DecoratedKey key , PrintStream out ) <nl> + { <nl> + ColumnFamily columnFamily = row . getColumnFamily ( ) ; <nl> + boolean isSuperCF = columnFamily . isSuper ( ) ; <nl> + CFMetaData cfMetaData = columnFamily . metadata ( ) ; <nl> + AbstractType comparator = columnFamily . getComparator ( ) ; <nl> + <nl> + out . print ( asKey ( bytesToHex ( key . key ) ) ) ; <nl> + out . print ( isSuperCF ? " { " : " [ " ) ; <nl> + <nl> + if ( isSuperCF ) <nl> + { <nl> + while ( row . hasNext ( ) ) <nl> + { <nl> + IColumn column = row . next ( ) ; <nl> + <nl> + out . print ( asKey ( comparator . getString ( column . name ( ) ) ) ) ; <nl> + out . print ( " { " ) ; <nl> + out . print ( asKey ( " deletedAt " ) ) ; <nl> + out . print ( column . getMarkedForDeleteAt ( ) ) ; <nl> + out . print ( " , " ) ; <nl> + out . print ( asKey ( " subColumns " ) ) ; <nl> + out . print ( " [ " ) ; <nl> + serializeColumns ( column . getSubColumns ( ) . iterator ( ) , out , columnFamily . getSubComparator ( ) , cfMetaData ) ; <nl> + out . print ( " ] " ) ; <nl> + out . print ( " } " ) ; <nl> + <nl> + if ( row . hasNext ( ) ) <nl> + out . print ( " , " ) ; <nl> + } <nl> + } <nl> + else <nl> + { <nl> + serializeColumns ( row , out , comparator , cfMetaData ) ; <nl> + } <nl> + <nl> + out . print ( isSuperCF ? " } " : " ] " ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Enumerate row keys from an SSTableReader and write the result to a PrintStream . <nl> + * <nl> + * @ param ssTableFile the file to export the rows from <nl> + * @ param outs PrintStream to write the output to <nl> + * @ throws IOException on failure to read / write input / output <nl> + * / <nl> + public static void enumeratekeys ( String ssTableFile , PrintStream outs ) <nl> + throws IOException <nl> + { <nl> + Descriptor desc = Descriptor . fromFilename ( ssTableFile ) ; <nl> + KeyIterator iter = new KeyIterator ( desc ) ; <nl> + DecoratedKey lastKey = null ; <nl> + while ( iter . hasNext ( ) ) <nl> + { <nl> + DecoratedKey key = iter . next ( ) ; <nl> + <nl> + / / validate order of the keys in the sstable <nl> + if ( lastKey ! = null & & lastKey . compareTo ( key ) > 0 ) <nl> + throw new IOException ( " Key out of order ! " + lastKey + " > " + key ) ; <nl> + lastKey = key ; <nl> + <nl> + outs . println ( bytesToHex ( key . key ) ) ; <nl> + } <nl> + iter . close ( ) ; <nl> + outs . flush ( ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Export specific rows from an SSTable and write the resulting JSON to a PrintStream . <nl> + * <nl> + * @ param ssTableFile the SSTableScanner to export the rows from <nl> + * @ param outs PrintStream to write the output to <nl> + * @ param toExport the keys corresponding to the rows to export <nl> + * @ param excludes keys to exclude from export <nl> + * @ throws IOException on failure to read / write input / output <nl> + * / <nl> + public static void export ( String ssTableFile , PrintStream outs , Collection < String > toExport , String [ ] excludes ) throws IOException <nl> + { <nl> + SSTableReader reader = SSTableReader . open ( Descriptor . fromFilename ( ssTableFile ) ) ; <nl> + SSTableScanner scanner = reader . getDirectScanner ( BufferedRandomAccessFile . DEFAULT _ BUFFER _ SIZE ) ; <nl> + <nl> + IPartitioner < ? > partitioner = StorageService . getPartitioner ( ) ; <nl> + <nl> + if ( excludes ! = null ) <nl> + toExport . removeAll ( Arrays . asList ( excludes ) ) ; <nl> + <nl> + outs . println ( " { " ) ; <nl> + <nl> + int i = 0 ; <nl> + <nl> + / / last key to compare order <nl> + DecoratedKey lastKey = null ; <nl> + <nl> + for ( String key : toExport ) <nl> + { <nl> + DecoratedKey decoratedKey = partitioner . decorateKey ( hexToBytes ( key ) ) ; <nl> + <nl> + if ( lastKey ! = null & & lastKey . compareTo ( decoratedKey ) > 0 ) <nl> + throw new IOException ( " Key out of order ! " + lastKey + " > " + decoratedKey ) ; <nl> + <nl> + lastKey = decoratedKey ; <nl> + <nl> + scanner . seekTo ( decoratedKey ) ; <nl> + <nl> + if ( ! scanner . hasNext ( ) ) <nl> + continue ; <nl> + <nl> + SSTableIdentityIterator row = ( SSTableIdentityIterator ) scanner . next ( ) ; <nl> + if ( ! row . getKey ( ) . equals ( decoratedKey ) ) <nl> + continue ; <nl> + <nl> + serializeRow ( row , decoratedKey , outs ) ; <nl> + <nl> + if ( i ! = 0 ) <nl> + outs . println ( " , " ) ; <nl> + <nl> + i + + ; <nl> + } <nl> + <nl> + outs . println ( " \ n } " ) ; <nl> + outs . flush ( ) ; <nl> + <nl> + scanner . close ( ) ; <nl> + } <nl> + <nl> + / / This is necessary to accommodate the test suite since you cannot open a Reader more <nl> + / / than once from within the same process . <nl> + static void export ( SSTableReader reader , PrintStream outs , String [ ] excludes ) throws IOException <nl> + { <nl> + Set < String > excludeSet = new HashSet < String > ( ) ; <nl> + <nl> + if ( excludes ! = null ) <nl> + excludeSet = new HashSet < String > ( Arrays . asList ( excludes ) ) ; <nl> + <nl> + <nl> + SSTableIdentityIterator row ; <nl> + SSTableScanner scanner = reader . getDirectScanner ( BufferedRandomAccessFile . DEFAULT _ BUFFER _ SIZE ) ; <nl> + <nl> + outs . println ( " { " ) ; <nl> + <nl> + int i = 0 ; <nl> + <nl> + / / collecting keys to export <nl> + while ( scanner . hasNext ( ) ) <nl> + { <nl> + row = ( SSTableIdentityIterator ) scanner . next ( ) ; <nl> + <nl> + String currentKey = bytesToHex ( row . getKey ( ) . key ) ; <nl> + <nl> + if ( excludeSet . contains ( currentKey ) ) <nl> + continue ; <nl> + else if ( i ! = 0 ) <nl> + outs . println ( " , " ) ; <nl> + <nl> + serializeRow ( row , row . getKey ( ) , outs ) ; <nl> + <nl> + i + + ; <nl> + } <nl> + <nl> + outs . println ( " \ n } " ) ; <nl> + outs . flush ( ) ; <nl> + <nl> + scanner . close ( ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Export an SSTable and write the resulting JSON to a PrintStream . <nl> + * <nl> + * @ param ssTableFile the SSTable to export <nl> + * @ param outs PrintStream to write the output to <nl> + * @ param excludes keys to exclude from export <nl> + * <nl> + * @ throws IOException on failure to read / write input / output <nl> + * / <nl> + public static void export ( String ssTableFile , PrintStream outs , String [ ] excludes ) throws IOException <nl> + { <nl> + export ( SSTableReader . open ( Descriptor . fromFilename ( ssTableFile ) ) , outs , excludes ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Export an SSTable and write the resulting JSON to standard out . <nl> + * <nl> + * @ param ssTableFile SSTable to export <nl> + * @ param excludes keys to exclude from export <nl> + * <nl> + * @ throws IOException on failure to read / write SSTable / standard out <nl> + * / <nl> + public static void export ( String ssTableFile , String [ ] excludes ) throws IOException <nl> + { <nl> + export ( ssTableFile , System . out , excludes ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Given arguments specifying an SSTable , and optionally an output file , <nl> + * export the contents of the SSTable to JSON . <nl> + * <nl> + * @ param args command lines arguments <nl> + * <nl> + * @ throws IOException on failure to open / read / write files or output streams <nl> + * @ throws ConfigurationException on configuration failure ( wrong params given ) <nl> + * / <nl> + public static void main ( String [ ] args ) throws IOException , ConfigurationException <nl> + { <nl> + String usage = String . format ( " Usage : % s < sstable > [ - k key [ - k key [ . . . ] ] - x key [ - x key [ . . . ] ] ] % n " , SSTableExport . class . getName ( ) ) ; <nl> + <nl> + CommandLineParser parser = new PosixParser ( ) ; <nl> + try <nl> + { <nl> + cmd = parser . parse ( options , args ) ; <nl> + } <nl> + catch ( ParseException e1 ) <nl> + { <nl> + System . err . println ( e1 . getMessage ( ) ) ; <nl> + System . err . println ( usage ) ; <nl> + System . exit ( 1 ) ; <nl> + } <nl> + <nl> + <nl> + if ( cmd . getArgs ( ) . length ! = 1 ) <nl> + { <nl> + System . err . println ( " You must supply exactly one sstable " ) ; <nl> + System . err . println ( usage ) ; <nl> + System . exit ( 1 ) ; <nl> + } <nl> + <nl> + <nl> + String [ ] keys = cmd . getOptionValues ( KEY _ OPTION ) ; <nl> + String [ ] excludes = cmd . getOptionValues ( EXCLUDEKEY _ OPTION ) ; <nl> + String ssTableFileName = new File ( cmd . getArgs ( ) [ 0 ] ) . getAbsolutePath ( ) ; <nl> + <nl> + DatabaseDescriptor . loadSchemas ( ) ; <nl> + if ( DatabaseDescriptor . getNonSystemTables ( ) . size ( ) < 1 ) <nl> + { <nl> + String msg = " no non - system tables are defined " ; <nl> + System . err . println ( msg ) ; <nl> + throw new ConfigurationException ( msg ) ; <nl> + } <nl> + <nl> + if ( cmd . hasOption ( ENUMERATEKEYS _ OPTION ) ) <nl> + { <nl> + enumeratekeys ( ssTableFileName , System . out ) ; <nl> + } <nl> + else <nl> + { <nl> + if ( ( keys ! = null ) & & ( keys . length > 0 ) ) <nl> + export ( ssTableFileName , System . out , Arrays . asList ( keys ) , excludes ) ; <nl> + else <nl> + export ( ssTableFileName , excludes ) ; <nl> + } <nl> + <nl> + System . exit ( 0 ) ; <nl> + } <nl> + } <nl> diff - - git a / src / java / org / apache / cassandra / tools / SSTableImport . java b / src / java / org / apache / cassandra / tools / SSTableImport . java <nl> new file mode 100644 <nl> index 0000000 . . 1b53563 <nl> - - - / dev / null <nl> + + + b / src / java / org / apache / cassandra / tools / SSTableImport . java <nl> @ @ - 0 , 0 + 1 , 528 @ @ <nl> + / * * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , software <nl> + * distributed under the License is distributed on an " AS IS " BASIS , <nl> + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> + * See the License for the specific language governing permissions and <nl> + * limitations under the License . <nl> + * / <nl> + <nl> + package org . apache . cassandra . tools ; <nl> + <nl> + import java . io . File ; <nl> + import java . io . IOException ; <nl> + import java . nio . ByteBuffer ; <nl> + import java . util . * ; <nl> + <nl> + import org . apache . cassandra . db . marshal . AbstractType ; <nl> + import org . apache . cassandra . db . marshal . BytesType ; <nl> + import org . apache . cassandra . db . marshal . MarshalException ; <nl> + import org . apache . commons . cli . * ; <nl> + <nl> + import org . apache . cassandra . config . CFMetaData ; <nl> + import org . apache . cassandra . config . ConfigurationException ; <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + import org . apache . cassandra . db . * ; <nl> + import org . apache . cassandra . db . filter . QueryPath ; <nl> + import org . apache . cassandra . dht . IPartitioner ; <nl> + import org . apache . cassandra . io . sstable . SSTableWriter ; <nl> + import org . codehaus . jackson . type . TypeReference ; <nl> + <nl> + import org . codehaus . jackson . JsonFactory ; <nl> + import org . codehaus . jackson . map . MappingJsonFactory ; <nl> + <nl> + import org . codehaus . jackson . JsonParser ; <nl> + <nl> + import static org . apache . cassandra . utils . ByteBufferUtil . hexToBytes ; <nl> + <nl> + / * * <nl> + * Create SSTables from JSON input <nl> + * / <nl> + public class SSTableImport <nl> + { <nl> + private static final String KEYSPACE _ OPTION = " K " ; <nl> + private static final String COLUMN _ FAMILY _ OPTION = " c " ; <nl> + private static final String KEY _ COUNT _ OPTION = " n " ; <nl> + private static final String IS _ SORTED _ OPTION = " s " ; <nl> + <nl> + private static Options options ; <nl> + private static CommandLine cmd ; <nl> + <nl> + private static Integer keyCountToImport = null ; <nl> + private static boolean isSorted = false ; <nl> + <nl> + private static JsonFactory factory = new MappingJsonFactory ( ) ; <nl> + <nl> + static <nl> + { <nl> + options = new Options ( ) ; <nl> + <nl> + Option optKeyspace = new Option ( KEYSPACE _ OPTION , true , " Keyspace name . " ) ; <nl> + optKeyspace . setRequired ( true ) ; <nl> + options . addOption ( optKeyspace ) ; <nl> + <nl> + Option optColfamily = new Option ( COLUMN _ FAMILY _ OPTION , true , " Column Family name . " ) ; <nl> + optColfamily . setRequired ( true ) ; <nl> + options . addOption ( optColfamily ) ; <nl> + <nl> + options . addOption ( new Option ( KEY _ COUNT _ OPTION , true , " Number of keys to import ( Optional ) . " ) ) ; <nl> + options . addOption ( new Option ( IS _ SORTED _ OPTION , false , " Assume JSON file as already sorted ( e . g . created by sstable2json tool ) ( Optional ) . " ) ) ; <nl> + } <nl> + <nl> + private static class JsonColumn < T > <nl> + { <nl> + private ByteBuffer name ; <nl> + private ByteBuffer value ; <nl> + private long timestamp ; <nl> + <nl> + private String kind ; <nl> + / / Expiring columns <nl> + private int ttl ; <nl> + private int localExpirationTime ; <nl> + <nl> + / / Counter columns <nl> + private long timestampOfLastDelete ; <nl> + <nl> + public JsonColumn ( T json , CFMetaData meta , boolean isSubColumn ) <nl> + { <nl> + AbstractType comparator = ( isSubColumn ) ? meta . subcolumnComparator : meta . comparator ; <nl> + <nl> + if ( json instanceof List ) <nl> + { <nl> + List fields = ( List < ? > ) json ; <nl> + <nl> + assert fields . size ( ) > = 3 : " Column definition should have at least 3 " ; <nl> + <nl> + name = stringAsType ( ( String ) fields . get ( 0 ) , comparator ) ; <nl> + value = stringAsType ( ( String ) fields . get ( 1 ) , meta . getValueValidator ( name . duplicate ( ) ) ) ; <nl> + timestamp = ( Long ) fields . get ( 2 ) ; <nl> + kind = " " ; <nl> + <nl> + if ( fields . size ( ) > 3 ) <nl> + { <nl> + if ( fields . get ( 3 ) instanceof Boolean ) <nl> + { <nl> + / / old format , reading this for backward compatibility sake <nl> + if ( fields . size ( ) = = 6 ) <nl> + { <nl> + kind = " e " ; <nl> + ttl = ( Integer ) fields . get ( 4 ) ; <nl> + localExpirationTime = ( int ) ( long ) ( ( Long ) fields . get ( 5 ) ) ; <nl> + } <nl> + else <nl> + { <nl> + kind = ( ( Boolean ) fields . get ( 3 ) ) ? " d " : " " ; <nl> + } <nl> + } <nl> + else <nl> + { <nl> + kind = ( String ) fields . get ( 3 ) ; <nl> + if ( isExpiring ( ) ) <nl> + { <nl> + ttl = ( Integer ) fields . get ( 4 ) ; <nl> + localExpirationTime = ( int ) ( long ) ( ( Long ) fields . get ( 5 ) ) ; <nl> + } <nl> + else if ( isCounter ( ) ) <nl> + { <nl> + timestampOfLastDelete = ( long ) ( ( Integer ) fields . get ( 4 ) ) ; <nl> + } <nl> + } <nl> + } <nl> + } <nl> + } <nl> + <nl> + public boolean isDeleted ( ) <nl> + { <nl> + return kind . equals ( " d " ) ; <nl> + } <nl> + <nl> + public boolean isExpiring ( ) <nl> + { <nl> + return kind . equals ( " e " ) ; <nl> + } <nl> + <nl> + public boolean isCounter ( ) <nl> + { <nl> + return kind . equals ( " c " ) ; <nl> + } <nl> + <nl> + public ByteBuffer getName ( ) <nl> + { <nl> + return name . duplicate ( ) ; <nl> + } <nl> + <nl> + public ByteBuffer getValue ( ) <nl> + { <nl> + return value . duplicate ( ) ; <nl> + } <nl> + } <nl> + <nl> + private static void addToStandardCF ( List < ? > row , ColumnFamily cfamily ) <nl> + { <nl> + addColumnsToCF ( row , null , cfamily ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Add columns to a column family . <nl> + * <nl> + * @ param row the columns associated with a row <nl> + * @ param superName name of the super column if any <nl> + * @ param cfamily the column family to add columns to <nl> + * / <nl> + private static void addColumnsToCF ( List < ? > row , ByteBuffer superName , ColumnFamily cfamily ) <nl> + { <nl> + CFMetaData cfm = cfamily . metadata ( ) ; <nl> + assert cfm ! = null ; <nl> + <nl> + for ( Object c : row ) <nl> + { <nl> + JsonColumn col = new JsonColumn < List > ( ( List ) c , cfm , ( superName ! = null ) ) ; <nl> + QueryPath path = new QueryPath ( cfm . cfName , superName , col . getName ( ) ) ; <nl> + <nl> + if ( col . isExpiring ( ) ) <nl> + { <nl> + cfamily . addColumn ( null , new ExpiringColumn ( col . getName ( ) , col . getValue ( ) , col . timestamp , col . ttl , col . localExpirationTime ) ) ; <nl> + } <nl> + else if ( col . isCounter ( ) ) <nl> + { <nl> + cfamily . addColumn ( null , new CounterColumn ( col . getName ( ) , col . getValue ( ) , col . timestamp , col . timestampOfLastDelete ) ) ; <nl> + } <nl> + else if ( col . isDeleted ( ) ) <nl> + { <nl> + cfamily . addTombstone ( path , col . getValue ( ) , col . timestamp ) ; <nl> + } <nl> + else <nl> + { <nl> + cfamily . addColumn ( path , col . getValue ( ) , col . timestamp ) ; <nl> + } <nl> + } <nl> + } <nl> + <nl> + / * * <nl> + * Add super columns to a column family . <nl> + * <nl> + * @ param row the super columns associated with a row <nl> + * @ param cfamily the column family to add columns to <nl> + * / <nl> + private static void addToSuperCF ( Map < ? , ? > row , ColumnFamily cfamily ) <nl> + { <nl> + CFMetaData metaData = cfamily . metadata ( ) ; <nl> + assert metaData ! = null ; <nl> + <nl> + AbstractType comparator = metaData . comparator ; <nl> + <nl> + / / Super columns <nl> + for ( Map . Entry < ? , ? > entry : row . entrySet ( ) ) <nl> + { <nl> + Map < ? , ? > data = ( Map < ? , ? > ) entry . getValue ( ) ; <nl> + <nl> + addColumnsToCF ( ( List < ? > ) data . get ( " subColumns " ) , stringAsType ( ( String ) entry . getKey ( ) , comparator ) , cfamily ) ; <nl> + <nl> + / / * WARNING * markForDeleteAt has been DEPRECATED at Cassandra side <nl> + / / BigInteger deletedAt = ( BigInteger ) data . get ( " deletedAt " ) ; <nl> + / / SuperColumn superColumn = ( SuperColumn ) cfamily . getColumn ( superName ) ; <nl> + / / superColumn . markForDeleteAt ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , deletedAt ) ; <nl> + } <nl> + } <nl> + <nl> + / * * <nl> + * Convert a JSON formatted file to an SSTable . <nl> + * <nl> + * @ param jsonFile the file containing JSON formatted data <nl> + * @ param keyspace keyspace the data belongs to <nl> + * @ param cf column family the data belongs to <nl> + * @ param ssTablePath file to write the SSTable to <nl> + * <nl> + * @ throws IOException for errors reading / writing input / output <nl> + * / <nl> + public static void importJson ( String jsonFile , String keyspace , String cf , String ssTablePath ) throws IOException <nl> + { <nl> + ColumnFamily columnFamily = ColumnFamily . create ( keyspace , cf ) ; <nl> + IPartitioner < ? > partitioner = DatabaseDescriptor . getPartitioner ( ) ; <nl> + <nl> + int importedKeys = ( isSorted ) ? importSorted ( jsonFile , columnFamily , ssTablePath , partitioner ) <nl> + : importUnsorted ( getParser ( jsonFile ) , columnFamily , ssTablePath , partitioner ) ; <nl> + <nl> + if ( importedKeys ! = - 1 ) <nl> + System . out . printf ( " % d keys imported successfully . % n " , importedKeys ) ; <nl> + } <nl> + <nl> + private static int importUnsorted ( JsonParser parser , ColumnFamily columnFamily , String ssTablePath , IPartitioner < ? > partitioner ) throws IOException <nl> + { <nl> + int importedKeys = 0 ; <nl> + long start = System . currentTimeMillis ( ) ; <nl> + Map < ? , ? > data = parser . readValueAs ( new TypeReference < Map < ? , ? > > ( ) { } ) ; <nl> + <nl> + keyCountToImport = ( keyCountToImport = = null ) ? data . size ( ) : keyCountToImport ; <nl> + SSTableWriter writer = new SSTableWriter ( ssTablePath , keyCountToImport ) ; <nl> + <nl> + System . out . printf ( " Importing % s keys . . . % n " , keyCountToImport ) ; <nl> + <nl> + / / sort by dk representation , but hold onto the hex version <nl> + SortedMap < DecoratedKey , String > decoratedKeys = new TreeMap < DecoratedKey , String > ( ) ; <nl> + <nl> + for ( Object keyObject : data . keySet ( ) ) <nl> + { <nl> + String key = ( String ) keyObject ; <nl> + decoratedKeys . put ( partitioner . decorateKey ( hexToBytes ( key ) ) , key ) ; <nl> + } <nl> + <nl> + for ( Map . Entry < DecoratedKey , String > rowKey : decoratedKeys . entrySet ( ) ) <nl> + { <nl> + if ( columnFamily . getColumnFamilyType ( ) = = ColumnFamilyType . Super ) <nl> + { <nl> + addToSuperCF ( ( Map < ? , ? > ) data . get ( rowKey . getValue ( ) ) , columnFamily ) ; <nl> + } <nl> + else <nl> + { <nl> + addToStandardCF ( ( List < ? > ) data . get ( rowKey . getValue ( ) ) , columnFamily ) ; <nl> + } <nl> + <nl> + writer . append ( rowKey . getKey ( ) , columnFamily ) ; <nl> + columnFamily . clear ( ) ; <nl> + <nl> + importedKeys + + ; <nl> + <nl> + long current = System . currentTimeMillis ( ) ; <nl> + <nl> + if ( current - start > = 5000 ) / / 5 secs . <nl> + { <nl> + System . out . printf ( " Currently imported % d keys . % n " , importedKeys ) ; <nl> + start = current ; <nl> + } <nl> + <nl> + if ( keyCountToImport = = importedKeys ) <nl> + break ; <nl> + } <nl> + <nl> + writer . closeAndOpenReader ( ) ; <nl> + <nl> + return importedKeys ; <nl> + } <nl> + <nl> + public static int importSorted ( String jsonFile , ColumnFamily columnFamily , String ssTablePath , IPartitioner < ? > partitioner ) throws IOException <nl> + { <nl> + int importedKeys = 0 ; / / already imported keys count <nl> + long start = System . currentTimeMillis ( ) ; <nl> + <nl> + JsonParser parser = getParser ( jsonFile ) ; <nl> + <nl> + if ( keyCountToImport = = null ) <nl> + { <nl> + keyCountToImport = 0 ; <nl> + System . out . println ( " Counting keys to import , please wait . . . ( NOTE : to skip this use - n < num _ keys > ) " ) ; <nl> + <nl> + parser . nextToken ( ) ; / / START _ OBJECT <nl> + while ( parser . nextToken ( ) ! = null ) <nl> + { <nl> + parser . nextToken ( ) ; <nl> + parser . skipChildren ( ) ; <nl> + if ( parser . getCurrentName ( ) = = null ) continue ; <nl> + <nl> + keyCountToImport + + ; <nl> + } <nl> + } <nl> + <nl> + System . out . printf ( " Importing % s keys . . . % n " , keyCountToImport ) ; <nl> + <nl> + parser = getParser ( jsonFile ) ; / / renewing parser <nl> + SSTableWriter writer = new SSTableWriter ( ssTablePath , keyCountToImport ) ; <nl> + <nl> + int lineNumber = 1 ; <nl> + DecoratedKey prevStoredKey = null ; <nl> + <nl> + while ( parser . nextToken ( ) ! = null ) <nl> + { <nl> + String key = parser . getCurrentName ( ) ; <nl> + <nl> + if ( key ! = null ) <nl> + { <nl> + String tokenName = parser . nextToken ( ) . name ( ) ; <nl> + <nl> + if ( tokenName . equals ( " START _ ARRAY " ) ) <nl> + { <nl> + if ( columnFamily . getColumnFamilyType ( ) = = ColumnFamilyType . Super ) <nl> + { <nl> + throw new RuntimeException ( " Can ' t write Standard columns to the Super Column Family . " ) ; <nl> + } <nl> + <nl> + List < ? > columns = parser . readValueAs ( new TypeReference < List < ? > > ( ) { } ) ; <nl> + addToStandardCF ( columns , columnFamily ) ; <nl> + } <nl> + else if ( tokenName . equals ( " START _ OBJECT " ) ) <nl> + { <nl> + if ( columnFamily . getColumnFamilyType ( ) = = ColumnFamilyType . Standard ) <nl> + { <nl> + throw new RuntimeException ( " Can ' t write Super columns to the Standard Column Family . " ) ; <nl> + } <nl> + <nl> + Map < ? , ? > columns = parser . readValueAs ( new TypeReference < Map < ? , ? > > ( ) { } ) ; <nl> + addToSuperCF ( columns , columnFamily ) ; <nl> + } <nl> + else <nl> + { <nl> + throw new UnsupportedOperationException ( " Only Array or Hash allowed as row content . " ) ; <nl> + } <nl> + <nl> + DecoratedKey currentKey = partitioner . decorateKey ( hexToBytes ( key ) ) ; <nl> + <nl> + if ( prevStoredKey ! = null & & prevStoredKey . compareTo ( currentKey ) ! = - 1 ) <nl> + { <nl> + System . err . printf ( " Line % d : Key % s is greater than previous , collection is not sorted properly . Aborting import . You might need to delete SSTables manually . % n " , lineNumber , key ) ; <nl> + return - 1 ; <nl> + } <nl> + <nl> + / / saving decorated key <nl> + writer . append ( currentKey , columnFamily ) ; <nl> + columnFamily . clear ( ) ; <nl> + <nl> + prevStoredKey = currentKey ; <nl> + importedKeys + + ; <nl> + lineNumber + + ; <nl> + <nl> + long current = System . currentTimeMillis ( ) ; <nl> + <nl> + if ( current - start > = 5000 ) / / 5 secs . <nl> + { <nl> + System . out . printf ( " Currently imported % d keys . % n " , importedKeys ) ; <nl> + start = current ; <nl> + } <nl> + <nl> + if ( keyCountToImport = = importedKeys ) <nl> + break ; <nl> + } <nl> + } <nl> + <nl> + writer . closeAndOpenReader ( ) ; <nl> + <nl> + return importedKeys ; <nl> + } <nl> + <nl> + / * * <nl> + * Get JsonParser object for file <nl> + * @ param fileName name of the file <nl> + * @ return json parser instance for given file <nl> + * @ throws IOException if any I / O error . <nl> + * / <nl> + private static JsonParser getParser ( String fileName ) throws IOException <nl> + { <nl> + return factory . createJsonParser ( new File ( fileName ) ) . configure ( JsonParser . Feature . INTERN _ FIELD _ NAMES , false ) ; <nl> + } <nl> + <nl> + / * * <nl> + * Converts JSON to an SSTable file . JSON input can either be a file specified <nl> + * using an optional command line argument , or supplied on standard in . <nl> + * <nl> + * @ param args command line arguments <nl> + * @ throws IOException on failure to open / read / write files or output streams <nl> + * @ throws ParseException on failure to parse JSON input <nl> + * @ throws ConfigurationException on configuration error . <nl> + * / <nl> + public static void main ( String [ ] args ) throws IOException , ParseException , ConfigurationException <nl> + { <nl> + CommandLineParser parser = new PosixParser ( ) ; <nl> + <nl> + try <nl> + { <nl> + cmd = parser . parse ( options , args ) ; <nl> + } <nl> + catch ( org . apache . commons . cli . ParseException e ) <nl> + { <nl> + System . err . println ( e . getMessage ( ) ) ; <nl> + printProgramUsage ( ) ; <nl> + System . exit ( 1 ) ; <nl> + } <nl> + <nl> + if ( cmd . getArgs ( ) . length ! = 2 ) <nl> + { <nl> + printProgramUsage ( ) ; <nl> + System . exit ( 1 ) ; <nl> + } <nl> + <nl> + String json = cmd . getArgs ( ) [ 0 ] ; <nl> + String ssTable = cmd . getArgs ( ) [ 1 ] ; <nl> + String keyspace = cmd . getOptionValue ( KEYSPACE _ OPTION ) ; <nl> + String cfamily = cmd . getOptionValue ( COLUMN _ FAMILY _ OPTION ) ; <nl> + <nl> + if ( cmd . hasOption ( KEY _ COUNT _ OPTION ) ) <nl> + { <nl> + keyCountToImport = Integer . valueOf ( cmd . getOptionValue ( KEY _ COUNT _ OPTION ) ) ; <nl> + } <nl> + <nl> + if ( cmd . hasOption ( IS _ SORTED _ OPTION ) ) <nl> + { <nl> + isSorted = true ; <nl> + } <nl> + <nl> + DatabaseDescriptor . loadSchemas ( ) ; <nl> + if ( DatabaseDescriptor . getNonSystemTables ( ) . size ( ) < 1 ) <nl> + { <nl> + String msg = " no non - system tables are defined " ; <nl> + System . err . println ( msg ) ; <nl> + throw new ConfigurationException ( msg ) ; <nl> + } <nl> + <nl> + try <nl> + { <nl> + importJson ( json , keyspace , cfamily , ssTable ) ; <nl> + } <nl> + catch ( Exception e ) <nl> + { <nl> + e . printStackTrace ( ) ; <nl> + System . err . println ( " ERROR : " + e . getMessage ( ) ) ; <nl> + System . exit ( - 1 ) ; <nl> + } <nl> + <nl> + System . exit ( 0 ) ; <nl> + } <nl> + <nl> + private static void printProgramUsage ( ) <nl> + { <nl> + System . out . printf ( " Usage : % s - s - K < keyspace > - c < column _ family > - n < num _ keys > < json > < sstable > % n % n " , <nl> + SSTableImport . class . getName ( ) ) ; <nl> + <nl> + System . out . println ( " Options : " ) ; <nl> + for ( Object o : options . getOptions ( ) ) <nl> + { <nl> + Option opt = ( Option ) o ; <nl> + System . out . println ( " - " + opt . getOpt ( ) + " - " + opt . getDescription ( ) ) ; <nl> + } <nl> + } <nl> + <nl> + / * * <nl> + * Used by test framework to set key count <nl> + * @ param keyCount numbers of keys to import <nl> + * / <nl> + public static void setKeyCountToImport ( Integer keyCount ) <nl> + { <nl> + keyCountToImport = keyCount ; <nl> + } <nl> + <nl> + / * * <nl> + * Convert a string to bytes ( ByteBuffer ) according to type <nl> + * @ param content string to convert <nl> + * @ param type type to use for conversion <nl> + * @ return byte buffer representation of the given string <nl> + * / <nl> + private static ByteBuffer stringAsType ( String content , AbstractType type ) <nl> + { <nl> + try <nl> + { <nl> + return ( type = = BytesType . instance ) ? hexToBytes ( content ) : type . fromString ( content ) ; <nl> + } <nl> + catch ( MarshalException e ) <nl> + { <nl> + throw new RuntimeException ( e . getMessage ( ) ) ; <nl> + } <nl> + } <nl> + <nl> + }

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 5033722 . . b91711d 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 5 + 1 , 12 @ @ 
 2 . 1 . 0 - final 
 + * cqlsh DESC CLUSTER fails retrieving ring information ( CASSANDRA - 7687 ) 
 + * Fix binding null values inside UDT ( CASSANDRA - 7685 ) 
 + * Fix UDT field selection with empty fields ( CASSANDRA - 7670 ) 
 + * Bogus deserialization of static cells from sstable ( CASSANDRA - 7684 ) 
 Merged from 2 . 0 : 
 + * Remove CqlPagingRecordReader / CqlPagingInputFormat ( CASSANDRA - 7570 ) 
 + * Add stop method to EmbeddedCassandraService ( CASSANDRA - 7595 ) 
 + * Update java driver ( for hadoop ) ( CASSANDRA - 7618 ) 
 * Support connecting to ipv6 jmx with nodetool ( CASSANDRA - 7669 ) 
 
 
 @ @ - 114 , 61 + 121 , 6 @ @ Merged from 2 . 0 : 
 * Fix NPE when listing saved caches dir ( CASSANDRA - 7632 ) 
 
 
 - 2 . 0 . 9 
 - * Fix CC # collectTimeOrderedData ( ) tombstone optimisations ( CASSANDRA - 7394 ) 
 - * Fix assertion error in CL . ANY timeout handling ( CASSANDRA - 7364 ) 
 - * Handle empty CFs in Memtable # maybeUpdateLiveRatio ( ) ( CASSANDRA - 7401 ) 
 - * Fix native protocol CAS batches ( CASSANDRA - 7337 ) 
 - * Reduce likelihood of contention on local paxos locking ( CASSANDRA - 7359 ) 
 - * Upgrade to Pig 0 . 12 . 1 ( CASSANDRA - 6556 ) 
 - * Make sure we clear out repair sessions from netstats ( CASSANDRA - 7329 ) 
 - * Don ' t fail streams on failure detector downs ( CASSANDRA - 3569 ) 
 - * Add optional keyspace to DROP INDEX statement ( CASSANDRA - 7314 ) 
 - * Reduce run time for CQL tests ( CASSANDRA - 7327 ) 
 - * Fix heap size calculation on Windows ( CASSANDRA - 7352 , 7353 ) 
 - * RefCount native frames from netty ( CASSANDRA - 7245 ) 
 - * Use tarball dir instead of / var for default paths ( CASSANDRA - 7136 ) 
 - * Remove rows _ per _ partition _ to _ cache keyword ( CASSANDRA - 7193 ) 
 - * Fix schema change response in native protocol v3 ( CASSANDRA - 7413 ) 
 - Merged from 2 . 0 : 
 - * Fix assertion error in CL . ANY timeout handling ( CASSANDRA - 7364 ) 
 - * Add per - CF range read request latency metrics ( CASSANDRA - 7338 ) 
 - * Fix NPE in StreamTransferTask . createMessageForRetry ( ) ( CASSANDRA - 7323 ) 
 - * Make StreamSession # closeSession ( ) idempotent ( CASSANDRA - 7262 ) 
 - * Fix infinite loop on exception while streaming ( CASSANDRA - 7330 ) 
 - * Account for range tombstones in min / max column names ( CASSANDRA - 7235 ) 
 - * Improve sub range repair validation ( CASSANDRA - 7317 ) 
 - * Accept subtypes for function results , type casts ( CASSANDRA - 6766 ) 
 - * Support DISTINCT for static columns and fix behaviour when DISTINC is 
 - not use ( CASSANDRA - 7305 ) . 
 - * Refuse range queries with strict bounds on compact tables since they 
 - are broken ( CASSANDRA - 7059 ) 
 - Merged from 1 . 2 : 
 - * Expose global ColumnFamily metrics ( CASSANDRA - 7273 ) 
 - | | | | | | | merged common ancestors 
 - 1 . 2 . 17 
 - = = = = = = = 
 - 
 - 1 . 2 . 17 
 - > > > > > > > cassandra - 1 . 2 
 - * cqlsh : Fix CompositeType columns in DESCRIBE TABLE output ( CASSANDRA - 7399 ) 
 - * Expose global ColumnFamily metrics ( CASSANDRA - 7273 ) 
 - * Handle possible integer overflow in FastByteArrayOutputStream ( CASSANDRA - 7373 ) 
 - * cqlsh : ' ascii ' values weren ' t formatted as text ( CASSANDRA - 7407 ) 
 - * cqlsh : ignore . cassandra permission errors ( CASSANDRA - 7266 ) 
 - * reduce failure detector initial value to 2s ( CASSANDRA - 7307 ) 
 - * Fix problem truncating on a node that was previously in a dead state ( CASSANDRA - 7318 ) 
 - * Don ' t insert tombstones that hide indexed values into 2i ( CASSANDRA - 7268 ) 
 - * Track metrics at a keyspace level ( CASSANDRA - 6539 ) 
 - * Add replace _ address _ first _ boot flag to only replace if not bootstrapped 
 - ( CASSANDRA - 7356 ) 
 - * Enable keepalive for native protocol ( CASSANDRA - 7380 ) 
 - * Check internal addresses for seeds ( CASSANDRA - 6523 ) 
 - * Fix potential / by 0 in HHOM page size calculation ( CASSANDRA - 7354 ) 
 - * Use LOCAL _ ONE for non - superuser auth queries ( CASSANDRA - 7328 ) 
 - * Fix RangeTombstone copy bug ( CASSANDRA - 7371 ) 
 - 
 - 
 2 . 1 . 0 - rc1 
 * Revert flush directory ( CASSANDRA - 6357 ) 
 * More efficient executor service for fast operations ( CASSANDRA - 4718 ) 
 diff - - git a / NEWS . txt b / NEWS . txt 
 index b42db60 . . 79212f8 100644 
 - - - a / NEWS . txt 
 + + + b / NEWS . txt 
 @ @ - 75 , 7 + 75 , 10 @ @ Upgrading 
 = = = = 
 New features 
 - - - - - - - - - - - - 
 - - If you are using Leveled Compaction , you can now disable doing size - tiered 
 + - CqlPaginRecordReader and CqlPagingInputFormat have both been removed . 
 + Use CqlInputFormat instead . 
 + - If you are using Leveled Compaction , you can now disable doing 
 + size - tiered 
 compaction in L0 by starting Cassandra with - Dcassandra . disable _ stcs _ in _ l0 
 ( see CASSANDRA - 6621 for details ) . 
 - Shuffle and taketoken have been removed . For clusters that choose to 
 diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java 
 deleted file mode 100644 
 index 96f2f94 . . 0000000 
 - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingInputFormat . java 
 + + + / dev / null 
 @ @ - 1 , 85 + 0 , 0 @ @ 
 - / * 
 - * Licensed to the Apache Software Foundation ( ASF ) under one 
 - * or more contributor license agreements . See the NOTICE file 
 - * distributed with this work for additional information 
 - * regarding copyright ownership . The ASF licenses this file 
 - * to you under the Apache License , Version 2 . 0 ( the 
 - * " License " ) ; you may not use this file except in compliance 
 - * with the License . You may obtain a copy of the License at 
 - * 
 - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 - * 
 - * Unless required by applicable law or agreed to in writing , software 
 - * distributed under the License is distributed on an " AS IS " BASIS , 
 - * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 - * See the License for the specific language governing permissions and 
 - * limitations under the License . 
 - * / 
 - package org . apache . cassandra . hadoop . cql3 ; 
 - 
 - import java . io . IOException ; 
 - import java . nio . ByteBuffer ; 
 - import java . util . Map ; 
 - 
 - import org . apache . cassandra . hadoop . HadoopCompat ; 
 - import org . apache . cassandra . hadoop . AbstractColumnFamilyInputFormat ; 
 - import org . apache . cassandra . hadoop . ReporterWrapper ; 
 - import org . apache . hadoop . mapred . InputSplit ; 
 - import org . apache . hadoop . mapred . JobConf ; 
 - import org . apache . hadoop . mapred . RecordReader ; 
 - import org . apache . hadoop . mapred . Reporter ; 
 - import org . apache . hadoop . mapreduce . TaskAttemptContext ; 
 - import org . apache . hadoop . mapreduce . TaskAttemptID ; 
 - 
 - / * * 
 - * Hadoop InputFormat allowing map / reduce against Cassandra rows within one ColumnFamily . 
 - * 
 - * At minimum , you need to set the KS and CF in your Hadoop job Configuration . 
 - * The ConfigHelper class is provided to make this 
 - * simple : 
 - * ConfigHelper . setInputColumnFamily 
 - * 
 - * You can also configure the number of rows per InputSplit with 
 - * ConfigHelper . setInputSplitSize . The default split size is 64k rows . 
 - * the number of CQL rows per page 
 - * 
 - * the number of CQL rows per page 
 - * CQLConfigHelper . setInputCQLPageRowSize . The default page row size is 1000 . You 
 - * should set it to " as big as possible , but no bigger . " It set the LIMIT for the CQL 
 - * query , so you need set it big enough to minimize the network overhead , and also 
 - * not too big to avoid out of memory issue . 
 - * 
 - * the column names of the select CQL query . The default is all columns 
 - * CQLConfigHelper . setInputColumns 
 - * 
 - * the user defined the where clause 
 - * CQLConfigHelper . setInputWhereClauses . The default is no user defined where clause 
 - * / 
 - public class CqlPagingInputFormat extends AbstractColumnFamilyInputFormat < Map < String , ByteBuffer > , Map < String , ByteBuffer > > 
 - { 
 - public RecordReader < Map < String , ByteBuffer > , Map < String , ByteBuffer > > getRecordReader ( InputSplit split , JobConf jobConf , final Reporter reporter ) 
 - throws IOException 
 - { 
 - TaskAttemptContext tac = HadoopCompat . newMapContext ( 
 - jobConf , 
 - TaskAttemptID . forName ( jobConf . get ( MAPRED _ TASK _ ID ) ) , 
 - null , 
 - null , 
 - null , 
 - new ReporterWrapper ( reporter ) , 
 - null ) ; 
 - 
 - CqlPagingRecordReader recordReader = new CqlPagingRecordReader ( ) ; 
 - recordReader . initialize ( ( org . apache . hadoop . mapreduce . InputSplit ) split , tac ) ; 
 - return recordReader ; 
 - } 
 - 
 - @ Override 
 - public org . apache . hadoop . mapreduce . RecordReader < Map < String , ByteBuffer > , Map < String , ByteBuffer > > createRecordReader ( 
 - org . apache . hadoop . mapreduce . InputSplit arg0 , TaskAttemptContext arg1 ) throws IOException , 
 - InterruptedException 
 - { 
 - return new CqlPagingRecordReader ( ) ; 
 - } 
 - 
 - } 
 diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java 
 deleted file mode 100644 
 index 2427e9c . . 0000000 
 - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlPagingRecordReader . java 
 + + + / dev / null 
 @ @ - 1 , 800 + 0 , 0 @ @ 
 - / * 
 - * Licensed to the Apache Software Foundation ( ASF ) under one 
 - * or more contributor license agreements . See the NOTICE file 
 - * distributed with this work for additional information 
 - * regarding copyright ownership . The ASF licenses this file 
 - * to you under the Apache License , Version 2 . 0 ( the 
 - * " License " ) ; you may not use this file except in compliance 
 - * with the License . You may obtain a copy of the License at 
 - * 
 - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 - * 
 - * Unless required by applicable law or agreed to in writing , software 
 - * distributed under the License is distributed on an " AS IS " BASIS , 
 - * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 - * See the License for the specific language governing permissions and 
 - * limitations under the License . 
 - * / 
 - package org . apache . cassandra . hadoop . cql3 ; 
 - 
 - import java . io . IOException ; 
 - import java . net . InetAddress ; 
 - import java . net . UnknownHostException ; 
 - import java . nio . ByteBuffer ; 
 - import java . nio . charset . CharacterCodingException ; 
 - import java . util . * ; 
 - 
 - import com . google . common . base . Optional ; 
 - import com . google . common . collect . AbstractIterator ; 
 - import com . google . common . collect . Iterables ; 
 - 
 - import org . apache . cassandra . hadoop . HadoopCompat ; 
 - import org . slf4j . Logger ; 
 - import org . slf4j . LoggerFactory ; 
 - import org . apache . cassandra . db . marshal . AbstractType ; 
 - import org . apache . cassandra . db . marshal . CompositeType ; 
 - import org . apache . cassandra . db . marshal . LongType ; 
 - import org . apache . cassandra . db . marshal . ReversedType ; 
 - import org . apache . cassandra . db . marshal . TypeParser ; 
 - import org . apache . cassandra . dht . IPartitioner ; 
 - import org . apache . cassandra . exceptions . ConfigurationException ; 
 - import org . apache . cassandra . exceptions . SyntaxException ; 
 - import org . apache . cassandra . hadoop . ColumnFamilySplit ; 
 - import org . apache . cassandra . hadoop . ConfigHelper ; 
 - import org . apache . cassandra . thrift . * ; 
 - import org . apache . cassandra . utils . ByteBufferUtil ; 
 - import org . apache . cassandra . utils . FBUtilities ; 
 - import org . apache . cassandra . utils . Pair ; 
 - import org . apache . hadoop . conf . Configuration ; 
 - import org . apache . hadoop . mapreduce . InputSplit ; 
 - import org . apache . hadoop . mapreduce . RecordReader ; 
 - import org . apache . hadoop . mapreduce . TaskAttemptContext ; 
 - import org . apache . thrift . TException ; 
 - import org . apache . thrift . transport . TTransport ; 
 - 
 - / * * 
 - * Hadoop RecordReader read the values return from the CQL query 
 - * It use CQL key range query to page through the wide rows . 
 - * < p / > 
 - * Return List < IColumn > as keys columns 
 - * < p / > 
 - * Map < ByteBuffer , IColumn > as column name to columns mappings 
 - * / 
 - public class CqlPagingRecordReader extends RecordReader < Map < String , ByteBuffer > , Map < String , ByteBuffer > > 
 - implements org . apache . hadoop . mapred . RecordReader < Map < String , ByteBuffer > , Map < String , ByteBuffer > > 
 - { 
 - private static final Logger logger = LoggerFactory . getLogger ( CqlPagingRecordReader . class ) ; 
 - 
 - public static final int DEFAULT _ CQL _ PAGE _ LIMIT = 1000 ; / / TODO : find the number large enough but not OOM 
 - 
 - private ColumnFamilySplit split ; 
 - protected RowIterator rowIterator ; 
 - 
 - private Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > currentRow ; 
 - private int totalRowCount ; / / total number of rows to fetch 
 - private String keyspace ; 
 - private String cfName ; 
 - private Cassandra . Client client ; 
 - private ConsistencyLevel consistencyLevel ; 
 - 
 - / / partition keys - - key aliases 
 - private List < BoundColumn > partitionBoundColumns = new ArrayList < BoundColumn > ( ) ; 
 - 
 - / / cluster keys - - column aliases 
 - private List < BoundColumn > clusterColumns = new ArrayList < BoundColumn > ( ) ; 
 - 
 - / / map prepared query type to item id 
 - private Map < Integer , Integer > preparedQueryIds = new HashMap < Integer , Integer > ( ) ; 
 - 
 - / / cql query select columns 
 - private String columns ; 
 - 
 - / / the number of cql rows per page 
 - private int pageRowSize ; 
 - 
 - / / user defined where clauses 
 - private String userDefinedWhereClauses ; 
 - 
 - private IPartitioner partitioner ; 
 - 
 - private AbstractType < ? > keyValidator ; 
 - 
 - public CqlPagingRecordReader ( ) 
 - { 
 - super ( ) ; 
 - } 
 - 
 - public void initialize ( InputSplit split , TaskAttemptContext context ) throws IOException 
 - { 
 - this . split = ( ColumnFamilySplit ) split ; 
 - Configuration conf = HadoopCompat . getConfiguration ( context ) ; 
 - totalRowCount = ( this . split . getLength ( ) < Long . MAX _ VALUE ) 
 - ? ( int ) this . split . getLength ( ) 
 - : ConfigHelper . getInputSplitSize ( conf ) ; 
 - cfName = ConfigHelper . getInputColumnFamily ( conf ) ; 
 - consistencyLevel = ConsistencyLevel . valueOf ( ConfigHelper . getReadConsistencyLevel ( conf ) ) ; 
 - keyspace = ConfigHelper . getInputKeyspace ( conf ) ; 
 - columns = CqlConfigHelper . getInputcolumns ( conf ) ; 
 - userDefinedWhereClauses = CqlConfigHelper . getInputWhereClauses ( conf ) ; 
 - 
 - Optional < Integer > pageRowSizeOptional = CqlConfigHelper . getInputPageRowSize ( conf ) ; 
 - try 
 - { 
 - 	 pageRowSize = pageRowSizeOptional . isPresent ( ) ? pageRowSizeOptional . get ( ) : DEFAULT _ CQL _ PAGE _ LIMIT ; 
 - } 
 - catch ( NumberFormatException e ) 
 - { 
 - 	 pageRowSize = DEFAULT _ CQL _ PAGE _ LIMIT ; 
 - } 
 - 
 - partitioner = ConfigHelper . getInputPartitioner ( HadoopCompat . getConfiguration ( context ) ) ; 
 - 
 - try 
 - { 
 - if ( client ! = null ) 
 - return ; 
 - 
 - / / create connection using thrift 
 - String [ ] locations = split . getLocations ( ) ; 
 - Exception lastException = null ; 
 - for ( String location : locations ) 
 - { 
 - int port = ConfigHelper . getInputRpcPort ( conf ) ; 
 - try 
 - { 
 - client = CqlPagingInputFormat . createAuthenticatedClient ( location , port , conf ) ; 
 - break ; 
 - } 
 - catch ( Exception e ) 
 - { 
 - lastException = e ; 
 - logger . warn ( " Failed to create authenticated client to { } : { } " , location , port ) ; 
 - } 
 - } 
 - if ( client = = null & & lastException ! = null ) 
 - throw lastException ; 
 - 
 - / / retrieve partition keys and cluster keys from system . schema _ columnfamilies table 
 - retrieveKeys ( ) ; 
 - 
 - client . set _ keyspace ( keyspace ) ; 
 - } 
 - catch ( Exception e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - 
 - rowIterator = new RowIterator ( ) ; 
 - 
 - logger . debug ( " created { } " , rowIterator ) ; 
 - } 
 - 
 - public void close ( ) 
 - { 
 - if ( client ! = null ) 
 - { 
 - TTransport transport = client . getOutputProtocol ( ) . getTransport ( ) ; 
 - if ( transport . isOpen ( ) ) 
 - transport . close ( ) ; 
 - client = null ; 
 - } 
 - } 
 - 
 - public Map < String , ByteBuffer > getCurrentKey ( ) 
 - { 
 - return currentRow . left ; 
 - } 
 - 
 - public Map < String , ByteBuffer > getCurrentValue ( ) 
 - { 
 - return currentRow . right ; 
 - } 
 - 
 - public float getProgress ( ) 
 - { 
 - if ( ! rowIterator . hasNext ( ) ) 
 - return 1 . 0F ; 
 - 
 - / / the progress is likely to be reported slightly off the actual but close enough 
 - float progress = ( ( float ) rowIterator . totalRead / totalRowCount ) ; 
 - return progress > 1 . 0F ? 1 . 0F : progress ; 
 - } 
 - 
 - public boolean nextKeyValue ( ) throws IOException 
 - { 
 - if ( ! rowIterator . hasNext ( ) ) 
 - { 
 - logger . debug ( " Finished scanning { } rows ( estimate was : { } ) " , rowIterator . totalRead , totalRowCount ) ; 
 - return false ; 
 - } 
 - 
 - try 
 - { 
 - currentRow = rowIterator . next ( ) ; 
 - } 
 - catch ( Exception e ) 
 - { 
 - / / throw it as IOException , so client can catch it and handle it at client side 
 - IOException ioe = new IOException ( e . getMessage ( ) ) ; 
 - ioe . initCause ( ioe . getCause ( ) ) ; 
 - throw ioe ; 
 - } 
 - return true ; 
 - } 
 - 
 - / / we don ' t use endpointsnitch since we are trying to support hadoop nodes that are 
 - / / not necessarily on Cassandra machines , too . This should be adequate for single - DC clusters , at least . 
 - private String [ ] getLocations ( ) 
 - { 
 - Collection < InetAddress > localAddresses = FBUtilities . getAllLocalAddresses ( ) ; 
 - 
 - for ( InetAddress address : localAddresses ) 
 - { 
 - for ( String location : split . getLocations ( ) ) 
 - { 
 - InetAddress locationAddress ; 
 - try 
 - { 
 - locationAddress = InetAddress . getByName ( location ) ; 
 - } 
 - catch ( UnknownHostException e ) 
 - { 
 - throw new AssertionError ( e ) ; 
 - } 
 - if ( address . equals ( locationAddress ) ) 
 - { 
 - return new String [ ] { location } ; 
 - } 
 - } 
 - } 
 - return split . getLocations ( ) ; 
 - } 
 - 
 - / / Because the old Hadoop API wants us to write to the key and value 
 - / / and the new asks for them , we need to copy the output of the new API 
 - / / to the old . Thus , expect a small performance hit . 
 - / / And obviously this wouldn ' t work for wide rows . But since ColumnFamilyInputFormat 
 - / / and ColumnFamilyRecordReader don ' t support them , it should be fine for now . 
 - public boolean next ( Map < String , ByteBuffer > keys , Map < String , ByteBuffer > value ) throws IOException 
 - { 
 - if ( nextKeyValue ( ) ) 
 - { 
 - value . clear ( ) ; 
 - value . putAll ( getCurrentValue ( ) ) ; 
 - 
 - keys . clear ( ) ; 
 - keys . putAll ( getCurrentKey ( ) ) ; 
 - 
 - return true ; 
 - } 
 - return false ; 
 - } 
 - 
 - public long getPos ( ) throws IOException 
 - { 
 - return ( long ) rowIterator . totalRead ; 
 - } 
 - 
 - public Map < String , ByteBuffer > createKey ( ) 
 - { 
 - return new LinkedHashMap < String , ByteBuffer > ( ) ; 
 - } 
 - 
 - public Map < String , ByteBuffer > createValue ( ) 
 - { 
 - return new LinkedHashMap < String , ByteBuffer > ( ) ; 
 - } 
 - 
 - / * * CQL row iterator * / 
 - protected class RowIterator extends AbstractIterator < Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > > 
 - { 
 - protected int totalRead = 0 ; / / total number of cf rows read 
 - protected Iterator < CqlRow > rows ; 
 - private int pageRows = 0 ; / / the number of cql rows read of this page 
 - private String previousRowKey = null ; / / previous CF row key 
 - private String partitionKeyString ; / / keys in < key1 > , < key2 > , < key3 > string format 
 - private String partitionKeyMarkers ; / / question marks in ? , ? , ? format which matches the number of keys 
 - 
 - public RowIterator ( ) 
 - { 
 - / / initial page 
 - executeQuery ( ) ; 
 - } 
 - 
 - protected Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > computeNext ( ) 
 - { 
 - if ( rows = = null ) 
 - return endOfData ( ) ; 
 - 
 - int index = - 2 ; 
 - / / check there are more page to read 
 - while ( ! rows . hasNext ( ) ) 
 - { 
 - / / no more data 
 - if ( index = = - 1 | | emptyPartitionKeyValues ( ) ) 
 - { 
 - logger . debug ( " no more data " ) ; 
 - return endOfData ( ) ; 
 - } 
 - 
 - index = setTailNull ( clusterColumns ) ; 
 - logger . debug ( " set tail to null , index : { } " , index ) ; 
 - executeQuery ( ) ; 
 - pageRows = 0 ; 
 - 
 - if ( rows = = null | | ! rows . hasNext ( ) & & index < 0 ) 
 - { 
 - logger . debug ( " no more data " ) ; 
 - return endOfData ( ) ; 
 - } 
 - } 
 - 
 - Map < String , ByteBuffer > valueColumns = createValue ( ) ; 
 - Map < String , ByteBuffer > keyColumns = createKey ( ) ; 
 - int i = 0 ; 
 - CqlRow row = rows . next ( ) ; 
 - for ( Column column : row . columns ) 
 - { 
 - String columnName = stringValue ( ByteBuffer . wrap ( column . getName ( ) ) ) ; 
 - logger . debug ( " column : { } " , columnName ) ; 
 - 
 - if ( i < partitionBoundColumns . size ( ) + clusterColumns . size ( ) ) 
 - keyColumns . put ( stringValue ( column . name ) , column . value ) ; 
 - else 
 - valueColumns . put ( stringValue ( column . name ) , column . value ) ; 
 - 
 - i + + ; 
 - } 
 - 
 - / / increase total CQL row read for this page 
 - pageRows + + ; 
 - 
 - / / increase total CF row read 
 - if ( newRow ( keyColumns , previousRowKey ) ) 
 - totalRead + + ; 
 - 
 - / / read full page 
 - if ( pageRows > = pageRowSize | | ! rows . hasNext ( ) ) 
 - { 
 - Iterator < String > newKeys = keyColumns . keySet ( ) . iterator ( ) ; 
 - for ( BoundColumn column : partitionBoundColumns ) 
 - column . value = keyColumns . get ( newKeys . next ( ) ) ; 
 - 
 - for ( BoundColumn column : clusterColumns ) 
 - column . value = keyColumns . get ( newKeys . next ( ) ) ; 
 - 
 - executeQuery ( ) ; 
 - pageRows = 0 ; 
 - } 
 - 
 - return Pair . create ( keyColumns , valueColumns ) ; 
 - } 
 - 
 - / * * check whether start to read a new CF row by comparing the partition keys * / 
 - private boolean newRow ( Map < String , ByteBuffer > keyColumns , String previousRowKey ) 
 - { 
 - if ( keyColumns . isEmpty ( ) ) 
 - return false ; 
 - 
 - String rowKey = " " ; 
 - if ( keyColumns . size ( ) = = 1 ) 
 - { 
 - rowKey = partitionBoundColumns . get ( 0 ) . validator . getString ( keyColumns . get ( partitionBoundColumns . get ( 0 ) . name ) ) ; 
 - } 
 - else 
 - { 
 - Iterator < ByteBuffer > iter = keyColumns . values ( ) . iterator ( ) ; 
 - for ( BoundColumn column : partitionBoundColumns ) 
 - rowKey = rowKey + column . validator . getString ( ByteBufferUtil . clone ( iter . next ( ) ) ) + " : " ; 
 - } 
 - 
 - logger . debug ( " previous RowKey : { } , new row key : { } " , previousRowKey , rowKey ) ; 
 - if ( previousRowKey = = null ) 
 - { 
 - this . previousRowKey = rowKey ; 
 - return true ; 
 - } 
 - 
 - if ( rowKey . equals ( previousRowKey ) ) 
 - return false ; 
 - 
 - this . previousRowKey = rowKey ; 
 - return true ; 
 - } 
 - 
 - / * * set the last non - null key value to null , and return the previous index * / 
 - private int setTailNull ( List < BoundColumn > values ) 
 - { 
 - if ( values . isEmpty ( ) ) 
 - return - 1 ; 
 - 
 - Iterator < BoundColumn > iterator = values . iterator ( ) ; 
 - int previousIndex = - 1 ; 
 - BoundColumn current ; 
 - while ( iterator . hasNext ( ) ) 
 - { 
 - current = iterator . next ( ) ; 
 - if ( current . value = = null ) 
 - { 
 - int index = previousIndex > 0 ? previousIndex : 0 ; 
 - BoundColumn column = values . get ( index ) ; 
 - logger . debug ( " set key { } value to null " , column . name ) ; 
 - column . value = null ; 
 - return previousIndex - 1 ; 
 - } 
 - 
 - previousIndex + + ; 
 - } 
 - 
 - BoundColumn column = values . get ( previousIndex ) ; 
 - logger . debug ( " set key { } value to null " , column . name ) ; 
 - column . value = null ; 
 - return previousIndex - 1 ; 
 - } 
 - 
 - / * * serialize the prepared query , pair . left is query id , pair . right is query * / 
 - private Pair < Integer , String > composeQuery ( String columns ) 
 - { 
 - Pair < Integer , String > clause = whereClause ( ) ; 
 - if ( columns = = null ) 
 - { 
 - columns = " * " ; 
 - } 
 - else 
 - { 
 - / / add keys in the front in order 
 - String partitionKey = keyString ( partitionBoundColumns ) ; 
 - String clusterKey = keyString ( clusterColumns ) ; 
 - 
 - columns = withoutKeyColumns ( columns ) ; 
 - columns = ( clusterKey = = null | | " " . equals ( clusterKey ) ) 
 - ? partitionKey + ( columns ! = null ? ( " , " + columns ) : " " ) 
 - : partitionKey + " , " + clusterKey + ( columns ! = null ? ( " , " + columns ) : " " ) ; 
 - } 
 - 
 - String whereStr = userDefinedWhereClauses = = null ? " " : " AND " + userDefinedWhereClauses ; 
 - return Pair . create ( clause . left , 
 - String . format ( " SELECT % s FROM % s % s % s LIMIT % d ALLOW FILTERING " , 
 - columns , quote ( cfName ) , clause . right , whereStr , pageRowSize ) ) ; 
 - } 
 - 
 - 
 - / * * remove key columns from the column string * / 
 - private String withoutKeyColumns ( String columnString ) 
 - { 
 - Set < String > keyNames = new HashSet < String > ( ) ; 
 - for ( BoundColumn column : Iterables . concat ( partitionBoundColumns , clusterColumns ) ) 
 - keyNames . add ( column . name ) ; 
 - 
 - String [ ] columns = columnString . split ( " , " ) ; 
 - String result = null ; 
 - for ( String column : columns ) 
 - { 
 - String trimmed = column . trim ( ) ; 
 - if ( keyNames . contains ( trimmed ) ) 
 - continue ; 
 - 
 - String quoted = quote ( trimmed ) ; 
 - result = result = = null ? quoted : result + " , " + quoted ; 
 - } 
 - return result ; 
 - } 
 - 
 - / * * serialize the where clause * / 
 - private Pair < Integer , String > whereClause ( ) 
 - { 
 - if ( partitionKeyString = = null ) 
 - partitionKeyString = keyString ( partitionBoundColumns ) ; 
 - 
 - if ( partitionKeyMarkers = = null ) 
 - partitionKeyMarkers = partitionKeyMarkers ( ) ; 
 - / / initial query token ( k ) > = start _ token and token ( k ) < = end _ token 
 - if ( emptyPartitionKeyValues ( ) ) 
 - return Pair . create ( 0 , String . format ( " WHERE token ( % s ) > ? AND token ( % s ) < = ? " , partitionKeyString , partitionKeyString ) ) ; 
 - 
 - / / query token ( k ) > token ( pre _ partition _ key ) and token ( k ) < = end _ token 
 - if ( clusterColumns . size ( ) = = 0 | | clusterColumns . get ( 0 ) . value = = null ) 
 - return Pair . create ( 1 , 
 - String . format ( " WHERE token ( % s ) > token ( % s ) AND token ( % s ) < = ? " , 
 - partitionKeyString , partitionKeyMarkers , partitionKeyString ) ) ; 
 - 
 - / / query token ( k ) = token ( pre _ partition _ key ) and m = pre _ cluster _ key _ m and n > pre _ cluster _ key _ n 
 - Pair < Integer , String > clause = whereClause ( clusterColumns , 0 ) ; 
 - return Pair . create ( clause . left , 
 - String . format ( " WHERE token ( % s ) = token ( % s ) % s " , partitionKeyString , partitionKeyMarkers , clause . right ) ) ; 
 - } 
 - 
 - / * * recursively serialize the where clause * / 
 - private Pair < Integer , String > whereClause ( List < BoundColumn > column , int position ) 
 - { 
 - if ( position = = column . size ( ) - 1 | | column . get ( position + 1 ) . value = = null ) 
 - return Pair . create ( position + 2 , String . format ( " AND % s % s ? " , quote ( column . get ( position ) . name ) , column . get ( position ) . reversed ? " < " : " > " ) ) ; 
 - 
 - Pair < Integer , String > clause = whereClause ( column , position + 1 ) ; 
 - return Pair . create ( clause . left , String . format ( " AND % s = ? % s " , quote ( column . get ( position ) . name ) , clause . right ) ) ; 
 - } 
 - 
 - / * * check whether all key values are null * / 
 - private boolean emptyPartitionKeyValues ( ) 
 - { 
 - for ( BoundColumn column : partitionBoundColumns ) 
 - { 
 - if ( column . value ! = null ) 
 - return false ; 
 - } 
 - return true ; 
 - } 
 - 
 - / * * serialize the partition key string in format of < key1 > , < key2 > , < key3 > * / 
 - private String keyString ( List < BoundColumn > columns ) 
 - { 
 - String result = null ; 
 - for ( BoundColumn column : columns ) 
 - result = result = = null ? quote ( column . name ) : result + " , " + quote ( column . name ) ; 
 - 
 - return result = = null ? " " : result ; 
 - } 
 - 
 - / * * serialize the question marks for partition key string in format of ? , ? , ? * / 
 - private String partitionKeyMarkers ( ) 
 - { 
 - String result = null ; 
 - for ( BoundColumn column : partitionBoundColumns ) 
 - result = result = = null ? " ? " : result + " , ? " ; 
 - 
 - return result ; 
 - } 
 - 
 - / * * serialize the query binding variables , pair . left is query id , pair . right is the binding variables * / 
 - private Pair < Integer , List < ByteBuffer > > preparedQueryBindValues ( ) 
 - { 
 - List < ByteBuffer > values = new LinkedList < ByteBuffer > ( ) ; 
 - 
 - / / initial query token ( k ) > = start _ token and token ( k ) < = end _ token 
 - if ( emptyPartitionKeyValues ( ) ) 
 - { 
 - values . add ( partitioner . getTokenValidator ( ) . fromString ( split . getStartToken ( ) ) ) ; 
 - values . add ( partitioner . getTokenValidator ( ) . fromString ( split . getEndToken ( ) ) ) ; 
 - return Pair . create ( 0 , values ) ; 
 - } 
 - else 
 - { 
 - for ( BoundColumn partitionBoundColumn1 : partitionBoundColumns ) 
 - values . add ( partitionBoundColumn1 . value ) ; 
 - 
 - if ( clusterColumns . size ( ) = = 0 | | clusterColumns . get ( 0 ) . value = = null ) 
 - { 
 - / / query token ( k ) > token ( pre _ partition _ key ) and token ( k ) < = end _ token 
 - values . add ( partitioner . getTokenValidator ( ) . fromString ( split . getEndToken ( ) ) ) ; 
 - return Pair . create ( 1 , values ) ; 
 - } 
 - else 
 - { 
 - / / query token ( k ) = token ( pre _ partition _ key ) and m = pre _ cluster _ key _ m and n > pre _ cluster _ key _ n 
 - int type = preparedQueryBindValues ( clusterColumns , 0 , values ) ; 
 - return Pair . create ( type , values ) ; 
 - } 
 - } 
 - } 
 - 
 - / * * recursively serialize the query binding variables * / 
 - private int preparedQueryBindValues ( List < BoundColumn > column , int position , List < ByteBuffer > bindValues ) 
 - { 
 - if ( position = = column . size ( ) - 1 | | column . get ( position + 1 ) . value = = null ) 
 - { 
 - bindValues . add ( column . get ( position ) . value ) ; 
 - return position + 2 ; 
 - } 
 - else 
 - { 
 - bindValues . add ( column . get ( position ) . value ) ; 
 - return preparedQueryBindValues ( column , position + 1 , bindValues ) ; 
 - } 
 - } 
 - 
 - / * * get the prepared query item Id * / 
 - private int prepareQuery ( int type ) throws InvalidRequestException , TException 
 - { 
 - Integer itemId = preparedQueryIds . get ( type ) ; 
 - if ( itemId ! = null ) 
 - return itemId ; 
 - 
 - Pair < Integer , String > query = null ; 
 - query = composeQuery ( columns ) ; 
 - logger . debug ( " type : { } , query : { } " , query . left , query . right ) ; 
 - CqlPreparedResult cqlPreparedResult = client . prepare _ cql3 _ query ( ByteBufferUtil . bytes ( query . right ) , Compression . NONE ) ; 
 - preparedQueryIds . put ( query . left , cqlPreparedResult . itemId ) ; 
 - return cqlPreparedResult . itemId ; 
 - } 
 - 
 - / * * Quoting for working with uppercase * / 
 - private String quote ( String identifier ) 
 - { 
 - return " \ " " + identifier . replaceAll ( " \ " " , " \ " \ " " ) + " \ " " ; 
 - } 
 - 
 - / * * execute the prepared query * / 
 - private void executeQuery ( ) 
 - { 
 - Pair < Integer , List < ByteBuffer > > bindValues = preparedQueryBindValues ( ) ; 
 - logger . debug ( " query type : { } " , bindValues . left ) ; 
 - 
 - / / check whether it reach end of range for type 1 query CASSANDRA - 5573 
 - if ( bindValues . left = = 1 & & reachEndRange ( ) ) 
 - { 
 - rows = null ; 
 - return ; 
 - } 
 - 
 - int retries = 0 ; 
 - / / only try three times for TimedOutException and UnavailableException 
 - while ( retries < 3 ) 
 - { 
 - try 
 - { 
 - CqlResult cqlResult = client . execute _ prepared _ cql3 _ query ( prepareQuery ( bindValues . left ) , bindValues . right , consistencyLevel ) ; 
 - if ( cqlResult ! = null & & cqlResult . rows ! = null ) 
 - rows = cqlResult . rows . iterator ( ) ; 
 - return ; 
 - } 
 - catch ( TimedOutException e ) 
 - { 
 - retries + + ; 
 - if ( retries > = 3 ) 
 - { 
 - rows = null ; 
 - RuntimeException rte = new RuntimeException ( e . getMessage ( ) ) ; 
 - rte . initCause ( e ) ; 
 - throw rte ; 
 - } 
 - } 
 - catch ( UnavailableException e ) 
 - { 
 - retries + + ; 
 - if ( retries > = 3 ) 
 - { 
 - rows = null ; 
 - RuntimeException rte = new RuntimeException ( e . getMessage ( ) ) ; 
 - rte . initCause ( e ) ; 
 - throw rte ; 
 - } 
 - } 
 - catch ( Exception e ) 
 - { 
 - rows = null ; 
 - RuntimeException rte = new RuntimeException ( e . getMessage ( ) ) ; 
 - rte . initCause ( e ) ; 
 - throw rte ; 
 - } 
 - } 
 - } 
 - } 
 - 
 - / * * retrieve the partition keys and cluster keys from system . schema _ columnfamilies table * / 
 - private void retrieveKeys ( ) throws Exception 
 - { 
 - String query = " select key _ aliases , " + 
 - " column _ aliases , " + 
 - " key _ validator , " + 
 - " comparator " + 
 - " from system . schema _ columnfamilies " + 
 - " where keyspace _ name = ' % s ' and columnfamily _ name = ' % s ' " ; 
 - String formatted = String . format ( query , keyspace , cfName ) ; 
 - CqlResult result = client . execute _ cql3 _ query ( ByteBufferUtil . bytes ( formatted ) , Compression . NONE , ConsistencyLevel . ONE ) ; 
 - 
 - CqlRow cqlRow = result . rows . get ( 0 ) ; 
 - String keyString = ByteBufferUtil . string ( ByteBuffer . wrap ( cqlRow . columns . get ( 0 ) . getValue ( ) ) ) ; 
 - logger . debug ( " partition keys : { } " , keyString ) ; 
 - List < String > keys = FBUtilities . fromJsonList ( keyString ) ; 
 - 
 - for ( String key : keys ) 
 - partitionBoundColumns . add ( new BoundColumn ( key ) ) ; 
 - 
 - keyString = ByteBufferUtil . string ( ByteBuffer . wrap ( cqlRow . columns . get ( 1 ) . getValue ( ) ) ) ; 
 - logger . debug ( " cluster columns : { } " , keyString ) ; 
 - keys = FBUtilities . fromJsonList ( keyString ) ; 
 - 
 - for ( String key : keys ) 
 - clusterColumns . add ( new BoundColumn ( key ) ) ; 
 - 
 - Column rawKeyValidator = cqlRow . columns . get ( 2 ) ; 
 - String validator = ByteBufferUtil . string ( ByteBuffer . wrap ( rawKeyValidator . getValue ( ) ) ) ; 
 - logger . debug ( " row key validator : { } " , validator ) ; 
 - keyValidator = parseType ( validator ) ; 
 - 
 - if ( keyValidator instanceof CompositeType ) 
 - { 
 - List < AbstractType < ? > > types = ( ( CompositeType ) keyValidator ) . types ; 
 - for ( int i = 0 ; i < partitionBoundColumns . size ( ) ; i + + ) 
 - partitionBoundColumns . get ( i ) . validator = types . get ( i ) ; 
 - } 
 - else 
 - { 
 - partitionBoundColumns . get ( 0 ) . validator = keyValidator ; 
 - } 
 - 
 - Column rawComparator = cqlRow . columns . get ( 3 ) ; 
 - String comparator = ByteBufferUtil . string ( ByteBuffer . wrap ( rawComparator . getValue ( ) ) ) ; 
 - logger . debug ( " comparator : { } " , comparator ) ; 
 - AbstractType comparatorValidator = parseType ( comparator ) ; 
 - if ( comparatorValidator instanceof CompositeType ) 
 - { 
 - for ( int i = 0 ; i < clusterColumns . size ( ) ; i + + ) 
 - clusterColumns . get ( i ) . reversed = ( ( ( CompositeType ) comparatorValidator ) . types . get ( i ) instanceof ReversedType ) ; 
 - } 
 - else if ( comparatorValidator instanceof ReversedType ) 
 - { 
 - clusterColumns . get ( 0 ) . reversed = true ; 
 - } 
 - } 
 - 
 - / * * check whether current row is at the end of range * / 
 - private boolean reachEndRange ( ) 
 - { 
 - / / current row key 
 - ByteBuffer rowKey ; 
 - if ( keyValidator instanceof CompositeType ) 
 - { 
 - ByteBuffer [ ] keys = new ByteBuffer [ partitionBoundColumns . size ( ) ] ; 
 - for ( int i = 0 ; i < partitionBoundColumns . size ( ) ; i + + ) 
 - keys [ i ] = partitionBoundColumns . get ( i ) . value . duplicate ( ) ; 
 - 
 - rowKey = CompositeType . build ( keys ) ; 
 - } 
 - else 
 - { 
 - rowKey = partitionBoundColumns . get ( 0 ) . value ; 
 - } 
 - 
 - String endToken = split . getEndToken ( ) ; 
 - String currentToken = partitioner . getToken ( rowKey ) . toString ( ) ; 
 - logger . debug ( " End token : { } , current token : { } " , endToken , currentToken ) ; 
 - 
 - return endToken . equals ( currentToken ) ; 
 - } 
 - 
 - private static AbstractType < ? > parseType ( String type ) throws IOException 
 - { 
 - try 
 - { 
 - / / always treat counters like longs , specifically CCT . serialize is not what we need 
 - if ( type ! = null & & type . equals ( " org . apache . cassandra . db . marshal . CounterColumnType " ) ) 
 - return LongType . instance ; 
 - return TypeParser . parse ( type ) ; 
 - } 
 - catch ( ConfigurationException e ) 
 - { 
 - throw new IOException ( e ) ; 
 - } 
 - catch ( SyntaxException e ) 
 - { 
 - throw new IOException ( e ) ; 
 - } 
 - } 
 - 
 - private static class BoundColumn 
 - { 
 - final String name ; 
 - ByteBuffer value ; 
 - AbstractType < ? > validator ; 
 - boolean reversed = false ; 
 - 
 - public BoundColumn ( String name ) 
 - { 
 - this . name = name ; 
 - } 
 - } 
 - 
 - / * * get string from a ByteBuffer , catch the exception and throw it as runtime exception * / 
 - private static String stringValue ( ByteBuffer value ) 
 - { 
 - try 
 - { 
 - return ByteBufferUtil . string ( value ) ; 
 - } 
 - catch ( CharacterCodingException e ) 
 - { 
 - throw new RuntimeException ( e ) ; 
 - } 
 - } 
 - } 
 diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / IterableCqlPagingRecordReader . java b / src / java / org / apache / cassandra / hadoop / cql3 / IterableCqlPagingRecordReader . java 
 deleted file mode 100644 
 index d7348b0 . . 0000000 
 - - - a / src / java / org / apache / cassandra / hadoop / cql3 / IterableCqlPagingRecordReader . java 
 + + + / dev / null 
 @ @ - 1 , 73 + 0 , 0 @ @ 
 - / * 
 - * Licensed to the Apache Software Foundation ( ASF ) under one 
 - * or more contributor license agreements . See the NOTICE file 
 - * distributed with this work for additional information 
 - * regarding copyright ownership . The ASF licenses this file 
 - * to you under the Apache License , Version 2 . 0 ( the 
 - * " License " ) ; you may not use this file except in compliance 
 - * with the License . You may obtain a copy of the License at 
 - * 
 - * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 - * 
 - * Unless required by applicable law or agreed to in writing , software 
 - * distributed under the License is distributed on an " AS IS " BASIS , 
 - * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 - * See the License for the specific language governing permissions and 
 - * limitations under the License . 
 - * / 
 - package org . apache . cassandra . hadoop . cql3 ; 
 - 
 - import java . io . Closeable ; 
 - import java . io . IOException ; 
 - import java . nio . ByteBuffer ; 
 - import java . util . Iterator ; 
 - import java . util . Map ; 
 - 
 - import org . apache . cassandra . utils . Pair ; 
 - 
 - / * * 
 - * Implements an iterable - friendly { @ link CqlPagingRecordReader } . 
 - * / 
 - public class IterableCqlPagingRecordReader extends CqlPagingRecordReader 
 - implements Iterable < Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > > , Closeable 
 - { 
 - public Iterator < Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > > iterator ( ) 
 - { 
 - return new Iterator < Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > > ( ) 
 - { 
 - public boolean hasNext ( ) 
 - { 
 - return rowIterator . hasNext ( ) ; 
 - } 
 - 
 - public Pair < Map < String , ByteBuffer > , Map < String , ByteBuffer > > next ( ) 
 - { 
 - return rowIterator . next ( ) ; 
 - } 
 - 
 - public void remove ( ) 
 - { 
 - throw new UnsupportedOperationException ( " Cannot remove an element on this iterator ! " ) ; 
 - 
 - } 
 - } ; 
 - } 
 - 
 - / * * 
 - * @ throws NotImplementedException Always throws this exception , this operation does not make sense in this implementation . 
 - * / 
 - @ Override 
 - public boolean nextKeyValue ( ) throws IOException 
 - { 
 - throw new UnsupportedOperationException ( " Calling method nextKeyValue ( ) does not make sense in this implementation " ) ; 
 - } 
 - 
 - / * * 
 - * @ throws NotImplementedException Always throws this exception , this operation does not make sense in this implementation . 
 - * / 
 - @ Override 
 - public boolean next ( Map < String , ByteBuffer > keys , Map < String , ByteBuffer > value ) throws IOException 
 - { 
 - throw new UnsupportedOperationException ( " Calling method next ( ) does not make sense in this implementation " ) ; 
 - } 
 - }

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / tools / SSTableExport . java b / src / java / org / apache / cassandra / tools / SSTableExport . java 
 new file mode 100644 
 index 0000000 . . e5aca7d 
 - - - / dev / null 
 + + + b / src / java / org / apache / cassandra / tools / SSTableExport . java 
 @ @ - 0 , 0 + 1 , 420 @ @ 
 + / * * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , software 
 + * distributed under the License is distributed on an " AS IS " BASIS , 
 + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 + * See the License for the specific language governing permissions and 
 + * limitations under the License . 
 + * / 
 + 
 + package org . apache . cassandra . tools ; 
 + 
 + import java . io . File ; 
 + import java . io . IOException ; 
 + import java . io . PrintStream ; 
 + import java . nio . ByteBuffer ; 
 + import java . util . * ; 
 + 
 + import org . apache . cassandra . config . CFMetaData ; 
 + import org . apache . cassandra . db . * ; 
 + import org . apache . cassandra . db . marshal . AbstractType ; 
 + import org . apache . cassandra . io . util . BufferedRandomAccessFile ; 
 + import org . apache . cassandra . service . StorageService ; 
 + 
 + import org . apache . commons . cli . * ; 
 + 
 + import org . apache . cassandra . config . ConfigurationException ; 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 + import org . apache . cassandra . dht . IPartitioner ; 
 + import org . apache . cassandra . io . sstable . * ; 
 + import org . apache . cassandra . utils . ByteBufferUtil ; 
 + import org . apache . cassandra . utils . Pair ; 
 + 
 + import static org . apache . cassandra . utils . ByteBufferUtil . bytesToHex ; 
 + import static org . apache . cassandra . utils . ByteBufferUtil . hexToBytes ; 
 + 
 + / * * 
 + * Export SSTables to JSON format . 
 + * / 
 + public class SSTableExport 
 + { 
 + / / size of the columns page 
 + private static final int PAGE _ SIZE = 1000 ; 
 + 
 + private static final String KEY _ OPTION = " k " ; 
 + private static final String EXCLUDEKEY _ OPTION = " x " ; 
 + private static final String ENUMERATEKEYS _ OPTION = " e " ; 
 + private static Options options ; 
 + private static CommandLine cmd ; 
 + 
 + static 
 + { 
 + options = new Options ( ) ; 
 + 
 + Option optKey = new Option ( KEY _ OPTION , true , " Row key " ) ; 
 + / / Number of times - k < key > can be passed on the command line . 
 + optKey . setArgs ( 500 ) ; 
 + options . addOption ( optKey ) ; 
 + 
 + Option excludeKey = new Option ( EXCLUDEKEY _ OPTION , true , " Excluded row key " ) ; 
 + / / Number of times - x < key > can be passed on the command line . 
 + excludeKey . setArgs ( 500 ) ; 
 + options . addOption ( excludeKey ) ; 
 + 
 + Option optEnumerate = new Option ( ENUMERATEKEYS _ OPTION , false , " enumerate keys only " ) ; 
 + options . addOption ( optEnumerate ) ; 
 + } 
 + 
 + / * * 
 + * Wraps given string into quotes 
 + * @ param val string to quote 
 + * @ return quoted string 
 + * / 
 + private static String quote ( String val ) 
 + { 
 + return String . format ( " \ " % s \ " " , val ) ; 
 + } 
 + 
 + / * * 
 + * JSON Hash Key serializer 
 + * @ param val value to set as a key 
 + * @ return JSON Hash key 
 + * / 
 + private static String asKey ( String val ) 
 + { 
 + return String . format ( " % s : " , quote ( val ) ) ; 
 + } 
 + 
 + / * * 
 + * Serialize columns using given column iterator 
 + * @ param columns column iterator 
 + * @ param out output stream 
 + * @ param comparator columns comparator 
 + * @ param cfMetaData Column Family metadata ( to get validator ) 
 + * @ return pair of ( number of columns serialized , last column serialized ) 
 + * / 
 + private static void serializeColumns ( Iterator < IColumn > columns , PrintStream out , AbstractType comparator , CFMetaData cfMetaData ) 
 + { 
 + while ( columns . hasNext ( ) ) 
 + { 
 + IColumn column = columns . next ( ) ; 
 + serializeColumn ( column , out , comparator , cfMetaData ) ; 
 + 
 + if ( columns . hasNext ( ) ) 
 + out . print ( " , " ) ; 
 + } 
 + } 
 + 
 + / * * 
 + * Serialize a given column to the JSON format 
 + * @ param column column presentation 
 + * @ param out output stream 
 + * @ param comparator columns comparator 
 + * @ param cfMetaData Column Family metadata ( to get validator ) 
 + * / 
 + private static void serializeColumn ( IColumn column , PrintStream out , AbstractType comparator , CFMetaData cfMetaData ) 
 + { 
 + ByteBuffer name = ByteBufferUtil . clone ( column . name ( ) ) ; 
 + ByteBuffer value = ByteBufferUtil . clone ( column . value ( ) ) ; 
 + AbstractType validator = cfMetaData . getValueValidator ( name ) ; 
 + 
 + out . print ( " [ " ) ; 
 + out . print ( quote ( comparator . getString ( name ) ) ) ; 
 + out . print ( " , " ) ; 
 + out . print ( quote ( validator . getString ( value ) ) ) ; 
 + out . print ( " , " ) ; 
 + out . print ( column . timestamp ( ) ) ; 
 + 
 + if ( column instanceof DeletedColumn ) 
 + { 
 + out . print ( " , " ) ; 
 + out . print ( " \ " d \ " " ) ; 
 + } 
 + else if ( column instanceof ExpiringColumn ) 
 + { 
 + out . print ( " , " ) ; 
 + out . print ( " \ " e \ " " ) ; 
 + out . print ( " , " ) ; 
 + out . print ( ( ( ExpiringColumn ) column ) . getTimeToLive ( ) ) ; 
 + out . print ( " , " ) ; 
 + out . print ( column . getLocalDeletionTime ( ) ) ; 
 + } 
 + else if ( column instanceof CounterColumn ) 
 + { 
 + out . print ( " , " ) ; 
 + out . print ( " \ " c \ " " ) ; 
 + out . print ( " , " ) ; 
 + out . print ( ( ( CounterColumn ) column ) . timestampOfLastDelete ( ) ) ; 
 + } 
 + 
 + out . print ( " ] " ) ; 
 + } 
 + 
 + / * * 
 + * Get portion of the columns and serialize in loop while not more columns left in the row 
 + * @ param row SSTableIdentityIterator row representation with Column Family 
 + * @ param key Decorated Key for the required row 
 + * @ param out output stream 
 + * / 
 + private static void serializeRow ( SSTableIdentityIterator row , DecoratedKey key , PrintStream out ) 
 + { 
 + ColumnFamily columnFamily = row . getColumnFamily ( ) ; 
 + boolean isSuperCF = columnFamily . isSuper ( ) ; 
 + CFMetaData cfMetaData = columnFamily . metadata ( ) ; 
 + AbstractType comparator = columnFamily . getComparator ( ) ; 
 + 
 + out . print ( asKey ( bytesToHex ( key . key ) ) ) ; 
 + out . print ( isSuperCF ? " { " : " [ " ) ; 
 + 
 + if ( isSuperCF ) 
 + { 
 + while ( row . hasNext ( ) ) 
 + { 
 + IColumn column = row . next ( ) ; 
 + 
 + out . print ( asKey ( comparator . getString ( column . name ( ) ) ) ) ; 
 + out . print ( " { " ) ; 
 + out . print ( asKey ( " deletedAt " ) ) ; 
 + out . print ( column . getMarkedForDeleteAt ( ) ) ; 
 + out . print ( " , " ) ; 
 + out . print ( asKey ( " subColumns " ) ) ; 
 + out . print ( " [ " ) ; 
 + serializeColumns ( column . getSubColumns ( ) . iterator ( ) , out , columnFamily . getSubComparator ( ) , cfMetaData ) ; 
 + out . print ( " ] " ) ; 
 + out . print ( " } " ) ; 
 + 
 + if ( row . hasNext ( ) ) 
 + out . print ( " , " ) ; 
 + } 
 + } 
 + else 
 + { 
 + serializeColumns ( row , out , comparator , cfMetaData ) ; 
 + } 
 + 
 + out . print ( isSuperCF ? " } " : " ] " ) ; 
 + } 
 + 
 + / * * 
 + * Enumerate row keys from an SSTableReader and write the result to a PrintStream . 
 + * 
 + * @ param ssTableFile the file to export the rows from 
 + * @ param outs PrintStream to write the output to 
 + * @ throws IOException on failure to read / write input / output 
 + * / 
 + public static void enumeratekeys ( String ssTableFile , PrintStream outs ) 
 + throws IOException 
 + { 
 + Descriptor desc = Descriptor . fromFilename ( ssTableFile ) ; 
 + KeyIterator iter = new KeyIterator ( desc ) ; 
 + DecoratedKey lastKey = null ; 
 + while ( iter . hasNext ( ) ) 
 + { 
 + DecoratedKey key = iter . next ( ) ; 
 + 
 + / / validate order of the keys in the sstable 
 + if ( lastKey ! = null & & lastKey . compareTo ( key ) > 0 ) 
 + throw new IOException ( " Key out of order ! " + lastKey + " > " + key ) ; 
 + lastKey = key ; 
 + 
 + outs . println ( bytesToHex ( key . key ) ) ; 
 + } 
 + iter . close ( ) ; 
 + outs . flush ( ) ; 
 + } 
 + 
 + / * * 
 + * Export specific rows from an SSTable and write the resulting JSON to a PrintStream . 
 + * 
 + * @ param ssTableFile the SSTableScanner to export the rows from 
 + * @ param outs PrintStream to write the output to 
 + * @ param toExport the keys corresponding to the rows to export 
 + * @ param excludes keys to exclude from export 
 + * @ throws IOException on failure to read / write input / output 
 + * / 
 + public static void export ( String ssTableFile , PrintStream outs , Collection < String > toExport , String [ ] excludes ) throws IOException 
 + { 
 + SSTableReader reader = SSTableReader . open ( Descriptor . fromFilename ( ssTableFile ) ) ; 
 + SSTableScanner scanner = reader . getDirectScanner ( BufferedRandomAccessFile . DEFAULT _ BUFFER _ SIZE ) ; 
 + 
 + IPartitioner < ? > partitioner = StorageService . getPartitioner ( ) ; 
 + 
 + if ( excludes ! = null ) 
 + toExport . removeAll ( Arrays . asList ( excludes ) ) ; 
 + 
 + outs . println ( " { " ) ; 
 + 
 + int i = 0 ; 
 + 
 + / / last key to compare order 
 + DecoratedKey lastKey = null ; 
 + 
 + for ( String key : toExport ) 
 + { 
 + DecoratedKey decoratedKey = partitioner . decorateKey ( hexToBytes ( key ) ) ; 
 + 
 + if ( lastKey ! = null & & lastKey . compareTo ( decoratedKey ) > 0 ) 
 + throw new IOException ( " Key out of order ! " + lastKey + " > " + decoratedKey ) ; 
 + 
 + lastKey = decoratedKey ; 
 + 
 + scanner . seekTo ( decoratedKey ) ; 
 + 
 + if ( ! scanner . hasNext ( ) ) 
 + continue ; 
 + 
 + SSTableIdentityIterator row = ( SSTableIdentityIterator ) scanner . next ( ) ; 
 + if ( ! row . getKey ( ) . equals ( decoratedKey ) ) 
 + continue ; 
 + 
 + serializeRow ( row , decoratedKey , outs ) ; 
 + 
 + if ( i ! = 0 ) 
 + outs . println ( " , " ) ; 
 + 
 + i + + ; 
 + } 
 + 
 + outs . println ( " \ n } " ) ; 
 + outs . flush ( ) ; 
 + 
 + scanner . close ( ) ; 
 + } 
 + 
 + / / This is necessary to accommodate the test suite since you cannot open a Reader more 
 + / / than once from within the same process . 
 + static void export ( SSTableReader reader , PrintStream outs , String [ ] excludes ) throws IOException 
 + { 
 + Set < String > excludeSet = new HashSet < String > ( ) ; 
 + 
 + if ( excludes ! = null ) 
 + excludeSet = new HashSet < String > ( Arrays . asList ( excludes ) ) ; 
 + 
 + 
 + SSTableIdentityIterator row ; 
 + SSTableScanner scanner = reader . getDirectScanner ( BufferedRandomAccessFile . DEFAULT _ BUFFER _ SIZE ) ; 
 + 
 + outs . println ( " { " ) ; 
 + 
 + int i = 0 ; 
 + 
 + / / collecting keys to export 
 + while ( scanner . hasNext ( ) ) 
 + { 
 + row = ( SSTableIdentityIterator ) scanner . next ( ) ; 
 + 
 + String currentKey = bytesToHex ( row . getKey ( ) . key ) ; 
 + 
 + if ( excludeSet . contains ( currentKey ) ) 
 + continue ; 
 + else if ( i ! = 0 ) 
 + outs . println ( " , " ) ; 
 + 
 + serializeRow ( row , row . getKey ( ) , outs ) ; 
 + 
 + i + + ; 
 + } 
 + 
 + outs . println ( " \ n } " ) ; 
 + outs . flush ( ) ; 
 + 
 + scanner . close ( ) ; 
 + } 
 + 
 + / * * 
 + * Export an SSTable and write the resulting JSON to a PrintStream . 
 + * 
 + * @ param ssTableFile the SSTable to export 
 + * @ param outs PrintStream to write the output to 
 + * @ param excludes keys to exclude from export 
 + * 
 + * @ throws IOException on failure to read / write input / output 
 + * / 
 + public static void export ( String ssTableFile , PrintStream outs , String [ ] excludes ) throws IOException 
 + { 
 + export ( SSTableReader . open ( Descriptor . fromFilename ( ssTableFile ) ) , outs , excludes ) ; 
 + } 
 + 
 + / * * 
 + * Export an SSTable and write the resulting JSON to standard out . 
 + * 
 + * @ param ssTableFile SSTable to export 
 + * @ param excludes keys to exclude from export 
 + * 
 + * @ throws IOException on failure to read / write SSTable / standard out 
 + * / 
 + public static void export ( String ssTableFile , String [ ] excludes ) throws IOException 
 + { 
 + export ( ssTableFile , System . out , excludes ) ; 
 + } 
 + 
 + / * * 
 + * Given arguments specifying an SSTable , and optionally an output file , 
 + * export the contents of the SSTable to JSON . 
 + * 
 + * @ param args command lines arguments 
 + * 
 + * @ throws IOException on failure to open / read / write files or output streams 
 + * @ throws ConfigurationException on configuration failure ( wrong params given ) 
 + * / 
 + public static void main ( String [ ] args ) throws IOException , ConfigurationException 
 + { 
 + String usage = String . format ( " Usage : % s < sstable > [ - k key [ - k key [ . . . ] ] - x key [ - x key [ . . . ] ] ] % n " , SSTableExport . class . getName ( ) ) ; 
 + 
 + CommandLineParser parser = new PosixParser ( ) ; 
 + try 
 + { 
 + cmd = parser . parse ( options , args ) ; 
 + } 
 + catch ( ParseException e1 ) 
 + { 
 + System . err . println ( e1 . getMessage ( ) ) ; 
 + System . err . println ( usage ) ; 
 + System . exit ( 1 ) ; 
 + } 
 + 
 + 
 + if ( cmd . getArgs ( ) . length ! = 1 ) 
 + { 
 + System . err . println ( " You must supply exactly one sstable " ) ; 
 + System . err . println ( usage ) ; 
 + System . exit ( 1 ) ; 
 + } 
 + 
 + 
 + String [ ] keys = cmd . getOptionValues ( KEY _ OPTION ) ; 
 + String [ ] excludes = cmd . getOptionValues ( EXCLUDEKEY _ OPTION ) ; 
 + String ssTableFileName = new File ( cmd . getArgs ( ) [ 0 ] ) . getAbsolutePath ( ) ; 
 + 
 + DatabaseDescriptor . loadSchemas ( ) ; 
 + if ( DatabaseDescriptor . getNonSystemTables ( ) . size ( ) < 1 ) 
 + { 
 + String msg = " no non - system tables are defined " ; 
 + System . err . println ( msg ) ; 
 + throw new ConfigurationException ( msg ) ; 
 + } 
 + 
 + if ( cmd . hasOption ( ENUMERATEKEYS _ OPTION ) ) 
 + { 
 + enumeratekeys ( ssTableFileName , System . out ) ; 
 + } 
 + else 
 + { 
 + if ( ( keys ! = null ) & & ( keys . length > 0 ) ) 
 + export ( ssTableFileName , System . out , Arrays . asList ( keys ) , excludes ) ; 
 + else 
 + export ( ssTableFileName , excludes ) ; 
 + } 
 + 
 + System . exit ( 0 ) ; 
 + } 
 + } 
 diff - - git a / src / java / org / apache / cassandra / tools / SSTableImport . java b / src / java / org / apache / cassandra / tools / SSTableImport . java 
 new file mode 100644 
 index 0000000 . . 1b53563 
 - - - / dev / null 
 + + + b / src / java / org / apache / cassandra / tools / SSTableImport . java 
 @ @ - 0 , 0 + 1 , 528 @ @ 
 + / * * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , software 
 + * distributed under the License is distributed on an " AS IS " BASIS , 
 + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 + * See the License for the specific language governing permissions and 
 + * limitations under the License . 
 + * / 
 + 
 + package org . apache . cassandra . tools ; 
 + 
 + import java . io . File ; 
 + import java . io . IOException ; 
 + import java . nio . ByteBuffer ; 
 + import java . util . * ; 
 + 
 + import org . apache . cassandra . db . marshal . AbstractType ; 
 + import org . apache . cassandra . db . marshal . BytesType ; 
 + import org . apache . cassandra . db . marshal . MarshalException ; 
 + import org . apache . commons . cli . * ; 
 + 
 + import org . apache . cassandra . config . CFMetaData ; 
 + import org . apache . cassandra . config . ConfigurationException ; 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 + import org . apache . cassandra . db . * ; 
 + import org . apache . cassandra . db . filter . QueryPath ; 
 + import org . apache . cassandra . dht . IPartitioner ; 
 + import org . apache . cassandra . io . sstable . SSTableWriter ; 
 + import org . codehaus . jackson . type . TypeReference ; 
 + 
 + import org . codehaus . jackson . JsonFactory ; 
 + import org . codehaus . jackson . map . MappingJsonFactory ; 
 + 
 + import org . codehaus . jackson . JsonParser ; 
 + 
 + import static org . apache . cassandra . utils . ByteBufferUtil . hexToBytes ; 
 + 
 + / * * 
 + * Create SSTables from JSON input 
 + * / 
 + public class SSTableImport 
 + { 
 + private static final String KEYSPACE _ OPTION = " K " ; 
 + private static final String COLUMN _ FAMILY _ OPTION = " c " ; 
 + private static final String KEY _ COUNT _ OPTION = " n " ; 
 + private static final String IS _ SORTED _ OPTION = " s " ; 
 + 
 + private static Options options ; 
 + private static CommandLine cmd ; 
 + 
 + private static Integer keyCountToImport = null ; 
 + private static boolean isSorted = false ; 
 + 
 + private static JsonFactory factory = new MappingJsonFactory ( ) ; 
 + 
 + static 
 + { 
 + options = new Options ( ) ; 
 + 
 + Option optKeyspace = new Option ( KEYSPACE _ OPTION , true , " Keyspace name . " ) ; 
 + optKeyspace . setRequired ( true ) ; 
 + options . addOption ( optKeyspace ) ; 
 + 
 + Option optColfamily = new Option ( COLUMN _ FAMILY _ OPTION , true , " Column Family name . " ) ; 
 + optColfamily . setRequired ( true ) ; 
 + options . addOption ( optColfamily ) ; 
 + 
 + options . addOption ( new Option ( KEY _ COUNT _ OPTION , true , " Number of keys to import ( Optional ) . " ) ) ; 
 + options . addOption ( new Option ( IS _ SORTED _ OPTION , false , " Assume JSON file as already sorted ( e . g . created by sstable2json tool ) ( Optional ) . " ) ) ; 
 + } 
 + 
 + private static class JsonColumn < T > 
 + { 
 + private ByteBuffer name ; 
 + private ByteBuffer value ; 
 + private long timestamp ; 
 + 
 + private String kind ; 
 + / / Expiring columns 
 + private int ttl ; 
 + private int localExpirationTime ; 
 + 
 + / / Counter columns 
 + private long timestampOfLastDelete ; 
 + 
 + public JsonColumn ( T json , CFMetaData meta , boolean isSubColumn ) 
 + { 
 + AbstractType comparator = ( isSubColumn ) ? meta . subcolumnComparator : meta . comparator ; 
 + 
 + if ( json instanceof List ) 
 + { 
 + List fields = ( List < ? > ) json ; 
 + 
 + assert fields . size ( ) > = 3 : " Column definition should have at least 3 " ; 
 + 
 + name = stringAsType ( ( String ) fields . get ( 0 ) , comparator ) ; 
 + value = stringAsType ( ( String ) fields . get ( 1 ) , meta . getValueValidator ( name . duplicate ( ) ) ) ; 
 + timestamp = ( Long ) fields . get ( 2 ) ; 
 + kind = " " ; 
 + 
 + if ( fields . size ( ) > 3 ) 
 + { 
 + if ( fields . get ( 3 ) instanceof Boolean ) 
 + { 
 + / / old format , reading this for backward compatibility sake 
 + if ( fields . size ( ) = = 6 ) 
 + { 
 + kind = " e " ; 
 + ttl = ( Integer ) fields . get ( 4 ) ; 
 + localExpirationTime = ( int ) ( long ) ( ( Long ) fields . get ( 5 ) ) ; 
 + } 
 + else 
 + { 
 + kind = ( ( Boolean ) fields . get ( 3 ) ) ? " d " : " " ; 
 + } 
 + } 
 + else 
 + { 
 + kind = ( String ) fields . get ( 3 ) ; 
 + if ( isExpiring ( ) ) 
 + { 
 + ttl = ( Integer ) fields . get ( 4 ) ; 
 + localExpirationTime = ( int ) ( long ) ( ( Long ) fields . get ( 5 ) ) ; 
 + } 
 + else if ( isCounter ( ) ) 
 + { 
 + timestampOfLastDelete = ( long ) ( ( Integer ) fields . get ( 4 ) ) ; 
 + } 
 + } 
 + } 
 + } 
 + } 
 + 
 + public boolean isDeleted ( ) 
 + { 
 + return kind . equals ( " d " ) ; 
 + } 
 + 
 + public boolean isExpiring ( ) 
 + { 
 + return kind . equals ( " e " ) ; 
 + } 
 + 
 + public boolean isCounter ( ) 
 + { 
 + return kind . equals ( " c " ) ; 
 + } 
 + 
 + public ByteBuffer getName ( ) 
 + { 
 + return name . duplicate ( ) ; 
 + } 
 + 
 + public ByteBuffer getValue ( ) 
 + { 
 + return value . duplicate ( ) ; 
 + } 
 + } 
 + 
 + private static void addToStandardCF ( List < ? > row , ColumnFamily cfamily ) 
 + { 
 + addColumnsToCF ( row , null , cfamily ) ; 
 + } 
 + 
 + / * * 
 + * Add columns to a column family . 
 + * 
 + * @ param row the columns associated with a row 
 + * @ param superName name of the super column if any 
 + * @ param cfamily the column family to add columns to 
 + * / 
 + private static void addColumnsToCF ( List < ? > row , ByteBuffer superName , ColumnFamily cfamily ) 
 + { 
 + CFMetaData cfm = cfamily . metadata ( ) ; 
 + assert cfm ! = null ; 
 + 
 + for ( Object c : row ) 
 + { 
 + JsonColumn col = new JsonColumn < List > ( ( List ) c , cfm , ( superName ! = null ) ) ; 
 + QueryPath path = new QueryPath ( cfm . cfName , superName , col . getName ( ) ) ; 
 + 
 + if ( col . isExpiring ( ) ) 
 + { 
 + cfamily . addColumn ( null , new ExpiringColumn ( col . getName ( ) , col . getValue ( ) , col . timestamp , col . ttl , col . localExpirationTime ) ) ; 
 + } 
 + else if ( col . isCounter ( ) ) 
 + { 
 + cfamily . addColumn ( null , new CounterColumn ( col . getName ( ) , col . getValue ( ) , col . timestamp , col . timestampOfLastDelete ) ) ; 
 + } 
 + else if ( col . isDeleted ( ) ) 
 + { 
 + cfamily . addTombstone ( path , col . getValue ( ) , col . timestamp ) ; 
 + } 
 + else 
 + { 
 + cfamily . addColumn ( path , col . getValue ( ) , col . timestamp ) ; 
 + } 
 + } 
 + } 
 + 
 + / * * 
 + * Add super columns to a column family . 
 + * 
 + * @ param row the super columns associated with a row 
 + * @ param cfamily the column family to add columns to 
 + * / 
 + private static void addToSuperCF ( Map < ? , ? > row , ColumnFamily cfamily ) 
 + { 
 + CFMetaData metaData = cfamily . metadata ( ) ; 
 + assert metaData ! = null ; 
 + 
 + AbstractType comparator = metaData . comparator ; 
 + 
 + / / Super columns 
 + for ( Map . Entry < ? , ? > entry : row . entrySet ( ) ) 
 + { 
 + Map < ? , ? > data = ( Map < ? , ? > ) entry . getValue ( ) ; 
 + 
 + addColumnsToCF ( ( List < ? > ) data . get ( " subColumns " ) , stringAsType ( ( String ) entry . getKey ( ) , comparator ) , cfamily ) ; 
 + 
 + / / * WARNING * markForDeleteAt has been DEPRECATED at Cassandra side 
 + / / BigInteger deletedAt = ( BigInteger ) data . get ( " deletedAt " ) ; 
 + / / SuperColumn superColumn = ( SuperColumn ) cfamily . getColumn ( superName ) ; 
 + / / superColumn . markForDeleteAt ( ( int ) ( System . currentTimeMillis ( ) / 1000 ) , deletedAt ) ; 
 + } 
 + } 
 + 
 + / * * 
 + * Convert a JSON formatted file to an SSTable . 
 + * 
 + * @ param jsonFile the file containing JSON formatted data 
 + * @ param keyspace keyspace the data belongs to 
 + * @ param cf column family the data belongs to 
 + * @ param ssTablePath file to write the SSTable to 
 + * 
 + * @ throws IOException for errors reading / writing input / output 
 + * / 
 + public static void importJson ( String jsonFile , String keyspace , String cf , String ssTablePath ) throws IOException 
 + { 
 + ColumnFamily columnFamily = ColumnFamily . create ( keyspace , cf ) ; 
 + IPartitioner < ? > partitioner = DatabaseDescriptor . getPartitioner ( ) ; 
 + 
 + int importedKeys = ( isSorted ) ? importSorted ( jsonFile , columnFamily , ssTablePath , partitioner ) 
 + : importUnsorted ( getParser ( jsonFile ) , columnFamily , ssTablePath , partitioner ) ; 
 + 
 + if ( importedKeys ! = - 1 ) 
 + System . out . printf ( " % d keys imported successfully . % n " , importedKeys ) ; 
 + } 
 + 
 + private static int importUnsorted ( JsonParser parser , ColumnFamily columnFamily , String ssTablePath , IPartitioner < ? > partitioner ) throws IOException 
 + { 
 + int importedKeys = 0 ; 
 + long start = System . currentTimeMillis ( ) ; 
 + Map < ? , ? > data = parser . readValueAs ( new TypeReference < Map < ? , ? > > ( ) { } ) ; 
 + 
 + keyCountToImport = ( keyCountToImport = = null ) ? data . size ( ) : keyCountToImport ; 
 + SSTableWriter writer = new SSTableWriter ( ssTablePath , keyCountToImport ) ; 
 + 
 + System . out . printf ( " Importing % s keys . . . % n " , keyCountToImport ) ; 
 + 
 + / / sort by dk representation , but hold onto the hex version 
 + SortedMap < DecoratedKey , String > decoratedKeys = new TreeMap < DecoratedKey , String > ( ) ; 
 + 
 + for ( Object keyObject : data . keySet ( ) ) 
 + { 
 + String key = ( String ) keyObject ; 
 + decoratedKeys . put ( partitioner . decorateKey ( hexToBytes ( key ) ) , key ) ; 
 + } 
 + 
 + for ( Map . Entry < DecoratedKey , String > rowKey : decoratedKeys . entrySet ( ) ) 
 + { 
 + if ( columnFamily . getColumnFamilyType ( ) = = ColumnFamilyType . Super ) 
 + { 
 + addToSuperCF ( ( Map < ? , ? > ) data . get ( rowKey . getValue ( ) ) , columnFamily ) ; 
 + } 
 + else 
 + { 
 + addToStandardCF ( ( List < ? > ) data . get ( rowKey . getValue ( ) ) , columnFamily ) ; 
 + } 
 + 
 + writer . append ( rowKey . getKey ( ) , columnFamily ) ; 
 + columnFamily . clear ( ) ; 
 + 
 + importedKeys + + ; 
 + 
 + long current = System . currentTimeMillis ( ) ; 
 + 
 + if ( current - start > = 5000 ) / / 5 secs . 
 + { 
 + System . out . printf ( " Currently imported % d keys . % n " , importedKeys ) ; 
 + start = current ; 
 + } 
 + 
 + if ( keyCountToImport = = importedKeys ) 
 + break ; 
 + } 
 + 
 + writer . closeAndOpenReader ( ) ; 
 + 
 + return importedKeys ; 
 + } 
 + 
 + public static int importSorted ( String jsonFile , ColumnFamily columnFamily , String ssTablePath , IPartitioner < ? > partitioner ) throws IOException 
 + { 
 + int importedKeys = 0 ; / / already imported keys count 
 + long start = System . currentTimeMillis ( ) ; 
 + 
 + JsonParser parser = getParser ( jsonFile ) ; 
 + 
 + if ( keyCountToImport = = null ) 
 + { 
 + keyCountToImport = 0 ; 
 + System . out . println ( " Counting keys to import , please wait . . . ( NOTE : to skip this use - n < num _ keys > ) " ) ; 
 + 
 + parser . nextToken ( ) ; / / START _ OBJECT 
 + while ( parser . nextToken ( ) ! = null ) 
 + { 
 + parser . nextToken ( ) ; 
 + parser . skipChildren ( ) ; 
 + if ( parser . getCurrentName ( ) = = null ) continue ; 
 + 
 + keyCountToImport + + ; 
 + } 
 + } 
 + 
 + System . out . printf ( " Importing % s keys . . . % n " , keyCountToImport ) ; 
 + 
 + parser = getParser ( jsonFile ) ; / / renewing parser 
 + SSTableWriter writer = new SSTableWriter ( ssTablePath , keyCountToImport ) ; 
 + 
 + int lineNumber = 1 ; 
 + DecoratedKey prevStoredKey = null ; 
 + 
 + while ( parser . nextToken ( ) ! = null ) 
 + { 
 + String key = parser . getCurrentName ( ) ; 
 + 
 + if ( key ! = null ) 
 + { 
 + String tokenName = parser . nextToken ( ) . name ( ) ; 
 + 
 + if ( tokenName . equals ( " START _ ARRAY " ) ) 
 + { 
 + if ( columnFamily . getColumnFamilyType ( ) = = ColumnFamilyType . Super ) 
 + { 
 + throw new RuntimeException ( " Can ' t write Standard columns to the Super Column Family . " ) ; 
 + } 
 + 
 + List < ? > columns = parser . readValueAs ( new TypeReference < List < ? > > ( ) { } ) ; 
 + addToStandardCF ( columns , columnFamily ) ; 
 + } 
 + else if ( tokenName . equals ( " START _ OBJECT " ) ) 
 + { 
 + if ( columnFamily . getColumnFamilyType ( ) = = ColumnFamilyType . Standard ) 
 + { 
 + throw new RuntimeException ( " Can ' t write Super columns to the Standard Column Family . " ) ; 
 + } 
 + 
 + Map < ? , ? > columns = parser . readValueAs ( new TypeReference < Map < ? , ? > > ( ) { } ) ; 
 + addToSuperCF ( columns , columnFamily ) ; 
 + } 
 + else 
 + { 
 + throw new UnsupportedOperationException ( " Only Array or Hash allowed as row content . " ) ; 
 + } 
 + 
 + DecoratedKey currentKey = partitioner . decorateKey ( hexToBytes ( key ) ) ; 
 + 
 + if ( prevStoredKey ! = null & & prevStoredKey . compareTo ( currentKey ) ! = - 1 ) 
 + { 
 + System . err . printf ( " Line % d : Key % s is greater than previous , collection is not sorted properly . Aborting import . You might need to delete SSTables manually . % n " , lineNumber , key ) ; 
 + return - 1 ; 
 + } 
 + 
 + / / saving decorated key 
 + writer . append ( currentKey , columnFamily ) ; 
 + columnFamily . clear ( ) ; 
 + 
 + prevStoredKey = currentKey ; 
 + importedKeys + + ; 
 + lineNumber + + ; 
 + 
 + long current = System . currentTimeMillis ( ) ; 
 + 
 + if ( current - start > = 5000 ) / / 5 secs . 
 + { 
 + System . out . printf ( " Currently imported % d keys . % n " , importedKeys ) ; 
 + start = current ; 
 + } 
 + 
 + if ( keyCountToImport = = importedKeys ) 
 + break ; 
 + } 
 + } 
 + 
 + writer . closeAndOpenReader ( ) ; 
 + 
 + return importedKeys ; 
 + } 
 + 
 + / * * 
 + * Get JsonParser object for file 
 + * @ param fileName name of the file 
 + * @ return json parser instance for given file 
 + * @ throws IOException if any I / O error . 
 + * / 
 + private static JsonParser getParser ( String fileName ) throws IOException 
 + { 
 + return factory . createJsonParser ( new File ( fileName ) ) . configure ( JsonParser . Feature . INTERN _ FIELD _ NAMES , false ) ; 
 + } 
 + 
 + / * * 
 + * Converts JSON to an SSTable file . JSON input can either be a file specified 
 + * using an optional command line argument , or supplied on standard in . 
 + * 
 + * @ param args command line arguments 
 + * @ throws IOException on failure to open / read / write files or output streams 
 + * @ throws ParseException on failure to parse JSON input 
 + * @ throws ConfigurationException on configuration error . 
 + * / 
 + public static void main ( String [ ] args ) throws IOException , ParseException , ConfigurationException 
 + { 
 + CommandLineParser parser = new PosixParser ( ) ; 
 + 
 + try 
 + { 
 + cmd = parser . parse ( options , args ) ; 
 + } 
 + catch ( org . apache . commons . cli . ParseException e ) 
 + { 
 + System . err . println ( e . getMessage ( ) ) ; 
 + printProgramUsage ( ) ; 
 + System . exit ( 1 ) ; 
 + } 
 + 
 + if ( cmd . getArgs ( ) . length ! = 2 ) 
 + { 
 + printProgramUsage ( ) ; 
 + System . exit ( 1 ) ; 
 + } 
 + 
 + String json = cmd . getArgs ( ) [ 0 ] ; 
 + String ssTable = cmd . getArgs ( ) [ 1 ] ; 
 + String keyspace = cmd . getOptionValue ( KEYSPACE _ OPTION ) ; 
 + String cfamily = cmd . getOptionValue ( COLUMN _ FAMILY _ OPTION ) ; 
 + 
 + if ( cmd . hasOption ( KEY _ COUNT _ OPTION ) ) 
 + { 
 + keyCountToImport = Integer . valueOf ( cmd . getOptionValue ( KEY _ COUNT _ OPTION ) ) ; 
 + } 
 + 
 + if ( cmd . hasOption ( IS _ SORTED _ OPTION ) ) 
 + { 
 + isSorted = true ; 
 + } 
 + 
 + DatabaseDescriptor . loadSchemas ( ) ; 
 + if ( DatabaseDescriptor . getNonSystemTables ( ) . size ( ) < 1 ) 
 + { 
 + String msg = " no non - system tables are defined " ; 
 + System . err . println ( msg ) ; 
 + throw new ConfigurationException ( msg ) ; 
 + } 
 + 
 + try 
 + { 
 + importJson ( json , keyspace , cfamily , ssTable ) ; 
 + } 
 + catch ( Exception e ) 
 + { 
 + e . printStackTrace ( ) ; 
 + System . err . println ( " ERROR : " + e . getMessage ( ) ) ; 
 + System . exit ( - 1 ) ; 
 + } 
 + 
 + System . exit ( 0 ) ; 
 + } 
 + 
 + private static void printProgramUsage ( ) 
 + { 
 + System . out . printf ( " Usage : % s - s - K < keyspace > - c < column _ family > - n < num _ keys > < json > < sstable > % n % n " , 
 + SSTableImport . class . getName ( ) ) ; 
 + 
 + System . out . println ( " Options : " ) ; 
 + for ( Object o : options . getOptions ( ) ) 
 + { 
 + Option opt = ( Option ) o ; 
 + System . out . println ( " - " + opt . getOpt ( ) + " - " + opt . getDescription ( ) ) ; 
 + } 
 + } 
 + 
 + / * * 
 + * Used by test framework to set key count 
 + * @ param keyCount numbers of keys to import 
 + * / 
 + public static void setKeyCountToImport ( Integer keyCount ) 
 + { 
 + keyCountToImport = keyCount ; 
 + } 
 + 
 + / * * 
 + * Convert a string to bytes ( ByteBuffer ) according to type 
 + * @ param content string to convert 
 + * @ param type type to use for conversion 
 + * @ return byte buffer representation of the given string 
 + * / 
 + private static ByteBuffer stringAsType ( String content , AbstractType type ) 
 + { 
 + try 
 + { 
 + return ( type = = BytesType . instance ) ? hexToBytes ( content ) : type . fromString ( content ) ; 
 + } 
 + catch ( MarshalException e ) 
 + { 
 + throw new RuntimeException ( e . getMessage ( ) ) ; 
 + } 
 + } 
 + 
 + }
