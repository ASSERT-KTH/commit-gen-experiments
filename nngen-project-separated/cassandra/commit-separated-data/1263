BLEU SCORE: 0.04767707020457096

TEST MSG: Add row size to sstable format for faster skipping
GENERATED MSG: remove per - row bloom filter of column names

TEST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / ColumnIndex . java b / src / java / org / apache / cassandra / db / ColumnIndex . java <nl> index add5fa7 . . ede3f79 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnIndex . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnIndex . java <nl> @ @ - 76 , 6 + 76 , 7 @ @ public class ColumnIndex <nl> private long startPosition = - 1 ; <nl> <nl> private int written ; <nl> + private long previousRowStart ; <nl> <nl> private ClusteringPrefix firstClustering ; <nl> private ClusteringPrefix lastClustering ; <nl> @ @ - 99 , 7 + 100 , 7 @ @ public class ColumnIndex <nl> ByteBufferUtil . writeWithShortLength ( iterator . partitionKey ( ) . getKey ( ) , writer ) ; <nl> DeletionTime . serializer . serialize ( iterator . partitionLevelDeletion ( ) , writer ) ; <nl> if ( header . hasStatic ( ) ) <nl> - UnfilteredSerializer . serializer . serialize ( iterator . staticRow ( ) , header , writer , version ) ; <nl> + UnfilteredSerializer . serializer . serializeStaticRow ( iterator . staticRow ( ) , header , writer , version ) ; <nl> } <nl> <nl> public ColumnIndex build ( ) throws IOException <nl> @ @ - 131 , 15 + 132 , 18 @ @ public class ColumnIndex <nl> <nl> private void add ( Unfiltered unfiltered ) throws IOException <nl> { <nl> + long pos = currentPosition ( ) ; <nl> + <nl> if ( firstClustering = = null ) <nl> { <nl> / / Beginning of an index block . Remember the start and position <nl> firstClustering = unfiltered . clustering ( ) ; <nl> - startPosition = currentPosition ( ) ; <nl> + startPosition = pos ; <nl> } <nl> <nl> - UnfilteredSerializer . serializer . serialize ( unfiltered , header , writer , version ) ; <nl> + UnfilteredSerializer . serializer . serialize ( unfiltered , header , writer , pos - previousRowStart , version ) ; <nl> lastClustering = unfiltered . clustering ( ) ; <nl> + previousRowStart = pos ; <nl> + + written ; <nl> <nl> if ( unfiltered . kind ( ) = = Unfiltered . Kind . RANGE _ TOMBSTONE _ MARKER ) <nl> diff - - git a / src / java / org / apache / cassandra / db / Memtable . java b / src / java / org / apache / cassandra / db / Memtable . java <nl> index 014467e . . f47efe3 100644 <nl> - - - a / src / java / org / apache / cassandra / db / Memtable . java <nl> + + + b / src / java / org / apache / cassandra / db / Memtable . java <nl> @ @ - 430 , 7 + 430 , 7 @ @ public class Memtable implements Comparable < Memtable > <nl> ( long ) partitions . size ( ) , <nl> ActiveRepairService . UNREPAIRED _ SSTABLE , <nl> sstableMetadataCollector , <nl> - new SerializationHeader ( cfs . metadata , columns , stats ) , <nl> + new SerializationHeader ( true , cfs . metadata , columns , stats ) , <nl> txn ) ) ; <nl> } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / SerializationHeader . java b / src / java / org / apache / cassandra / db / SerializationHeader . java <nl> index decac49 . . 0706d06 100644 <nl> - - - a / src / java / org / apache / cassandra / db / SerializationHeader . java <nl> + + + b / src / java / org / apache / cassandra / db / SerializationHeader . java <nl> @ @ - 45 , 6 + 45 , 8 @ @ public class SerializationHeader <nl> { <nl> public static final Serializer serializer = new Serializer ( ) ; <nl> <nl> + private final boolean isForSSTable ; <nl> + <nl> private final AbstractType < ? > keyType ; <nl> private final List < AbstractType < ? > > clusteringTypes ; <nl> <nl> @ @ - 53 , 12 + 55 , 14 @ @ public class SerializationHeader <nl> <nl> private final Map < ByteBuffer , AbstractType < ? > > typeMap ; <nl> <nl> - private SerializationHeader ( AbstractType < ? > keyType , <nl> + private SerializationHeader ( boolean isForSSTable , <nl> + AbstractType < ? > keyType , <nl> List < AbstractType < ? > > clusteringTypes , <nl> PartitionColumns columns , <nl> EncodingStats stats , <nl> Map < ByteBuffer , AbstractType < ? > > typeMap ) <nl> { <nl> + this . isForSSTable = isForSSTable ; <nl> this . keyType = keyType ; <nl> this . clusteringTypes = clusteringTypes ; <nl> this . columns = columns ; <nl> @ @ - 77 , 7 + 81 , 8 @ @ public class SerializationHeader <nl> List < AbstractType < ? > > clusteringTypes = new ArrayList < > ( size ) ; <nl> for ( int i = 0 ; i < size ; i + + ) <nl> clusteringTypes . add ( BytesType . instance ) ; <nl> - return new SerializationHeader ( BytesType . instance , <nl> + return new SerializationHeader ( false , <nl> + BytesType . instance , <nl> clusteringTypes , <nl> PartitionColumns . NONE , <nl> EncodingStats . NO _ STATS , <nl> @ @ - 108 , 14 + 113 , 16 @ @ public class SerializationHeader <nl> else <nl> columns . addAll ( sstable . header . columns ( ) ) ; <nl> } <nl> - return new SerializationHeader ( metadata , columns . build ( ) , stats . get ( ) ) ; <nl> + return new SerializationHeader ( true , metadata , columns . build ( ) , stats . get ( ) ) ; <nl> } <nl> <nl> - public SerializationHeader ( CFMetaData metadata , <nl> + public SerializationHeader ( boolean isForSSTable , <nl> + CFMetaData metadata , <nl> PartitionColumns columns , <nl> EncodingStats stats ) <nl> { <nl> - this ( metadata . getKeyValidator ( ) , <nl> + this ( isForSSTable , <nl> + metadata . getKeyValidator ( ) , <nl> typesOf ( metadata . clusteringColumns ( ) ) , <nl> columns , <nl> stats , <nl> @ @ - 137 , 6 + 144 , 11 @ @ public class SerializationHeader <nl> return ! columns . statics . isEmpty ( ) ; <nl> } <nl> <nl> + public boolean isForSSTable ( ) <nl> + { <nl> + return isForSSTable ; <nl> + } <nl> + <nl> public EncodingStats stats ( ) <nl> { <nl> return stats ; <nl> @ @ - 320 , 7 + 332 , 7 @ @ public class SerializationHeader <nl> } <nl> builder . add ( column ) ; <nl> } <nl> - return new SerializationHeader ( keyType , clusteringTypes , builder . build ( ) , stats , typeMap ) ; <nl> + return new SerializationHeader ( true , keyType , clusteringTypes , builder . build ( ) , stats , typeMap ) ; <nl> } <nl> <nl> @ Override <nl> @ @ - 390 , 7 + 402 , 7 @ @ public class SerializationHeader <nl> regulars = Columns . serializer . deserializeSubset ( selection . fetchedColumns ( ) . regulars , in ) ; <nl> } <nl> <nl> - return new SerializationHeader ( keyType , clusteringTypes , new PartitionColumns ( statics , regulars ) , stats , null ) ; <nl> + return new SerializationHeader ( false , keyType , clusteringTypes , new PartitionColumns ( statics , regulars ) , stats , null ) ; <nl> } <nl> <nl> public long serializedSizeForMessaging ( SerializationHeader header , ColumnFilter selection , boolean hasStatic ) <nl> diff - - git a / src / java / org / apache / cassandra / db / UnfilteredDeserializer . java b / src / java / org / apache / cassandra / db / UnfilteredDeserializer . java <nl> index 5c76c63 . . ef30289 100644 <nl> - - - a / src / java / org / apache / cassandra / db / UnfilteredDeserializer . java <nl> + + + b / src / java / org / apache / cassandra / db / UnfilteredDeserializer . java <nl> @ @ - 147 , 7 + 147 , 7 @ @ public abstract class UnfilteredDeserializer <nl> return ; <nl> } <nl> <nl> - nextExtendedFlags = UnfilteredSerializer . isExtended ( nextFlags ) ? in . readUnsignedByte ( ) : 0 ; <nl> + nextExtendedFlags = UnfilteredSerializer . readExtendedFlags ( in , nextFlags ) ; <nl> <nl> clusteringDeserializer . prepare ( nextFlags , nextExtendedFlags ) ; <nl> isReady = true ; <nl> @ @ - 195 , 14 + 195 , 14 @ @ public abstract class UnfilteredDeserializer <nl> public void skipNext ( ) throws IOException <nl> { <nl> isReady = false ; <nl> - ClusteringPrefix . Kind kind = clusteringDeserializer . skipNext ( ) ; <nl> + clusteringDeserializer . skipNext ( ) ; <nl> if ( UnfilteredSerializer . kind ( nextFlags ) = = Unfiltered . Kind . RANGE _ TOMBSTONE _ MARKER ) <nl> { <nl> - UnfilteredSerializer . serializer . skipMarkerBody ( in , header , kind . isBoundary ( ) ) ; <nl> + UnfilteredSerializer . serializer . skipMarkerBody ( in ) ; <nl> } <nl> else <nl> { <nl> - UnfilteredSerializer . serializer . skipRowBody ( in , header , nextFlags , nextExtendedFlags ) ; <nl> + UnfilteredSerializer . serializer . skipRowBody ( in ) ; <nl> } <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java b / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java <nl> index df006d7 . . 3a0558e 100644 <nl> - - - a / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java <nl> + + + b / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java <nl> @ @ - 80 , 7 + 80 , 8 @ @ public class UnfilteredRowIteratorSerializer <nl> / / Should only be used for the on - wire format . <nl> public void serialize ( UnfilteredRowIterator iterator , ColumnFilter selection , DataOutputPlus out , int version , int rowEstimate ) throws IOException <nl> { <nl> - SerializationHeader header = new SerializationHeader ( iterator . metadata ( ) , <nl> + SerializationHeader header = new SerializationHeader ( false , <nl> + iterator . metadata ( ) , <nl> iterator . columns ( ) , <nl> iterator . stats ( ) ) ; <nl> serialize ( iterator , header , selection , out , version , rowEstimate ) ; <nl> @ @ - 89 , 6 + 90 , 8 @ @ public class UnfilteredRowIteratorSerializer <nl> / / Should only be used for the on - wire format . <nl> public void serialize ( UnfilteredRowIterator iterator , SerializationHeader header , ColumnFilter selection , DataOutputPlus out , int version , int rowEstimate ) throws IOException <nl> { <nl> + assert ! header . isForSSTable ( ) ; <nl> + <nl> ByteBufferUtil . writeWithVIntLength ( iterator . partitionKey ( ) . getKey ( ) , out ) ; <nl> <nl> int flags = 0 ; <nl> @ @ - 134 , 7 + 137 , 8 @ @ public class UnfilteredRowIteratorSerializer <nl> / / recreate an iterator for both serialize and serializedSize , which is mostly only PartitionUpdate / ArrayBackedCachedPartition . <nl> public long serializedSize ( UnfilteredRowIterator iterator , ColumnFilter selection , int version , int rowEstimate ) <nl> { <nl> - SerializationHeader header = new SerializationHeader ( iterator . metadata ( ) , <nl> + SerializationHeader header = new SerializationHeader ( false , <nl> + iterator . metadata ( ) , <nl> iterator . columns ( ) , <nl> iterator . stats ( ) ) ; <nl> <nl> @ @ - 175 , 7 + 179 , 7 @ @ public class UnfilteredRowIteratorSerializer <nl> boolean isReversed = ( flags & IS _ REVERSED ) ! = 0 ; <nl> if ( ( flags & IS _ EMPTY ) ! = 0 ) <nl> { <nl> - SerializationHeader sh = new SerializationHeader ( metadata , PartitionColumns . NONE , EncodingStats . NO _ STATS ) ; <nl> + SerializationHeader sh = new SerializationHeader ( false , metadata , PartitionColumns . NONE , EncodingStats . NO _ STATS ) ; <nl> return new Header ( sh , key , isReversed , true , null , null , 0 ) ; <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java b / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java <nl> index b83ccf9 . . 4efc5eb 100644 <nl> - - - a / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java <nl> + + + b / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java <nl> @ @ - 35 , 10 + 35 , 12 @ @ import org . apache . cassandra . io . util . DataOutputPlus ; <nl> * flag is defined / explained below as the " Unfiltered flags " constants . One of those flags <nl> * is an extension flag , and if present , trigger the rid of another byte that contains more <nl> * flags . If the extension is not set , defaults are assumed for the flags of that 2nd byte . <nl> - * < row > is < clustering > [ < timestamp > ] [ < ttl > ] [ < deletion > ] < sc1 > . . . < sci > < cc1 > . . . < ccj > where <nl> - * < clustering > is the row clustering as serialized by <nl> - * { @ code Clustering . serializer } . Note that static row are an exception and <nl> - * don ' t have this . < timestamp > , < ttl > and < deletion > are the row timestamp , ttl and deletion <nl> + * < row > is < clustering > < size > [ < timestamp > ] [ < ttl > ] [ < deletion > ] < sc1 > . . . < sci > < cc1 > . . . < ccj > where <nl> + * < clustering > is the row clustering as serialized by { @ code Clustering . serializer } ( note <nl> + * that static row are an exception and don ' t have this ) . <nl> + * < size > is the size of the whole unfiltered on disk ( it ' s only used for sstables and is <nl> + * used to efficiently skip rows ) . <nl> + * < timestamp > , < ttl > and < deletion > are the row timestamp , ttl and deletion <nl> * whose presence is determined by the flags . < sci > is the simple columns of the row and < ccj > the <nl> * complex ones . <nl> * The columns for the row are then serialized if they differ from those in the header , <nl> @ @ - 90 , 22 + 92 , 35 @ @ public class UnfilteredSerializer <nl> public void serialize ( Unfiltered unfiltered , SerializationHeader header , DataOutputPlus out , int version ) <nl> throws IOException <nl> { <nl> + assert ! header . isForSSTable ( ) ; <nl> + serialize ( unfiltered , header , out , 0 , version ) ; <nl> + } <nl> + <nl> + public void serialize ( Unfiltered unfiltered , SerializationHeader header , DataOutputPlus out , long previousUnfilteredSize , int version ) <nl> + throws IOException <nl> + { <nl> if ( unfiltered . kind ( ) = = Unfiltered . Kind . RANGE _ TOMBSTONE _ MARKER ) <nl> { <nl> - serialize ( ( RangeTombstoneMarker ) unfiltered , header , out , version ) ; <nl> + serialize ( ( RangeTombstoneMarker ) unfiltered , header , out , previousUnfilteredSize , version ) ; <nl> } <nl> else <nl> { <nl> - serialize ( ( Row ) unfiltered , header , out , version ) ; <nl> + serialize ( ( Row ) unfiltered , header , out , previousUnfilteredSize , version ) ; <nl> } <nl> } <nl> <nl> - public void serialize ( Row row , SerializationHeader header , DataOutputPlus out , int version ) <nl> + public void serializeStaticRow ( Row row , SerializationHeader header , DataOutputPlus out , int version ) <nl> + throws IOException <nl> + { <nl> + assert row . isStatic ( ) ; <nl> + serialize ( row , header , out , 0 , version ) ; <nl> + } <nl> + <nl> + private void serialize ( Row row , SerializationHeader header , DataOutputPlus out , long previousUnfilteredSize , int version ) <nl> throws IOException <nl> { <nl> int flags = 0 ; <nl> int extendedFlags = 0 ; <nl> - boolean hasExtendedFlags = false ; <nl> <nl> boolean isStatic = row . isStatic ( ) ; <nl> Columns headerColumns = header . columns ( isStatic ) ; <nl> @ @ - 113 , 12 + 128 , 10 @ @ public class UnfilteredSerializer <nl> Row . Deletion deletion = row . deletion ( ) ; <nl> boolean hasComplexDeletion = row . hasComplexDeletion ( ) ; <nl> boolean hasAllColumns = ( row . size ( ) = = headerColumns . size ( ) ) ; <nl> + boolean hasExtendedFlags = hasExtendedFlags ( row ) ; <nl> <nl> if ( isStatic ) <nl> - { <nl> - hasExtendedFlags = true ; <nl> extendedFlags | = IS _ STATIC ; <nl> - } <nl> <nl> if ( ! pkLiveness . isEmpty ( ) ) <nl> flags | = HAS _ TIMESTAMP ; <nl> @ @ - 128 , 10 + 141 , 7 @ @ public class UnfilteredSerializer <nl> { <nl> flags | = HAS _ DELETION ; <nl> if ( deletion . isShadowable ( ) ) <nl> - { <nl> - hasExtendedFlags = true ; <nl> extendedFlags | = HAS _ SHADOWABLE _ DELETION ; <nl> - } <nl> } <nl> if ( hasComplexDeletion ) <nl> flags | = HAS _ COMPLEX _ DELETION ; <nl> @ @ - 148 , 6 + 158 , 12 @ @ public class UnfilteredSerializer <nl> if ( ! isStatic ) <nl> Clustering . serializer . serialize ( row . clustering ( ) , out , version , header . clusteringTypes ( ) ) ; <nl> <nl> + if ( header . isForSSTable ( ) ) <nl> + { <nl> + out . writeUnsignedVInt ( serializedRowBodySize ( row , header , previousUnfilteredSize , version ) ) ; <nl> + out . writeUnsignedVInt ( previousUnfilteredSize ) ; <nl> + } <nl> + <nl> if ( ( flags & HAS _ TIMESTAMP ) ! = 0 ) <nl> header . writeTimestamp ( pkLiveness . timestamp ( ) , out ) ; <nl> if ( ( flags & HAS _ TTL ) ! = 0 ) <nl> @ @ - 181 , 12 + 197 , 18 @ @ public class UnfilteredSerializer <nl> Cell . serializer . serialize ( cell , out , rowLiveness , header ) ; <nl> } <nl> <nl> - public void serialize ( RangeTombstoneMarker marker , SerializationHeader header , DataOutputPlus out , int version ) <nl> + private void serialize ( RangeTombstoneMarker marker , SerializationHeader header , DataOutputPlus out , long previousUnfilteredSize , int version ) <nl> throws IOException <nl> { <nl> out . writeByte ( ( byte ) IS _ MARKER ) ; <nl> RangeTombstone . Bound . serializer . serialize ( marker . clustering ( ) , out , version , header . clusteringTypes ( ) ) ; <nl> <nl> + if ( header . isForSSTable ( ) ) <nl> + { <nl> + out . writeUnsignedVInt ( serializedMarkerBodySize ( marker , header , previousUnfilteredSize , version ) ) ; <nl> + out . writeUnsignedVInt ( previousUnfilteredSize ) ; <nl> + } <nl> + <nl> if ( marker . isBoundary ( ) ) <nl> { <nl> RangeTombstoneBoundaryMarker bm = ( RangeTombstoneBoundaryMarker ) marker ; <nl> @ @ - 201 , 15 + 223 , 37 @ @ public class UnfilteredSerializer <nl> <nl> public long serializedSize ( Unfiltered unfiltered , SerializationHeader header , int version ) <nl> { <nl> + assert ! header . isForSSTable ( ) ; <nl> + return serializedSize ( unfiltered , header , 0 , version ) ; <nl> + } <nl> + <nl> + public long serializedSize ( Unfiltered unfiltered , SerializationHeader header , long previousUnfilteredSize , int version ) <nl> + { <nl> return unfiltered . kind ( ) = = Unfiltered . Kind . RANGE _ TOMBSTONE _ MARKER <nl> - ? serializedSize ( ( RangeTombstoneMarker ) unfiltered , header , version ) <nl> - : serializedSize ( ( Row ) unfiltered , header , version ) ; <nl> + ? serializedSize ( ( RangeTombstoneMarker ) unfiltered , header , previousUnfilteredSize , version ) <nl> + : serializedSize ( ( Row ) unfiltered , header , previousUnfilteredSize , version ) ; <nl> } <nl> <nl> - public long serializedSize ( Row row , SerializationHeader header , int version ) <nl> + private long serializedSize ( Row row , SerializationHeader header , long previousUnfilteredSize , int version ) <nl> { <nl> long size = 1 ; / / flags <nl> <nl> + if ( hasExtendedFlags ( row ) ) <nl> + size + = 1 ; / / extended flags <nl> + <nl> + if ( ! row . isStatic ( ) ) <nl> + size + = Clustering . serializer . serializedSize ( row . clustering ( ) , version , header . clusteringTypes ( ) ) ; <nl> + <nl> + return size + serializedRowBodySize ( row , header , previousUnfilteredSize , version ) ; <nl> + } <nl> + <nl> + private long serializedRowBodySize ( Row row , SerializationHeader header , long previousUnfilteredSize , int version ) <nl> + { <nl> + long size = 0 ; <nl> + <nl> + if ( header . isForSSTable ( ) ) <nl> + size + = TypeSizes . sizeofUnsignedVInt ( previousUnfilteredSize ) ; <nl> + <nl> boolean isStatic = row . isStatic ( ) ; <nl> Columns headerColumns = header . columns ( isStatic ) ; <nl> LivenessInfo pkLiveness = row . primaryKeyLivenessInfo ( ) ; <nl> @ @ - 217 , 12 + 261 , 6 @ @ public class UnfilteredSerializer <nl> boolean hasComplexDeletion = row . hasComplexDeletion ( ) ; <nl> boolean hasAllColumns = ( row . size ( ) = = headerColumns . size ( ) ) ; <nl> <nl> - if ( isStatic | | deletion . isShadowable ( ) ) <nl> - size + = 1 ; / / extended flags <nl> - <nl> - if ( ! isStatic ) <nl> - size + = Clustering . serializer . serializedSize ( row . clustering ( ) , version , header . clusteringTypes ( ) ) ; <nl> - <nl> if ( ! pkLiveness . isEmpty ( ) ) <nl> size + = header . timestampSerializedSize ( pkLiveness . timestamp ( ) ) ; <nl> if ( pkLiveness . isExpiring ( ) ) <nl> @ @ - 261 , 10 + 299 , 19 @ @ public class UnfilteredSerializer <nl> return size ; <nl> } <nl> <nl> - public long serializedSize ( RangeTombstoneMarker marker , SerializationHeader header , int version ) <nl> + private long serializedSize ( RangeTombstoneMarker marker , SerializationHeader header , long previousUnfilteredSize , int version ) <nl> { <nl> - long size = 1 / / flags <nl> - + RangeTombstone . Bound . serializer . serializedSize ( marker . clustering ( ) , version , header . clusteringTypes ( ) ) ; <nl> + assert ! header . isForSSTable ( ) ; <nl> + return 1 / / flags <nl> + + RangeTombstone . Bound . serializer . serializedSize ( marker . clustering ( ) , version , header . clusteringTypes ( ) ) <nl> + + serializedMarkerBodySize ( marker , header , previousUnfilteredSize , version ) ; <nl> + } <nl> + <nl> + private long serializedMarkerBodySize ( RangeTombstoneMarker marker , SerializationHeader header , long previousUnfilteredSize , int version ) <nl> + { <nl> + long size = 0 ; <nl> + if ( header . isForSSTable ( ) ) <nl> + size + = TypeSizes . sizeofUnsignedVInt ( previousUnfilteredSize ) ; <nl> <nl> if ( marker . isBoundary ( ) ) <nl> { <nl> @ @ - 299 , 7 + 346 , 7 @ @ public class UnfilteredSerializer <nl> if ( isEndOfPartition ( flags ) ) <nl> return null ; <nl> <nl> - int extendedFlags = isExtended ( flags ) ? in . readUnsignedByte ( ) : 0 ; <nl> + int extendedFlags = readExtendedFlags ( in , flags ) ; <nl> <nl> if ( kind ( flags ) = = Unfiltered . Kind . RANGE _ TOMBSTONE _ MARKER ) <nl> { <nl> @ @ - 328 , 6 + 375 , 12 @ @ public class UnfilteredSerializer <nl> public RangeTombstoneMarker deserializeMarkerBody ( DataInputPlus in , SerializationHeader header , RangeTombstone . Bound bound ) <nl> throws IOException <nl> { <nl> + if ( header . isForSSTable ( ) ) <nl> + { <nl> + in . readUnsignedVInt ( ) ; / / marker size <nl> + in . readUnsignedVInt ( ) ; / / previous unfiltered size <nl> + } <nl> + <nl> if ( bound . isBoundary ( ) ) <nl> return new RangeTombstoneBoundaryMarker ( bound , header . readDeletionTime ( in ) , header . readDeletionTime ( in ) ) ; <nl> else <nl> @ @ - 353 , 6 + 406 , 12 @ @ public class UnfilteredSerializer <nl> boolean hasAllColumns = ( flags & HAS _ ALL _ COLUMNS ) ! = 0 ; <nl> Columns headerColumns = header . columns ( isStatic ) ; <nl> <nl> + if ( header . isForSSTable ( ) ) <nl> + { <nl> + in . readUnsignedVInt ( ) ; / / Skip row size <nl> + in . readUnsignedVInt ( ) ; / / previous unfiltered size <nl> + } <nl> + <nl> LivenessInfo rowLiveness = LivenessInfo . EMPTY ; <nl> if ( hasTimestamp ) <nl> { <nl> @ @ - 430 , 36 + 489 , 10 @ @ public class UnfilteredSerializer <nl> } <nl> } <nl> <nl> - public void skipRowBody ( DataInputPlus in , SerializationHeader header , int flags , int extendedFlags ) throws IOException <nl> + public void skipRowBody ( DataInputPlus in ) throws IOException <nl> { <nl> - boolean isStatic = isStatic ( extendedFlags ) ; <nl> - boolean hasTimestamp = ( flags & HAS _ TIMESTAMP ) ! = 0 ; <nl> - boolean hasTTL = ( flags & HAS _ TTL ) ! = 0 ; <nl> - boolean hasDeletion = ( flags & HAS _ DELETION ) ! = 0 ; <nl> - boolean hasComplexDeletion = ( flags & HAS _ COMPLEX _ DELETION ) ! = 0 ; <nl> - boolean hasAllColumns = ( flags & HAS _ ALL _ COLUMNS ) ! = 0 ; <nl> - Columns headerColumns = header . columns ( isStatic ) ; <nl> - <nl> - / / Note that we don ' t want want to use FileUtils . skipBytesFully for anything that may not have <nl> - / / the size we think due to VINT encoding <nl> - if ( hasTimestamp ) <nl> - header . skipTimestamp ( in ) ; <nl> - if ( hasTTL ) <nl> - { <nl> - header . skipLocalDeletionTime ( in ) ; <nl> - header . skipTTL ( in ) ; <nl> - } <nl> - if ( hasDeletion ) <nl> - header . skipDeletionTime ( in ) ; <nl> - <nl> - Columns columns = hasAllColumns ? headerColumns : Columns . serializer . deserializeSubset ( headerColumns , in ) ; <nl> - for ( ColumnDefinition column : columns ) <nl> - { <nl> - if ( column . isSimple ( ) ) <nl> - Cell . serializer . skip ( in , column , header ) ; <nl> - else <nl> - skipComplexColumn ( in , column , header , hasComplexDeletion ) ; <nl> - } <nl> + int rowSize = ( int ) in . readUnsignedVInt ( ) ; <nl> + in . skipBytesFully ( rowSize ) ; <nl> } <nl> <nl> public void skipStaticRow ( DataInputPlus in , SerializationHeader header , SerializationHelper helper ) throws IOException <nl> @ @ - 468 , 20 + 501 , 13 @ @ public class UnfilteredSerializer <nl> assert ! isEndOfPartition ( flags ) & & kind ( flags ) = = Unfiltered . Kind . ROW & & isExtended ( flags ) : " Flags is " + flags ; <nl> int extendedFlags = in . readUnsignedByte ( ) ; <nl> assert isStatic ( extendedFlags ) ; <nl> - skipRowBody ( in , header , flags , extendedFlags ) ; <nl> + skipRowBody ( in ) ; <nl> } <nl> <nl> - public void skipMarkerBody ( DataInputPlus in , SerializationHeader header , boolean isBoundary ) throws IOException <nl> + public void skipMarkerBody ( DataInputPlus in ) throws IOException <nl> { <nl> - if ( isBoundary ) <nl> - { <nl> - header . skipDeletionTime ( in ) ; <nl> - header . skipDeletionTime ( in ) ; <nl> - } <nl> - else <nl> - { <nl> - header . skipDeletionTime ( in ) ; <nl> - } <nl> + int markerSize = ( int ) in . readUnsignedVInt ( ) ; <nl> + in . skipBytesFully ( markerSize ) ; <nl> } <nl> <nl> private void skipComplexColumn ( DataInputPlus in , ColumnDefinition column , SerializationHeader header , boolean hasComplexDeletion ) <nl> @ @ - 510 , 8 + 536 , 18 @ @ public class UnfilteredSerializer <nl> return ( extendedFlags & IS _ STATIC ) ! = 0 ; <nl> } <nl> <nl> - public static boolean isExtended ( int flags ) <nl> + private static boolean isExtended ( int flags ) <nl> { <nl> return ( flags & EXTENSION _ FLAG ) ! = 0 ; <nl> } <nl> + <nl> + public static int readExtendedFlags ( DataInputPlus in , int flags ) throws IOException <nl> + { <nl> + return isExtended ( flags ) ? in . readUnsignedByte ( ) : 0 ; <nl> + } <nl> + <nl> + public static boolean hasExtendedFlags ( Row row ) <nl> + { <nl> + return row . isStatic ( ) | | row . deletion ( ) . isShadowable ( ) ; <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / AbstractSSTableSimpleWriter . java b / src / java / org / apache / cassandra / io / sstable / AbstractSSTableSimpleWriter . java <nl> index d94b219 . . 62348ec 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / AbstractSSTableSimpleWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / AbstractSSTableSimpleWriter . java <nl> @ @ - 65 , 7 + 65 , 7 @ @ abstract class AbstractSSTableSimpleWriter implements Closeable <nl> 0 , <nl> ActiveRepairService . UNREPAIRED _ SSTABLE , <nl> 0 , <nl> - new SerializationHeader ( metadata , columns , EncodingStats . NO _ STATS ) ) ; <nl> + new SerializationHeader ( true , metadata , columns , EncodingStats . NO _ STATS ) ) ; <nl> } <nl> <nl> private static Descriptor createDescriptor ( File directory , final String keyspace , final String columnFamily , final SSTableFormat . Type fmt ) <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableSimpleUnsortedWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableSimpleUnsortedWriter . java <nl> index f4b9adf . . 6d3a714 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableSimpleUnsortedWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableSimpleUnsortedWriter . java <nl> @ @ - 63 , 7 + 63 , 7 @ @ class SSTableSimpleUnsortedWriter extends AbstractSSTableSimpleWriter <nl> { <nl> super ( directory , metadata , columns ) ; <nl> this . bufferSize = bufferSizeInMB * 1024L * 1024L ; <nl> - this . header = new SerializationHeader ( metadata , columns , EncodingStats . NO _ STATS ) ; <nl> + this . header = new SerializationHeader ( true , metadata , columns , EncodingStats . NO _ STATS ) ; <nl> diskWriter . start ( ) ; <nl> } <nl> <nl> @ @ - 89 , 7 + 89 , 7 @ @ class SSTableSimpleUnsortedWriter extends AbstractSSTableSimpleWriter <nl> / / improve that . In particular , what we count is closer to the serialized value , but it ' s debatable that it ' s the right thing <nl> / / to count since it will take a lot more space in memory and the bufferSize if first and foremost used to avoid OOM when <nl> / / using this writer . <nl> - currentSize + = UnfilteredSerializer . serializer . serializedSize ( row , header , formatType . info . getLatestVersion ( ) . correspondingMessagingVersion ( ) ) ; <nl> + currentSize + = UnfilteredSerializer . serializer . serializedSize ( row , header , 0 , formatType . info . getLatestVersion ( ) . correspondingMessagingVersion ( ) ) ; <nl> } <nl> <nl> private void maybeSync ( ) throws SyncException <nl> diff - - git a / test / unit / org / apache / cassandra / db / RowIndexEntryTest . java b / test / unit / org / apache / cassandra / db / RowIndexEntryTest . java <nl> index 25baa4e . . 62c88a0 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / RowIndexEntryTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / RowIndexEntryTest . java <nl> @ @ - 60 , 7 + 60 , 7 @ @ public class RowIndexEntryTest extends CQLTester <nl> <nl> DeletionTime deletionInfo = new DeletionTime ( FBUtilities . timestampMicros ( ) , FBUtilities . nowInSeconds ( ) ) ; <nl> <nl> - SerializationHeader header = new SerializationHeader ( cfMeta , cfMeta . partitionColumns ( ) , EncodingStats . NO _ STATS ) ; <nl> + SerializationHeader header = new SerializationHeader ( true , cfMeta , cfMeta . partitionColumns ( ) , EncodingStats . NO _ STATS ) ; <nl> IndexHelper . IndexInfo . Serializer indexSerializer = new IndexHelper . IndexInfo . Serializer ( cfMeta , BigFormat . latestVersion , header ) ; <nl> <nl> DataOutputBuffer dob = new DataOutputBuffer ( ) ; <nl> @ @ - 119 , 7 + 119 , 7 @ @ public class RowIndexEntryTest extends CQLTester <nl> final RowIndexEntry simple = new RowIndexEntry ( 123 ) ; <nl> <nl> DataOutputBuffer buffer = new DataOutputBuffer ( ) ; <nl> - SerializationHeader header = new SerializationHeader ( cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ; <nl> + SerializationHeader header = new SerializationHeader ( true , cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ; <nl> RowIndexEntry . Serializer serializer = new RowIndexEntry . Serializer ( cfs . metadata , BigFormat . latestVersion , header ) ; <nl> <nl> serializer . serialize ( simple , buffer ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / db / ScrubTest . java b / test / unit / org / apache / cassandra / db / ScrubTest . java <nl> index 2fc8436 . . ab99750 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / ScrubTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / ScrubTest . java <nl> @ @ - 325 , 7 + 325 , 8 @ @ public class ScrubTest <nl> keys . size ( ) , <nl> 0L , <nl> 0 , <nl> - new SerializationHeader ( cfs . metadata , <nl> + new SerializationHeader ( true , <nl> + cfs . metadata , <nl> cfs . metadata . partitionColumns ( ) , <nl> EncodingStats . NO _ STATS ) ) ) <nl> { <nl> diff - - git a / test / unit / org / apache / cassandra / db / compaction / AntiCompactionTest . java b / test / unit / org / apache / cassandra / db / compaction / AntiCompactionTest . java <nl> index cd82b19 . . db07eb8 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / compaction / AntiCompactionTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / compaction / AntiCompactionTest . java <nl> @ @ - 166 , 7 + 166 , 7 @ @ public class AntiCompactionTest <nl> File dir = cfs . getDirectories ( ) . getDirectoryForNewSSTables ( ) ; <nl> String filename = cfs . getSSTablePath ( dir ) ; <nl> <nl> - try ( SSTableTxnWriter writer = SSTableTxnWriter . create ( cfs , filename , 0 , 0 , new SerializationHeader ( cfm , cfm . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) <nl> + try ( SSTableTxnWriter writer = SSTableTxnWriter . create ( cfs , filename , 0 , 0 , new SerializationHeader ( true , cfm , cfm . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) <nl> { <nl> for ( int i = 0 ; i < count ; i + + ) <nl> { <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / BigTableWriterTest . java b / test / unit / org / apache / cassandra / io / sstable / BigTableWriterTest . java <nl> index e1ab48f . . 78964f4 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / BigTableWriterTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / BigTableWriterTest . java <nl> @ @ - 69 , 7 + 69 , 7 @ @ public class BigTableWriterTest extends AbstractTransactionalTest <nl> <nl> private TestableBTW ( String file ) <nl> { <nl> - this ( file , SSTableTxnWriter . create ( cfs , file , 0 , 0 , new SerializationHeader ( cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) ; <nl> + this ( file , SSTableTxnWriter . create ( cfs , file , 0 , 0 , new SerializationHeader ( true , cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) ; <nl> } <nl> <nl> private TestableBTW ( String file , SSTableTxnWriter sw ) <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java <nl> index 093bffd . . bd286e4 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java <nl> @ @ - 943 , 7 + 943 , 7 @ @ public class SSTableRewriterTest extends SchemaLoader <nl> File dir = cfs . getDirectories ( ) . getDirectoryForNewSSTables ( ) ; <nl> String filename = cfs . getSSTablePath ( dir ) ; <nl> <nl> - try ( SSTableTxnWriter writer = SSTableTxnWriter . create ( cfs , filename , 0 , 0 , new SerializationHeader ( cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) <nl> + try ( SSTableTxnWriter writer = SSTableTxnWriter . create ( cfs , filename , 0 , 0 , new SerializationHeader ( true , cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) <nl> { <nl> int end = f = = fileCount - 1 ? partitionCount : ( ( f + 1 ) * partitionCount ) / fileCount ; <nl> for ( ; i < end ; i + + ) <nl> @ @ - 1011 , 7 + 1011 , 7 @ @ public class SSTableRewriterTest extends SchemaLoader <nl> public static SSTableWriter getWriter ( ColumnFamilyStore cfs , File directory , LifecycleTransaction txn ) <nl> { <nl> String filename = cfs . getSSTablePath ( directory ) ; <nl> - return SSTableWriter . create ( filename , 0 , 0 , new SerializationHeader ( cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) , txn ) ; <nl> + return SSTableWriter . create ( filename , 0 , 0 , new SerializationHeader ( true , cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) , txn ) ; <nl> } <nl> <nl> public static ByteBuffer random ( int i , int size ) <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java b / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java <nl> index fcd2d71 . . 5c7ff02 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java <nl> @ @ - 187 , 7 + 187 , 7 @ @ public class SSTableUtils <nl> { <nl> public SerializationHeader header ( ) <nl> { <nl> - return new SerializationHeader ( Schema . instance . getCFMetaData ( ksname , cfname ) , builder . build ( ) , EncodingStats . NO _ STATS ) ; <nl> + return new SerializationHeader ( true , Schema . instance . getCFMetaData ( ksname , cfname ) , builder . build ( ) , EncodingStats . NO _ STATS ) ; <nl> } <nl> <nl> @ Override
NEAREST DIFF (one line): diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db <nl> deleted file mode 100644 <nl> index 0f95b07 . . 0000000 <nl> Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db and / dev / null differ <nl> diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db <nl> deleted file mode 100644 <nl> index 88aac99 . . 0000000 <nl> Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db and / dev / null differ <nl> diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db <nl> deleted file mode 100644 <nl> index a7787c5 . . 0000000 <nl> Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db and / dev / null differ <nl> diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db <nl> deleted file mode 100644 <nl> index 6312c71 . . 0000000 <nl> Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db and / dev / null differ <nl> diff - - git a / test / unit / org / apache / cassandra / db / ScrubTest . java b / test / unit / org / apache / cassandra / db / ScrubTest . java <nl> new file mode 100644 <nl> index 0000000 . . 26f0e78 <nl> - - - / dev / null <nl> + + + b / test / unit / org / apache / cassandra / db / ScrubTest . java <nl> @ @ - 0 , 0 + 1 , 211 @ @ <nl> + package org . apache . cassandra . db ; <nl> + / * <nl> + * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , <nl> + * software distributed under the License is distributed on an <nl> + * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY <nl> + * KIND , either express or implied . See the License for the <nl> + * specific language governing permissions and limitations <nl> + * under the License . <nl> + * <nl> + * / <nl> + <nl> + <nl> + import java . io . File ; <nl> + import java . io . IOException ; <nl> + import java . util . List ; <nl> + import java . util . concurrent . ExecutionException ; <nl> + <nl> + import org . junit . Test ; <nl> + <nl> + import org . apache . cassandra . SchemaLoader ; <nl> + import org . apache . cassandra . Util ; <nl> + import org . apache . cassandra . exceptions . ConfigurationException ; <nl> + import org . apache . cassandra . db . columniterator . IdentityQueryFilter ; <nl> + import org . apache . cassandra . db . compaction . CompactionManager ; <nl> + import org . apache . cassandra . io . util . FileUtils ; <nl> + import org . apache . cassandra . utils . ByteBufferUtil ; <nl> + import org . apache . cassandra . utils . CLibrary ; <nl> + <nl> + import static org . apache . cassandra . Util . column ; <nl> + import static org . junit . Assert . assertEquals ; <nl> + import static org . junit . Assert . fail ; <nl> + <nl> + public class ScrubTest extends SchemaLoader <nl> + { <nl> + public String TABLE = " Keyspace1 " ; <nl> + public String CF = " Standard1 " ; <nl> + public String CF2 = " Super5 " ; <nl> + public String CF3 = " Standard2 " ; <nl> + <nl> + public String copySSTables ( String cf ) throws IOException <nl> + { <nl> + String root = System . getProperty ( " corrupt - sstable - root " ) ; <nl> + assert root ! = null ; <nl> + File rootDir = new File ( root ) ; <nl> + assert rootDir . isDirectory ( ) ; <nl> + <nl> + File destDir = Directories . create ( TABLE , cf ) . getDirectoryForNewSSTables ( 1 ) ; <nl> + <nl> + String corruptSSTableName = null ; <nl> + <nl> + FileUtils . createDirectory ( destDir ) ; <nl> + for ( File srcFile : rootDir . listFiles ( ) ) <nl> + { <nl> + if ( srcFile . getName ( ) . equals ( " . svn " ) ) <nl> + continue ; <nl> + if ( ! srcFile . getName ( ) . contains ( cf ) ) <nl> + continue ; <nl> + File destFile = new File ( destDir , srcFile . getName ( ) ) ; <nl> + CLibrary . createHardLink ( srcFile , destFile ) ; <nl> + <nl> + assert destFile . exists ( ) : destFile . getAbsoluteFile ( ) ; <nl> + <nl> + if ( destFile . getName ( ) . endsWith ( " Data . db " ) ) <nl> + corruptSSTableName = destFile . getCanonicalPath ( ) ; <nl> + } <nl> + <nl> + assert corruptSSTableName ! = null ; <nl> + return corruptSSTableName ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testScrubOneRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException <nl> + { <nl> + CompactionManager . instance . disableAutoCompaction ( ) ; <nl> + Table table = Table . open ( TABLE ) ; <nl> + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF ) ; <nl> + <nl> + List < Row > rows ; <nl> + <nl> + / / insert data and verify we get it back w / range query <nl> + fillCF ( cfs , 1 ) ; <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assertEquals ( 1 , rows . size ( ) ) ; <nl> + <nl> + CompactionManager . instance . performScrub ( cfs ) ; <nl> + <nl> + / / check data is still there <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assertEquals ( 1 , rows . size ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testScrubDeletedRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException <nl> + { <nl> + CompactionManager . instance . disableAutoCompaction ( ) ; <nl> + Table table = Table . open ( TABLE ) ; <nl> + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF3 ) ; <nl> + <nl> + RowMutation rm ; <nl> + rm = new RowMutation ( TABLE , ByteBufferUtil . bytes ( 1 ) ) ; <nl> + ColumnFamily cf = ColumnFamily . create ( TABLE , CF3 ) ; <nl> + cf . delete ( new DeletionInfo ( 0 , 1 ) ) ; / / expired tombstone <nl> + rm . add ( cf ) ; <nl> + rm . applyUnsafe ( ) ; <nl> + cfs . forceBlockingFlush ( ) ; <nl> + <nl> + CompactionManager . instance . performScrub ( cfs ) ; <nl> + assert cfs . getSSTables ( ) . isEmpty ( ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testScrubMultiRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException <nl> + { <nl> + CompactionManager . instance . disableAutoCompaction ( ) ; <nl> + Table table = Table . open ( TABLE ) ; <nl> + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF ) ; <nl> + <nl> + List < Row > rows ; <nl> + <nl> + / / insert data and verify we get it back w / range query <nl> + fillCF ( cfs , 10 ) ; <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assertEquals ( 10 , rows . size ( ) ) ; <nl> + <nl> + CompactionManager . instance . performScrub ( cfs ) ; <nl> + <nl> + / / check data is still there <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assertEquals ( 10 , rows . size ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testScubOutOfOrder ( ) throws Exception <nl> + { <nl> + CompactionManager . instance . disableAutoCompaction ( ) ; <nl> + Table table = Table . open ( TABLE ) ; <nl> + String columnFamily = " Standard3 " ; <nl> + ColumnFamilyStore cfs = table . getColumnFamilyStore ( columnFamily ) ; <nl> + <nl> + / * <nl> + * Code used to generate an outOfOrder sstable . The test for out - of - order key in SSTableWriter must also be commented out . <nl> + * The test also assumes an ordered partitioner . <nl> + * <nl> + * ColumnFamily cf = ColumnFamily . create ( TABLE , columnFamily ) ; <nl> + * cf . addColumn ( new Column ( ByteBufferUtil . bytes ( " someName " ) , ByteBufferUtil . bytes ( " someValue " ) , 0L ) ) ; <nl> + <nl> + * SSTableWriter writer = cfs . createCompactionWriter ( ( long ) DatabaseDescriptor . getIndexInterval ( ) , new File ( " . " ) , Collections . < SSTableReader > emptyList ( ) ) ; <nl> + * writer . append ( Util . dk ( " a " ) , cf ) ; <nl> + * writer . append ( Util . dk ( " b " ) , cf ) ; <nl> + * writer . append ( Util . dk ( " z " ) , cf ) ; <nl> + * writer . append ( Util . dk ( " c " ) , cf ) ; <nl> + * writer . append ( Util . dk ( " y " ) , cf ) ; <nl> + * writer . append ( Util . dk ( " d " ) , cf ) ; <nl> + * writer . closeAndOpenReader ( ) ; <nl> + * / <nl> + <nl> + copySSTables ( columnFamily ) ; <nl> + cfs . loadNewSSTables ( ) ; <nl> + assert cfs . getSSTables ( ) . size ( ) > 0 ; <nl> + <nl> + List < Row > rows ; <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assert ! isRowOrdered ( rows ) : " ' corrupt ' test file actually was not " ; <nl> + <nl> + CompactionManager . instance . performScrub ( cfs ) ; <nl> + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; <nl> + assert isRowOrdered ( rows ) : " Scrub failed : " + rows ; <nl> + assert rows . size ( ) = = 6 : " Got " + rows . size ( ) ; <nl> + } <nl> + <nl> + private static boolean isRowOrdered ( List < Row > rows ) <nl> + { <nl> + DecoratedKey prev = null ; <nl> + for ( Row row : rows ) <nl> + { <nl> + if ( prev ! = null & & prev . compareTo ( row . key ) > 0 ) <nl> + return false ; <nl> + prev = row . key ; <nl> + } <nl> + return true ; <nl> + } <nl> + <nl> + protected void fillCF ( ColumnFamilyStore cfs , int rowsPerSSTable ) throws ExecutionException , InterruptedException , IOException <nl> + { <nl> + for ( int i = 0 ; i < rowsPerSSTable ; i + + ) <nl> + { <nl> + String key = String . valueOf ( i ) ; <nl> + / / create a row and update the birthdate value , test that the index query fetches the new version <nl> + RowMutation rm ; <nl> + rm = new RowMutation ( TABLE , ByteBufferUtil . bytes ( key ) ) ; <nl> + ColumnFamily cf = ColumnFamily . create ( TABLE , CF ) ; <nl> + cf . addColumn ( column ( " c1 " , " 1 " , 1L ) ) ; <nl> + cf . addColumn ( column ( " c2 " , " 2 " , 1L ) ) ; <nl> + rm . add ( cf ) ; <nl> + rm . applyUnsafe ( ) ; <nl> + } <nl> + <nl> + cfs . forceBlockingFlush ( ) ; <nl> + } <nl> + }

TEST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / ColumnIndex . java b / src / java / org / apache / cassandra / db / ColumnIndex . java 
 index add5fa7 . . ede3f79 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnIndex . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnIndex . java 
 @ @ - 76 , 6 + 76 , 7 @ @ public class ColumnIndex 
 private long startPosition = - 1 ; 
 
 private int written ; 
 + private long previousRowStart ; 
 
 private ClusteringPrefix firstClustering ; 
 private ClusteringPrefix lastClustering ; 
 @ @ - 99 , 7 + 100 , 7 @ @ public class ColumnIndex 
 ByteBufferUtil . writeWithShortLength ( iterator . partitionKey ( ) . getKey ( ) , writer ) ; 
 DeletionTime . serializer . serialize ( iterator . partitionLevelDeletion ( ) , writer ) ; 
 if ( header . hasStatic ( ) ) 
 - UnfilteredSerializer . serializer . serialize ( iterator . staticRow ( ) , header , writer , version ) ; 
 + UnfilteredSerializer . serializer . serializeStaticRow ( iterator . staticRow ( ) , header , writer , version ) ; 
 } 
 
 public ColumnIndex build ( ) throws IOException 
 @ @ - 131 , 15 + 132 , 18 @ @ public class ColumnIndex 
 
 private void add ( Unfiltered unfiltered ) throws IOException 
 { 
 + long pos = currentPosition ( ) ; 
 + 
 if ( firstClustering = = null ) 
 { 
 / / Beginning of an index block . Remember the start and position 
 firstClustering = unfiltered . clustering ( ) ; 
 - startPosition = currentPosition ( ) ; 
 + startPosition = pos ; 
 } 
 
 - UnfilteredSerializer . serializer . serialize ( unfiltered , header , writer , version ) ; 
 + UnfilteredSerializer . serializer . serialize ( unfiltered , header , writer , pos - previousRowStart , version ) ; 
 lastClustering = unfiltered . clustering ( ) ; 
 + previousRowStart = pos ; 
 + + written ; 
 
 if ( unfiltered . kind ( ) = = Unfiltered . Kind . RANGE _ TOMBSTONE _ MARKER ) 
 diff - - git a / src / java / org / apache / cassandra / db / Memtable . java b / src / java / org / apache / cassandra / db / Memtable . java 
 index 014467e . . f47efe3 100644 
 - - - a / src / java / org / apache / cassandra / db / Memtable . java 
 + + + b / src / java / org / apache / cassandra / db / Memtable . java 
 @ @ - 430 , 7 + 430 , 7 @ @ public class Memtable implements Comparable < Memtable > 
 ( long ) partitions . size ( ) , 
 ActiveRepairService . UNREPAIRED _ SSTABLE , 
 sstableMetadataCollector , 
 - new SerializationHeader ( cfs . metadata , columns , stats ) , 
 + new SerializationHeader ( true , cfs . metadata , columns , stats ) , 
 txn ) ) ; 
 } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / SerializationHeader . java b / src / java / org / apache / cassandra / db / SerializationHeader . java 
 index decac49 . . 0706d06 100644 
 - - - a / src / java / org / apache / cassandra / db / SerializationHeader . java 
 + + + b / src / java / org / apache / cassandra / db / SerializationHeader . java 
 @ @ - 45 , 6 + 45 , 8 @ @ public class SerializationHeader 
 { 
 public static final Serializer serializer = new Serializer ( ) ; 
 
 + private final boolean isForSSTable ; 
 + 
 private final AbstractType < ? > keyType ; 
 private final List < AbstractType < ? > > clusteringTypes ; 
 
 @ @ - 53 , 12 + 55 , 14 @ @ public class SerializationHeader 
 
 private final Map < ByteBuffer , AbstractType < ? > > typeMap ; 
 
 - private SerializationHeader ( AbstractType < ? > keyType , 
 + private SerializationHeader ( boolean isForSSTable , 
 + AbstractType < ? > keyType , 
 List < AbstractType < ? > > clusteringTypes , 
 PartitionColumns columns , 
 EncodingStats stats , 
 Map < ByteBuffer , AbstractType < ? > > typeMap ) 
 { 
 + this . isForSSTable = isForSSTable ; 
 this . keyType = keyType ; 
 this . clusteringTypes = clusteringTypes ; 
 this . columns = columns ; 
 @ @ - 77 , 7 + 81 , 8 @ @ public class SerializationHeader 
 List < AbstractType < ? > > clusteringTypes = new ArrayList < > ( size ) ; 
 for ( int i = 0 ; i < size ; i + + ) 
 clusteringTypes . add ( BytesType . instance ) ; 
 - return new SerializationHeader ( BytesType . instance , 
 + return new SerializationHeader ( false , 
 + BytesType . instance , 
 clusteringTypes , 
 PartitionColumns . NONE , 
 EncodingStats . NO _ STATS , 
 @ @ - 108 , 14 + 113 , 16 @ @ public class SerializationHeader 
 else 
 columns . addAll ( sstable . header . columns ( ) ) ; 
 } 
 - return new SerializationHeader ( metadata , columns . build ( ) , stats . get ( ) ) ; 
 + return new SerializationHeader ( true , metadata , columns . build ( ) , stats . get ( ) ) ; 
 } 
 
 - public SerializationHeader ( CFMetaData metadata , 
 + public SerializationHeader ( boolean isForSSTable , 
 + CFMetaData metadata , 
 PartitionColumns columns , 
 EncodingStats stats ) 
 { 
 - this ( metadata . getKeyValidator ( ) , 
 + this ( isForSSTable , 
 + metadata . getKeyValidator ( ) , 
 typesOf ( metadata . clusteringColumns ( ) ) , 
 columns , 
 stats , 
 @ @ - 137 , 6 + 144 , 11 @ @ public class SerializationHeader 
 return ! columns . statics . isEmpty ( ) ; 
 } 
 
 + public boolean isForSSTable ( ) 
 + { 
 + return isForSSTable ; 
 + } 
 + 
 public EncodingStats stats ( ) 
 { 
 return stats ; 
 @ @ - 320 , 7 + 332 , 7 @ @ public class SerializationHeader 
 } 
 builder . add ( column ) ; 
 } 
 - return new SerializationHeader ( keyType , clusteringTypes , builder . build ( ) , stats , typeMap ) ; 
 + return new SerializationHeader ( true , keyType , clusteringTypes , builder . build ( ) , stats , typeMap ) ; 
 } 
 
 @ Override 
 @ @ - 390 , 7 + 402 , 7 @ @ public class SerializationHeader 
 regulars = Columns . serializer . deserializeSubset ( selection . fetchedColumns ( ) . regulars , in ) ; 
 } 
 
 - return new SerializationHeader ( keyType , clusteringTypes , new PartitionColumns ( statics , regulars ) , stats , null ) ; 
 + return new SerializationHeader ( false , keyType , clusteringTypes , new PartitionColumns ( statics , regulars ) , stats , null ) ; 
 } 
 
 public long serializedSizeForMessaging ( SerializationHeader header , ColumnFilter selection , boolean hasStatic ) 
 diff - - git a / src / java / org / apache / cassandra / db / UnfilteredDeserializer . java b / src / java / org / apache / cassandra / db / UnfilteredDeserializer . java 
 index 5c76c63 . . ef30289 100644 
 - - - a / src / java / org / apache / cassandra / db / UnfilteredDeserializer . java 
 + + + b / src / java / org / apache / cassandra / db / UnfilteredDeserializer . java 
 @ @ - 147 , 7 + 147 , 7 @ @ public abstract class UnfilteredDeserializer 
 return ; 
 } 
 
 - nextExtendedFlags = UnfilteredSerializer . isExtended ( nextFlags ) ? in . readUnsignedByte ( ) : 0 ; 
 + nextExtendedFlags = UnfilteredSerializer . readExtendedFlags ( in , nextFlags ) ; 
 
 clusteringDeserializer . prepare ( nextFlags , nextExtendedFlags ) ; 
 isReady = true ; 
 @ @ - 195 , 14 + 195 , 14 @ @ public abstract class UnfilteredDeserializer 
 public void skipNext ( ) throws IOException 
 { 
 isReady = false ; 
 - ClusteringPrefix . Kind kind = clusteringDeserializer . skipNext ( ) ; 
 + clusteringDeserializer . skipNext ( ) ; 
 if ( UnfilteredSerializer . kind ( nextFlags ) = = Unfiltered . Kind . RANGE _ TOMBSTONE _ MARKER ) 
 { 
 - UnfilteredSerializer . serializer . skipMarkerBody ( in , header , kind . isBoundary ( ) ) ; 
 + UnfilteredSerializer . serializer . skipMarkerBody ( in ) ; 
 } 
 else 
 { 
 - UnfilteredSerializer . serializer . skipRowBody ( in , header , nextFlags , nextExtendedFlags ) ; 
 + UnfilteredSerializer . serializer . skipRowBody ( in ) ; 
 } 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java b / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java 
 index df006d7 . . 3a0558e 100644 
 - - - a / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java 
 + + + b / src / java / org / apache / cassandra / db / rows / UnfilteredRowIteratorSerializer . java 
 @ @ - 80 , 7 + 80 , 8 @ @ public class UnfilteredRowIteratorSerializer 
 / / Should only be used for the on - wire format . 
 public void serialize ( UnfilteredRowIterator iterator , ColumnFilter selection , DataOutputPlus out , int version , int rowEstimate ) throws IOException 
 { 
 - SerializationHeader header = new SerializationHeader ( iterator . metadata ( ) , 
 + SerializationHeader header = new SerializationHeader ( false , 
 + iterator . metadata ( ) , 
 iterator . columns ( ) , 
 iterator . stats ( ) ) ; 
 serialize ( iterator , header , selection , out , version , rowEstimate ) ; 
 @ @ - 89 , 6 + 90 , 8 @ @ public class UnfilteredRowIteratorSerializer 
 / / Should only be used for the on - wire format . 
 public void serialize ( UnfilteredRowIterator iterator , SerializationHeader header , ColumnFilter selection , DataOutputPlus out , int version , int rowEstimate ) throws IOException 
 { 
 + assert ! header . isForSSTable ( ) ; 
 + 
 ByteBufferUtil . writeWithVIntLength ( iterator . partitionKey ( ) . getKey ( ) , out ) ; 
 
 int flags = 0 ; 
 @ @ - 134 , 7 + 137 , 8 @ @ public class UnfilteredRowIteratorSerializer 
 / / recreate an iterator for both serialize and serializedSize , which is mostly only PartitionUpdate / ArrayBackedCachedPartition . 
 public long serializedSize ( UnfilteredRowIterator iterator , ColumnFilter selection , int version , int rowEstimate ) 
 { 
 - SerializationHeader header = new SerializationHeader ( iterator . metadata ( ) , 
 + SerializationHeader header = new SerializationHeader ( false , 
 + iterator . metadata ( ) , 
 iterator . columns ( ) , 
 iterator . stats ( ) ) ; 
 
 @ @ - 175 , 7 + 179 , 7 @ @ public class UnfilteredRowIteratorSerializer 
 boolean isReversed = ( flags & IS _ REVERSED ) ! = 0 ; 
 if ( ( flags & IS _ EMPTY ) ! = 0 ) 
 { 
 - SerializationHeader sh = new SerializationHeader ( metadata , PartitionColumns . NONE , EncodingStats . NO _ STATS ) ; 
 + SerializationHeader sh = new SerializationHeader ( false , metadata , PartitionColumns . NONE , EncodingStats . NO _ STATS ) ; 
 return new Header ( sh , key , isReversed , true , null , null , 0 ) ; 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java b / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java 
 index b83ccf9 . . 4efc5eb 100644 
 - - - a / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java 
 + + + b / src / java / org / apache / cassandra / db / rows / UnfilteredSerializer . java 
 @ @ - 35 , 10 + 35 , 12 @ @ import org . apache . cassandra . io . util . DataOutputPlus ; 
 * flag is defined / explained below as the " Unfiltered flags " constants . One of those flags 
 * is an extension flag , and if present , trigger the rid of another byte that contains more 
 * flags . If the extension is not set , defaults are assumed for the flags of that 2nd byte . 
 - * < row > is < clustering > [ < timestamp > ] [ < ttl > ] [ < deletion > ] < sc1 > . . . < sci > < cc1 > . . . < ccj > where 
 - * < clustering > is the row clustering as serialized by 
 - * { @ code Clustering . serializer } . Note that static row are an exception and 
 - * don ' t have this . < timestamp > , < ttl > and < deletion > are the row timestamp , ttl and deletion 
 + * < row > is < clustering > < size > [ < timestamp > ] [ < ttl > ] [ < deletion > ] < sc1 > . . . < sci > < cc1 > . . . < ccj > where 
 + * < clustering > is the row clustering as serialized by { @ code Clustering . serializer } ( note 
 + * that static row are an exception and don ' t have this ) . 
 + * < size > is the size of the whole unfiltered on disk ( it ' s only used for sstables and is 
 + * used to efficiently skip rows ) . 
 + * < timestamp > , < ttl > and < deletion > are the row timestamp , ttl and deletion 
 * whose presence is determined by the flags . < sci > is the simple columns of the row and < ccj > the 
 * complex ones . 
 * The columns for the row are then serialized if they differ from those in the header , 
 @ @ - 90 , 22 + 92 , 35 @ @ public class UnfilteredSerializer 
 public void serialize ( Unfiltered unfiltered , SerializationHeader header , DataOutputPlus out , int version ) 
 throws IOException 
 { 
 + assert ! header . isForSSTable ( ) ; 
 + serialize ( unfiltered , header , out , 0 , version ) ; 
 + } 
 + 
 + public void serialize ( Unfiltered unfiltered , SerializationHeader header , DataOutputPlus out , long previousUnfilteredSize , int version ) 
 + throws IOException 
 + { 
 if ( unfiltered . kind ( ) = = Unfiltered . Kind . RANGE _ TOMBSTONE _ MARKER ) 
 { 
 - serialize ( ( RangeTombstoneMarker ) unfiltered , header , out , version ) ; 
 + serialize ( ( RangeTombstoneMarker ) unfiltered , header , out , previousUnfilteredSize , version ) ; 
 } 
 else 
 { 
 - serialize ( ( Row ) unfiltered , header , out , version ) ; 
 + serialize ( ( Row ) unfiltered , header , out , previousUnfilteredSize , version ) ; 
 } 
 } 
 
 - public void serialize ( Row row , SerializationHeader header , DataOutputPlus out , int version ) 
 + public void serializeStaticRow ( Row row , SerializationHeader header , DataOutputPlus out , int version ) 
 + throws IOException 
 + { 
 + assert row . isStatic ( ) ; 
 + serialize ( row , header , out , 0 , version ) ; 
 + } 
 + 
 + private void serialize ( Row row , SerializationHeader header , DataOutputPlus out , long previousUnfilteredSize , int version ) 
 throws IOException 
 { 
 int flags = 0 ; 
 int extendedFlags = 0 ; 
 - boolean hasExtendedFlags = false ; 
 
 boolean isStatic = row . isStatic ( ) ; 
 Columns headerColumns = header . columns ( isStatic ) ; 
 @ @ - 113 , 12 + 128 , 10 @ @ public class UnfilteredSerializer 
 Row . Deletion deletion = row . deletion ( ) ; 
 boolean hasComplexDeletion = row . hasComplexDeletion ( ) ; 
 boolean hasAllColumns = ( row . size ( ) = = headerColumns . size ( ) ) ; 
 + boolean hasExtendedFlags = hasExtendedFlags ( row ) ; 
 
 if ( isStatic ) 
 - { 
 - hasExtendedFlags = true ; 
 extendedFlags | = IS _ STATIC ; 
 - } 
 
 if ( ! pkLiveness . isEmpty ( ) ) 
 flags | = HAS _ TIMESTAMP ; 
 @ @ - 128 , 10 + 141 , 7 @ @ public class UnfilteredSerializer 
 { 
 flags | = HAS _ DELETION ; 
 if ( deletion . isShadowable ( ) ) 
 - { 
 - hasExtendedFlags = true ; 
 extendedFlags | = HAS _ SHADOWABLE _ DELETION ; 
 - } 
 } 
 if ( hasComplexDeletion ) 
 flags | = HAS _ COMPLEX _ DELETION ; 
 @ @ - 148 , 6 + 158 , 12 @ @ public class UnfilteredSerializer 
 if ( ! isStatic ) 
 Clustering . serializer . serialize ( row . clustering ( ) , out , version , header . clusteringTypes ( ) ) ; 
 
 + if ( header . isForSSTable ( ) ) 
 + { 
 + out . writeUnsignedVInt ( serializedRowBodySize ( row , header , previousUnfilteredSize , version ) ) ; 
 + out . writeUnsignedVInt ( previousUnfilteredSize ) ; 
 + } 
 + 
 if ( ( flags & HAS _ TIMESTAMP ) ! = 0 ) 
 header . writeTimestamp ( pkLiveness . timestamp ( ) , out ) ; 
 if ( ( flags & HAS _ TTL ) ! = 0 ) 
 @ @ - 181 , 12 + 197 , 18 @ @ public class UnfilteredSerializer 
 Cell . serializer . serialize ( cell , out , rowLiveness , header ) ; 
 } 
 
 - public void serialize ( RangeTombstoneMarker marker , SerializationHeader header , DataOutputPlus out , int version ) 
 + private void serialize ( RangeTombstoneMarker marker , SerializationHeader header , DataOutputPlus out , long previousUnfilteredSize , int version ) 
 throws IOException 
 { 
 out . writeByte ( ( byte ) IS _ MARKER ) ; 
 RangeTombstone . Bound . serializer . serialize ( marker . clustering ( ) , out , version , header . clusteringTypes ( ) ) ; 
 
 + if ( header . isForSSTable ( ) ) 
 + { 
 + out . writeUnsignedVInt ( serializedMarkerBodySize ( marker , header , previousUnfilteredSize , version ) ) ; 
 + out . writeUnsignedVInt ( previousUnfilteredSize ) ; 
 + } 
 + 
 if ( marker . isBoundary ( ) ) 
 { 
 RangeTombstoneBoundaryMarker bm = ( RangeTombstoneBoundaryMarker ) marker ; 
 @ @ - 201 , 15 + 223 , 37 @ @ public class UnfilteredSerializer 
 
 public long serializedSize ( Unfiltered unfiltered , SerializationHeader header , int version ) 
 { 
 + assert ! header . isForSSTable ( ) ; 
 + return serializedSize ( unfiltered , header , 0 , version ) ; 
 + } 
 + 
 + public long serializedSize ( Unfiltered unfiltered , SerializationHeader header , long previousUnfilteredSize , int version ) 
 + { 
 return unfiltered . kind ( ) = = Unfiltered . Kind . RANGE _ TOMBSTONE _ MARKER 
 - ? serializedSize ( ( RangeTombstoneMarker ) unfiltered , header , version ) 
 - : serializedSize ( ( Row ) unfiltered , header , version ) ; 
 + ? serializedSize ( ( RangeTombstoneMarker ) unfiltered , header , previousUnfilteredSize , version ) 
 + : serializedSize ( ( Row ) unfiltered , header , previousUnfilteredSize , version ) ; 
 } 
 
 - public long serializedSize ( Row row , SerializationHeader header , int version ) 
 + private long serializedSize ( Row row , SerializationHeader header , long previousUnfilteredSize , int version ) 
 { 
 long size = 1 ; / / flags 
 
 + if ( hasExtendedFlags ( row ) ) 
 + size + = 1 ; / / extended flags 
 + 
 + if ( ! row . isStatic ( ) ) 
 + size + = Clustering . serializer . serializedSize ( row . clustering ( ) , version , header . clusteringTypes ( ) ) ; 
 + 
 + return size + serializedRowBodySize ( row , header , previousUnfilteredSize , version ) ; 
 + } 
 + 
 + private long serializedRowBodySize ( Row row , SerializationHeader header , long previousUnfilteredSize , int version ) 
 + { 
 + long size = 0 ; 
 + 
 + if ( header . isForSSTable ( ) ) 
 + size + = TypeSizes . sizeofUnsignedVInt ( previousUnfilteredSize ) ; 
 + 
 boolean isStatic = row . isStatic ( ) ; 
 Columns headerColumns = header . columns ( isStatic ) ; 
 LivenessInfo pkLiveness = row . primaryKeyLivenessInfo ( ) ; 
 @ @ - 217 , 12 + 261 , 6 @ @ public class UnfilteredSerializer 
 boolean hasComplexDeletion = row . hasComplexDeletion ( ) ; 
 boolean hasAllColumns = ( row . size ( ) = = headerColumns . size ( ) ) ; 
 
 - if ( isStatic | | deletion . isShadowable ( ) ) 
 - size + = 1 ; / / extended flags 
 - 
 - if ( ! isStatic ) 
 - size + = Clustering . serializer . serializedSize ( row . clustering ( ) , version , header . clusteringTypes ( ) ) ; 
 - 
 if ( ! pkLiveness . isEmpty ( ) ) 
 size + = header . timestampSerializedSize ( pkLiveness . timestamp ( ) ) ; 
 if ( pkLiveness . isExpiring ( ) ) 
 @ @ - 261 , 10 + 299 , 19 @ @ public class UnfilteredSerializer 
 return size ; 
 } 
 
 - public long serializedSize ( RangeTombstoneMarker marker , SerializationHeader header , int version ) 
 + private long serializedSize ( RangeTombstoneMarker marker , SerializationHeader header , long previousUnfilteredSize , int version ) 
 { 
 - long size = 1 / / flags 
 - + RangeTombstone . Bound . serializer . serializedSize ( marker . clustering ( ) , version , header . clusteringTypes ( ) ) ; 
 + assert ! header . isForSSTable ( ) ; 
 + return 1 / / flags 
 + + RangeTombstone . Bound . serializer . serializedSize ( marker . clustering ( ) , version , header . clusteringTypes ( ) ) 
 + + serializedMarkerBodySize ( marker , header , previousUnfilteredSize , version ) ; 
 + } 
 + 
 + private long serializedMarkerBodySize ( RangeTombstoneMarker marker , SerializationHeader header , long previousUnfilteredSize , int version ) 
 + { 
 + long size = 0 ; 
 + if ( header . isForSSTable ( ) ) 
 + size + = TypeSizes . sizeofUnsignedVInt ( previousUnfilteredSize ) ; 
 
 if ( marker . isBoundary ( ) ) 
 { 
 @ @ - 299 , 7 + 346 , 7 @ @ public class UnfilteredSerializer 
 if ( isEndOfPartition ( flags ) ) 
 return null ; 
 
 - int extendedFlags = isExtended ( flags ) ? in . readUnsignedByte ( ) : 0 ; 
 + int extendedFlags = readExtendedFlags ( in , flags ) ; 
 
 if ( kind ( flags ) = = Unfiltered . Kind . RANGE _ TOMBSTONE _ MARKER ) 
 { 
 @ @ - 328 , 6 + 375 , 12 @ @ public class UnfilteredSerializer 
 public RangeTombstoneMarker deserializeMarkerBody ( DataInputPlus in , SerializationHeader header , RangeTombstone . Bound bound ) 
 throws IOException 
 { 
 + if ( header . isForSSTable ( ) ) 
 + { 
 + in . readUnsignedVInt ( ) ; / / marker size 
 + in . readUnsignedVInt ( ) ; / / previous unfiltered size 
 + } 
 + 
 if ( bound . isBoundary ( ) ) 
 return new RangeTombstoneBoundaryMarker ( bound , header . readDeletionTime ( in ) , header . readDeletionTime ( in ) ) ; 
 else 
 @ @ - 353 , 6 + 406 , 12 @ @ public class UnfilteredSerializer 
 boolean hasAllColumns = ( flags & HAS _ ALL _ COLUMNS ) ! = 0 ; 
 Columns headerColumns = header . columns ( isStatic ) ; 
 
 + if ( header . isForSSTable ( ) ) 
 + { 
 + in . readUnsignedVInt ( ) ; / / Skip row size 
 + in . readUnsignedVInt ( ) ; / / previous unfiltered size 
 + } 
 + 
 LivenessInfo rowLiveness = LivenessInfo . EMPTY ; 
 if ( hasTimestamp ) 
 { 
 @ @ - 430 , 36 + 489 , 10 @ @ public class UnfilteredSerializer 
 } 
 } 
 
 - public void skipRowBody ( DataInputPlus in , SerializationHeader header , int flags , int extendedFlags ) throws IOException 
 + public void skipRowBody ( DataInputPlus in ) throws IOException 
 { 
 - boolean isStatic = isStatic ( extendedFlags ) ; 
 - boolean hasTimestamp = ( flags & HAS _ TIMESTAMP ) ! = 0 ; 
 - boolean hasTTL = ( flags & HAS _ TTL ) ! = 0 ; 
 - boolean hasDeletion = ( flags & HAS _ DELETION ) ! = 0 ; 
 - boolean hasComplexDeletion = ( flags & HAS _ COMPLEX _ DELETION ) ! = 0 ; 
 - boolean hasAllColumns = ( flags & HAS _ ALL _ COLUMNS ) ! = 0 ; 
 - Columns headerColumns = header . columns ( isStatic ) ; 
 - 
 - / / Note that we don ' t want want to use FileUtils . skipBytesFully for anything that may not have 
 - / / the size we think due to VINT encoding 
 - if ( hasTimestamp ) 
 - header . skipTimestamp ( in ) ; 
 - if ( hasTTL ) 
 - { 
 - header . skipLocalDeletionTime ( in ) ; 
 - header . skipTTL ( in ) ; 
 - } 
 - if ( hasDeletion ) 
 - header . skipDeletionTime ( in ) ; 
 - 
 - Columns columns = hasAllColumns ? headerColumns : Columns . serializer . deserializeSubset ( headerColumns , in ) ; 
 - for ( ColumnDefinition column : columns ) 
 - { 
 - if ( column . isSimple ( ) ) 
 - Cell . serializer . skip ( in , column , header ) ; 
 - else 
 - skipComplexColumn ( in , column , header , hasComplexDeletion ) ; 
 - } 
 + int rowSize = ( int ) in . readUnsignedVInt ( ) ; 
 + in . skipBytesFully ( rowSize ) ; 
 } 
 
 public void skipStaticRow ( DataInputPlus in , SerializationHeader header , SerializationHelper helper ) throws IOException 
 @ @ - 468 , 20 + 501 , 13 @ @ public class UnfilteredSerializer 
 assert ! isEndOfPartition ( flags ) & & kind ( flags ) = = Unfiltered . Kind . ROW & & isExtended ( flags ) : " Flags is " + flags ; 
 int extendedFlags = in . readUnsignedByte ( ) ; 
 assert isStatic ( extendedFlags ) ; 
 - skipRowBody ( in , header , flags , extendedFlags ) ; 
 + skipRowBody ( in ) ; 
 } 
 
 - public void skipMarkerBody ( DataInputPlus in , SerializationHeader header , boolean isBoundary ) throws IOException 
 + public void skipMarkerBody ( DataInputPlus in ) throws IOException 
 { 
 - if ( isBoundary ) 
 - { 
 - header . skipDeletionTime ( in ) ; 
 - header . skipDeletionTime ( in ) ; 
 - } 
 - else 
 - { 
 - header . skipDeletionTime ( in ) ; 
 - } 
 + int markerSize = ( int ) in . readUnsignedVInt ( ) ; 
 + in . skipBytesFully ( markerSize ) ; 
 } 
 
 private void skipComplexColumn ( DataInputPlus in , ColumnDefinition column , SerializationHeader header , boolean hasComplexDeletion ) 
 @ @ - 510 , 8 + 536 , 18 @ @ public class UnfilteredSerializer 
 return ( extendedFlags & IS _ STATIC ) ! = 0 ; 
 } 
 
 - public static boolean isExtended ( int flags ) 
 + private static boolean isExtended ( int flags ) 
 { 
 return ( flags & EXTENSION _ FLAG ) ! = 0 ; 
 } 
 + 
 + public static int readExtendedFlags ( DataInputPlus in , int flags ) throws IOException 
 + { 
 + return isExtended ( flags ) ? in . readUnsignedByte ( ) : 0 ; 
 + } 
 + 
 + public static boolean hasExtendedFlags ( Row row ) 
 + { 
 + return row . isStatic ( ) | | row . deletion ( ) . isShadowable ( ) ; 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / AbstractSSTableSimpleWriter . java b / src / java / org / apache / cassandra / io / sstable / AbstractSSTableSimpleWriter . java 
 index d94b219 . . 62348ec 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / AbstractSSTableSimpleWriter . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / AbstractSSTableSimpleWriter . java 
 @ @ - 65 , 7 + 65 , 7 @ @ abstract class AbstractSSTableSimpleWriter implements Closeable 
 0 , 
 ActiveRepairService . UNREPAIRED _ SSTABLE , 
 0 , 
 - new SerializationHeader ( metadata , columns , EncodingStats . NO _ STATS ) ) ; 
 + new SerializationHeader ( true , metadata , columns , EncodingStats . NO _ STATS ) ) ; 
 } 
 
 private static Descriptor createDescriptor ( File directory , final String keyspace , final String columnFamily , final SSTableFormat . Type fmt ) 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableSimpleUnsortedWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableSimpleUnsortedWriter . java 
 index f4b9adf . . 6d3a714 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableSimpleUnsortedWriter . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableSimpleUnsortedWriter . java 
 @ @ - 63 , 7 + 63 , 7 @ @ class SSTableSimpleUnsortedWriter extends AbstractSSTableSimpleWriter 
 { 
 super ( directory , metadata , columns ) ; 
 this . bufferSize = bufferSizeInMB * 1024L * 1024L ; 
 - this . header = new SerializationHeader ( metadata , columns , EncodingStats . NO _ STATS ) ; 
 + this . header = new SerializationHeader ( true , metadata , columns , EncodingStats . NO _ STATS ) ; 
 diskWriter . start ( ) ; 
 } 
 
 @ @ - 89 , 7 + 89 , 7 @ @ class SSTableSimpleUnsortedWriter extends AbstractSSTableSimpleWriter 
 / / improve that . In particular , what we count is closer to the serialized value , but it ' s debatable that it ' s the right thing 
 / / to count since it will take a lot more space in memory and the bufferSize if first and foremost used to avoid OOM when 
 / / using this writer . 
 - currentSize + = UnfilteredSerializer . serializer . serializedSize ( row , header , formatType . info . getLatestVersion ( ) . correspondingMessagingVersion ( ) ) ; 
 + currentSize + = UnfilteredSerializer . serializer . serializedSize ( row , header , 0 , formatType . info . getLatestVersion ( ) . correspondingMessagingVersion ( ) ) ; 
 } 
 
 private void maybeSync ( ) throws SyncException 
 diff - - git a / test / unit / org / apache / cassandra / db / RowIndexEntryTest . java b / test / unit / org / apache / cassandra / db / RowIndexEntryTest . java 
 index 25baa4e . . 62c88a0 100644 
 - - - a / test / unit / org / apache / cassandra / db / RowIndexEntryTest . java 
 + + + b / test / unit / org / apache / cassandra / db / RowIndexEntryTest . java 
 @ @ - 60 , 7 + 60 , 7 @ @ public class RowIndexEntryTest extends CQLTester 
 
 DeletionTime deletionInfo = new DeletionTime ( FBUtilities . timestampMicros ( ) , FBUtilities . nowInSeconds ( ) ) ; 
 
 - SerializationHeader header = new SerializationHeader ( cfMeta , cfMeta . partitionColumns ( ) , EncodingStats . NO _ STATS ) ; 
 + SerializationHeader header = new SerializationHeader ( true , cfMeta , cfMeta . partitionColumns ( ) , EncodingStats . NO _ STATS ) ; 
 IndexHelper . IndexInfo . Serializer indexSerializer = new IndexHelper . IndexInfo . Serializer ( cfMeta , BigFormat . latestVersion , header ) ; 
 
 DataOutputBuffer dob = new DataOutputBuffer ( ) ; 
 @ @ - 119 , 7 + 119 , 7 @ @ public class RowIndexEntryTest extends CQLTester 
 final RowIndexEntry simple = new RowIndexEntry ( 123 ) ; 
 
 DataOutputBuffer buffer = new DataOutputBuffer ( ) ; 
 - SerializationHeader header = new SerializationHeader ( cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ; 
 + SerializationHeader header = new SerializationHeader ( true , cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ; 
 RowIndexEntry . Serializer serializer = new RowIndexEntry . Serializer ( cfs . metadata , BigFormat . latestVersion , header ) ; 
 
 serializer . serialize ( simple , buffer ) ; 
 diff - - git a / test / unit / org / apache / cassandra / db / ScrubTest . java b / test / unit / org / apache / cassandra / db / ScrubTest . java 
 index 2fc8436 . . ab99750 100644 
 - - - a / test / unit / org / apache / cassandra / db / ScrubTest . java 
 + + + b / test / unit / org / apache / cassandra / db / ScrubTest . java 
 @ @ - 325 , 7 + 325 , 8 @ @ public class ScrubTest 
 keys . size ( ) , 
 0L , 
 0 , 
 - new SerializationHeader ( cfs . metadata , 
 + new SerializationHeader ( true , 
 + cfs . metadata , 
 cfs . metadata . partitionColumns ( ) , 
 EncodingStats . NO _ STATS ) ) ) 
 { 
 diff - - git a / test / unit / org / apache / cassandra / db / compaction / AntiCompactionTest . java b / test / unit / org / apache / cassandra / db / compaction / AntiCompactionTest . java 
 index cd82b19 . . db07eb8 100644 
 - - - a / test / unit / org / apache / cassandra / db / compaction / AntiCompactionTest . java 
 + + + b / test / unit / org / apache / cassandra / db / compaction / AntiCompactionTest . java 
 @ @ - 166 , 7 + 166 , 7 @ @ public class AntiCompactionTest 
 File dir = cfs . getDirectories ( ) . getDirectoryForNewSSTables ( ) ; 
 String filename = cfs . getSSTablePath ( dir ) ; 
 
 - try ( SSTableTxnWriter writer = SSTableTxnWriter . create ( cfs , filename , 0 , 0 , new SerializationHeader ( cfm , cfm . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) 
 + try ( SSTableTxnWriter writer = SSTableTxnWriter . create ( cfs , filename , 0 , 0 , new SerializationHeader ( true , cfm , cfm . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) 
 { 
 for ( int i = 0 ; i < count ; i + + ) 
 { 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / BigTableWriterTest . java b / test / unit / org / apache / cassandra / io / sstable / BigTableWriterTest . java 
 index e1ab48f . . 78964f4 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / BigTableWriterTest . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / BigTableWriterTest . java 
 @ @ - 69 , 7 + 69 , 7 @ @ public class BigTableWriterTest extends AbstractTransactionalTest 
 
 private TestableBTW ( String file ) 
 { 
 - this ( file , SSTableTxnWriter . create ( cfs , file , 0 , 0 , new SerializationHeader ( cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) ; 
 + this ( file , SSTableTxnWriter . create ( cfs , file , 0 , 0 , new SerializationHeader ( true , cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) ; 
 } 
 
 private TestableBTW ( String file , SSTableTxnWriter sw ) 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java 
 index 093bffd . . bd286e4 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableRewriterTest . java 
 @ @ - 943 , 7 + 943 , 7 @ @ public class SSTableRewriterTest extends SchemaLoader 
 File dir = cfs . getDirectories ( ) . getDirectoryForNewSSTables ( ) ; 
 String filename = cfs . getSSTablePath ( dir ) ; 
 
 - try ( SSTableTxnWriter writer = SSTableTxnWriter . create ( cfs , filename , 0 , 0 , new SerializationHeader ( cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) 
 + try ( SSTableTxnWriter writer = SSTableTxnWriter . create ( cfs , filename , 0 , 0 , new SerializationHeader ( true , cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) ) ) 
 { 
 int end = f = = fileCount - 1 ? partitionCount : ( ( f + 1 ) * partitionCount ) / fileCount ; 
 for ( ; i < end ; i + + ) 
 @ @ - 1011 , 7 + 1011 , 7 @ @ public class SSTableRewriterTest extends SchemaLoader 
 public static SSTableWriter getWriter ( ColumnFamilyStore cfs , File directory , LifecycleTransaction txn ) 
 { 
 String filename = cfs . getSSTablePath ( directory ) ; 
 - return SSTableWriter . create ( filename , 0 , 0 , new SerializationHeader ( cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) , txn ) ; 
 + return SSTableWriter . create ( filename , 0 , 0 , new SerializationHeader ( true , cfs . metadata , cfs . metadata . partitionColumns ( ) , EncodingStats . NO _ STATS ) , txn ) ; 
 } 
 
 public static ByteBuffer random ( int i , int size ) 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java b / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java 
 index fcd2d71 . . 5c7ff02 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java 
 @ @ - 187 , 7 + 187 , 7 @ @ public class SSTableUtils 
 { 
 public SerializationHeader header ( ) 
 { 
 - return new SerializationHeader ( Schema . instance . getCFMetaData ( ksname , cfname ) , builder . build ( ) , EncodingStats . NO _ STATS ) ; 
 + return new SerializationHeader ( true , Schema . instance . getCFMetaData ( ksname , cfname ) , builder . build ( ) , EncodingStats . NO _ STATS ) ; 
 } 
 
 @ Override

NEAREST DIFF:
diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db 
 deleted file mode 100644 
 index 0f95b07 . . 0000000 
 Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Data . db and / dev / null differ 
 diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db 
 deleted file mode 100644 
 index 88aac99 . . 0000000 
 Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Filter . db and / dev / null differ 
 diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db 
 deleted file mode 100644 
 index a7787c5 . . 0000000 
 Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Index . db and / dev / null differ 
 diff - - git a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db b / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db 
 deleted file mode 100644 
 index 6312c71 . . 0000000 
 Binary files a / test / data / corrupt - sstables / Keyspace1 - Super5 - f - 2 - Statistics . db and / dev / null differ 
 diff - - git a / test / unit / org / apache / cassandra / db / ScrubTest . java b / test / unit / org / apache / cassandra / db / ScrubTest . java 
 new file mode 100644 
 index 0000000 . . 26f0e78 
 - - - / dev / null 
 + + + b / test / unit / org / apache / cassandra / db / ScrubTest . java 
 @ @ - 0 , 0 + 1 , 211 @ @ 
 + package org . apache . cassandra . db ; 
 + / * 
 + * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , 
 + * software distributed under the License is distributed on an 
 + * " AS IS " BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY 
 + * KIND , either express or implied . See the License for the 
 + * specific language governing permissions and limitations 
 + * under the License . 
 + * 
 + * / 
 + 
 + 
 + import java . io . File ; 
 + import java . io . IOException ; 
 + import java . util . List ; 
 + import java . util . concurrent . ExecutionException ; 
 + 
 + import org . junit . Test ; 
 + 
 + import org . apache . cassandra . SchemaLoader ; 
 + import org . apache . cassandra . Util ; 
 + import org . apache . cassandra . exceptions . ConfigurationException ; 
 + import org . apache . cassandra . db . columniterator . IdentityQueryFilter ; 
 + import org . apache . cassandra . db . compaction . CompactionManager ; 
 + import org . apache . cassandra . io . util . FileUtils ; 
 + import org . apache . cassandra . utils . ByteBufferUtil ; 
 + import org . apache . cassandra . utils . CLibrary ; 
 + 
 + import static org . apache . cassandra . Util . column ; 
 + import static org . junit . Assert . assertEquals ; 
 + import static org . junit . Assert . fail ; 
 + 
 + public class ScrubTest extends SchemaLoader 
 + { 
 + public String TABLE = " Keyspace1 " ; 
 + public String CF = " Standard1 " ; 
 + public String CF2 = " Super5 " ; 
 + public String CF3 = " Standard2 " ; 
 + 
 + public String copySSTables ( String cf ) throws IOException 
 + { 
 + String root = System . getProperty ( " corrupt - sstable - root " ) ; 
 + assert root ! = null ; 
 + File rootDir = new File ( root ) ; 
 + assert rootDir . isDirectory ( ) ; 
 + 
 + File destDir = Directories . create ( TABLE , cf ) . getDirectoryForNewSSTables ( 1 ) ; 
 + 
 + String corruptSSTableName = null ; 
 + 
 + FileUtils . createDirectory ( destDir ) ; 
 + for ( File srcFile : rootDir . listFiles ( ) ) 
 + { 
 + if ( srcFile . getName ( ) . equals ( " . svn " ) ) 
 + continue ; 
 + if ( ! srcFile . getName ( ) . contains ( cf ) ) 
 + continue ; 
 + File destFile = new File ( destDir , srcFile . getName ( ) ) ; 
 + CLibrary . createHardLink ( srcFile , destFile ) ; 
 + 
 + assert destFile . exists ( ) : destFile . getAbsoluteFile ( ) ; 
 + 
 + if ( destFile . getName ( ) . endsWith ( " Data . db " ) ) 
 + corruptSSTableName = destFile . getCanonicalPath ( ) ; 
 + } 
 + 
 + assert corruptSSTableName ! = null ; 
 + return corruptSSTableName ; 
 + } 
 + 
 + @ Test 
 + public void testScrubOneRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException 
 + { 
 + CompactionManager . instance . disableAutoCompaction ( ) ; 
 + Table table = Table . open ( TABLE ) ; 
 + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF ) ; 
 + 
 + List < Row > rows ; 
 + 
 + / / insert data and verify we get it back w / range query 
 + fillCF ( cfs , 1 ) ; 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assertEquals ( 1 , rows . size ( ) ) ; 
 + 
 + CompactionManager . instance . performScrub ( cfs ) ; 
 + 
 + / / check data is still there 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assertEquals ( 1 , rows . size ( ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testScrubDeletedRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException 
 + { 
 + CompactionManager . instance . disableAutoCompaction ( ) ; 
 + Table table = Table . open ( TABLE ) ; 
 + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF3 ) ; 
 + 
 + RowMutation rm ; 
 + rm = new RowMutation ( TABLE , ByteBufferUtil . bytes ( 1 ) ) ; 
 + ColumnFamily cf = ColumnFamily . create ( TABLE , CF3 ) ; 
 + cf . delete ( new DeletionInfo ( 0 , 1 ) ) ; / / expired tombstone 
 + rm . add ( cf ) ; 
 + rm . applyUnsafe ( ) ; 
 + cfs . forceBlockingFlush ( ) ; 
 + 
 + CompactionManager . instance . performScrub ( cfs ) ; 
 + assert cfs . getSSTables ( ) . isEmpty ( ) ; 
 + } 
 + 
 + @ Test 
 + public void testScrubMultiRow ( ) throws IOException , ExecutionException , InterruptedException , ConfigurationException 
 + { 
 + CompactionManager . instance . disableAutoCompaction ( ) ; 
 + Table table = Table . open ( TABLE ) ; 
 + ColumnFamilyStore cfs = table . getColumnFamilyStore ( CF ) ; 
 + 
 + List < Row > rows ; 
 + 
 + / / insert data and verify we get it back w / range query 
 + fillCF ( cfs , 10 ) ; 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assertEquals ( 10 , rows . size ( ) ) ; 
 + 
 + CompactionManager . instance . performScrub ( cfs ) ; 
 + 
 + / / check data is still there 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assertEquals ( 10 , rows . size ( ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testScubOutOfOrder ( ) throws Exception 
 + { 
 + CompactionManager . instance . disableAutoCompaction ( ) ; 
 + Table table = Table . open ( TABLE ) ; 
 + String columnFamily = " Standard3 " ; 
 + ColumnFamilyStore cfs = table . getColumnFamilyStore ( columnFamily ) ; 
 + 
 + / * 
 + * Code used to generate an outOfOrder sstable . The test for out - of - order key in SSTableWriter must also be commented out . 
 + * The test also assumes an ordered partitioner . 
 + * 
 + * ColumnFamily cf = ColumnFamily . create ( TABLE , columnFamily ) ; 
 + * cf . addColumn ( new Column ( ByteBufferUtil . bytes ( " someName " ) , ByteBufferUtil . bytes ( " someValue " ) , 0L ) ) ; 
 + 
 + * SSTableWriter writer = cfs . createCompactionWriter ( ( long ) DatabaseDescriptor . getIndexInterval ( ) , new File ( " . " ) , Collections . < SSTableReader > emptyList ( ) ) ; 
 + * writer . append ( Util . dk ( " a " ) , cf ) ; 
 + * writer . append ( Util . dk ( " b " ) , cf ) ; 
 + * writer . append ( Util . dk ( " z " ) , cf ) ; 
 + * writer . append ( Util . dk ( " c " ) , cf ) ; 
 + * writer . append ( Util . dk ( " y " ) , cf ) ; 
 + * writer . append ( Util . dk ( " d " ) , cf ) ; 
 + * writer . closeAndOpenReader ( ) ; 
 + * / 
 + 
 + copySSTables ( columnFamily ) ; 
 + cfs . loadNewSSTables ( ) ; 
 + assert cfs . getSSTables ( ) . size ( ) > 0 ; 
 + 
 + List < Row > rows ; 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assert ! isRowOrdered ( rows ) : " ' corrupt ' test file actually was not " ; 
 + 
 + CompactionManager . instance . performScrub ( cfs ) ; 
 + rows = cfs . getRangeSlice ( null , Util . range ( " " , " " ) , 1000 , new IdentityQueryFilter ( ) , null ) ; 
 + assert isRowOrdered ( rows ) : " Scrub failed : " + rows ; 
 + assert rows . size ( ) = = 6 : " Got " + rows . size ( ) ; 
 + } 
 + 
 + private static boolean isRowOrdered ( List < Row > rows ) 
 + { 
 + DecoratedKey prev = null ; 
 + for ( Row row : rows ) 
 + { 
 + if ( prev ! = null & & prev . compareTo ( row . key ) > 0 ) 
 + return false ; 
 + prev = row . key ; 
 + } 
 + return true ; 
 + } 
 + 
 + protected void fillCF ( ColumnFamilyStore cfs , int rowsPerSSTable ) throws ExecutionException , InterruptedException , IOException 
 + { 
 + for ( int i = 0 ; i < rowsPerSSTable ; i + + ) 
 + { 
 + String key = String . valueOf ( i ) ; 
 + / / create a row and update the birthdate value , test that the index query fetches the new version 
 + RowMutation rm ; 
 + rm = new RowMutation ( TABLE , ByteBufferUtil . bytes ( key ) ) ; 
 + ColumnFamily cf = ColumnFamily . create ( TABLE , CF ) ; 
 + cf . addColumn ( column ( " c1 " , " 1 " , 1L ) ) ; 
 + cf . addColumn ( column ( " c2 " , " 2 " , 1L ) ) ; 
 + rm . add ( cf ) ; 
 + rm . applyUnsafe ( ) ; 
 + } 
 + 
 + cfs . forceBlockingFlush ( ) ; 
 + } 
 + }
