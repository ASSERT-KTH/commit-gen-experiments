BLEU SCORE: 0.04767707020457096

TEST MSG: ( cqlsh ) Add support for native protocol 4
GENERATED MSG: cqlsh : Remove ASSUME command

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index a97cf2f . . 8b59309 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 9 + 1 , 9 @ @ <nl> 2 . 2 <nl> + * ( cqlsh ) Add support for native protocol 4 ( CASSANDRA - 9399 ) <nl> * Ensure that UDF and UDAs are keyspace - isolated ( CASSANDRA - 9409 ) <nl> * Revert CASSANDRA - 7807 ( tracing completion client notifications ) ( CASSANDRA - 9429 ) <nl> * Add ability to stop compaction by ID ( CASSANDRA - 7207 ) <nl> Merged from 2 . 1 : <nl> - 2 . 1 . 6 <nl> * Improve estimated row count ( CASSANDRA - 9107 ) <nl> * Optimize range tombstone memory footprint ( CASSANDRA - 8603 ) <nl> * Use configured gcgs in anticompaction ( CASSANDRA - 9397 ) <nl> diff - - git a / bin / cqlsh b / bin / cqlsh <nl> index c73f9a4 . . b2a729c 100755 <nl> - - - a / bin / cqlsh <nl> + + + b / bin / cqlsh <nl> @ @ - 135 , 7 + 135 , 7 @ @ from cqlshlib . tracing import print _ trace _ session , print _ trace <nl> DEFAULT _ HOST = ' 127 . 0 . 0 . 1 ' <nl> DEFAULT _ PORT = 9042 <nl> DEFAULT _ CQLVER = ' 3 . 2 . 0 ' <nl> - DEFAULT _ PROTOCOL _ VERSION = 3 <nl> + DEFAULT _ PROTOCOL _ VERSION = 4 <nl> <nl> DEFAULT _ FLOAT _ PRECISION = 5 <nl> DEFAULT _ MAX _ TRACE _ WAIT = 10 <nl> @ @ - 550 , 7 + 550 , 6 @ @ class FrozenType ( cassandra . cqltypes . _ ParameterizedType ) : <nl> subtype , = cls . subtypes <nl> return subtype . to _ binary ( val , protocol _ version ) <nl> <nl> - <nl> class Shell ( cmd . Cmd ) : <nl> custom _ prompt = os . getenv ( ' CQLSH _ PROMPT ' , ' ' ) <nl> if custom _ prompt is not ' ' : <nl> @ @ - 581 , 7 + 580 , 8 @ @ class Shell ( cmd . Cmd ) : <nl> max _ trace _ wait = DEFAULT _ MAX _ TRACE _ WAIT , <nl> ssl = False , <nl> single _ statement = None , <nl> - client _ timeout = 10 ) : <nl> + client _ timeout = 10 , <nl> + protocol _ version = DEFAULT _ PROTOCOL _ VERSION ) : <nl> cmd . Cmd . _ _ init _ _ ( self , completekey = completekey ) <nl> self . hostname = hostname <nl> self . port = port <nl> @ @ - 599 , 7 + 599 , 7 @ @ class Shell ( cmd . Cmd ) : <nl> self . conn = use _ conn <nl> else : <nl> self . conn = Cluster ( contact _ points = ( self . hostname , ) , port = self . port , cql _ version = cqlver , <nl> - protocol _ version = DEFAULT _ PROTOCOL _ VERSION , <nl> + protocol _ version = protocol _ version , <nl> auth _ provider = self . auth _ provider , <nl> ssl _ options = sslhandling . ssl _ settings ( hostname , CONFIG _ FILE ) if ssl else None , <nl> load _ balancing _ policy = WhiteListRoundRobinPolicy ( [ self . hostname ] ) ) <nl> @ @ - 1048 , 13 + 1048 , 22 @ @ class Shell ( cmd . Cmd ) : <nl> <nl> def perform _ statement ( self , statement ) : <nl> stmt = SimpleStatement ( statement , consistency _ level = self . consistency _ level , serial _ consistency _ level = self . serial _ consistency _ level , fetch _ size = self . default _ page _ size if self . use _ paging else None ) <nl> - result = self . perform _ simple _ statement ( stmt ) <nl> - if self . tracing _ enabled : <nl> - if stmt . trace : <nl> - print _ trace ( self , stmt . trace ) <nl> - else : <nl> - msg = " Statement trace did not complete within % d seconds " % ( self . session . max _ trace _ wait ) <nl> - self . writeresult ( msg , color = RED ) <nl> + result , future = self . perform _ simple _ statement ( stmt ) <nl> + <nl> + if future : <nl> + if future . warnings : <nl> + self . print _ warnings ( future . warnings ) <nl> + <nl> + if self . tracing _ enabled : <nl> + try : <nl> + trace = future . get _ query _ trace ( self . max _ trace _ wait ) <nl> + if trace : <nl> + print _ trace ( self , trace ) <nl> + else : <nl> + msg = " Statement trace did not complete within % d seconds " % ( self . session . max _ trace _ wait ) <nl> + self . writeresult ( msg , color = RED ) <nl> + except Exception , err : <nl> + self . printerr ( " Unable to fetch query trace : % s " % ( str ( err ) , ) ) <nl> <nl> return result <nl> <nl> @ @ - 1069 , 19 + 1078 , 20 @ @ class Shell ( cmd . Cmd ) : <nl> <nl> def perform _ simple _ statement ( self , statement ) : <nl> if not statement : <nl> - return False <nl> + return False , None <nl> rows = None <nl> while True : <nl> try : <nl> - rows = self . session . execute ( statement , trace = self . tracing _ enabled ) <nl> + future = self . session . execute _ async ( statement , trace = self . tracing _ enabled ) <nl> + rows = future . result ( self . session . default _ timeout ) <nl> break <nl> except CQL _ ERRORS , err : <nl> self . printerr ( str ( err . _ _ class _ _ . _ _ name _ _ ) + " : " + str ( err ) ) <nl> - return False <nl> + return False , None <nl> except Exception , err : <nl> import traceback <nl> self . printerr ( traceback . format _ exc ( ) ) <nl> - return False <nl> + return False , None <nl> <nl> if statement . query _ string [ : 6 ] . lower ( ) = = ' select ' : <nl> self . print _ result ( rows , self . parse _ for _ table _ meta ( statement . query _ string ) ) <nl> @ @ - 1094 , 7 + 1104 , 7 @ @ class Shell ( cmd . Cmd ) : <nl> self . writeresult ( " " ) <nl> self . print _ static _ result ( rows , self . parse _ for _ table _ meta ( statement . query _ string ) ) <nl> self . flush _ output ( ) <nl> - return True <nl> + return True , future <nl> <nl> def print _ result ( self , rows , table _ meta ) : <nl> self . decoding _ errors = [ ] <nl> @ @ - 1189 , 6 + 1199 , 16 @ @ class Shell ( cmd . Cmd ) : <nl> self . writeresult ( ' ' + " | " . join ( [ column , value ] ) ) <nl> self . writeresult ( ' ' ) <nl> <nl> + def print _ warnings ( self , warnings ) : <nl> + if warnings is None or len ( warnings ) = = 0 : <nl> + return ; <nl> + <nl> + self . writeresult ( ' ' ) <nl> + self . writeresult ( ' Warnings : ' ) <nl> + for warning in warnings : <nl> + self . writeresult ( warning ) <nl> + self . writeresult ( ' ' ) <nl> + <nl> def emptyline ( self ) : <nl> pass <nl> <nl> @ @ - 1614 , 7 + 1634 , 7 @ @ class Shell ( cmd . Cmd ) : <nl> contact _ points = ( self . hostname , ) , <nl> port = self . port , <nl> cql _ version = self . conn . cql _ version , <nl> - protocol _ version = DEFAULT _ PROTOCOL _ VERSION , <nl> + protocol _ version = self . conn . protocol _ version , <nl> auth _ provider = self . auth _ provider , <nl> ssl _ options = sslhandling . ssl _ settings ( self . hostname , CONFIG _ FILE ) if self . ssl else None , <nl> load _ balancing _ policy = WhiteListRoundRobinPolicy ( [ self . hostname ] ) , <nl> @ @ - 1703 , 7 + 1723 , 7 @ @ class Shell ( cmd . Cmd ) : <nl> <nl> request _ id = conn . get _ request _ id ( ) <nl> binary _ message = query _ message . to _ binary ( <nl> - stream _ id = request _ id , protocol _ version = DEFAULT _ PROTOCOL _ VERSION , compression = None ) <nl> + stream _ id = request _ id , protocol _ version = self . conn . protocol _ version , compression = None ) <nl> <nl> # add the message directly to the connection ' s queue <nl> with conn . lock : <nl> @ @ - 2036 , 7 + 2056 , 7 @ @ class Shell ( cmd . Cmd ) : <nl> auth _ provider = PlainTextAuthProvider ( username = username , password = password ) <nl> <nl> conn = Cluster ( contact _ points = ( self . hostname , ) , port = self . port , cql _ version = self . conn . cql _ version , <nl> - protocol _ version = DEFAULT _ PROTOCOL _ VERSION , <nl> + protocol _ version = self . conn . protocol _ version , <nl> auth _ provider = auth _ provider , <nl> ssl _ options = self . conn . ssl _ options , <nl> load _ balancing _ policy = WhiteListRoundRobinPolicy ( [ self . hostname ] ) ) <nl> diff - - git a / lib / cassandra - driver - internal - only - 2 . 5 . 1 . post0 - 074650b . zip b / lib / cassandra - driver - internal - only - 2 . 5 . 1 . post0 - 074650b . zip <nl> new file mode 100644 <nl> index 0000000 . . ce21a7a <nl> Binary files / dev / null and b / lib / cassandra - driver - internal - only - 2 . 5 . 1 . post0 - 074650b . zip differ <nl> diff - - git a / lib / cassandra - driver - internal - only - 2 . 5 . 1 . zip b / lib / cassandra - driver - internal - only - 2 . 5 . 1 . zip <nl> deleted file mode 100644 <nl> index ee6ace0 . . 0000000 <nl> Binary files a / lib / cassandra - driver - internal - only - 2 . 5 . 1 . zip and / dev / null differ <nl> diff - - git a / pylib / cqlshlib / cql3handling . py b / pylib / cqlshlib / cql3handling . py <nl> index 3e155d0 . . ae66a4e 100644 <nl> - - - a / pylib / cqlshlib / cql3handling . py <nl> + + + b / pylib / cqlshlib / cql3handling . py <nl> @ @ - 20 , 7 + 20 , 7 @ @ from cassandra . metadata import escape _ name <nl> <nl> <nl> simple _ cql _ types = set ( ( ' ascii ' , ' bigint ' , ' blob ' , ' boolean ' , ' counter ' , ' date ' , ' decimal ' , ' double ' , ' float ' , ' inet ' , ' int ' , <nl> - ' text ' , ' time ' , ' timestamp ' , ' timeuuid ' , ' uuid ' , ' varchar ' , ' varint ' ) ) <nl> + ' smallint ' , ' text ' , ' time ' , ' timestamp ' , ' timeuuid ' , ' tinyint ' , ' uuid ' , ' varchar ' , ' varint ' ) ) <nl> simple _ cql _ types . difference _ update ( ( ' set ' , ' map ' , ' list ' ) ) <nl> <nl> from . import helptopics <nl> diff - - git a / pylib / cqlshlib / formatting . py b / pylib / cqlshlib / formatting . py <nl> index 868ec28 . . 2310fa9 100644 <nl> - - - a / pylib / cqlshlib / formatting . py <nl> + + + b / pylib / cqlshlib / formatting . py <nl> @ @ - 192 , 8 + 192 , 8 @ @ def strftime ( time _ format , seconds ) : <nl> hours , minutes = divmod ( abs ( offset ) / 60 , 60 ) <nl> return formatted [ : - 5 ] + sign + ' { 0 : 0 = 2 } { 1 : 0 = 2 } ' . format ( hours , minutes ) <nl> <nl> - @ formatter _ for ( ' date ' ) <nl> - def format _ value _ uuid ( val , colormap , * * _ ) : <nl> + @ formatter _ for ( ' Date ' ) <nl> + def format _ value _ date ( val , colormap , * * _ ) : <nl> return format _ python _ formatted _ type ( val , colormap , ' date ' ) <nl> <nl> @ formatter _ for ( ' Time ' ) <nl> diff - - git a / pylib / cqlshlib / helptopics . py b / pylib / cqlshlib / helptopics . py <nl> index 0a43882 . . b38b235 100644 <nl> - - - a / pylib / cqlshlib / helptopics . py <nl> + + + b / pylib / cqlshlib / helptopics . py <nl> @ @ - 38 , 6 + 38 , 7 @ @ class CQLHelpTopics ( object ) : <nl> HELP BLOB _ INPUT <nl> HELP UUID _ INPUT <nl> HELP BOOLEAN _ INPUT <nl> + HELP INT _ INPUT <nl> <nl> HELP TEXT _ OUTPUT <nl> HELP TIMESTAMP _ OUTPUT <nl> @ @ - 119 , 6 + 120 , 17 @ @ class CQLHelpTopics ( object ) : <nl> as input for boolean types . <nl> " " " <nl> <nl> + def help _ int _ input ( self ) : <nl> + print " " " <nl> + Integer input <nl> + <nl> + CQL accepts the following integer types : <nl> + tinyint - 1 - byte signed integer <nl> + smallint - 2 - byte signed integer <nl> + int - 4 - byte signed integer <nl> + bigint - 8 - byte signed integer <nl> + " " " <nl> + <nl> def help _ timestamp _ output ( self ) : <nl> print " " " <nl> Timestamp output <nl> diff - - git a / pylib / cqlshlib / test / test _ cqlsh _ completion . py b / pylib / cqlshlib / test / test _ cqlsh _ completion . py <nl> index cf7cab2 . . d6ccaf7 100644 <nl> - - - a / pylib / cqlshlib / test / test _ cqlsh _ completion . py <nl> + + + b / pylib / cqlshlib / test / test _ cqlsh _ completion . py <nl> @ @ - 143 , 8 + 143 , 8 @ @ class TestCqlshCompletion ( CqlshCompletionCase ) : <nl> def test _ complete _ on _ empty _ string ( self ) : <nl> self . trycompletions ( ' ' , choices = ( ' ? ' , ' ALTER ' , ' BEGIN ' , ' CAPTURE ' , ' CONSISTENCY ' , <nl> ' COPY ' , ' CREATE ' , ' DEBUG ' , ' DELETE ' , ' DESC ' , ' DESCRIBE ' , <nl> - ' DROP ' , ' GRANT ' , ' HELP ' , ' INSERT ' , ' LIST ' , ' PAGING ' , ' REVOKE ' , <nl> - ' SELECT ' , ' SHOW ' , ' SOURCE ' , ' TRACING ' , ' EXPAND ' , ' TRUNCATE ' , <nl> + ' DROP ' , ' GRANT ' , ' HELP ' , ' INSERT ' , ' LIST ' , ' LOGIN ' , ' PAGING ' , ' REVOKE ' , <nl> + ' SELECT ' , ' SHOW ' , ' SOURCE ' , ' TRACING ' , ' EXPAND ' , ' SERIAL ' , ' TRUNCATE ' , <nl> ' UPDATE ' , ' USE ' , ' exit ' , ' quit ' ) ) <nl> <nl> def test _ complete _ command _ words ( self ) : <nl> @ @ - 229 , 8 + 229 , 8 @ @ class TestCqlshCompletion ( CqlshCompletionCase ) : <nl> " VALUES ( ' eggs ' , ' sausage ' , ' spam ' ) ; " ) , <nl> choices = [ ' ? ' , ' ALTER ' , ' BEGIN ' , ' CAPTURE ' , ' CONSISTENCY ' , ' COPY ' , <nl> ' CREATE ' , ' DEBUG ' , ' DELETE ' , ' DESC ' , ' DESCRIBE ' , ' DROP ' , <nl> - ' EXPAND ' , ' GRANT ' , ' HELP ' , ' INSERT ' , ' LIST ' , ' PAGING ' , <nl> - ' REVOKE ' , ' SELECT ' , ' SHOW ' , ' SOURCE ' , ' TRACING ' , <nl> + ' EXPAND ' , ' GRANT ' , ' HELP ' , ' INSERT ' , ' LIST ' , ' LOGIN ' , ' PAGING ' , <nl> + ' REVOKE ' , ' SELECT ' , ' SHOW ' , ' SOURCE ' , ' SERIAL ' , ' TRACING ' , <nl> ' TRUNCATE ' , ' UPDATE ' , ' USE ' , ' exit ' , ' quit ' ] ) <nl> <nl> self . trycompletions ( <nl> diff - - git a / pylib / cqlshlib / test / test _ cqlsh _ output . py b / pylib / cqlshlib / test / test _ cqlsh _ output . py <nl> index 40c7efc . . 2fd0ac7 100644 <nl> - - - a / pylib / cqlshlib / test / test _ cqlsh _ output . py <nl> + + + b / pylib / cqlshlib / test / test _ cqlsh _ output . py <nl> @ @ - 275 , 9 + 275 , 9 @ @ class TestCqlshOutput ( BaseTestCase ) : <nl> # same query should show up as empty in cql 3 <nl> self . assertQueriesGiveColoredOutput ( ( <nl> ( q , " " " <nl> - num | asciicol | bigintcol | blobcol | booleancol | decimalcol | doublecol | floatcol | intcol | textcol | timestampcol | uuidcol | varcharcol | varintcol <nl> - RRR MMMMMMMM MMMMMMMMM MMMMMMM MMMMMMMMMM MMMMMMMMMM MMMMMMMMM MMMMMMMM MMMMMM MMMMMMM MMMMMMMMMMMM MMMMMMM MMMMMMMMMM MMMMMMMMM <nl> - - - - - - + - - - - - - - - - - + - - - - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - + - - - - - - - - - - + - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - <nl> + num | asciicol | bigintcol | blobcol | booleancol | decimalcol | doublecol | floatcol | intcol | smallintcol | textcol | timestampcol | tinyintcol | uuidcol | varcharcol | varintcol <nl> + RRR MMMMMMMM MMMMMMMMM MMMMMMM MMMMMMMMMM MMMMMMMMMM MMMMMMMMM MMMMMMMM MMMMMM MMMMMMMMMMM MMMMMMM MMMMMMMMMMMM MMMMMMMMMM MMMMMMM MMMMMMMMMM MMMMMMMMM <nl> + - - - - - + - - - - - - - - - - + - - - - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - + - - - - - - - - - - + - - - - - - - - + - - - - - - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - <nl> <nl> <nl> ( 0 rows ) <nl> @ @ - 602 , 8 + 602 , 10 @ @ class TestCqlshOutput ( BaseTestCase ) : <nl> doublecol double , <nl> floatcol float , <nl> intcol int , <nl> + smallintcol smallint , <nl> textcol text , <nl> timestampcol timestamp , <nl> + tinyintcol tinyint , <nl> uuidcol uuid , <nl> varcharcol text , <nl> varintcol varint <nl> diff - - git a / pylib / cqlshlib / test / test _ keyspace _ init . cql b / pylib / cqlshlib / test / test _ keyspace _ init . cql <nl> index 2433ca0 . . fda629e 100644 <nl> - - - a / pylib / cqlshlib / test / test _ keyspace _ init . cql <nl> + + + b / pylib / cqlshlib / test / test _ keyspace _ init . cql <nl> @ @ - 8 , 49 + 8 , 51 @ @ CREATE TABLE has _ all _ types ( <nl> decimalcol decimal , <nl> doublecol double , <nl> floatcol float , <nl> + smallintcol smallint , <nl> textcol text , <nl> timestampcol timestamp , <nl> + tinyintcol tinyint , <nl> uuidcol uuid , <nl> varcharcol varchar , <nl> varintcol varint <nl> ) WITH compression = { ' sstable _ compression ' : ' LZ4Compressor ' } ; <nl> <nl> INSERT INTO has _ all _ types ( num , intcol , asciicol , bigintcol , blobcol , booleancol , <nl> - decimalcol , doublecol , floatcol , textcol , <nl> - timestampcol , uuidcol , varcharcol , varintcol ) <nl> + decimalcol , doublecol , floatcol , smallintcol , textcol , <nl> + timestampcol , tinyintcol , uuidcol , varcharcol , varintcol ) <nl> VALUES ( 0 , - 12 , ' abcdefg ' , 1234567890123456789 , 0x000102030405fffefd , true , <nl> - 19952 . 11882 , 1 . 0 , - 2 . 1 , ' Voil á ! ' , <nl> - ' 2012 - 05 - 14 12 : 53 : 20 + 0000 ' , bd1924e1 - 6af8 - 44ae - b5e1 - f24131dbd460 , ' " ' , 10000000000000000000000000 ) ; <nl> + 19952 . 11882 , 1 . 0 , - 2 . 1 , 32767 , ' Voil á ! ' , <nl> + ' 2012 - 05 - 14 12 : 53 : 20 + 0000 ' , 127 , bd1924e1 - 6af8 - 44ae - b5e1 - f24131dbd460 , ' " ' , 10000000000000000000000000 ) ; <nl> <nl> INSERT INTO has _ all _ types ( num , intcol , asciicol , bigintcol , blobcol , booleancol , <nl> - decimalcol , doublecol , floatcol , textcol , <nl> - timestampcol , uuidcol , varcharcol , varintcol ) <nl> + decimalcol , doublecol , floatcol , smallintcol , textcol , <nl> + timestampcol , tinyintcol , uuidcol , varcharcol , varintcol ) <nl> VALUES ( 1 , 2147483647 , ' _ _ ! ' ' $ # @ ! ~ " ' , 9223372036854775807 , 0xffffffffffffffffff , true , <nl> - 0 . 00000000000001 , 9999999 . 999 , 99999 . 99 , ' ∭ Ƕ ⑮ ฑ ➳ ❏ ' ' ' , <nl> - ' 1950 - 01 - 01 + 0000 ' , ffffffff - ffff - ffff - ffff - ffffffffffff , ' newline - > <nl> + 0 . 00000000000001 , 9999999 . 999 , 99999 . 99 , 32767 , ' ∭ Ƕ ⑮ ฑ ➳ ❏ ' ' ' , <nl> + ' 1950 - 01 - 01 + 0000 ' , 127 , ffffffff - ffff - ffff - ffff - ffffffffffff , ' newline - > <nl> < - ' , 9 ) ; <nl> <nl> INSERT INTO has _ all _ types ( num , intcol , asciicol , bigintcol , blobcol , booleancol , <nl> - decimalcol , doublecol , floatcol , textcol , <nl> - timestampcol , uuidcol , varcharcol , varintcol ) <nl> + decimalcol , doublecol , floatcol , smallintcol , textcol , <nl> + timestampcol , tinyintcol , uuidcol , varcharcol , varintcol ) <nl> VALUES ( 2 , 0 , ' ' , 0 , 0x , false , <nl> - 0 . 0 , 0 . 0 , 0 . 0 , ' ' , <nl> - 0 , 00000000 - 0000 - 0000 - 0000 - 000000000000 , ' ' , 0 ) ; <nl> + 0 . 0 , 0 . 0 , 0 . 0 , 0 , ' ' , <nl> + 0 , 0 , 00000000 - 0000 - 0000 - 0000 - 000000000000 , ' ' , 0 ) ; <nl> <nl> INSERT INTO has _ all _ types ( num , intcol , asciicol , bigintcol , blobcol , booleancol , <nl> - decimalcol , doublecol , floatcol , textcol , <nl> - timestampcol , uuidcol , varcharcol , varintcol ) <nl> + decimalcol , doublecol , floatcol , smallintcol , textcol , <nl> + timestampcol , tinyintcol , uuidcol , varcharcol , varintcol ) <nl> VALUES ( 3 , - 2147483648 , ' ' ' ' ' ' ' ' , - 9223372036854775808 , 0x80 , false , <nl> - 10 . 0000000000000 , - 1004 . 10 , 100000000 . 9 , ' 龍 馭 鬱 ' , <nl> - ' 2038 - 01 - 19T03 : 14 - 1200 ' , ffffffff - ffff - 1fff - 8fff - ffffffffffff , <nl> + 10 . 0000000000000 , - 1004 . 10 , 100000000 . 9 , 32767 , ' 龍 馭 鬱 ' , <nl> + ' 2038 - 01 - 19T03 : 14 - 1200 ' , 127 , ffffffff - ffff - 1fff - 8fff - ffffffffffff , <nl> 	 ' ' ' ' , - 10000000000000000000000000 ) ; <nl> <nl> INSERT INTO has _ all _ types ( num , intcol , asciicol , bigintcol , blobcol , booleancol , <nl> - decimalcol , doublecol , floatcol , textcol , <nl> - timestampcol , uuidcol , varcharcol , varintcol ) <nl> + decimalcol , doublecol , floatcol , smallintcol , textcol , <nl> + timestampcol , tinyintcol , uuidcol , varcharcol , varintcol ) <nl> VALUES ( 4 , blobAsInt ( 0x ) , ' ' , blobAsBigint ( 0x ) , 0x , blobAsBoolean ( 0x ) , <nl> - 	 blobAsDecimal ( 0x ) , blobAsDouble ( 0x ) , blobAsFloat ( 0x ) , ' ' , <nl> - 	 blobAsTimestamp ( 0x ) , blobAsUuid ( 0x ) , ' ' , blobAsVarint ( 0x ) ) ; <nl> + 	 blobAsDecimal ( 0x ) , blobAsDouble ( 0x ) , blobAsFloat ( 0x ) , blobAsSmallInt ( 0x0000 ) , ' ' , <nl> + 	 blobAsTimestamp ( 0x ) , blobAsTinyInt ( 0x00 ) , blobAsUuid ( 0x ) , ' ' , blobAsVarint ( 0x ) ) ; <nl> <nl> <nl>
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 0959ac9 . . e309eae 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 5 , 6 + 5 , 8 @ @ <nl> * Improve asynchronous hint delivery ( CASSANDRA - 5179 ) <nl> * Fix Guava dependency version ( 12 . 0 - > 13 . 0 . 1 ) for Maven ( CASSANDRA - 5364 ) <nl> * Validate that provided CQL3 collection value are < 64K ( CASSANDRA - 5355 ) <nl> + * Change Kernel Page Cache skipping into row preheating ( disabled by default ) <nl> + ( CASSANDRA - 4937 ) <nl> <nl> <nl> 1 . 2 . 3 <nl> diff - - git a / conf / cassandra . yaml b / conf / cassandra . yaml <nl> index 0a8102d . . 178487d 100644 <nl> - - - a / conf / cassandra . yaml <nl> + + + b / conf / cassandra . yaml <nl> @ @ - 684 , 3 + 684 , 9 @ @ internode _ compression : all <nl> # reducing overhead from the TCP protocol itself , at the cost of increasing <nl> # latency if you block for cross - datacenter responses . <nl> inter _ dc _ tcp _ nodelay : true <nl> + <nl> + # Enable or disable kernel page cache preheating from contents of the key cache after compaction . <nl> + # When enabled it would preheat only first " page " ( 4KB ) of each row to optimize <nl> + # for sequential access . Note : This could be harmful for fat rows , see CASSANDRA - 4937 <nl> + # for further details on that topic . <nl> + preheat _ kernel _ page _ cache : false <nl> diff - - git a / src / java / org / apache / cassandra / config / Config . java b / src / java / org / apache / cassandra / config / Config . java <nl> index 02324ee . . 212147a 100644 <nl> - - - a / src / java / org / apache / cassandra / config / Config . java <nl> + + + b / src / java / org / apache / cassandra / config / Config . java <nl> @ @ - 167 , 6 + 167 , 8 @ @ public class Config <nl> <nl> public boolean inter _ dc _ tcp _ nodelay = true ; <nl> <nl> + public boolean preheat _ kernel _ page _ cache = false ; <nl> + <nl> private static boolean loadYaml = true ; <nl> private static boolean outboundBindAny = false ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> index 2d3cbb5 . . 5fcef07 100644 <nl> - - - a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> + + + b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> @ @ - 1275 , 4 + 1275 , 9 @ @ public class DatabaseDescriptor <nl> { <nl> return conf . inter _ dc _ tcp _ nodelay ; <nl> } <nl> + <nl> + public static boolean shouldPreheatPageCache ( ) <nl> + { <nl> + return conf . preheat _ kernel _ page _ cache ; <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java <nl> index 2728970 . . 8dcdaad 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java <nl> @ @ - 121 , 7 + 121 , 7 @ @ public class CommitLogReplayer <nl> CommitLogDescriptor desc = CommitLogDescriptor . fromFileName ( file . getName ( ) ) ; <nl> final long segment = desc . id ; <nl> int version = desc . getMessagingVersion ( ) ; <nl> - RandomAccessReader reader = RandomAccessReader . open ( new File ( file . getAbsolutePath ( ) ) , true ) ; <nl> + RandomAccessReader reader = RandomAccessReader . open ( new File ( file . getAbsolutePath ( ) ) ) ; <nl> try <nl> { <nl> assert reader . length ( ) < = Integer . MAX _ VALUE ; <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / AbstractCompactionStrategy . java b / src / java / org / apache / cassandra / db / compaction / AbstractCompactionStrategy . java <nl> index cb15109 . . 85b09c1 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / AbstractCompactionStrategy . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / AbstractCompactionStrategy . java <nl> @ @ - 152 , 7 + 152 , 7 @ @ public abstract class AbstractCompactionStrategy <nl> { <nl> ArrayList < ICompactionScanner > scanners = new ArrayList < ICompactionScanner > ( ) ; <nl> for ( SSTableReader sstable : sstables ) <nl> - scanners . add ( sstable . getDirectScanner ( range ) ) ; <nl> + scanners . add ( sstable . getScanner ( range ) ) ; <nl> return scanners ; <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> index 14e1b13 . . 0fe3a7a 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java <nl> @ @ - 568 , 7 + 568 , 7 @ @ public class CompactionManager implements CompactionManagerMBean <nl> if ( compactionFileLocation = = null ) <nl> throw new IOException ( " disk full " ) ; <nl> <nl> - SSTableScanner scanner = sstable . getDirectScanner ( ) ; <nl> + SSTableScanner scanner = sstable . getScanner ( ) ; <nl> long rowsRead = 0 ; <nl> List < IColumn > indexedColumnsInRow = null ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionTask . java b / src / java / org / apache / cassandra / db / compaction / CompactionTask . java <nl> index 75ea1cb . . 8c2af4d 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / CompactionTask . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / CompactionTask . java <nl> @ @ - 231 , 10 + 231 , 7 @ @ public class CompactionTask extends AbstractCompactionTask <nl> cfs . replaceCompactedSSTables ( toCompact , sstables , compactionType ) ; <nl> / / TODO : this doesn ' t belong here , it should be part of the reader to load when the tracker is wired up <nl> for ( SSTableReader sstable : sstables ) <nl> - { <nl> - for ( Map . Entry < DecoratedKey , RowIndexEntry > entry : cachedKeyMap . get ( sstable . descriptor ) . entrySet ( ) ) <nl> - sstable . cacheKey ( entry . getKey ( ) , entry . getValue ( ) ) ; <nl> - } <nl> + sstable . preheat ( cachedKeyMap . get ( sstable . descriptor ) ) ; <nl> <nl> if ( logger . isInfoEnabled ( ) ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java <nl> index 6a1bf4b . . d916c48 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java <nl> @ @ - 179 , 7 + 179 , 7 @ @ public class LeveledCompactionStrategy extends AbstractCompactionStrategy implem <nl> { <nl> / / L0 makes no guarantees about overlapping - ness . Just create a direct scanner for each <nl> for ( SSTableReader sstable : byLevel . get ( level ) ) <nl> - scanners . add ( sstable . getDirectScanner ( range ) ) ; <nl> + scanners . add ( sstable . getScanner ( range ) ) ; <nl> } <nl> else <nl> { <nl> @ @ - 209 , 7 + 209 , 7 @ @ public class LeveledCompactionStrategy extends AbstractCompactionStrategy implem <nl> this . sstables = new ArrayList < SSTableReader > ( sstables ) ; <nl> Collections . sort ( this . sstables , SSTable . sstableComparator ) ; <nl> sstableIterator = this . sstables . iterator ( ) ; <nl> - currentScanner = sstableIterator . next ( ) . getDirectScanner ( range ) ; <nl> + currentScanner = sstableIterator . next ( ) . getScanner ( range ) ; <nl> <nl> long length = 0 ; <nl> for ( SSTableReader sstable : sstables ) <nl> @ @ - 234 , 7 + 234 , 7 @ @ public class LeveledCompactionStrategy extends AbstractCompactionStrategy implem <nl> currentScanner = null ; <nl> return endOfData ( ) ; <nl> } <nl> - currentScanner = sstableIterator . next ( ) . getDirectScanner ( range ) ; <nl> + currentScanner = sstableIterator . next ( ) . getScanner ( range ) ; <nl> } <nl> } <nl> catch ( IOException e ) <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / Scrubber . java b / src / java / org / apache / cassandra / db / compaction / Scrubber . java <nl> index 0601857 . . 30929cc 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / Scrubber . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / Scrubber . java <nl> @ @ - 94 , 8 + 94 , 8 @ @ public class Scrubber implements Closeable <nl> / / we ' ll also loop through the index at the same time , using the position from the index to recover if the <nl> / / row header ( key or data size ) is corrupt . ( This means our position in the index file will be one row <nl> / / " ahead " of the data file . ) <nl> - this . dataFile = sstable . openDataReader ( true ) ; <nl> - this . indexFile = RandomAccessReader . open ( new File ( sstable . descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) , true ) ; <nl> + this . dataFile = sstable . openDataReader ( ) ; <nl> + this . indexFile = RandomAccessReader . open ( new File ( sstable . descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) ) ; <nl> this . scrubInfo = new ScrubInfo ( dataFile , sstable ) ; <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java <nl> index bbd2466 . . aa686c2 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java <nl> @ @ - 42 , 7 + 42 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> { <nl> try <nl> { <nl> - return new CompressedRandomAccessReader ( path , metadata , false , owner ) ; <nl> + return new CompressedRandomAccessReader ( path , metadata , owner ) ; <nl> } <nl> catch ( FileNotFoundException e ) <nl> { <nl> @ @ - 50 , 11 + 50 , 11 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> } <nl> } <nl> <nl> - public static CompressedRandomAccessReader open ( String dataFilePath , CompressionMetadata metadata , boolean skipIOCache ) <nl> + public static CompressedRandomAccessReader open ( String dataFilePath , CompressionMetadata metadata ) <nl> { <nl> try <nl> { <nl> - return new CompressedRandomAccessReader ( dataFilePath , metadata , skipIOCache , null ) ; <nl> + return new CompressedRandomAccessReader ( dataFilePath , metadata , null ) ; <nl> } <nl> catch ( FileNotFoundException e ) <nl> { <nl> @ @ - 73 , 9 + 73 , 9 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> / / raw checksum bytes <nl> private final ByteBuffer checksumBytes = ByteBuffer . wrap ( new byte [ 4 ] ) ; <nl> <nl> - private CompressedRandomAccessReader ( String dataFilePath , CompressionMetadata metadata , boolean skipIOCache , PoolingSegmentedFile owner ) throws FileNotFoundException <nl> + private CompressedRandomAccessReader ( String dataFilePath , CompressionMetadata metadata , PoolingSegmentedFile owner ) throws FileNotFoundException <nl> { <nl> - super ( new File ( dataFilePath ) , metadata . chunkLength ( ) , skipIOCache , owner ) ; <nl> + super ( new File ( dataFilePath ) , metadata . chunkLength ( ) , owner ) ; <nl> this . metadata = metadata ; <nl> compressed = ByteBuffer . wrap ( new byte [ metadata . compressor ( ) . initialCompressedBufferLength ( metadata . chunkLength ( ) ) ] ) ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> index e581b22 . . 9fe1309 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> @ @ - 38 , 7 + 38 , 7 @ @ public class KeyIterator extends AbstractIterator < DecoratedKey > implements Close <nl> { <nl> this . desc = desc ; <nl> File path = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; <nl> - in = RandomAccessReader . open ( path , true ) ; <nl> + in = RandomAccessReader . open ( path ) ; <nl> } <nl> <nl> protected DecoratedKey computeNext ( ) <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableBoundedScanner . java b / src / java / org / apache / cassandra / io / sstable / SSTableBoundedScanner . java <nl> index a571901 . . d5bec82 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableBoundedScanner . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableBoundedScanner . java <nl> @ @ - 35 , 9 + 35 , 9 @ @ public class SSTableBoundedScanner extends SSTableScanner <nl> private final Iterator < Pair < Long , Long > > rangeIterator ; <nl> private Pair < Long , Long > currentRange ; <nl> <nl> - SSTableBoundedScanner ( SSTableReader sstable , boolean skipCache , Iterator < Pair < Long , Long > > rangeIterator ) <nl> + SSTableBoundedScanner ( SSTableReader sstable , Iterator < Pair < Long , Long > > rangeIterator ) <nl> { <nl> - super ( sstable , skipCache ) ; <nl> + super ( sstable ) ; <nl> this . rangeIterator = rangeIterator ; <nl> assert rangeIterator . hasNext ( ) ; / / use EmptyCompactionScanner otherwise <nl> currentRange = rangeIterator . next ( ) ; <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> index 51f2344 . . dbc45d8 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> @ @ - 352 , 7 + 352 , 7 @ @ public class SSTableReader extends SSTable <nl> : SegmentedFile . getBuilder ( DatabaseDescriptor . getDiskAccessMode ( ) ) ; <nl> <nl> / / we read the positions in a BRAF so we don ' t have to worry about an entry spanning a mmap boundary . <nl> - RandomAccessReader primaryIndex = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) , true ) ; <nl> + RandomAccessReader primaryIndex = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) ) ; <nl> <nl> / / try to load summaries from the disk and check if we need <nl> / / to read primary index because we should re - create a BloomFilter or pre - load KeyCache <nl> @ @ - 698 , 6 + 698 , 29 @ @ public class SSTableReader extends SSTable <nl> keyCache . put ( cacheKey , info ) ; <nl> } <nl> <nl> + public void preheat ( Map < DecoratedKey , RowIndexEntry > cachedKeys ) throws IOException <nl> + { <nl> + RandomAccessFile f = new RandomAccessFile ( getFilename ( ) , " r " ) ; <nl> + <nl> + try <nl> + { <nl> + int fd = CLibrary . getfd ( f . getFD ( ) ) ; <nl> + <nl> + for ( Map . Entry < DecoratedKey , RowIndexEntry > entry : cachedKeys . entrySet ( ) ) <nl> + { <nl> + cacheKey ( entry . getKey ( ) , entry . getValue ( ) ) ; <nl> + <nl> + / / add to the cache but don ' t do actual preheating if we have it disabled in the config <nl> + if ( DatabaseDescriptor . shouldPreheatPageCache ( ) & & fd > 0 ) <nl> + CLibrary . preheatPage ( fd , entry . getValue ( ) . position ) ; <nl> + } <nl> + } <nl> + finally <nl> + { <nl> + FileUtils . closeQuietly ( f ) ; <nl> + } <nl> + } <nl> + <nl> public RowIndexEntry getCachedPosition ( DecoratedKey key , boolean updateStats ) <nl> { <nl> return getCachedPosition ( new KeyCacheKey ( descriptor , key . key ) , updateStats ) ; <nl> @ @ - 897 , 6 + 920 , 15 @ @ public class SSTableReader extends SSTable <nl> { <nl> if ( references . decrementAndGet ( ) = = 0 & & isCompacted . get ( ) ) <nl> { <nl> + / * * <nl> + * Make OS a favour and suggest ( using fadvice call ) that we <nl> + * don ' t want to see pages of this SSTable in memory anymore . <nl> + * <nl> + * NOTE : We can ' t use madvice in java because it requires address of <nl> + * the mapping , so instead we always open a file and run fadvice ( fd , 0 , 0 ) on it <nl> + * / <nl> + dropPageCache ( ) ; <nl> + <nl> / / Force finalizing mmapping if necessary <nl> ifile . cleanup ( ) ; <nl> dfile . cleanup ( ) ; <nl> @ @ - 948 , 12 + 980 , 12 @ @ public class SSTableReader extends SSTable <nl> } <nl> <nl> / * * <nl> - * Direct I / O SSTableScanner <nl> + * I / O SSTableScanner <nl> * @ return A Scanner for seeking over the rows of the SSTable . <nl> * / <nl> - public SSTableScanner getDirectScanner ( ) <nl> + public SSTableScanner getScanner ( ) <nl> { <nl> - return new SSTableScanner ( this , true ) ; <nl> + return new SSTableScanner ( this ) ; <nl> } <nl> <nl> / * * <nl> @ @ - 962 , 14 + 994 , 14 @ @ public class SSTableReader extends SSTable <nl> * @ param range the range of keys to cover <nl> * @ return A Scanner for seeking over the rows of the SSTable . <nl> * / <nl> - public ICompactionScanner getDirectScanner ( Range < Token > range ) <nl> + public ICompactionScanner getScanner ( Range < Token > range ) <nl> { <nl> if ( range = = null ) <nl> - return getDirectScanner ( ) ; <nl> + return getScanner ( ) ; <nl> <nl> Iterator < Pair < Long , Long > > rangeIterator = getPositionsForRanges ( Collections . singletonList ( range ) ) . iterator ( ) ; <nl> return rangeIterator . hasNext ( ) <nl> - ? new SSTableBoundedScanner ( this , true , rangeIterator ) <nl> + ? new SSTableBoundedScanner ( this , rangeIterator ) <nl> : new EmptyCompactionScanner ( getFilename ( ) ) ; <nl> } <nl> <nl> @ @ - 1122 , 16 + 1154 , 16 @ @ public class SSTableReader extends SSTable <nl> return sstableMetadata . ancestors ; <nl> } <nl> <nl> - public RandomAccessReader openDataReader ( boolean skipIOCache ) <nl> + public RandomAccessReader openDataReader ( ) <nl> { <nl> return compression <nl> - ? CompressedRandomAccessReader . open ( getFilename ( ) , getCompressionMetadata ( ) , skipIOCache ) <nl> - : RandomAccessReader . open ( new File ( getFilename ( ) ) , skipIOCache ) ; <nl> + ? CompressedRandomAccessReader . open ( getFilename ( ) , getCompressionMetadata ( ) ) <nl> + : RandomAccessReader . open ( new File ( getFilename ( ) ) ) ; <nl> } <nl> <nl> - public RandomAccessReader openIndexReader ( boolean skipIOCache ) <nl> + public RandomAccessReader openIndexReader ( ) <nl> { <nl> - return RandomAccessReader . open ( new File ( getIndexFilename ( ) ) , skipIOCache ) ; <nl> + return RandomAccessReader . open ( new File ( getIndexFilename ( ) ) ) ; <nl> } <nl> <nl> / * * <nl> @ @ - 1222 , 4 + 1254 , 38 @ @ public class SSTableReader extends SSTable <nl> throw new UnsupportedOperationException ( ) ; <nl> } <nl> } <nl> + <nl> + private void dropPageCache ( ) <nl> + { <nl> + dropPageCache ( dfile . path ) ; <nl> + dropPageCache ( ifile . path ) ; <nl> + } <nl> + <nl> + private void dropPageCache ( String filePath ) <nl> + { <nl> + RandomAccessFile file = null ; <nl> + <nl> + try <nl> + { <nl> + file = new RandomAccessFile ( filePath , " r " ) ; <nl> + <nl> + int fd = CLibrary . getfd ( file . getFD ( ) ) ; <nl> + <nl> + if ( fd > 0 ) <nl> + { <nl> + if ( logger . isDebugEnabled ( ) ) <nl> + logger . debug ( String . format ( " Dropping page cache of file % s . " , filePath ) ) ; <nl> + <nl> + CLibrary . trySkipCache ( fd , 0 , 0 ) ; <nl> + } <nl> + } <nl> + catch ( IOException e ) <nl> + { <nl> + / / we don ' t care if cache cleanup fails <nl> + } <nl> + finally <nl> + { <nl> + FileUtils . closeQuietly ( file ) ; <nl> + } <nl> + } <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableScanner . java b / src / java / org / apache / cassandra / io / sstable / SSTableScanner . java <nl> index 22ac485 . . 949acda 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableScanner . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableScanner . java <nl> @ @ - 46 , 10 + 46 , 10 @ @ public class SSTableScanner implements ICompactionScanner <nl> / * * <nl> * @ param sstable SSTable to scan . <nl> * / <nl> - SSTableScanner ( SSTableReader sstable , boolean skipCache ) <nl> + SSTableScanner ( SSTableReader sstable ) <nl> { <nl> - this . dfile = sstable . openDataReader ( skipCache ) ; <nl> - this . ifile = sstable . openIndexReader ( skipCache ) ; <nl> + this . dfile = sstable . openDataReader ( ) ; <nl> + this . ifile = sstable . openIndexReader ( ) ; <nl> this . sstable = sstable ; <nl> this . filter = null ; <nl> } <nl> @ @ - 60 , 8 + 60 , 8 @ @ public class SSTableScanner implements ICompactionScanner <nl> * / <nl> SSTableScanner ( SSTableReader sstable , QueryFilter filter ) <nl> { <nl> - this . dfile = sstable . openDataReader ( false ) ; <nl> - this . ifile = sstable . openIndexReader ( false ) ; <nl> + this . dfile = sstable . openDataReader ( ) ; <nl> + this . ifile = sstable . openIndexReader ( ) ; <nl> this . sstable = sstable ; <nl> this . filter = filter ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / util / RandomAccessReader . java b / src / java / org / apache / cassandra / io / util / RandomAccessReader . java <nl> index 3210372 . . 4d7bfbb 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / RandomAccessReader . java <nl> + + + b / src / java / org / apache / cassandra / io / util / RandomAccessReader . java <nl> @ @ - 50 , 19 + 50 , 11 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu <nl> / / channel liked with the file , used to retrieve data and force updates . <nl> protected final FileChannel channel ; <nl> <nl> - private final boolean skipIOCache ; <nl> - <nl> - / / file descriptor <nl> - private final int fd ; <nl> - <nl> - / / used if skip I / O cache was enabled <nl> - private long bytesSinceCacheFlush = 0 ; <nl> - <nl> private final long fileLength ; <nl> <nl> protected final PoolingSegmentedFile owner ; <nl> <nl> - protected RandomAccessReader ( File file , int bufferSize , boolean skipIOCache , PoolingSegmentedFile owner ) throws FileNotFoundException <nl> + protected RandomAccessReader ( File file , int bufferSize , PoolingSegmentedFile owner ) throws FileNotFoundException <nl> { <nl> super ( file , " r " ) ; <nl> <nl> @ @ - 74 , 18 + 66 , 8 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu <nl> / / allocating required size of the buffer <nl> if ( bufferSize < = 0 ) <nl> throw new IllegalArgumentException ( " bufferSize must be positive " ) ; <nl> - buffer = new byte [ bufferSize ] ; <nl> <nl> - this . skipIOCache = skipIOCache ; <nl> - try <nl> - { <nl> - fd = CLibrary . getfd ( getFD ( ) ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - / / fd = = null , Not Supposed To Happen <nl> - throw new RuntimeException ( e ) ; <nl> - } <nl> + buffer = new byte [ bufferSize ] ; <nl> <nl> / / we can cache file length in read - only mode <nl> try <nl> @ @ - 99 , 27 + 81 , 22 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu <nl> validBufferBytes = - 1 ; / / that will trigger reBuffer ( ) on demand by read / seek operations <nl> } <nl> <nl> - public static RandomAccessReader open ( File file ) <nl> - { <nl> - return open ( file , false ) ; <nl> - } <nl> - <nl> public static RandomAccessReader open ( File file , PoolingSegmentedFile owner ) <nl> { <nl> - return open ( file , DEFAULT _ BUFFER _ SIZE , false , owner ) ; <nl> + return open ( file , DEFAULT _ BUFFER _ SIZE , owner ) ; <nl> } <nl> <nl> - public static RandomAccessReader open ( File file , boolean skipIOCache ) <nl> + public static RandomAccessReader open ( File file ) <nl> { <nl> - return open ( file , DEFAULT _ BUFFER _ SIZE , skipIOCache , null ) ; <nl> + return open ( file , DEFAULT _ BUFFER _ SIZE , null ) ; <nl> } <nl> <nl> @ VisibleForTesting <nl> - static RandomAccessReader open ( File file , int bufferSize , boolean skipIOCache , PoolingSegmentedFile owner ) <nl> + static RandomAccessReader open ( File file , int bufferSize , PoolingSegmentedFile owner ) <nl> { <nl> try <nl> { <nl> - return new RandomAccessReader ( file , bufferSize , skipIOCache , owner ) ; <nl> + return new RandomAccessReader ( file , bufferSize , owner ) ; <nl> } <nl> catch ( FileNotFoundException e ) <nl> { <nl> @ @ - 130 , 7 + 107 , 7 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu <nl> @ VisibleForTesting <nl> static RandomAccessReader open ( SequentialWriter writer ) <nl> { <nl> - return open ( new File ( writer . getPath ( ) ) , DEFAULT _ BUFFER _ SIZE , false , null ) ; <nl> + return open ( new File ( writer . getPath ( ) ) , DEFAULT _ BUFFER _ SIZE , null ) ; <nl> } <nl> <nl> / * * <nl> @ @ - 158 , 21 + 135 , 11 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu <nl> } <nl> <nl> validBufferBytes = read ; <nl> - bytesSinceCacheFlush + = read ; <nl> } <nl> catch ( IOException e ) <nl> { <nl> throw new FSReadError ( e , filePath ) ; <nl> } <nl> - <nl> - if ( skipIOCache & & bytesSinceCacheFlush > = CACHE _ FLUSH _ INTERVAL _ IN _ BYTES ) <nl> - { <nl> - / / with random I / O we can ' t control what we are skipping so <nl> - / / it will be more appropriate to just skip a whole file after <nl> - / / we reach threshold <nl> - CLibrary . trySkipCache ( this . fd , 0 , 0 ) ; <nl> - bytesSinceCacheFlush = 0 ; <nl> - } <nl> } <nl> <nl> @ Override <nl> @ @ - 264 , 9 + 231 , 6 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu <nl> { <nl> buffer = null ; / / makes sure we don ' t use this after it ' s ostensibly closed <nl> <nl> - if ( skipIOCache & & bytesSinceCacheFlush > 0 ) <nl> - CLibrary . trySkipCache ( fd , 0 , 0 ) ; <nl> - <nl> try <nl> { <nl> super . close ( ) ; <nl> @ @ - 280 , 7 + 244 , 7 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu <nl> @ Override <nl> public String toString ( ) <nl> { <nl> - return getClass ( ) . getSimpleName ( ) + " ( " + " filePath = ' " + filePath + " ' " + " , skipIOCache = " + skipIOCache + " ) " ; <nl> + return getClass ( ) . getSimpleName ( ) + " ( " + " filePath = ' " + filePath + " ' ) " ; <nl> } <nl> <nl> / * * <nl> diff - - git a / src / java / org / apache / cassandra / streaming / FileStreamTask . java b / src / java / org / apache / cassandra / streaming / FileStreamTask . java <nl> index 67d5c35 . . 979b2e1 100644 <nl> - - - a / src / java / org / apache / cassandra / streaming / FileStreamTask . java <nl> + + + b / src / java / org / apache / cassandra / streaming / FileStreamTask . java <nl> @ @ - 139 , 7 + 139 , 7 @ @ public class FileStreamTask extends WrappedRunnable <nl> return ; <nl> <nl> / / try to skip kernel page cache if possible <nl> - RandomAccessReader file = RandomAccessReader . open ( new File ( header . file . getFilename ( ) ) , true ) ; <nl> + RandomAccessReader file = RandomAccessReader . open ( new File ( header . file . getFilename ( ) ) ) ; <nl> <nl> / / setting up data compression stream <nl> compressedoutput = new LZFOutputStream ( output ) ; <nl> diff - - git a / src / java / org / apache / cassandra / streaming / compress / CompressedFileStreamTask . java b / src / java / org / apache / cassandra / streaming / compress / CompressedFileStreamTask . java <nl> index dda9d7d . . c1818ed 100644 <nl> - - - a / src / java / org / apache / cassandra / streaming / compress / CompressedFileStreamTask . java <nl> + + + b / src / java / org / apache / cassandra / streaming / compress / CompressedFileStreamTask . java <nl> @ @ - 65 , 7 + 65 , 7 @ @ public class CompressedFileStreamTask extends FileStreamTask <nl> ByteBuffer headerBuffer = MessagingService . instance ( ) . constructStreamHeader ( header , false , MessagingService . instance ( ) . getVersion ( to ) ) ; <nl> socket . getOutputStream ( ) . write ( ByteBufferUtil . getArray ( headerBuffer ) ) ; <nl> <nl> - RandomAccessReader file = RandomAccessReader . open ( new File ( header . file . getFilename ( ) ) , true ) ; <nl> + RandomAccessReader file = RandomAccessReader . open ( new File ( header . file . getFilename ( ) ) ) ; <nl> FileChannel fc = file . getChannel ( ) ; <nl> <nl> StreamingMetrics . activeStreamsOutbound . inc ( ) ; <nl> diff - - git a / src / java / org / apache / cassandra / tools / SSTableExport . java b / src / java / org / apache / cassandra / tools / SSTableExport . java <nl> index 51cdc72 . . 2e117ef 100644 <nl> - - - a / src / java / org / apache / cassandra / tools / SSTableExport . java <nl> + + + b / src / java / org / apache / cassandra / tools / SSTableExport . java <nl> @ @ - 349 , 7 + 349 , 7 @ @ public class SSTableExport <nl> public static void export ( Descriptor desc , PrintStream outs , Collection < String > toExport , String [ ] excludes ) throws IOException <nl> { <nl> SSTableReader reader = SSTableReader . open ( desc ) ; <nl> - SSTableScanner scanner = reader . getDirectScanner ( ) ; <nl> + SSTableScanner scanner = reader . getScanner ( ) ; <nl> <nl> IPartitioner < ? > partitioner = reader . partitioner ; <nl> <nl> @ @ - 406 , 7 + 406 , 7 @ @ public class SSTableExport <nl> <nl> <nl> SSTableIdentityIterator row ; <nl> - SSTableScanner scanner = reader . getDirectScanner ( ) ; <nl> + SSTableScanner scanner = reader . getScanner ( ) ; <nl> <nl> outs . println ( " [ " ) ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / utils / CLibrary . java b / src / java / org / apache / cassandra / utils / CLibrary . java <nl> index f8ad9d1 . . 2f6e088 100644 <nl> - - - a / src / java / org / apache / cassandra / utils / CLibrary . java <nl> + + + b / src / java / org / apache / cassandra / utils / CLibrary . java <nl> @ @ - 326 , 4 + 326 , 25 @ @ public final class CLibrary <nl> <nl> return - 1 ; <nl> } <nl> + <nl> + / * * <nl> + * Suggest kernel to preheat one page for the given file . <nl> + * <nl> + * @ param fd The file descriptor of file to preheat . <nl> + * @ param position The offset of the block . <nl> + * <nl> + * @ return On success , zero is returned . On error , an error number is returned . <nl> + * / <nl> + public static int preheatPage ( int fd , long position ) <nl> + { <nl> + try <nl> + { <nl> + / / 4096 is good for SSD because they operate on " Pages " 4KB in size <nl> + return posix _ fadvise ( fd , position , 4096 , POSIX _ FADV _ WILLNEED ) ; <nl> + } <nl> + catch ( UnsatisfiedLinkError e ) <nl> + { <nl> + return - 1 ; <nl> + } <nl> + } <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / db / KeyCacheTest . java b / test / unit / org / apache / cassandra / db / KeyCacheTest . java <nl> index 93f1fea . . b05a607 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / KeyCacheTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / KeyCacheTest . java <nl> @ @ - 130 , 13 + 130 , 13 @ @ public class KeyCacheTest extends SchemaLoader <nl> false , <nl> 10 ) ) ; <nl> <nl> - assert CacheService . instance . keyCache . size ( ) = = 2 ; <nl> + assertEquals ( 2 , CacheService . instance . keyCache . size ( ) ) ; <nl> <nl> Util . compactAll ( cfs ) . get ( ) ; <nl> keyCacheSize = CacheService . instance . keyCache . size ( ) ; <nl> / / after compaction cache should have entries for <nl> / / new SSTables , if we had 2 keys in cache previously it should become 4 <nl> - assert keyCacheSize = = 4 : keyCacheSize ; <nl> + assertEquals ( 4 , keyCacheSize ) ; <nl> <nl> / / re - read same keys to verify that key cache didn ' t grow further <nl> cfs . getColumnFamily ( QueryFilter . getSliceFilter ( key1 , <nl> diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> index 830c3e1 . . 437b778 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java <nl> @ @ - 75 , 7 + 75 , 7 @ @ public class CompressedRandomAccessReaderTest <nl> <nl> assert f . exists ( ) ; <nl> RandomAccessReader reader = compressed <nl> - ? CompressedRandomAccessReader . open ( filename , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) , false ) <nl> + ? CompressedRandomAccessReader . open ( filename , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) ) <nl> : RandomAccessReader . open ( f ) ; <nl> String expected = " The quick brown fox jumps over the lazy dog " ; <nl> assertEquals ( expected . length ( ) , reader . length ( ) ) ; <nl> @ @ - 115 , 7 + 115 , 7 @ @ public class CompressedRandomAccessReaderTest <nl> CompressionMetadata meta = new CompressionMetadata ( metadata . getPath ( ) , file . length ( ) ) ; <nl> CompressionMetadata . Chunk chunk = meta . chunkFor ( 0 ) ; <nl> <nl> - RandomAccessReader reader = CompressedRandomAccessReader . open ( file . getPath ( ) , meta , false ) ; <nl> + RandomAccessReader reader = CompressedRandomAccessReader . open ( file . getPath ( ) , meta ) ; <nl> / / read and verify compressed data <nl> assertEquals ( CONTENT , reader . readLine ( ) ) ; <nl> / / close reader <nl> @ @ - 142 , 7 + 142 , 7 @ @ public class CompressedRandomAccessReaderTest <nl> checksumModifier . write ( random . nextInt ( ) ) ; <nl> checksumModifier . getFD ( ) . sync ( ) ; / / making sure that change was synced with disk <nl> <nl> - final RandomAccessReader r = CompressedRandomAccessReader . open ( file . getPath ( ) , meta , false ) ; <nl> + final RandomAccessReader r = CompressedRandomAccessReader . open ( file . getPath ( ) , meta ) ; <nl> <nl> Throwable exception = null ; <nl> try <nl> @ @ - 163 , 7 + 163 , 7 @ @ public class CompressedRandomAccessReaderTest <nl> / / lets write original checksum and check if we can read data <nl> updateChecksum ( checksumModifier , chunk . length , checksum ) ; <nl> <nl> - reader = CompressedRandomAccessReader . open ( file . getPath ( ) , meta , false ) ; <nl> + reader = CompressedRandomAccessReader . open ( file . getPath ( ) , meta ) ; <nl> / / read and verify compressed data <nl> assertEquals ( CONTENT , reader . readLine ( ) ) ; <nl> / / close reader <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableTest . java <nl> index 12f2747 . . e944ed2 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableTest . java <nl> @ @ - 56 , 7 + 56 , 7 @ @ public class SSTableTest extends SchemaLoader <nl> <nl> private void verifySingle ( SSTableReader sstable , ByteBuffer bytes , ByteBuffer key ) throws IOException <nl> { <nl> - RandomAccessReader file = sstable . openDataReader ( false ) ; <nl> + RandomAccessReader file = sstable . openDataReader ( ) ; <nl> file . seek ( sstable . getPosition ( sstable . partitioner . decorateKey ( key ) , SSTableReader . Operator . EQ ) . position ) ; <nl> assert key . equals ( ByteBufferUtil . readWithShortLength ( file ) ) ; <nl> int size = ( int ) SSTableReader . readRowSize ( file , sstable . descriptor ) ; <nl> @ @ - 98 , 7 + 98 , 7 @ @ public class SSTableTest extends SchemaLoader <nl> { <nl> List < ByteBuffer > keys = new ArrayList < ByteBuffer > ( map . keySet ( ) ) ; <nl> / / Collections . shuffle ( keys ) ; <nl> - RandomAccessReader file = sstable . openDataReader ( false ) ; <nl> + RandomAccessReader file = sstable . openDataReader ( ) ; <nl> for ( ByteBuffer key : keys ) <nl> { <nl> file . seek ( sstable . getPosition ( sstable . partitioner . decorateKey ( key ) , SSTableReader . Operator . EQ ) . position ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java b / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java <nl> index 2b0a13a . . fcfaeec 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java <nl> @ @ - 72 , 8 + 72 , 8 @ @ public class SSTableUtils <nl> <nl> public static void assertContentEquals ( SSTableReader lhs , SSTableReader rhs ) throws IOException <nl> { <nl> - SSTableScanner slhs = lhs . getDirectScanner ( ) ; <nl> - SSTableScanner srhs = rhs . getDirectScanner ( ) ; <nl> + SSTableScanner slhs = lhs . getScanner ( ) ; <nl> + SSTableScanner srhs = rhs . getScanner ( ) ; <nl> while ( slhs . hasNext ( ) ) <nl> { <nl> OnDiskAtomIterator ilhs = slhs . next ( ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / io / util / BufferedRandomAccessFileTest . java b / test / unit / org / apache / cassandra / io / util / BufferedRandomAccessFileTest . java <nl> index 8059bbd . . 90c27e3 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / util / BufferedRandomAccessFileTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / util / BufferedRandomAccessFileTest . java <nl> @ @ - 338 , 7 + 338 , 7 @ @ public class BufferedRandomAccessFileTest <nl> for ( final int offset : Arrays . asList ( 0 , 8 ) ) <nl> { <nl> File file1 = writeTemporaryFile ( new byte [ 16 ] ) ; <nl> - final RandomAccessReader file = RandomAccessReader . open ( file1 , bufferSize , false , null ) ; <nl> + final RandomAccessReader file = RandomAccessReader . open ( file1 , bufferSize , null ) ; <nl> expectEOF ( new Callable < Object > ( ) <nl> { <nl> public Object call ( ) throws IOException <nl> @ @ - 353 , 7 + 353 , 7 @ @ public class BufferedRandomAccessFileTest <nl> for ( final int n : Arrays . asList ( 1 , 2 , 4 , 8 ) ) <nl> { <nl> File file1 = writeTemporaryFile ( new byte [ 16 ] ) ; <nl> - final RandomAccessReader file = RandomAccessReader . open ( file1 , bufferSize , false , null ) ; <nl> + final RandomAccessReader file = RandomAccessReader . open ( file1 , bufferSize , null ) ; <nl> expectEOF ( new Callable < Object > ( ) <nl> { <nl> public Object call ( ) throws IOException

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index a97cf2f . . 8b59309 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 9 + 1 , 9 @ @ 
 2 . 2 
 + * ( cqlsh ) Add support for native protocol 4 ( CASSANDRA - 9399 ) 
 * Ensure that UDF and UDAs are keyspace - isolated ( CASSANDRA - 9409 ) 
 * Revert CASSANDRA - 7807 ( tracing completion client notifications ) ( CASSANDRA - 9429 ) 
 * Add ability to stop compaction by ID ( CASSANDRA - 7207 ) 
 Merged from 2 . 1 : 
 - 2 . 1 . 6 
 * Improve estimated row count ( CASSANDRA - 9107 ) 
 * Optimize range tombstone memory footprint ( CASSANDRA - 8603 ) 
 * Use configured gcgs in anticompaction ( CASSANDRA - 9397 ) 
 diff - - git a / bin / cqlsh b / bin / cqlsh 
 index c73f9a4 . . b2a729c 100755 
 - - - a / bin / cqlsh 
 + + + b / bin / cqlsh 
 @ @ - 135 , 7 + 135 , 7 @ @ from cqlshlib . tracing import print _ trace _ session , print _ trace 
 DEFAULT _ HOST = ' 127 . 0 . 0 . 1 ' 
 DEFAULT _ PORT = 9042 
 DEFAULT _ CQLVER = ' 3 . 2 . 0 ' 
 - DEFAULT _ PROTOCOL _ VERSION = 3 
 + DEFAULT _ PROTOCOL _ VERSION = 4 
 
 DEFAULT _ FLOAT _ PRECISION = 5 
 DEFAULT _ MAX _ TRACE _ WAIT = 10 
 @ @ - 550 , 7 + 550 , 6 @ @ class FrozenType ( cassandra . cqltypes . _ ParameterizedType ) : 
 subtype , = cls . subtypes 
 return subtype . to _ binary ( val , protocol _ version ) 
 
 - 
 class Shell ( cmd . Cmd ) : 
 custom _ prompt = os . getenv ( ' CQLSH _ PROMPT ' , ' ' ) 
 if custom _ prompt is not ' ' : 
 @ @ - 581 , 7 + 580 , 8 @ @ class Shell ( cmd . Cmd ) : 
 max _ trace _ wait = DEFAULT _ MAX _ TRACE _ WAIT , 
 ssl = False , 
 single _ statement = None , 
 - client _ timeout = 10 ) : 
 + client _ timeout = 10 , 
 + protocol _ version = DEFAULT _ PROTOCOL _ VERSION ) : 
 cmd . Cmd . _ _ init _ _ ( self , completekey = completekey ) 
 self . hostname = hostname 
 self . port = port 
 @ @ - 599 , 7 + 599 , 7 @ @ class Shell ( cmd . Cmd ) : 
 self . conn = use _ conn 
 else : 
 self . conn = Cluster ( contact _ points = ( self . hostname , ) , port = self . port , cql _ version = cqlver , 
 - protocol _ version = DEFAULT _ PROTOCOL _ VERSION , 
 + protocol _ version = protocol _ version , 
 auth _ provider = self . auth _ provider , 
 ssl _ options = sslhandling . ssl _ settings ( hostname , CONFIG _ FILE ) if ssl else None , 
 load _ balancing _ policy = WhiteListRoundRobinPolicy ( [ self . hostname ] ) ) 
 @ @ - 1048 , 13 + 1048 , 22 @ @ class Shell ( cmd . Cmd ) : 
 
 def perform _ statement ( self , statement ) : 
 stmt = SimpleStatement ( statement , consistency _ level = self . consistency _ level , serial _ consistency _ level = self . serial _ consistency _ level , fetch _ size = self . default _ page _ size if self . use _ paging else None ) 
 - result = self . perform _ simple _ statement ( stmt ) 
 - if self . tracing _ enabled : 
 - if stmt . trace : 
 - print _ trace ( self , stmt . trace ) 
 - else : 
 - msg = " Statement trace did not complete within % d seconds " % ( self . session . max _ trace _ wait ) 
 - self . writeresult ( msg , color = RED ) 
 + result , future = self . perform _ simple _ statement ( stmt ) 
 + 
 + if future : 
 + if future . warnings : 
 + self . print _ warnings ( future . warnings ) 
 + 
 + if self . tracing _ enabled : 
 + try : 
 + trace = future . get _ query _ trace ( self . max _ trace _ wait ) 
 + if trace : 
 + print _ trace ( self , trace ) 
 + else : 
 + msg = " Statement trace did not complete within % d seconds " % ( self . session . max _ trace _ wait ) 
 + self . writeresult ( msg , color = RED ) 
 + except Exception , err : 
 + self . printerr ( " Unable to fetch query trace : % s " % ( str ( err ) , ) ) 
 
 return result 
 
 @ @ - 1069 , 19 + 1078 , 20 @ @ class Shell ( cmd . Cmd ) : 
 
 def perform _ simple _ statement ( self , statement ) : 
 if not statement : 
 - return False 
 + return False , None 
 rows = None 
 while True : 
 try : 
 - rows = self . session . execute ( statement , trace = self . tracing _ enabled ) 
 + future = self . session . execute _ async ( statement , trace = self . tracing _ enabled ) 
 + rows = future . result ( self . session . default _ timeout ) 
 break 
 except CQL _ ERRORS , err : 
 self . printerr ( str ( err . _ _ class _ _ . _ _ name _ _ ) + " : " + str ( err ) ) 
 - return False 
 + return False , None 
 except Exception , err : 
 import traceback 
 self . printerr ( traceback . format _ exc ( ) ) 
 - return False 
 + return False , None 
 
 if statement . query _ string [ : 6 ] . lower ( ) = = ' select ' : 
 self . print _ result ( rows , self . parse _ for _ table _ meta ( statement . query _ string ) ) 
 @ @ - 1094 , 7 + 1104 , 7 @ @ class Shell ( cmd . Cmd ) : 
 self . writeresult ( " " ) 
 self . print _ static _ result ( rows , self . parse _ for _ table _ meta ( statement . query _ string ) ) 
 self . flush _ output ( ) 
 - return True 
 + return True , future 
 
 def print _ result ( self , rows , table _ meta ) : 
 self . decoding _ errors = [ ] 
 @ @ - 1189 , 6 + 1199 , 16 @ @ class Shell ( cmd . Cmd ) : 
 self . writeresult ( ' ' + " | " . join ( [ column , value ] ) ) 
 self . writeresult ( ' ' ) 
 
 + def print _ warnings ( self , warnings ) : 
 + if warnings is None or len ( warnings ) = = 0 : 
 + return ; 
 + 
 + self . writeresult ( ' ' ) 
 + self . writeresult ( ' Warnings : ' ) 
 + for warning in warnings : 
 + self . writeresult ( warning ) 
 + self . writeresult ( ' ' ) 
 + 
 def emptyline ( self ) : 
 pass 
 
 @ @ - 1614 , 7 + 1634 , 7 @ @ class Shell ( cmd . Cmd ) : 
 contact _ points = ( self . hostname , ) , 
 port = self . port , 
 cql _ version = self . conn . cql _ version , 
 - protocol _ version = DEFAULT _ PROTOCOL _ VERSION , 
 + protocol _ version = self . conn . protocol _ version , 
 auth _ provider = self . auth _ provider , 
 ssl _ options = sslhandling . ssl _ settings ( self . hostname , CONFIG _ FILE ) if self . ssl else None , 
 load _ balancing _ policy = WhiteListRoundRobinPolicy ( [ self . hostname ] ) , 
 @ @ - 1703 , 7 + 1723 , 7 @ @ class Shell ( cmd . Cmd ) : 
 
 request _ id = conn . get _ request _ id ( ) 
 binary _ message = query _ message . to _ binary ( 
 - stream _ id = request _ id , protocol _ version = DEFAULT _ PROTOCOL _ VERSION , compression = None ) 
 + stream _ id = request _ id , protocol _ version = self . conn . protocol _ version , compression = None ) 
 
 # add the message directly to the connection ' s queue 
 with conn . lock : 
 @ @ - 2036 , 7 + 2056 , 7 @ @ class Shell ( cmd . Cmd ) : 
 auth _ provider = PlainTextAuthProvider ( username = username , password = password ) 
 
 conn = Cluster ( contact _ points = ( self . hostname , ) , port = self . port , cql _ version = self . conn . cql _ version , 
 - protocol _ version = DEFAULT _ PROTOCOL _ VERSION , 
 + protocol _ version = self . conn . protocol _ version , 
 auth _ provider = auth _ provider , 
 ssl _ options = self . conn . ssl _ options , 
 load _ balancing _ policy = WhiteListRoundRobinPolicy ( [ self . hostname ] ) ) 
 diff - - git a / lib / cassandra - driver - internal - only - 2 . 5 . 1 . post0 - 074650b . zip b / lib / cassandra - driver - internal - only - 2 . 5 . 1 . post0 - 074650b . zip 
 new file mode 100644 
 index 0000000 . . ce21a7a 
 Binary files / dev / null and b / lib / cassandra - driver - internal - only - 2 . 5 . 1 . post0 - 074650b . zip differ 
 diff - - git a / lib / cassandra - driver - internal - only - 2 . 5 . 1 . zip b / lib / cassandra - driver - internal - only - 2 . 5 . 1 . zip 
 deleted file mode 100644 
 index ee6ace0 . . 0000000 
 Binary files a / lib / cassandra - driver - internal - only - 2 . 5 . 1 . zip and / dev / null differ 
 diff - - git a / pylib / cqlshlib / cql3handling . py b / pylib / cqlshlib / cql3handling . py 
 index 3e155d0 . . ae66a4e 100644 
 - - - a / pylib / cqlshlib / cql3handling . py 
 + + + b / pylib / cqlshlib / cql3handling . py 
 @ @ - 20 , 7 + 20 , 7 @ @ from cassandra . metadata import escape _ name 
 
 
 simple _ cql _ types = set ( ( ' ascii ' , ' bigint ' , ' blob ' , ' boolean ' , ' counter ' , ' date ' , ' decimal ' , ' double ' , ' float ' , ' inet ' , ' int ' , 
 - ' text ' , ' time ' , ' timestamp ' , ' timeuuid ' , ' uuid ' , ' varchar ' , ' varint ' ) ) 
 + ' smallint ' , ' text ' , ' time ' , ' timestamp ' , ' timeuuid ' , ' tinyint ' , ' uuid ' , ' varchar ' , ' varint ' ) ) 
 simple _ cql _ types . difference _ update ( ( ' set ' , ' map ' , ' list ' ) ) 
 
 from . import helptopics 
 diff - - git a / pylib / cqlshlib / formatting . py b / pylib / cqlshlib / formatting . py 
 index 868ec28 . . 2310fa9 100644 
 - - - a / pylib / cqlshlib / formatting . py 
 + + + b / pylib / cqlshlib / formatting . py 
 @ @ - 192 , 8 + 192 , 8 @ @ def strftime ( time _ format , seconds ) : 
 hours , minutes = divmod ( abs ( offset ) / 60 , 60 ) 
 return formatted [ : - 5 ] + sign + ' { 0 : 0 = 2 } { 1 : 0 = 2 } ' . format ( hours , minutes ) 
 
 - @ formatter _ for ( ' date ' ) 
 - def format _ value _ uuid ( val , colormap , * * _ ) : 
 + @ formatter _ for ( ' Date ' ) 
 + def format _ value _ date ( val , colormap , * * _ ) : 
 return format _ python _ formatted _ type ( val , colormap , ' date ' ) 
 
 @ formatter _ for ( ' Time ' ) 
 diff - - git a / pylib / cqlshlib / helptopics . py b / pylib / cqlshlib / helptopics . py 
 index 0a43882 . . b38b235 100644 
 - - - a / pylib / cqlshlib / helptopics . py 
 + + + b / pylib / cqlshlib / helptopics . py 
 @ @ - 38 , 6 + 38 , 7 @ @ class CQLHelpTopics ( object ) : 
 HELP BLOB _ INPUT 
 HELP UUID _ INPUT 
 HELP BOOLEAN _ INPUT 
 + HELP INT _ INPUT 
 
 HELP TEXT _ OUTPUT 
 HELP TIMESTAMP _ OUTPUT 
 @ @ - 119 , 6 + 120 , 17 @ @ class CQLHelpTopics ( object ) : 
 as input for boolean types . 
 " " " 
 
 + def help _ int _ input ( self ) : 
 + print " " " 
 + Integer input 
 + 
 + CQL accepts the following integer types : 
 + tinyint - 1 - byte signed integer 
 + smallint - 2 - byte signed integer 
 + int - 4 - byte signed integer 
 + bigint - 8 - byte signed integer 
 + " " " 
 + 
 def help _ timestamp _ output ( self ) : 
 print " " " 
 Timestamp output 
 diff - - git a / pylib / cqlshlib / test / test _ cqlsh _ completion . py b / pylib / cqlshlib / test / test _ cqlsh _ completion . py 
 index cf7cab2 . . d6ccaf7 100644 
 - - - a / pylib / cqlshlib / test / test _ cqlsh _ completion . py 
 + + + b / pylib / cqlshlib / test / test _ cqlsh _ completion . py 
 @ @ - 143 , 8 + 143 , 8 @ @ class TestCqlshCompletion ( CqlshCompletionCase ) : 
 def test _ complete _ on _ empty _ string ( self ) : 
 self . trycompletions ( ' ' , choices = ( ' ? ' , ' ALTER ' , ' BEGIN ' , ' CAPTURE ' , ' CONSISTENCY ' , 
 ' COPY ' , ' CREATE ' , ' DEBUG ' , ' DELETE ' , ' DESC ' , ' DESCRIBE ' , 
 - ' DROP ' , ' GRANT ' , ' HELP ' , ' INSERT ' , ' LIST ' , ' PAGING ' , ' REVOKE ' , 
 - ' SELECT ' , ' SHOW ' , ' SOURCE ' , ' TRACING ' , ' EXPAND ' , ' TRUNCATE ' , 
 + ' DROP ' , ' GRANT ' , ' HELP ' , ' INSERT ' , ' LIST ' , ' LOGIN ' , ' PAGING ' , ' REVOKE ' , 
 + ' SELECT ' , ' SHOW ' , ' SOURCE ' , ' TRACING ' , ' EXPAND ' , ' SERIAL ' , ' TRUNCATE ' , 
 ' UPDATE ' , ' USE ' , ' exit ' , ' quit ' ) ) 
 
 def test _ complete _ command _ words ( self ) : 
 @ @ - 229 , 8 + 229 , 8 @ @ class TestCqlshCompletion ( CqlshCompletionCase ) : 
 " VALUES ( ' eggs ' , ' sausage ' , ' spam ' ) ; " ) , 
 choices = [ ' ? ' , ' ALTER ' , ' BEGIN ' , ' CAPTURE ' , ' CONSISTENCY ' , ' COPY ' , 
 ' CREATE ' , ' DEBUG ' , ' DELETE ' , ' DESC ' , ' DESCRIBE ' , ' DROP ' , 
 - ' EXPAND ' , ' GRANT ' , ' HELP ' , ' INSERT ' , ' LIST ' , ' PAGING ' , 
 - ' REVOKE ' , ' SELECT ' , ' SHOW ' , ' SOURCE ' , ' TRACING ' , 
 + ' EXPAND ' , ' GRANT ' , ' HELP ' , ' INSERT ' , ' LIST ' , ' LOGIN ' , ' PAGING ' , 
 + ' REVOKE ' , ' SELECT ' , ' SHOW ' , ' SOURCE ' , ' SERIAL ' , ' TRACING ' , 
 ' TRUNCATE ' , ' UPDATE ' , ' USE ' , ' exit ' , ' quit ' ] ) 
 
 self . trycompletions ( 
 diff - - git a / pylib / cqlshlib / test / test _ cqlsh _ output . py b / pylib / cqlshlib / test / test _ cqlsh _ output . py 
 index 40c7efc . . 2fd0ac7 100644 
 - - - a / pylib / cqlshlib / test / test _ cqlsh _ output . py 
 + + + b / pylib / cqlshlib / test / test _ cqlsh _ output . py 
 @ @ - 275 , 9 + 275 , 9 @ @ class TestCqlshOutput ( BaseTestCase ) : 
 # same query should show up as empty in cql 3 
 self . assertQueriesGiveColoredOutput ( ( 
 ( q , " " " 
 - num | asciicol | bigintcol | blobcol | booleancol | decimalcol | doublecol | floatcol | intcol | textcol | timestampcol | uuidcol | varcharcol | varintcol 
 - RRR MMMMMMMM MMMMMMMMM MMMMMMM MMMMMMMMMM MMMMMMMMMM MMMMMMMMM MMMMMMMM MMMMMM MMMMMMM MMMMMMMMMMMM MMMMMMM MMMMMMMMMM MMMMMMMMM 
 - - - - - - + - - - - - - - - - - + - - - - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - + - - - - - - - - - - + - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - 
 + num | asciicol | bigintcol | blobcol | booleancol | decimalcol | doublecol | floatcol | intcol | smallintcol | textcol | timestampcol | tinyintcol | uuidcol | varcharcol | varintcol 
 + RRR MMMMMMMM MMMMMMMMM MMMMMMM MMMMMMMMMM MMMMMMMMMM MMMMMMMMM MMMMMMMM MMMMMM MMMMMMMMMMM MMMMMMM MMMMMMMMMMMM MMMMMMMMMM MMMMMMM MMMMMMMMMM MMMMMMMMM 
 + - - - - - + - - - - - - - - - - + - - - - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - + - - - - - - - - - - + - - - - - - - - + - - - - - - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - + - - - - - - - - - - - - + - - - - - - - - - - - 
 
 
 ( 0 rows ) 
 @ @ - 602 , 8 + 602 , 10 @ @ class TestCqlshOutput ( BaseTestCase ) : 
 doublecol double , 
 floatcol float , 
 intcol int , 
 + smallintcol smallint , 
 textcol text , 
 timestampcol timestamp , 
 + tinyintcol tinyint , 
 uuidcol uuid , 
 varcharcol text , 
 varintcol varint 
 diff - - git a / pylib / cqlshlib / test / test _ keyspace _ init . cql b / pylib / cqlshlib / test / test _ keyspace _ init . cql 
 index 2433ca0 . . fda629e 100644 
 - - - a / pylib / cqlshlib / test / test _ keyspace _ init . cql 
 + + + b / pylib / cqlshlib / test / test _ keyspace _ init . cql 
 @ @ - 8 , 49 + 8 , 51 @ @ CREATE TABLE has _ all _ types ( 
 decimalcol decimal , 
 doublecol double , 
 floatcol float , 
 + smallintcol smallint , 
 textcol text , 
 timestampcol timestamp , 
 + tinyintcol tinyint , 
 uuidcol uuid , 
 varcharcol varchar , 
 varintcol varint 
 ) WITH compression = { ' sstable _ compression ' : ' LZ4Compressor ' } ; 
 
 INSERT INTO has _ all _ types ( num , intcol , asciicol , bigintcol , blobcol , booleancol , 
 - decimalcol , doublecol , floatcol , textcol , 
 - timestampcol , uuidcol , varcharcol , varintcol ) 
 + decimalcol , doublecol , floatcol , smallintcol , textcol , 
 + timestampcol , tinyintcol , uuidcol , varcharcol , varintcol ) 
 VALUES ( 0 , - 12 , ' abcdefg ' , 1234567890123456789 , 0x000102030405fffefd , true , 
 - 19952 . 11882 , 1 . 0 , - 2 . 1 , ' Voil á ! ' , 
 - ' 2012 - 05 - 14 12 : 53 : 20 + 0000 ' , bd1924e1 - 6af8 - 44ae - b5e1 - f24131dbd460 , ' " ' , 10000000000000000000000000 ) ; 
 + 19952 . 11882 , 1 . 0 , - 2 . 1 , 32767 , ' Voil á ! ' , 
 + ' 2012 - 05 - 14 12 : 53 : 20 + 0000 ' , 127 , bd1924e1 - 6af8 - 44ae - b5e1 - f24131dbd460 , ' " ' , 10000000000000000000000000 ) ; 
 
 INSERT INTO has _ all _ types ( num , intcol , asciicol , bigintcol , blobcol , booleancol , 
 - decimalcol , doublecol , floatcol , textcol , 
 - timestampcol , uuidcol , varcharcol , varintcol ) 
 + decimalcol , doublecol , floatcol , smallintcol , textcol , 
 + timestampcol , tinyintcol , uuidcol , varcharcol , varintcol ) 
 VALUES ( 1 , 2147483647 , ' _ _ ! ' ' $ # @ ! ~ " ' , 9223372036854775807 , 0xffffffffffffffffff , true , 
 - 0 . 00000000000001 , 9999999 . 999 , 99999 . 99 , ' ∭ Ƕ ⑮ ฑ ➳ ❏ ' ' ' , 
 - ' 1950 - 01 - 01 + 0000 ' , ffffffff - ffff - ffff - ffff - ffffffffffff , ' newline - > 
 + 0 . 00000000000001 , 9999999 . 999 , 99999 . 99 , 32767 , ' ∭ Ƕ ⑮ ฑ ➳ ❏ ' ' ' , 
 + ' 1950 - 01 - 01 + 0000 ' , 127 , ffffffff - ffff - ffff - ffff - ffffffffffff , ' newline - > 
 < - ' , 9 ) ; 
 
 INSERT INTO has _ all _ types ( num , intcol , asciicol , bigintcol , blobcol , booleancol , 
 - decimalcol , doublecol , floatcol , textcol , 
 - timestampcol , uuidcol , varcharcol , varintcol ) 
 + decimalcol , doublecol , floatcol , smallintcol , textcol , 
 + timestampcol , tinyintcol , uuidcol , varcharcol , varintcol ) 
 VALUES ( 2 , 0 , ' ' , 0 , 0x , false , 
 - 0 . 0 , 0 . 0 , 0 . 0 , ' ' , 
 - 0 , 00000000 - 0000 - 0000 - 0000 - 000000000000 , ' ' , 0 ) ; 
 + 0 . 0 , 0 . 0 , 0 . 0 , 0 , ' ' , 
 + 0 , 0 , 00000000 - 0000 - 0000 - 0000 - 000000000000 , ' ' , 0 ) ; 
 
 INSERT INTO has _ all _ types ( num , intcol , asciicol , bigintcol , blobcol , booleancol , 
 - decimalcol , doublecol , floatcol , textcol , 
 - timestampcol , uuidcol , varcharcol , varintcol ) 
 + decimalcol , doublecol , floatcol , smallintcol , textcol , 
 + timestampcol , tinyintcol , uuidcol , varcharcol , varintcol ) 
 VALUES ( 3 , - 2147483648 , ' ' ' ' ' ' ' ' , - 9223372036854775808 , 0x80 , false , 
 - 10 . 0000000000000 , - 1004 . 10 , 100000000 . 9 , ' 龍 馭 鬱 ' , 
 - ' 2038 - 01 - 19T03 : 14 - 1200 ' , ffffffff - ffff - 1fff - 8fff - ffffffffffff , 
 + 10 . 0000000000000 , - 1004 . 10 , 100000000 . 9 , 32767 , ' 龍 馭 鬱 ' , 
 + ' 2038 - 01 - 19T03 : 14 - 1200 ' , 127 , ffffffff - ffff - 1fff - 8fff - ffffffffffff , 
 	 ' ' ' ' , - 10000000000000000000000000 ) ; 
 
 INSERT INTO has _ all _ types ( num , intcol , asciicol , bigintcol , blobcol , booleancol , 
 - decimalcol , doublecol , floatcol , textcol , 
 - timestampcol , uuidcol , varcharcol , varintcol ) 
 + decimalcol , doublecol , floatcol , smallintcol , textcol , 
 + timestampcol , tinyintcol , uuidcol , varcharcol , varintcol ) 
 VALUES ( 4 , blobAsInt ( 0x ) , ' ' , blobAsBigint ( 0x ) , 0x , blobAsBoolean ( 0x ) , 
 - 	 blobAsDecimal ( 0x ) , blobAsDouble ( 0x ) , blobAsFloat ( 0x ) , ' ' , 
 - 	 blobAsTimestamp ( 0x ) , blobAsUuid ( 0x ) , ' ' , blobAsVarint ( 0x ) ) ; 
 + 	 blobAsDecimal ( 0x ) , blobAsDouble ( 0x ) , blobAsFloat ( 0x ) , blobAsSmallInt ( 0x0000 ) , ' ' , 
 + 	 blobAsTimestamp ( 0x ) , blobAsTinyInt ( 0x00 ) , blobAsUuid ( 0x ) , ' ' , blobAsVarint ( 0x ) ) ; 
 
 


NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 0959ac9 . . e309eae 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 5 , 6 + 5 , 8 @ @ 
 * Improve asynchronous hint delivery ( CASSANDRA - 5179 ) 
 * Fix Guava dependency version ( 12 . 0 - > 13 . 0 . 1 ) for Maven ( CASSANDRA - 5364 ) 
 * Validate that provided CQL3 collection value are < 64K ( CASSANDRA - 5355 ) 
 + * Change Kernel Page Cache skipping into row preheating ( disabled by default ) 
 + ( CASSANDRA - 4937 ) 
 
 
 1 . 2 . 3 
 diff - - git a / conf / cassandra . yaml b / conf / cassandra . yaml 
 index 0a8102d . . 178487d 100644 
 - - - a / conf / cassandra . yaml 
 + + + b / conf / cassandra . yaml 
 @ @ - 684 , 3 + 684 , 9 @ @ internode _ compression : all 
 # reducing overhead from the TCP protocol itself , at the cost of increasing 
 # latency if you block for cross - datacenter responses . 
 inter _ dc _ tcp _ nodelay : true 
 + 
 + # Enable or disable kernel page cache preheating from contents of the key cache after compaction . 
 + # When enabled it would preheat only first " page " ( 4KB ) of each row to optimize 
 + # for sequential access . Note : This could be harmful for fat rows , see CASSANDRA - 4937 
 + # for further details on that topic . 
 + preheat _ kernel _ page _ cache : false 
 diff - - git a / src / java / org / apache / cassandra / config / Config . java b / src / java / org / apache / cassandra / config / Config . java 
 index 02324ee . . 212147a 100644 
 - - - a / src / java / org / apache / cassandra / config / Config . java 
 + + + b / src / java / org / apache / cassandra / config / Config . java 
 @ @ - 167 , 6 + 167 , 8 @ @ public class Config 
 
 public boolean inter _ dc _ tcp _ nodelay = true ; 
 
 + public boolean preheat _ kernel _ page _ cache = false ; 
 + 
 private static boolean loadYaml = true ; 
 private static boolean outboundBindAny = false ; 
 
 diff - - git a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 index 2d3cbb5 . . 5fcef07 100644 
 - - - a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 + + + b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 @ @ - 1275 , 4 + 1275 , 9 @ @ public class DatabaseDescriptor 
 { 
 return conf . inter _ dc _ tcp _ nodelay ; 
 } 
 + 
 + public static boolean shouldPreheatPageCache ( ) 
 + { 
 + return conf . preheat _ kernel _ page _ cache ; 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java 
 index 2728970 . . 8dcdaad 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java 
 @ @ - 121 , 7 + 121 , 7 @ @ public class CommitLogReplayer 
 CommitLogDescriptor desc = CommitLogDescriptor . fromFileName ( file . getName ( ) ) ; 
 final long segment = desc . id ; 
 int version = desc . getMessagingVersion ( ) ; 
 - RandomAccessReader reader = RandomAccessReader . open ( new File ( file . getAbsolutePath ( ) ) , true ) ; 
 + RandomAccessReader reader = RandomAccessReader . open ( new File ( file . getAbsolutePath ( ) ) ) ; 
 try 
 { 
 assert reader . length ( ) < = Integer . MAX _ VALUE ; 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / AbstractCompactionStrategy . java b / src / java / org / apache / cassandra / db / compaction / AbstractCompactionStrategy . java 
 index cb15109 . . 85b09c1 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / AbstractCompactionStrategy . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / AbstractCompactionStrategy . java 
 @ @ - 152 , 7 + 152 , 7 @ @ public abstract class AbstractCompactionStrategy 
 { 
 ArrayList < ICompactionScanner > scanners = new ArrayList < ICompactionScanner > ( ) ; 
 for ( SSTableReader sstable : sstables ) 
 - scanners . add ( sstable . getDirectScanner ( range ) ) ; 
 + scanners . add ( sstable . getScanner ( range ) ) ; 
 return scanners ; 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 index 14e1b13 . . 0fe3a7a 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / CompactionManager . java 
 @ @ - 568 , 7 + 568 , 7 @ @ public class CompactionManager implements CompactionManagerMBean 
 if ( compactionFileLocation = = null ) 
 throw new IOException ( " disk full " ) ; 
 
 - SSTableScanner scanner = sstable . getDirectScanner ( ) ; 
 + SSTableScanner scanner = sstable . getScanner ( ) ; 
 long rowsRead = 0 ; 
 List < IColumn > indexedColumnsInRow = null ; 
 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionTask . java b / src / java / org / apache / cassandra / db / compaction / CompactionTask . java 
 index 75ea1cb . . 8c2af4d 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / CompactionTask . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / CompactionTask . java 
 @ @ - 231 , 10 + 231 , 7 @ @ public class CompactionTask extends AbstractCompactionTask 
 cfs . replaceCompactedSSTables ( toCompact , sstables , compactionType ) ; 
 / / TODO : this doesn ' t belong here , it should be part of the reader to load when the tracker is wired up 
 for ( SSTableReader sstable : sstables ) 
 - { 
 - for ( Map . Entry < DecoratedKey , RowIndexEntry > entry : cachedKeyMap . get ( sstable . descriptor ) . entrySet ( ) ) 
 - sstable . cacheKey ( entry . getKey ( ) , entry . getValue ( ) ) ; 
 - } 
 + sstable . preheat ( cachedKeyMap . get ( sstable . descriptor ) ) ; 
 
 if ( logger . isInfoEnabled ( ) ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java 
 index 6a1bf4b . . d916c48 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java 
 @ @ - 179 , 7 + 179 , 7 @ @ public class LeveledCompactionStrategy extends AbstractCompactionStrategy implem 
 { 
 / / L0 makes no guarantees about overlapping - ness . Just create a direct scanner for each 
 for ( SSTableReader sstable : byLevel . get ( level ) ) 
 - scanners . add ( sstable . getDirectScanner ( range ) ) ; 
 + scanners . add ( sstable . getScanner ( range ) ) ; 
 } 
 else 
 { 
 @ @ - 209 , 7 + 209 , 7 @ @ public class LeveledCompactionStrategy extends AbstractCompactionStrategy implem 
 this . sstables = new ArrayList < SSTableReader > ( sstables ) ; 
 Collections . sort ( this . sstables , SSTable . sstableComparator ) ; 
 sstableIterator = this . sstables . iterator ( ) ; 
 - currentScanner = sstableIterator . next ( ) . getDirectScanner ( range ) ; 
 + currentScanner = sstableIterator . next ( ) . getScanner ( range ) ; 
 
 long length = 0 ; 
 for ( SSTableReader sstable : sstables ) 
 @ @ - 234 , 7 + 234 , 7 @ @ public class LeveledCompactionStrategy extends AbstractCompactionStrategy implem 
 currentScanner = null ; 
 return endOfData ( ) ; 
 } 
 - currentScanner = sstableIterator . next ( ) . getDirectScanner ( range ) ; 
 + currentScanner = sstableIterator . next ( ) . getScanner ( range ) ; 
 } 
 } 
 catch ( IOException e ) 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / Scrubber . java b / src / java / org / apache / cassandra / db / compaction / Scrubber . java 
 index 0601857 . . 30929cc 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / Scrubber . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / Scrubber . java 
 @ @ - 94 , 8 + 94 , 8 @ @ public class Scrubber implements Closeable 
 / / we ' ll also loop through the index at the same time , using the position from the index to recover if the 
 / / row header ( key or data size ) is corrupt . ( This means our position in the index file will be one row 
 / / " ahead " of the data file . ) 
 - this . dataFile = sstable . openDataReader ( true ) ; 
 - this . indexFile = RandomAccessReader . open ( new File ( sstable . descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) , true ) ; 
 + this . dataFile = sstable . openDataReader ( ) ; 
 + this . indexFile = RandomAccessReader . open ( new File ( sstable . descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) ) ; 
 this . scrubInfo = new ScrubInfo ( dataFile , sstable ) ; 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java 
 index bbd2466 . . aa686c2 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java 
 + + + b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java 
 @ @ - 42 , 7 + 42 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 { 
 try 
 { 
 - return new CompressedRandomAccessReader ( path , metadata , false , owner ) ; 
 + return new CompressedRandomAccessReader ( path , metadata , owner ) ; 
 } 
 catch ( FileNotFoundException e ) 
 { 
 @ @ - 50 , 11 + 50 , 11 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 } 
 } 
 
 - public static CompressedRandomAccessReader open ( String dataFilePath , CompressionMetadata metadata , boolean skipIOCache ) 
 + public static CompressedRandomAccessReader open ( String dataFilePath , CompressionMetadata metadata ) 
 { 
 try 
 { 
 - return new CompressedRandomAccessReader ( dataFilePath , metadata , skipIOCache , null ) ; 
 + return new CompressedRandomAccessReader ( dataFilePath , metadata , null ) ; 
 } 
 catch ( FileNotFoundException e ) 
 { 
 @ @ - 73 , 9 + 73 , 9 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 / / raw checksum bytes 
 private final ByteBuffer checksumBytes = ByteBuffer . wrap ( new byte [ 4 ] ) ; 
 
 - private CompressedRandomAccessReader ( String dataFilePath , CompressionMetadata metadata , boolean skipIOCache , PoolingSegmentedFile owner ) throws FileNotFoundException 
 + private CompressedRandomAccessReader ( String dataFilePath , CompressionMetadata metadata , PoolingSegmentedFile owner ) throws FileNotFoundException 
 { 
 - super ( new File ( dataFilePath ) , metadata . chunkLength ( ) , skipIOCache , owner ) ; 
 + super ( new File ( dataFilePath ) , metadata . chunkLength ( ) , owner ) ; 
 this . metadata = metadata ; 
 compressed = ByteBuffer . wrap ( new byte [ metadata . compressor ( ) . initialCompressedBufferLength ( metadata . chunkLength ( ) ) ] ) ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 index e581b22 . . 9fe1309 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 @ @ - 38 , 7 + 38 , 7 @ @ public class KeyIterator extends AbstractIterator < DecoratedKey > implements Close 
 { 
 this . desc = desc ; 
 File path = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; 
 - in = RandomAccessReader . open ( path , true ) ; 
 + in = RandomAccessReader . open ( path ) ; 
 } 
 
 protected DecoratedKey computeNext ( ) 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableBoundedScanner . java b / src / java / org / apache / cassandra / io / sstable / SSTableBoundedScanner . java 
 index a571901 . . d5bec82 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableBoundedScanner . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableBoundedScanner . java 
 @ @ - 35 , 9 + 35 , 9 @ @ public class SSTableBoundedScanner extends SSTableScanner 
 private final Iterator < Pair < Long , Long > > rangeIterator ; 
 private Pair < Long , Long > currentRange ; 
 
 - SSTableBoundedScanner ( SSTableReader sstable , boolean skipCache , Iterator < Pair < Long , Long > > rangeIterator ) 
 + SSTableBoundedScanner ( SSTableReader sstable , Iterator < Pair < Long , Long > > rangeIterator ) 
 { 
 - super ( sstable , skipCache ) ; 
 + super ( sstable ) ; 
 this . rangeIterator = rangeIterator ; 
 assert rangeIterator . hasNext ( ) ; / / use EmptyCompactionScanner otherwise 
 currentRange = rangeIterator . next ( ) ; 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 index 51f2344 . . dbc45d8 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 @ @ - 352 , 7 + 352 , 7 @ @ public class SSTableReader extends SSTable 
 : SegmentedFile . getBuilder ( DatabaseDescriptor . getDiskAccessMode ( ) ) ; 
 
 / / we read the positions in a BRAF so we don ' t have to worry about an entry spanning a mmap boundary . 
 - RandomAccessReader primaryIndex = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) , true ) ; 
 + RandomAccessReader primaryIndex = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) ) ; 
 
 / / try to load summaries from the disk and check if we need 
 / / to read primary index because we should re - create a BloomFilter or pre - load KeyCache 
 @ @ - 698 , 6 + 698 , 29 @ @ public class SSTableReader extends SSTable 
 keyCache . put ( cacheKey , info ) ; 
 } 
 
 + public void preheat ( Map < DecoratedKey , RowIndexEntry > cachedKeys ) throws IOException 
 + { 
 + RandomAccessFile f = new RandomAccessFile ( getFilename ( ) , " r " ) ; 
 + 
 + try 
 + { 
 + int fd = CLibrary . getfd ( f . getFD ( ) ) ; 
 + 
 + for ( Map . Entry < DecoratedKey , RowIndexEntry > entry : cachedKeys . entrySet ( ) ) 
 + { 
 + cacheKey ( entry . getKey ( ) , entry . getValue ( ) ) ; 
 + 
 + / / add to the cache but don ' t do actual preheating if we have it disabled in the config 
 + if ( DatabaseDescriptor . shouldPreheatPageCache ( ) & & fd > 0 ) 
 + CLibrary . preheatPage ( fd , entry . getValue ( ) . position ) ; 
 + } 
 + } 
 + finally 
 + { 
 + FileUtils . closeQuietly ( f ) ; 
 + } 
 + } 
 + 
 public RowIndexEntry getCachedPosition ( DecoratedKey key , boolean updateStats ) 
 { 
 return getCachedPosition ( new KeyCacheKey ( descriptor , key . key ) , updateStats ) ; 
 @ @ - 897 , 6 + 920 , 15 @ @ public class SSTableReader extends SSTable 
 { 
 if ( references . decrementAndGet ( ) = = 0 & & isCompacted . get ( ) ) 
 { 
 + / * * 
 + * Make OS a favour and suggest ( using fadvice call ) that we 
 + * don ' t want to see pages of this SSTable in memory anymore . 
 + * 
 + * NOTE : We can ' t use madvice in java because it requires address of 
 + * the mapping , so instead we always open a file and run fadvice ( fd , 0 , 0 ) on it 
 + * / 
 + dropPageCache ( ) ; 
 + 
 / / Force finalizing mmapping if necessary 
 ifile . cleanup ( ) ; 
 dfile . cleanup ( ) ; 
 @ @ - 948 , 12 + 980 , 12 @ @ public class SSTableReader extends SSTable 
 } 
 
 / * * 
 - * Direct I / O SSTableScanner 
 + * I / O SSTableScanner 
 * @ return A Scanner for seeking over the rows of the SSTable . 
 * / 
 - public SSTableScanner getDirectScanner ( ) 
 + public SSTableScanner getScanner ( ) 
 { 
 - return new SSTableScanner ( this , true ) ; 
 + return new SSTableScanner ( this ) ; 
 } 
 
 / * * 
 @ @ - 962 , 14 + 994 , 14 @ @ public class SSTableReader extends SSTable 
 * @ param range the range of keys to cover 
 * @ return A Scanner for seeking over the rows of the SSTable . 
 * / 
 - public ICompactionScanner getDirectScanner ( Range < Token > range ) 
 + public ICompactionScanner getScanner ( Range < Token > range ) 
 { 
 if ( range = = null ) 
 - return getDirectScanner ( ) ; 
 + return getScanner ( ) ; 
 
 Iterator < Pair < Long , Long > > rangeIterator = getPositionsForRanges ( Collections . singletonList ( range ) ) . iterator ( ) ; 
 return rangeIterator . hasNext ( ) 
 - ? new SSTableBoundedScanner ( this , true , rangeIterator ) 
 + ? new SSTableBoundedScanner ( this , rangeIterator ) 
 : new EmptyCompactionScanner ( getFilename ( ) ) ; 
 } 
 
 @ @ - 1122 , 16 + 1154 , 16 @ @ public class SSTableReader extends SSTable 
 return sstableMetadata . ancestors ; 
 } 
 
 - public RandomAccessReader openDataReader ( boolean skipIOCache ) 
 + public RandomAccessReader openDataReader ( ) 
 { 
 return compression 
 - ? CompressedRandomAccessReader . open ( getFilename ( ) , getCompressionMetadata ( ) , skipIOCache ) 
 - : RandomAccessReader . open ( new File ( getFilename ( ) ) , skipIOCache ) ; 
 + ? CompressedRandomAccessReader . open ( getFilename ( ) , getCompressionMetadata ( ) ) 
 + : RandomAccessReader . open ( new File ( getFilename ( ) ) ) ; 
 } 
 
 - public RandomAccessReader openIndexReader ( boolean skipIOCache ) 
 + public RandomAccessReader openIndexReader ( ) 
 { 
 - return RandomAccessReader . open ( new File ( getIndexFilename ( ) ) , skipIOCache ) ; 
 + return RandomAccessReader . open ( new File ( getIndexFilename ( ) ) ) ; 
 } 
 
 / * * 
 @ @ - 1222 , 4 + 1254 , 38 @ @ public class SSTableReader extends SSTable 
 throw new UnsupportedOperationException ( ) ; 
 } 
 } 
 + 
 + private void dropPageCache ( ) 
 + { 
 + dropPageCache ( dfile . path ) ; 
 + dropPageCache ( ifile . path ) ; 
 + } 
 + 
 + private void dropPageCache ( String filePath ) 
 + { 
 + RandomAccessFile file = null ; 
 + 
 + try 
 + { 
 + file = new RandomAccessFile ( filePath , " r " ) ; 
 + 
 + int fd = CLibrary . getfd ( file . getFD ( ) ) ; 
 + 
 + if ( fd > 0 ) 
 + { 
 + if ( logger . isDebugEnabled ( ) ) 
 + logger . debug ( String . format ( " Dropping page cache of file % s . " , filePath ) ) ; 
 + 
 + CLibrary . trySkipCache ( fd , 0 , 0 ) ; 
 + } 
 + } 
 + catch ( IOException e ) 
 + { 
 + / / we don ' t care if cache cleanup fails 
 + } 
 + finally 
 + { 
 + FileUtils . closeQuietly ( file ) ; 
 + } 
 + } 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableScanner . java b / src / java / org / apache / cassandra / io / sstable / SSTableScanner . java 
 index 22ac485 . . 949acda 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableScanner . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableScanner . java 
 @ @ - 46 , 10 + 46 , 10 @ @ public class SSTableScanner implements ICompactionScanner 
 / * * 
 * @ param sstable SSTable to scan . 
 * / 
 - SSTableScanner ( SSTableReader sstable , boolean skipCache ) 
 + SSTableScanner ( SSTableReader sstable ) 
 { 
 - this . dfile = sstable . openDataReader ( skipCache ) ; 
 - this . ifile = sstable . openIndexReader ( skipCache ) ; 
 + this . dfile = sstable . openDataReader ( ) ; 
 + this . ifile = sstable . openIndexReader ( ) ; 
 this . sstable = sstable ; 
 this . filter = null ; 
 } 
 @ @ - 60 , 8 + 60 , 8 @ @ public class SSTableScanner implements ICompactionScanner 
 * / 
 SSTableScanner ( SSTableReader sstable , QueryFilter filter ) 
 { 
 - this . dfile = sstable . openDataReader ( false ) ; 
 - this . ifile = sstable . openIndexReader ( false ) ; 
 + this . dfile = sstable . openDataReader ( ) ; 
 + this . ifile = sstable . openIndexReader ( ) ; 
 this . sstable = sstable ; 
 this . filter = filter ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / util / RandomAccessReader . java b / src / java / org / apache / cassandra / io / util / RandomAccessReader . java 
 index 3210372 . . 4d7bfbb 100644 
 - - - a / src / java / org / apache / cassandra / io / util / RandomAccessReader . java 
 + + + b / src / java / org / apache / cassandra / io / util / RandomAccessReader . java 
 @ @ - 50 , 19 + 50 , 11 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu 
 / / channel liked with the file , used to retrieve data and force updates . 
 protected final FileChannel channel ; 
 
 - private final boolean skipIOCache ; 
 - 
 - / / file descriptor 
 - private final int fd ; 
 - 
 - / / used if skip I / O cache was enabled 
 - private long bytesSinceCacheFlush = 0 ; 
 - 
 private final long fileLength ; 
 
 protected final PoolingSegmentedFile owner ; 
 
 - protected RandomAccessReader ( File file , int bufferSize , boolean skipIOCache , PoolingSegmentedFile owner ) throws FileNotFoundException 
 + protected RandomAccessReader ( File file , int bufferSize , PoolingSegmentedFile owner ) throws FileNotFoundException 
 { 
 super ( file , " r " ) ; 
 
 @ @ - 74 , 18 + 66 , 8 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu 
 / / allocating required size of the buffer 
 if ( bufferSize < = 0 ) 
 throw new IllegalArgumentException ( " bufferSize must be positive " ) ; 
 - buffer = new byte [ bufferSize ] ; 
 
 - this . skipIOCache = skipIOCache ; 
 - try 
 - { 
 - fd = CLibrary . getfd ( getFD ( ) ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - / / fd = = null , Not Supposed To Happen 
 - throw new RuntimeException ( e ) ; 
 - } 
 + buffer = new byte [ bufferSize ] ; 
 
 / / we can cache file length in read - only mode 
 try 
 @ @ - 99 , 27 + 81 , 22 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu 
 validBufferBytes = - 1 ; / / that will trigger reBuffer ( ) on demand by read / seek operations 
 } 
 
 - public static RandomAccessReader open ( File file ) 
 - { 
 - return open ( file , false ) ; 
 - } 
 - 
 public static RandomAccessReader open ( File file , PoolingSegmentedFile owner ) 
 { 
 - return open ( file , DEFAULT _ BUFFER _ SIZE , false , owner ) ; 
 + return open ( file , DEFAULT _ BUFFER _ SIZE , owner ) ; 
 } 
 
 - public static RandomAccessReader open ( File file , boolean skipIOCache ) 
 + public static RandomAccessReader open ( File file ) 
 { 
 - return open ( file , DEFAULT _ BUFFER _ SIZE , skipIOCache , null ) ; 
 + return open ( file , DEFAULT _ BUFFER _ SIZE , null ) ; 
 } 
 
 @ VisibleForTesting 
 - static RandomAccessReader open ( File file , int bufferSize , boolean skipIOCache , PoolingSegmentedFile owner ) 
 + static RandomAccessReader open ( File file , int bufferSize , PoolingSegmentedFile owner ) 
 { 
 try 
 { 
 - return new RandomAccessReader ( file , bufferSize , skipIOCache , owner ) ; 
 + return new RandomAccessReader ( file , bufferSize , owner ) ; 
 } 
 catch ( FileNotFoundException e ) 
 { 
 @ @ - 130 , 7 + 107 , 7 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu 
 @ VisibleForTesting 
 static RandomAccessReader open ( SequentialWriter writer ) 
 { 
 - return open ( new File ( writer . getPath ( ) ) , DEFAULT _ BUFFER _ SIZE , false , null ) ; 
 + return open ( new File ( writer . getPath ( ) ) , DEFAULT _ BUFFER _ SIZE , null ) ; 
 } 
 
 / * * 
 @ @ - 158 , 21 + 135 , 11 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu 
 } 
 
 validBufferBytes = read ; 
 - bytesSinceCacheFlush + = read ; 
 } 
 catch ( IOException e ) 
 { 
 throw new FSReadError ( e , filePath ) ; 
 } 
 - 
 - if ( skipIOCache & & bytesSinceCacheFlush > = CACHE _ FLUSH _ INTERVAL _ IN _ BYTES ) 
 - { 
 - / / with random I / O we can ' t control what we are skipping so 
 - / / it will be more appropriate to just skip a whole file after 
 - / / we reach threshold 
 - CLibrary . trySkipCache ( this . fd , 0 , 0 ) ; 
 - bytesSinceCacheFlush = 0 ; 
 - } 
 } 
 
 @ Override 
 @ @ - 264 , 9 + 231 , 6 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu 
 { 
 buffer = null ; / / makes sure we don ' t use this after it ' s ostensibly closed 
 
 - if ( skipIOCache & & bytesSinceCacheFlush > 0 ) 
 - CLibrary . trySkipCache ( fd , 0 , 0 ) ; 
 - 
 try 
 { 
 super . close ( ) ; 
 @ @ - 280 , 7 + 244 , 7 @ @ public class RandomAccessReader extends RandomAccessFile implements FileDataInpu 
 @ Override 
 public String toString ( ) 
 { 
 - return getClass ( ) . getSimpleName ( ) + " ( " + " filePath = ' " + filePath + " ' " + " , skipIOCache = " + skipIOCache + " ) " ; 
 + return getClass ( ) . getSimpleName ( ) + " ( " + " filePath = ' " + filePath + " ' ) " ; 
 } 
 
 / * * 
 diff - - git a / src / java / org / apache / cassandra / streaming / FileStreamTask . java b / src / java / org / apache / cassandra / streaming / FileStreamTask . java 
 index 67d5c35 . . 979b2e1 100644 
 - - - a / src / java / org / apache / cassandra / streaming / FileStreamTask . java 
 + + + b / src / java / org / apache / cassandra / streaming / FileStreamTask . java 
 @ @ - 139 , 7 + 139 , 7 @ @ public class FileStreamTask extends WrappedRunnable 
 return ; 
 
 / / try to skip kernel page cache if possible 
 - RandomAccessReader file = RandomAccessReader . open ( new File ( header . file . getFilename ( ) ) , true ) ; 
 + RandomAccessReader file = RandomAccessReader . open ( new File ( header . file . getFilename ( ) ) ) ; 
 
 / / setting up data compression stream 
 compressedoutput = new LZFOutputStream ( output ) ; 
 diff - - git a / src / java / org / apache / cassandra / streaming / compress / CompressedFileStreamTask . java b / src / java / org / apache / cassandra / streaming / compress / CompressedFileStreamTask . java 
 index dda9d7d . . c1818ed 100644 
 - - - a / src / java / org / apache / cassandra / streaming / compress / CompressedFileStreamTask . java 
 + + + b / src / java / org / apache / cassandra / streaming / compress / CompressedFileStreamTask . java 
 @ @ - 65 , 7 + 65 , 7 @ @ public class CompressedFileStreamTask extends FileStreamTask 
 ByteBuffer headerBuffer = MessagingService . instance ( ) . constructStreamHeader ( header , false , MessagingService . instance ( ) . getVersion ( to ) ) ; 
 socket . getOutputStream ( ) . write ( ByteBufferUtil . getArray ( headerBuffer ) ) ; 
 
 - RandomAccessReader file = RandomAccessReader . open ( new File ( header . file . getFilename ( ) ) , true ) ; 
 + RandomAccessReader file = RandomAccessReader . open ( new File ( header . file . getFilename ( ) ) ) ; 
 FileChannel fc = file . getChannel ( ) ; 
 
 StreamingMetrics . activeStreamsOutbound . inc ( ) ; 
 diff - - git a / src / java / org / apache / cassandra / tools / SSTableExport . java b / src / java / org / apache / cassandra / tools / SSTableExport . java 
 index 51cdc72 . . 2e117ef 100644 
 - - - a / src / java / org / apache / cassandra / tools / SSTableExport . java 
 + + + b / src / java / org / apache / cassandra / tools / SSTableExport . java 
 @ @ - 349 , 7 + 349 , 7 @ @ public class SSTableExport 
 public static void export ( Descriptor desc , PrintStream outs , Collection < String > toExport , String [ ] excludes ) throws IOException 
 { 
 SSTableReader reader = SSTableReader . open ( desc ) ; 
 - SSTableScanner scanner = reader . getDirectScanner ( ) ; 
 + SSTableScanner scanner = reader . getScanner ( ) ; 
 
 IPartitioner < ? > partitioner = reader . partitioner ; 
 
 @ @ - 406 , 7 + 406 , 7 @ @ public class SSTableExport 
 
 
 SSTableIdentityIterator row ; 
 - SSTableScanner scanner = reader . getDirectScanner ( ) ; 
 + SSTableScanner scanner = reader . getScanner ( ) ; 
 
 outs . println ( " [ " ) ; 
 
 diff - - git a / src / java / org / apache / cassandra / utils / CLibrary . java b / src / java / org / apache / cassandra / utils / CLibrary . java 
 index f8ad9d1 . . 2f6e088 100644 
 - - - a / src / java / org / apache / cassandra / utils / CLibrary . java 
 + + + b / src / java / org / apache / cassandra / utils / CLibrary . java 
 @ @ - 326 , 4 + 326 , 25 @ @ public final class CLibrary 
 
 return - 1 ; 
 } 
 + 
 + / * * 
 + * Suggest kernel to preheat one page for the given file . 
 + * 
 + * @ param fd The file descriptor of file to preheat . 
 + * @ param position The offset of the block . 
 + * 
 + * @ return On success , zero is returned . On error , an error number is returned . 
 + * / 
 + public static int preheatPage ( int fd , long position ) 
 + { 
 + try 
 + { 
 + / / 4096 is good for SSD because they operate on " Pages " 4KB in size 
 + return posix _ fadvise ( fd , position , 4096 , POSIX _ FADV _ WILLNEED ) ; 
 + } 
 + catch ( UnsatisfiedLinkError e ) 
 + { 
 + return - 1 ; 
 + } 
 + } 
 } 
 diff - - git a / test / unit / org / apache / cassandra / db / KeyCacheTest . java b / test / unit / org / apache / cassandra / db / KeyCacheTest . java 
 index 93f1fea . . b05a607 100644 
 - - - a / test / unit / org / apache / cassandra / db / KeyCacheTest . java 
 + + + b / test / unit / org / apache / cassandra / db / KeyCacheTest . java 
 @ @ - 130 , 13 + 130 , 13 @ @ public class KeyCacheTest extends SchemaLoader 
 false , 
 10 ) ) ; 
 
 - assert CacheService . instance . keyCache . size ( ) = = 2 ; 
 + assertEquals ( 2 , CacheService . instance . keyCache . size ( ) ) ; 
 
 Util . compactAll ( cfs ) . get ( ) ; 
 keyCacheSize = CacheService . instance . keyCache . size ( ) ; 
 / / after compaction cache should have entries for 
 / / new SSTables , if we had 2 keys in cache previously it should become 4 
 - assert keyCacheSize = = 4 : keyCacheSize ; 
 + assertEquals ( 4 , keyCacheSize ) ; 
 
 / / re - read same keys to verify that key cache didn ' t grow further 
 cfs . getColumnFamily ( QueryFilter . getSliceFilter ( key1 , 
 diff - - git a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 index 830c3e1 . . 437b778 100644 
 - - - a / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 + + + b / test / unit / org / apache / cassandra / io / compress / CompressedRandomAccessReaderTest . java 
 @ @ - 75 , 7 + 75 , 7 @ @ public class CompressedRandomAccessReaderTest 
 
 assert f . exists ( ) ; 
 RandomAccessReader reader = compressed 
 - ? CompressedRandomAccessReader . open ( filename , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) , false ) 
 + ? CompressedRandomAccessReader . open ( filename , new CompressionMetadata ( filename + " . metadata " , f . length ( ) ) ) 
 : RandomAccessReader . open ( f ) ; 
 String expected = " The quick brown fox jumps over the lazy dog " ; 
 assertEquals ( expected . length ( ) , reader . length ( ) ) ; 
 @ @ - 115 , 7 + 115 , 7 @ @ public class CompressedRandomAccessReaderTest 
 CompressionMetadata meta = new CompressionMetadata ( metadata . getPath ( ) , file . length ( ) ) ; 
 CompressionMetadata . Chunk chunk = meta . chunkFor ( 0 ) ; 
 
 - RandomAccessReader reader = CompressedRandomAccessReader . open ( file . getPath ( ) , meta , false ) ; 
 + RandomAccessReader reader = CompressedRandomAccessReader . open ( file . getPath ( ) , meta ) ; 
 / / read and verify compressed data 
 assertEquals ( CONTENT , reader . readLine ( ) ) ; 
 / / close reader 
 @ @ - 142 , 7 + 142 , 7 @ @ public class CompressedRandomAccessReaderTest 
 checksumModifier . write ( random . nextInt ( ) ) ; 
 checksumModifier . getFD ( ) . sync ( ) ; / / making sure that change was synced with disk 
 
 - final RandomAccessReader r = CompressedRandomAccessReader . open ( file . getPath ( ) , meta , false ) ; 
 + final RandomAccessReader r = CompressedRandomAccessReader . open ( file . getPath ( ) , meta ) ; 
 
 Throwable exception = null ; 
 try 
 @ @ - 163 , 7 + 163 , 7 @ @ public class CompressedRandomAccessReaderTest 
 / / lets write original checksum and check if we can read data 
 updateChecksum ( checksumModifier , chunk . length , checksum ) ; 
 
 - reader = CompressedRandomAccessReader . open ( file . getPath ( ) , meta , false ) ; 
 + reader = CompressedRandomAccessReader . open ( file . getPath ( ) , meta ) ; 
 / / read and verify compressed data 
 assertEquals ( CONTENT , reader . readLine ( ) ) ; 
 / / close reader 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableTest . java 
 index 12f2747 . . e944ed2 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableTest . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableTest . java 
 @ @ - 56 , 7 + 56 , 7 @ @ public class SSTableTest extends SchemaLoader 
 
 private void verifySingle ( SSTableReader sstable , ByteBuffer bytes , ByteBuffer key ) throws IOException 
 { 
 - RandomAccessReader file = sstable . openDataReader ( false ) ; 
 + RandomAccessReader file = sstable . openDataReader ( ) ; 
 file . seek ( sstable . getPosition ( sstable . partitioner . decorateKey ( key ) , SSTableReader . Operator . EQ ) . position ) ; 
 assert key . equals ( ByteBufferUtil . readWithShortLength ( file ) ) ; 
 int size = ( int ) SSTableReader . readRowSize ( file , sstable . descriptor ) ; 
 @ @ - 98 , 7 + 98 , 7 @ @ public class SSTableTest extends SchemaLoader 
 { 
 List < ByteBuffer > keys = new ArrayList < ByteBuffer > ( map . keySet ( ) ) ; 
 / / Collections . shuffle ( keys ) ; 
 - RandomAccessReader file = sstable . openDataReader ( false ) ; 
 + RandomAccessReader file = sstable . openDataReader ( ) ; 
 for ( ByteBuffer key : keys ) 
 { 
 file . seek ( sstable . getPosition ( sstable . partitioner . decorateKey ( key ) , SSTableReader . Operator . EQ ) . position ) ; 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java b / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java 
 index 2b0a13a . . fcfaeec 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableUtils . java 
 @ @ - 72 , 8 + 72 , 8 @ @ public class SSTableUtils 
 
 public static void assertContentEquals ( SSTableReader lhs , SSTableReader rhs ) throws IOException 
 { 
 - SSTableScanner slhs = lhs . getDirectScanner ( ) ; 
 - SSTableScanner srhs = rhs . getDirectScanner ( ) ; 
 + SSTableScanner slhs = lhs . getScanner ( ) ; 
 + SSTableScanner srhs = rhs . getScanner ( ) ; 
 while ( slhs . hasNext ( ) ) 
 { 
 OnDiskAtomIterator ilhs = slhs . next ( ) ; 
 diff - - git a / test / unit / org / apache / cassandra / io / util / BufferedRandomAccessFileTest . java b / test / unit / org / apache / cassandra / io / util / BufferedRandomAccessFileTest . java 
 index 8059bbd . . 90c27e3 100644 
 - - - a / test / unit / org / apache / cassandra / io / util / BufferedRandomAccessFileTest . java 
 + + + b / test / unit / org / apache / cassandra / io / util / BufferedRandomAccessFileTest . java 
 @ @ - 338 , 7 + 338 , 7 @ @ public class BufferedRandomAccessFileTest 
 for ( final int offset : Arrays . asList ( 0 , 8 ) ) 
 { 
 File file1 = writeTemporaryFile ( new byte [ 16 ] ) ; 
 - final RandomAccessReader file = RandomAccessReader . open ( file1 , bufferSize , false , null ) ; 
 + final RandomAccessReader file = RandomAccessReader . open ( file1 , bufferSize , null ) ; 
 expectEOF ( new Callable < Object > ( ) 
 { 
 public Object call ( ) throws IOException 
 @ @ - 353 , 7 + 353 , 7 @ @ public class BufferedRandomAccessFileTest 
 for ( final int n : Arrays . asList ( 1 , 2 , 4 , 8 ) ) 
 { 
 File file1 = writeTemporaryFile ( new byte [ 16 ] ) ; 
 - final RandomAccessReader file = RandomAccessReader . open ( file1 , bufferSize , false , null ) ; 
 + final RandomAccessReader file = RandomAccessReader . open ( file1 , bufferSize , null ) ; 
 expectEOF ( new Callable < Object > ( ) 
 { 
 public Object call ( ) throws IOException
