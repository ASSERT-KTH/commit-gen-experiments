BLEU SCORE: 0.09688464563433238

TEST MSG: Add doc on compaction
GENERATED MSG: Add initial version of security section

TEST DIFF (one line): diff - - git a / doc / source / operations . rst b / doc / source / operations . rst <nl> index 9e79700 . . 9094766 100644 <nl> - - - a / doc / source / operations . rst <nl> + + + b / doc / source / operations . rst <nl> @ @ - 205 , 26 + 205 , 362 @ @ Hints <nl> <nl> . . todo : : todo <nl> <nl> + . . _ compaction : <nl> + <nl> Compaction <nl> - - - - - - - - - - <nl> <nl> - Size Tiered <nl> - ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + Types of compaction <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> <nl> - . . todo : : todo <nl> + The concept of compaction is used for different kinds of operations in Cassandra , the common thing about these <nl> + operations is that it takes one or more sstables and output new sstables . The types of compactions are ; <nl> + <nl> + Minor compaction <nl> + triggered automatically in Cassandra . <nl> + Major compaction <nl> + a user executes a compaction over all sstables on the node . <nl> + User defined compaction <nl> + a user triggers a compaction on a given set of sstables . <nl> + Scrub <nl> + try to fix any broken sstables . This can actually remove valid data if that data is corrupted , if that happens you <nl> + will need to run a full repair on the node . <nl> + Upgradesstables <nl> + upgrade sstables to the latest version . Run this after upgrading to a new major version . <nl> + Cleanup <nl> + remove any ranges this node does not own anymore , typically triggered on neighbouring nodes after a node has been <nl> + bootstrapped since that node will take ownership of some ranges from those nodes . <nl> + Secondary index rebuild <nl> + rebuild the secondary indexes on the node . <nl> + Anticompaction <nl> + after repair the ranges that were actually repaired are split out of the sstables that existed when repair started . <nl> + <nl> + When is a minor compaction triggered ? <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + # When an sstable is added to the node through flushing / streaming etc . <nl> + # When autocompaction is enabled after being disabled ( ` ` nodetool enableautocompaction ` ` ) <nl> + # When compaction adds new sstables . <nl> + # A check for new minor compactions every 5 minutes . <nl> + <nl> + Merging sstables <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> <nl> - Leveled <nl> - ^ ^ ^ ^ ^ ^ ^ <nl> + Compaction is about merging sstables , since partitions in sstables are sorted based on the hash of the partition key it <nl> + is possible to efficiently merge separate sstables . Content of each partition is also sorted so each partition can be <nl> + merged efficiently . <nl> <nl> - . . todo : : todo <nl> + Tombstones and gc _ grace <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> <nl> - TimeWindow <nl> - ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> - . . todo : : todo <nl> + When a delete is issued in Cassandra what happens is that a tombstone is written , this tombstone shadows the data it <nl> + deletes . This means that the tombstone can live in one sstable and the data it covers is in another sstable . To be able <nl> + to remove the actual data , a compaction where both the sstable containing the tombstone and the sstable containing the <nl> + data is included the same compaction is needed . <nl> + <nl> + ` ` gc _ grace _ seconds ` ` is the minimum time tombstones are kept around . If you generally run repair once a week , then <nl> + ` ` gc _ grace _ seconds ` ` needs to be at least 1 week ( you probably want some margin as well ) , otherwise you might drop a <nl> + tombstone that has not been propagated to all replicas and that could cause deleted data to become live again . <nl> + <nl> + To be able to drop an actual tombstone the following needs to be true ; <nl> + <nl> + - The tombstone must be older than ` ` gc _ grace _ seconds ` ` <nl> + - If partition X contains the tombstone , the sstable containing the partition plus all sstables containing data older <nl> + than the tombstone containing X must be included in the same compaction . We don ' t need to care if the partition is in <nl> + an sstable if we can guarantee that all data in that sstable is newer than the tombstone . If the tombstone is older <nl> + than the data it cannot shadow that data . <nl> + - If the option ` ` only _ purge _ repaired _ tombstones ` ` is enabled , tombstones are only removed if the data has also been <nl> + repaired . <nl> + <nl> + TTL <nl> + ^ ^ ^ <nl> + <nl> + Data in Cassandra can have an additional property called time to live - this is used to automatically drop data that has <nl> + expired once the time is reached . Once the TTL has expired the data is converted to a tombstone which stays around for <nl> + at least ` ` gc _ grace _ seconds ` ` . Note that if you mix data with TTL and data without TTL ( or just different length of the <nl> + TTL ) Cassandra will have a hard time dropping the tombstones created since the partition might span many sstables and <nl> + not all are compacted at once . <nl> + <nl> + Fully expired sstables <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + If an sstable contains only tombstones and it is guaranteed that that sstable is not shadowing data in any other sstable <nl> + compaction can drop that sstable . If you see sstables with only tombstones ( note that TTL : ed data is considered <nl> + tombstones once the time to live has expired ) but it is not being dropped by compaction , it is likely that other <nl> + sstables contain older data . There is a tool called ` ` sstableexpiredblockers ` ` that will list which sstables are <nl> + droppable and which are blocking them from being dropped . This is especially useful for time series compaction with <nl> + ` ` TimeWindowCompactionStrategy ` ` ( and the deprecated ` ` DateTieredCompactionStrategy ` ` ) . <nl> + <nl> + Repaired / unrepaired data <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + With incremental repairs Cassandra must keep track of what data is repaired and what data is unrepaired . With <nl> + anticompaction repaired data is split out into repaired and unrepaired sstables . To avoid mixing up the data again <nl> + separate compaction strategy instances are run on the two sets of data , each instance only knowing about either the <nl> + repaired or the unrepaired sstables . This means that if you only run incremental repair once and then never again , you <nl> + might have very old data in the repaired sstables that block compaction from dropping tombstones in the unrepaired <nl> + ( probably newer ) sstables . <nl> + <nl> + Data directories <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + Since tombstones and data can live in different sstables it is important to realize that losing an sstable might lead to <nl> + data becoming live again - the most common way of losing sstables is to have a hard drive break down . To avoid making <nl> + data live tombstones and actual data are always in the same data directory . This way , if a disk is lost , all versions of <nl> + a partition are lost and no data can get undeleted . To achieve this a compaction strategy instance per data directory is <nl> + run in addition to the compaction strategy instances containing repaired / unrepaired data , this means that if you have 4 <nl> + data directories there will be 8 compaction strategy instances running . This has a few more benefits than just avoiding <nl> + data getting undeleted : <nl> + <nl> + - It is possible to run more compactions in parallel - leveled compaction will have several totally separate levelings <nl> + and each one can run compactions independently from the others . <nl> + - Users can backup and restore a single data directory . <nl> + - Note though that currently all data directories are considered equal , so if you have a tiny disk and a big disk <nl> + backing two data directories , the big one will be limited the by the small one . One work around to this is to create <nl> + more data directories backed by the big disk . <nl> + <nl> + Single sstable tombstone compaction <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + When an sstable is written a histogram with the tombstone expiry times is created and this is used to try to find <nl> + sstables with very many tombstones and run single sstable compaction on that sstable in hope of being able to drop <nl> + tombstones in that sstable . Before starting this it is also checked how likely it is that any tombstones will actually <nl> + will be able to be dropped how much this sstable overlaps with other sstables . To avoid most of these checks the <nl> + compaction option ` ` unchecked _ tombstone _ compaction ` ` can be enabled . <nl> + <nl> + . . _ compaction - options : <nl> + <nl> + Common options <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + There is a number of common options for all the compaction strategies ; <nl> + <nl> + ` ` enabled ` ` ( default : true ) <nl> + Whether minor compactions should run . Note that you can have ' enabled ' : true as a compaction option and then do <nl> + ' nodetool enableautocompaction ' to start running compactions . <nl> + Default true . <nl> + ` ` tombstone _ threshold ` ` ( default : 0 . 2 ) <nl> + How much of the sstable should be tombstones for us to consider doing a single sstable compaction of that sstable . <nl> + ` ` tombstone _ compaction _ interval ` ` ( default : 86400s ( 1 day ) ) <nl> + Since it might not be possible to drop any tombstones when doing a single sstable compaction we need to make sure <nl> + that one sstable is not constantly getting recompacted - this option states how often we should try for a given <nl> + sstable . <nl> + ` ` log _ all ` ` ( default : false ) <nl> + New detailed compaction logging , see : ref : ` below < detailed - compaction - logging > ` . <nl> + ` ` unchecked _ tombstone _ compaction ` ` ( default : false ) <nl> + The single sstable compaction has quite strict checks for whether it should be started , this option disables those <nl> + checks and for some usecases this might be needed . Note that this does not change anything for the actual <nl> + compaction , tombstones are only dropped if it is safe to do so - it might just rewrite an sstable without being able <nl> + to drop any tombstones . <nl> + ` ` only _ purge _ repaired _ tombstone ` ` ( default : false ) <nl> + Option to enable the extra safety of making sure that tombstones are only dropped if the data has been repaired . <nl> + ` ` min _ threshold ` ` ( default : 4 ) <nl> + Lower limit of number of sstables before a compaction is triggered . Not used for ` ` LeveledCompactionStrategy ` ` . <nl> + ` ` max _ threshold ` ` ( default : 32 ) <nl> + Upper limit of number of sstables before a compaction is triggered . Not used for ` ` LeveledCompactionStrategy ` ` . <nl> + <nl> + Compaction nodetool commands <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + The : ref : ` nodetool < nodetool > ` utility provides a number of commands related to compaction : <nl> + <nl> + ` ` enableautocompaction ` ` <nl> + Enable compaction . <nl> + ` ` disableautocompaction ` ` <nl> + Disable compaction . <nl> + ` ` setcompactionthroughput ` ` <nl> + How fast compaction should run at most - defaults to 16MB / s , but note that it is likely not possible to reach this <nl> + throughput . <nl> + ` ` compactionstats ` ` <nl> + Statistics about current and pending compactions . <nl> + ` ` compactionhistory ` ` <nl> + List details about the last compactions . <nl> + ` ` setcompactionthreshold ` ` <nl> + Set the min / max sstable count for when to trigger compaction , defaults to 4 / 32 . <nl> + <nl> + Switching the compaction strategy and options using JMX <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + It is possible to switch compaction strategies and its options on just a single node using JMX , this is a great way to <nl> + experiment with settings without affecting the whole cluster . The mbean is : : <nl> + <nl> + org . apache . cassandra . db : type = ColumnFamilies , keyspace = < keyspace _ name > , columnfamily = < table _ name > <nl> + <nl> + and the attribute to change is ` ` CompactionParameters ` ` or ` ` CompactionParametersJson ` ` if you use jconsole or jmc . The <nl> + syntax for the json version is the same as you would use in an : ref : ` ALTER TABLE < alter - table - statement > ` statement - <nl> + for example : : <nl> + <nl> + { ' class ' : ' LeveledCompactionStrategy ' , ' sstable _ size _ in _ mb ' : 123 } <nl> + <nl> + The setting is kept until someone executes an : ref : ` ALTER TABLE < alter - table - statement > ` that touches the compaction <nl> + settings or restarts the node . <nl> + <nl> + . . _ detailed - compaction - logging : <nl> + <nl> + More detailed compaction logging <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + Enable with the compaction option ` ` log _ all ` ` and a more detailed compaction log file will be produced in your log <nl> + directory . <nl> + <nl> + Size Tiered Compaction Strategy <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + The basic idea of ` ` SizeTieredCompactionStrategy ` ` ( STCS ) is to merge sstables of approximately the same size . All <nl> + sstables are put in different buckets depending on their size . An sstable is added to the bucket if size of the sstable <nl> + is within ` ` bucket _ low ` ` and ` ` bucket _ high ` ` of the current average size of the sstables already in the bucket . This <nl> + will create several buckets and the most interesting of those buckets will be compacted . The most interesting one is <nl> + decided by figuring out which bucket ' s sstables takes the most reads . <nl> + <nl> + Major compaction <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + When running a major compaction with STCS you will end up with two sstables per data directory ( one for repaired data <nl> + and one for unrepaired data ) . There is also an option ( - s ) to do a major compaction that splits the output into several <nl> + sstables . The sizes of the sstables are approximately 50 % , 25 % , 12 . 5 % . . . of the total size . <nl> + <nl> + . . _ stcs - options : <nl> + <nl> + STCS options <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + ` ` min _ sstable _ size ` ` ( default : 50MB ) <nl> + Sstables smaller than this are put in the same bucket . <nl> + ` ` bucket _ low ` ` ( default : 0 . 5 ) <nl> + How much smaller than the average size of a bucket a sstable should be before not being included in the bucket . That <nl> + is , if ` ` bucket _ low * avg _ bucket _ size < sstable _ size ` ` ( and the ` ` bucket _ high ` ` condition holds , see below ) , then <nl> + the sstable is added to the bucket . <nl> + ` ` bucket _ high ` ` ( default : 1 . 5 ) <nl> + How much bigger than the average size of a bucket a sstable should be before not being included in the bucket . That <nl> + is , if ` ` sstable _ size < bucket _ high * avg _ bucket _ size ` ` ( and the ` ` bucket _ low ` ` condition holds , see above ) , then <nl> + the sstable is added to the bucket . <nl> + <nl> + Defragmentation <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + Defragmentation is done when many sstables are touched during a read . The result of the read is put in to the memtable <nl> + so that the next read will not have to touch as many sstables . This can cause writes on a read - only - cluster . <nl> + <nl> + Leveled Compaction Strategy <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + The idea of ` ` LeveledCompactionStrategy ` ` ( LCS ) is that all sstables are put into different levels where we guarantee <nl> + that no overlapping sstables are in the same level . By overlapping we mean that the first / last token of a single sstable <nl> + are never overlapping with other sstables . This means that for a SELECT we will only have to look for the partition key <nl> + in a single sstable per level . Each level is 10x the size of the previous one and each sstable is 160MB by default . L0 <nl> + is where sstables are streamed / flushed - no overlap guarantees are given here . <nl> + <nl> + When picking compaction candidates we have to make sure that the compaction does not create overlap in the target level . <nl> + This is done by always including all overlapping sstables in the next level . For example if we select an sstable in L3 , <nl> + we need to guarantee that we pick all overlapping sstables in L4 and make sure that no currently ongoing compactions <nl> + will create overlap if we start that compaction . We can start many parallel compactions in a level if we guarantee that <nl> + we wont create overlap . For L0 - > L1 compactions we almost always need to include all L1 sstables since most L0 sstables <nl> + cover the full range . We also can ' t compact all L0 sstables with all L1 sstables in a single compaction since that can <nl> + use too much memory . <nl> + <nl> + When deciding which level to compact LCS checks the higher levels first ( with LCS , a " higher " level is one with a higher <nl> + number , L0 being the lowest one ) and if the level is behind a compaction will be started in that level . <nl> + <nl> + Major compaction <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + It is possible to do a major compaction with LCS - it will currently start by filling out L1 and then once L1 is full , <nl> + it continues with L2 etc . This is sub optimal and will change to create all the sstables in a high level instead , <nl> + CASSANDRA - 11817 . <nl> + <nl> + Bootstrapping <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + During bootstrap sstables are streamed from other nodes . The level of the remote sstable is kept to avoid many <nl> + compactions after the bootstrap is done . During bootstrap the new node also takes writes while it is streaming the data <nl> + from a remote node - these writes are flushed to L0 like all other writes and to avoid those sstables blocking the <nl> + remote sstables from going to the correct level , we only do STCS in L0 until the bootstrap is done . <nl> + <nl> + STCS in L0 <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + If LCS gets very many L0 sstables reads are going to hit all ( or most ) of the L0 sstables since they are likely to be <nl> + overlapping . To more quickly remedy this LCS does STCS compactions in L0 if there are more than 32 sstables there . This <nl> + should improve read performance more quickly compared to letting LCS do its L0 - > L1 compactions . If you keep getting <nl> + too many sstables in L0 it is likely that LCS is not the best fit for your workload and STCS could work out better . <nl> + <nl> + Starved sstables <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + If a node ends up with a leveling where there are a few very high level sstables that are not getting compacted they <nl> + might make it impossible for lower levels to drop tombstones etc . For example , if there are sstables in L6 but there is <nl> + only enough data to actually get a L4 on the node the left over sstables in L6 will get starved and not compacted . This <nl> + can happen if a user changes sstable \ _ size \ _ in \ _ mb from 5MB to 160MB for example . To avoid this LCS tries to include <nl> + those starved high level sstables in other compactions if there has been 25 compaction rounds where the highest level <nl> + has not been involved . <nl> + <nl> + . . _ lcs - options : <nl> + <nl> + LCS options <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + ` ` sstable _ size _ in _ mb ` ` ( default : 160MB ) <nl> + The target compressed ( if using compression ) sstable size - the sstables can end up being larger if there are very <nl> + large partitions on the node . <nl> + <nl> + LCS also support the ` ` cassandra . disable _ stcs _ in _ l0 ` ` startup option ( ` ` - Dcassandra . disable _ stcs _ in _ l0 = true ` ` ) to avoid <nl> + doing STCS in L0 . <nl> + <nl> + . . _ twcs : <nl> + <nl> + Time Window CompactionStrategy <nl> + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> + <nl> + ` ` TimeWindowCompactionStrategy ` ` ( TWCS ) is designed specifically for workloads where it ' s beneficial to have data on <nl> + disk grouped by the timestamp of the data , a common goal when the workload is time - series in nature or when all data is <nl> + written with a TTL . In an expiring / TTL workload , the contents of an entire SSTable likely expire at approximately the <nl> + same time , allowing them to be dropped completely , and space reclaimed much more reliably than when using <nl> + ` ` SizeTieredCompactionStrategy ` ` or ` ` LeveledCompactionStrategy ` ` . The basic concept is that <nl> + ` ` TimeWindowCompactionStrategy ` ` will create 1 sstable per file for a given window , where a window is simply calculated <nl> + as the combination of two primary options : <nl> + <nl> + ` ` compaction _ window _ unit ` ` ( default : DAYS ) <nl> + A Java TimeUnit ( MINUTES , HOURS , or DAYS ) . <nl> + ` ` compaction _ window _ size ` ` ( default : 1 ) <nl> + The number of units that make up a window . <nl> + <nl> + Taken together , the operator can specify windows of virtually any size , and ` TimeWindowCompactionStrategy ` will work to <nl> + create a single sstable for writes within that window . For efficiency during writing , the newest window will be <nl> + compacted using ` SizeTieredCompactionStrategy ` . <nl> + <nl> + Ideally , operators should select a ` ` compaction _ window _ unit ` ` and ` ` compaction _ window _ size ` ` pair that produces <nl> + approximately 20 - 30 windows - if writing with a 90 day TTL , for example , a 3 Day window would be a reasonable choice <nl> + ( ` ` ' compaction _ window _ unit ' : ' DAYS ' , ' compaction _ window _ size ' : 3 ` ` ) . <nl> + <nl> + TimeWindowCompactionStrategy Operational Concerns <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + The primary motivation for TWCS is to separate data on disk by timestamp and to allow fully expired SSTables to drop <nl> + more efficiently . One potential way this optimal behavior can be subverted is if data is written to SSTables out of <nl> + order , with new data and old data in the same SSTable . Out of order data can appear in two ways : <nl> + <nl> + - If the user mixes old data and new data in the traditional write path , the data will be comingled in the memtables <nl> + and flushed into the same SSTable , where it will remain comingled . <nl> + - If the user ' s read requests for old data cause read repairs that pull old data into the current memtable , that data <nl> + will be comingled and flushed into the same SSTable . <nl> + <nl> + While TWCS tries to minimize the impact of comingled data , users should attempt to avoid this behavior . Specifically , <nl> + users should avoid queries that explicitly set the timestamp via CQL ` ` USING TIMESTAMP ` ` . Additionally , users should run <nl> + frequent repairs ( which streams data in such a way that it does not become comingled ) , and disable background read <nl> + repair by setting the table ' s ` ` read _ repair _ chance ` ` and ` ` dclocal _ read _ repair _ chance ` ` to 0 . <nl> + <nl> + Changing TimeWindowCompactionStrategy Options <nl> + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <nl> + <nl> + Operators wishing to enable ` ` TimeWindowCompactionStrategy ` ` on existing data should consider running a major compaction <nl> + first , placing all existing data into a single ( old ) window . Subsequent newer writes will then create typical SSTables <nl> + as expected . <nl> + <nl> + Operators wishing to change ` ` compaction _ window _ unit ` ` or ` ` compaction _ window _ size ` ` can do so , but may trigger <nl> + additional compactions as adjacent windows are joined together . If the window size is decrease d ( for example , from 24 <nl> + hours to 12 hours ) , then the existing SSTables will not be modified - TWCS can not split existing SSTables into multiple <nl> + windows . <nl> <nl> - DateTiered <nl> - ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ <nl> - . . todo : : todo <nl> <nl> Tombstones and Garbage Collection ( GC ) Grace <nl> - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
NEAREST DIFF (one line): ELIMINATEDSENTENCE

TEST DIFF:
diff - - git a / doc / source / operations . rst b / doc / source / operations . rst 
 index 9e79700 . . 9094766 100644 
 - - - a / doc / source / operations . rst 
 + + + b / doc / source / operations . rst 
 @ @ - 205 , 26 + 205 , 362 @ @ Hints 
 
 . . todo : : todo 
 
 + . . _ compaction : 
 + 
 Compaction 
 - - - - - - - - - - 
 
 - Size Tiered 
 - ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + Types of compaction 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 
 - . . todo : : todo 
 + The concept of compaction is used for different kinds of operations in Cassandra , the common thing about these 
 + operations is that it takes one or more sstables and output new sstables . The types of compactions are ; 
 + 
 + Minor compaction 
 + triggered automatically in Cassandra . 
 + Major compaction 
 + a user executes a compaction over all sstables on the node . 
 + User defined compaction 
 + a user triggers a compaction on a given set of sstables . 
 + Scrub 
 + try to fix any broken sstables . This can actually remove valid data if that data is corrupted , if that happens you 
 + will need to run a full repair on the node . 
 + Upgradesstables 
 + upgrade sstables to the latest version . Run this after upgrading to a new major version . 
 + Cleanup 
 + remove any ranges this node does not own anymore , typically triggered on neighbouring nodes after a node has been 
 + bootstrapped since that node will take ownership of some ranges from those nodes . 
 + Secondary index rebuild 
 + rebuild the secondary indexes on the node . 
 + Anticompaction 
 + after repair the ranges that were actually repaired are split out of the sstables that existed when repair started . 
 + 
 + When is a minor compaction triggered ? 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + # When an sstable is added to the node through flushing / streaming etc . 
 + # When autocompaction is enabled after being disabled ( ` ` nodetool enableautocompaction ` ` ) 
 + # When compaction adds new sstables . 
 + # A check for new minor compactions every 5 minutes . 
 + 
 + Merging sstables 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 
 - Leveled 
 - ^ ^ ^ ^ ^ ^ ^ 
 + Compaction is about merging sstables , since partitions in sstables are sorted based on the hash of the partition key it 
 + is possible to efficiently merge separate sstables . Content of each partition is also sorted so each partition can be 
 + merged efficiently . 
 
 - . . todo : : todo 
 + Tombstones and gc _ grace 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 
 - TimeWindow 
 - ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 - . . todo : : todo 
 + When a delete is issued in Cassandra what happens is that a tombstone is written , this tombstone shadows the data it 
 + deletes . This means that the tombstone can live in one sstable and the data it covers is in another sstable . To be able 
 + to remove the actual data , a compaction where both the sstable containing the tombstone and the sstable containing the 
 + data is included the same compaction is needed . 
 + 
 + ` ` gc _ grace _ seconds ` ` is the minimum time tombstones are kept around . If you generally run repair once a week , then 
 + ` ` gc _ grace _ seconds ` ` needs to be at least 1 week ( you probably want some margin as well ) , otherwise you might drop a 
 + tombstone that has not been propagated to all replicas and that could cause deleted data to become live again . 
 + 
 + To be able to drop an actual tombstone the following needs to be true ; 
 + 
 + - The tombstone must be older than ` ` gc _ grace _ seconds ` ` 
 + - If partition X contains the tombstone , the sstable containing the partition plus all sstables containing data older 
 + than the tombstone containing X must be included in the same compaction . We don ' t need to care if the partition is in 
 + an sstable if we can guarantee that all data in that sstable is newer than the tombstone . If the tombstone is older 
 + than the data it cannot shadow that data . 
 + - If the option ` ` only _ purge _ repaired _ tombstones ` ` is enabled , tombstones are only removed if the data has also been 
 + repaired . 
 + 
 + TTL 
 + ^ ^ ^ 
 + 
 + Data in Cassandra can have an additional property called time to live - this is used to automatically drop data that has 
 + expired once the time is reached . Once the TTL has expired the data is converted to a tombstone which stays around for 
 + at least ` ` gc _ grace _ seconds ` ` . Note that if you mix data with TTL and data without TTL ( or just different length of the 
 + TTL ) Cassandra will have a hard time dropping the tombstones created since the partition might span many sstables and 
 + not all are compacted at once . 
 + 
 + Fully expired sstables 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + If an sstable contains only tombstones and it is guaranteed that that sstable is not shadowing data in any other sstable 
 + compaction can drop that sstable . If you see sstables with only tombstones ( note that TTL : ed data is considered 
 + tombstones once the time to live has expired ) but it is not being dropped by compaction , it is likely that other 
 + sstables contain older data . There is a tool called ` ` sstableexpiredblockers ` ` that will list which sstables are 
 + droppable and which are blocking them from being dropped . This is especially useful for time series compaction with 
 + ` ` TimeWindowCompactionStrategy ` ` ( and the deprecated ` ` DateTieredCompactionStrategy ` ` ) . 
 + 
 + Repaired / unrepaired data 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + With incremental repairs Cassandra must keep track of what data is repaired and what data is unrepaired . With 
 + anticompaction repaired data is split out into repaired and unrepaired sstables . To avoid mixing up the data again 
 + separate compaction strategy instances are run on the two sets of data , each instance only knowing about either the 
 + repaired or the unrepaired sstables . This means that if you only run incremental repair once and then never again , you 
 + might have very old data in the repaired sstables that block compaction from dropping tombstones in the unrepaired 
 + ( probably newer ) sstables . 
 + 
 + Data directories 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + Since tombstones and data can live in different sstables it is important to realize that losing an sstable might lead to 
 + data becoming live again - the most common way of losing sstables is to have a hard drive break down . To avoid making 
 + data live tombstones and actual data are always in the same data directory . This way , if a disk is lost , all versions of 
 + a partition are lost and no data can get undeleted . To achieve this a compaction strategy instance per data directory is 
 + run in addition to the compaction strategy instances containing repaired / unrepaired data , this means that if you have 4 
 + data directories there will be 8 compaction strategy instances running . This has a few more benefits than just avoiding 
 + data getting undeleted : 
 + 
 + - It is possible to run more compactions in parallel - leveled compaction will have several totally separate levelings 
 + and each one can run compactions independently from the others . 
 + - Users can backup and restore a single data directory . 
 + - Note though that currently all data directories are considered equal , so if you have a tiny disk and a big disk 
 + backing two data directories , the big one will be limited the by the small one . One work around to this is to create 
 + more data directories backed by the big disk . 
 + 
 + Single sstable tombstone compaction 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + When an sstable is written a histogram with the tombstone expiry times is created and this is used to try to find 
 + sstables with very many tombstones and run single sstable compaction on that sstable in hope of being able to drop 
 + tombstones in that sstable . Before starting this it is also checked how likely it is that any tombstones will actually 
 + will be able to be dropped how much this sstable overlaps with other sstables . To avoid most of these checks the 
 + compaction option ` ` unchecked _ tombstone _ compaction ` ` can be enabled . 
 + 
 + . . _ compaction - options : 
 + 
 + Common options 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + There is a number of common options for all the compaction strategies ; 
 + 
 + ` ` enabled ` ` ( default : true ) 
 + Whether minor compactions should run . Note that you can have ' enabled ' : true as a compaction option and then do 
 + ' nodetool enableautocompaction ' to start running compactions . 
 + Default true . 
 + ` ` tombstone _ threshold ` ` ( default : 0 . 2 ) 
 + How much of the sstable should be tombstones for us to consider doing a single sstable compaction of that sstable . 
 + ` ` tombstone _ compaction _ interval ` ` ( default : 86400s ( 1 day ) ) 
 + Since it might not be possible to drop any tombstones when doing a single sstable compaction we need to make sure 
 + that one sstable is not constantly getting recompacted - this option states how often we should try for a given 
 + sstable . 
 + ` ` log _ all ` ` ( default : false ) 
 + New detailed compaction logging , see : ref : ` below < detailed - compaction - logging > ` . 
 + ` ` unchecked _ tombstone _ compaction ` ` ( default : false ) 
 + The single sstable compaction has quite strict checks for whether it should be started , this option disables those 
 + checks and for some usecases this might be needed . Note that this does not change anything for the actual 
 + compaction , tombstones are only dropped if it is safe to do so - it might just rewrite an sstable without being able 
 + to drop any tombstones . 
 + ` ` only _ purge _ repaired _ tombstone ` ` ( default : false ) 
 + Option to enable the extra safety of making sure that tombstones are only dropped if the data has been repaired . 
 + ` ` min _ threshold ` ` ( default : 4 ) 
 + Lower limit of number of sstables before a compaction is triggered . Not used for ` ` LeveledCompactionStrategy ` ` . 
 + ` ` max _ threshold ` ` ( default : 32 ) 
 + Upper limit of number of sstables before a compaction is triggered . Not used for ` ` LeveledCompactionStrategy ` ` . 
 + 
 + Compaction nodetool commands 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + The : ref : ` nodetool < nodetool > ` utility provides a number of commands related to compaction : 
 + 
 + ` ` enableautocompaction ` ` 
 + Enable compaction . 
 + ` ` disableautocompaction ` ` 
 + Disable compaction . 
 + ` ` setcompactionthroughput ` ` 
 + How fast compaction should run at most - defaults to 16MB / s , but note that it is likely not possible to reach this 
 + throughput . 
 + ` ` compactionstats ` ` 
 + Statistics about current and pending compactions . 
 + ` ` compactionhistory ` ` 
 + List details about the last compactions . 
 + ` ` setcompactionthreshold ` ` 
 + Set the min / max sstable count for when to trigger compaction , defaults to 4 / 32 . 
 + 
 + Switching the compaction strategy and options using JMX 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + It is possible to switch compaction strategies and its options on just a single node using JMX , this is a great way to 
 + experiment with settings without affecting the whole cluster . The mbean is : : 
 + 
 + org . apache . cassandra . db : type = ColumnFamilies , keyspace = < keyspace _ name > , columnfamily = < table _ name > 
 + 
 + and the attribute to change is ` ` CompactionParameters ` ` or ` ` CompactionParametersJson ` ` if you use jconsole or jmc . The 
 + syntax for the json version is the same as you would use in an : ref : ` ALTER TABLE < alter - table - statement > ` statement - 
 + for example : : 
 + 
 + { ' class ' : ' LeveledCompactionStrategy ' , ' sstable _ size _ in _ mb ' : 123 } 
 + 
 + The setting is kept until someone executes an : ref : ` ALTER TABLE < alter - table - statement > ` that touches the compaction 
 + settings or restarts the node . 
 + 
 + . . _ detailed - compaction - logging : 
 + 
 + More detailed compaction logging 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + Enable with the compaction option ` ` log _ all ` ` and a more detailed compaction log file will be produced in your log 
 + directory . 
 + 
 + Size Tiered Compaction Strategy 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + The basic idea of ` ` SizeTieredCompactionStrategy ` ` ( STCS ) is to merge sstables of approximately the same size . All 
 + sstables are put in different buckets depending on their size . An sstable is added to the bucket if size of the sstable 
 + is within ` ` bucket _ low ` ` and ` ` bucket _ high ` ` of the current average size of the sstables already in the bucket . This 
 + will create several buckets and the most interesting of those buckets will be compacted . The most interesting one is 
 + decided by figuring out which bucket ' s sstables takes the most reads . 
 + 
 + Major compaction 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + When running a major compaction with STCS you will end up with two sstables per data directory ( one for repaired data 
 + and one for unrepaired data ) . There is also an option ( - s ) to do a major compaction that splits the output into several 
 + sstables . The sizes of the sstables are approximately 50 % , 25 % , 12 . 5 % . . . of the total size . 
 + 
 + . . _ stcs - options : 
 + 
 + STCS options 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + ` ` min _ sstable _ size ` ` ( default : 50MB ) 
 + Sstables smaller than this are put in the same bucket . 
 + ` ` bucket _ low ` ` ( default : 0 . 5 ) 
 + How much smaller than the average size of a bucket a sstable should be before not being included in the bucket . That 
 + is , if ` ` bucket _ low * avg _ bucket _ size < sstable _ size ` ` ( and the ` ` bucket _ high ` ` condition holds , see below ) , then 
 + the sstable is added to the bucket . 
 + ` ` bucket _ high ` ` ( default : 1 . 5 ) 
 + How much bigger than the average size of a bucket a sstable should be before not being included in the bucket . That 
 + is , if ` ` sstable _ size < bucket _ high * avg _ bucket _ size ` ` ( and the ` ` bucket _ low ` ` condition holds , see above ) , then 
 + the sstable is added to the bucket . 
 + 
 + Defragmentation 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + Defragmentation is done when many sstables are touched during a read . The result of the read is put in to the memtable 
 + so that the next read will not have to touch as many sstables . This can cause writes on a read - only - cluster . 
 + 
 + Leveled Compaction Strategy 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + The idea of ` ` LeveledCompactionStrategy ` ` ( LCS ) is that all sstables are put into different levels where we guarantee 
 + that no overlapping sstables are in the same level . By overlapping we mean that the first / last token of a single sstable 
 + are never overlapping with other sstables . This means that for a SELECT we will only have to look for the partition key 
 + in a single sstable per level . Each level is 10x the size of the previous one and each sstable is 160MB by default . L0 
 + is where sstables are streamed / flushed - no overlap guarantees are given here . 
 + 
 + When picking compaction candidates we have to make sure that the compaction does not create overlap in the target level . 
 + This is done by always including all overlapping sstables in the next level . For example if we select an sstable in L3 , 
 + we need to guarantee that we pick all overlapping sstables in L4 and make sure that no currently ongoing compactions 
 + will create overlap if we start that compaction . We can start many parallel compactions in a level if we guarantee that 
 + we wont create overlap . For L0 - > L1 compactions we almost always need to include all L1 sstables since most L0 sstables 
 + cover the full range . We also can ' t compact all L0 sstables with all L1 sstables in a single compaction since that can 
 + use too much memory . 
 + 
 + When deciding which level to compact LCS checks the higher levels first ( with LCS , a " higher " level is one with a higher 
 + number , L0 being the lowest one ) and if the level is behind a compaction will be started in that level . 
 + 
 + Major compaction 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + It is possible to do a major compaction with LCS - it will currently start by filling out L1 and then once L1 is full , 
 + it continues with L2 etc . This is sub optimal and will change to create all the sstables in a high level instead , 
 + CASSANDRA - 11817 . 
 + 
 + Bootstrapping 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + During bootstrap sstables are streamed from other nodes . The level of the remote sstable is kept to avoid many 
 + compactions after the bootstrap is done . During bootstrap the new node also takes writes while it is streaming the data 
 + from a remote node - these writes are flushed to L0 like all other writes and to avoid those sstables blocking the 
 + remote sstables from going to the correct level , we only do STCS in L0 until the bootstrap is done . 
 + 
 + STCS in L0 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + If LCS gets very many L0 sstables reads are going to hit all ( or most ) of the L0 sstables since they are likely to be 
 + overlapping . To more quickly remedy this LCS does STCS compactions in L0 if there are more than 32 sstables there . This 
 + should improve read performance more quickly compared to letting LCS do its L0 - > L1 compactions . If you keep getting 
 + too many sstables in L0 it is likely that LCS is not the best fit for your workload and STCS could work out better . 
 + 
 + Starved sstables 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + If a node ends up with a leveling where there are a few very high level sstables that are not getting compacted they 
 + might make it impossible for lower levels to drop tombstones etc . For example , if there are sstables in L6 but there is 
 + only enough data to actually get a L4 on the node the left over sstables in L6 will get starved and not compacted . This 
 + can happen if a user changes sstable \ _ size \ _ in \ _ mb from 5MB to 160MB for example . To avoid this LCS tries to include 
 + those starved high level sstables in other compactions if there has been 25 compaction rounds where the highest level 
 + has not been involved . 
 + 
 + . . _ lcs - options : 
 + 
 + LCS options 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + ` ` sstable _ size _ in _ mb ` ` ( default : 160MB ) 
 + The target compressed ( if using compression ) sstable size - the sstables can end up being larger if there are very 
 + large partitions on the node . 
 + 
 + LCS also support the ` ` cassandra . disable _ stcs _ in _ l0 ` ` startup option ( ` ` - Dcassandra . disable _ stcs _ in _ l0 = true ` ` ) to avoid 
 + doing STCS in L0 . 
 + 
 + . . _ twcs : 
 + 
 + Time Window CompactionStrategy 
 + ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 + 
 + ` ` TimeWindowCompactionStrategy ` ` ( TWCS ) is designed specifically for workloads where it ' s beneficial to have data on 
 + disk grouped by the timestamp of the data , a common goal when the workload is time - series in nature or when all data is 
 + written with a TTL . In an expiring / TTL workload , the contents of an entire SSTable likely expire at approximately the 
 + same time , allowing them to be dropped completely , and space reclaimed much more reliably than when using 
 + ` ` SizeTieredCompactionStrategy ` ` or ` ` LeveledCompactionStrategy ` ` . The basic concept is that 
 + ` ` TimeWindowCompactionStrategy ` ` will create 1 sstable per file for a given window , where a window is simply calculated 
 + as the combination of two primary options : 
 + 
 + ` ` compaction _ window _ unit ` ` ( default : DAYS ) 
 + A Java TimeUnit ( MINUTES , HOURS , or DAYS ) . 
 + ` ` compaction _ window _ size ` ` ( default : 1 ) 
 + The number of units that make up a window . 
 + 
 + Taken together , the operator can specify windows of virtually any size , and ` TimeWindowCompactionStrategy ` will work to 
 + create a single sstable for writes within that window . For efficiency during writing , the newest window will be 
 + compacted using ` SizeTieredCompactionStrategy ` . 
 + 
 + Ideally , operators should select a ` ` compaction _ window _ unit ` ` and ` ` compaction _ window _ size ` ` pair that produces 
 + approximately 20 - 30 windows - if writing with a 90 day TTL , for example , a 3 Day window would be a reasonable choice 
 + ( ` ` ' compaction _ window _ unit ' : ' DAYS ' , ' compaction _ window _ size ' : 3 ` ` ) . 
 + 
 + TimeWindowCompactionStrategy Operational Concerns 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + The primary motivation for TWCS is to separate data on disk by timestamp and to allow fully expired SSTables to drop 
 + more efficiently . One potential way this optimal behavior can be subverted is if data is written to SSTables out of 
 + order , with new data and old data in the same SSTable . Out of order data can appear in two ways : 
 + 
 + - If the user mixes old data and new data in the traditional write path , the data will be comingled in the memtables 
 + and flushed into the same SSTable , where it will remain comingled . 
 + - If the user ' s read requests for old data cause read repairs that pull old data into the current memtable , that data 
 + will be comingled and flushed into the same SSTable . 
 + 
 + While TWCS tries to minimize the impact of comingled data , users should attempt to avoid this behavior . Specifically , 
 + users should avoid queries that explicitly set the timestamp via CQL ` ` USING TIMESTAMP ` ` . Additionally , users should run 
 + frequent repairs ( which streams data in such a way that it does not become comingled ) , and disable background read 
 + repair by setting the table ' s ` ` read _ repair _ chance ` ` and ` ` dclocal _ read _ repair _ chance ` ` to 0 . 
 + 
 + Changing TimeWindowCompactionStrategy Options 
 + ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
 + 
 + Operators wishing to enable ` ` TimeWindowCompactionStrategy ` ` on existing data should consider running a major compaction 
 + first , placing all existing data into a single ( old ) window . Subsequent newer writes will then create typical SSTables 
 + as expected . 
 + 
 + Operators wishing to change ` ` compaction _ window _ unit ` ` or ` ` compaction _ window _ size ` ` can do so , but may trigger 
 + additional compactions as adjacent windows are joined together . If the window size is decrease d ( for example , from 24 
 + hours to 12 hours ) , then the existing SSTables will not be modified - TWCS can not split existing SSTables into multiple 
 + windows . 
 
 - DateTiered 
 - ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
 - . . todo : : todo 
 
 Tombstones and Garbage Collection ( GC ) Grace 
 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

NEAREST DIFF:
ELIMINATEDSENTENCE
