BLEU SCORE: 0.016224821466016948

TEST MSG: fix 2 . 2 eclipse - warnings
GENERATED MSG: fat clients were creating local data . patch by gdusbabek , reviewed by tjake . CASSANDRA - 2223

TEST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / cache / AutoSavingCache . java b / src / java / org / apache / cassandra / cache / AutoSavingCache . java <nl> index c08925d . . 2c6820e 100644 <nl> - - - a / src / java / org / apache / cassandra / cache / AutoSavingCache . java <nl> + + + b / src / java / org / apache / cassandra / cache / AutoSavingCache . java <nl> @ @ - 318 , 6 + 318 , 7 @ @ public class AutoSavingCache < K extends CacheKey , V > extends InstrumentingCache < K <nl> return info . forProgress ( keysWritten , Math . max ( keysWritten , keysEstimate ) ) ; <nl> } <nl> <nl> + @ SuppressWarnings ( " resource " ) <nl> public void saveCache ( ) <nl> { <nl> logger . trace ( " Deleting old { } files . " , cacheType ) ; <nl> diff - - git a / src / java / org / apache / cassandra / db / WindowsFailedSnapshotTracker . java b / src / java / org / apache / cassandra / db / WindowsFailedSnapshotTracker . java <nl> index 9e6bb47 . . 7cc7893 100644 <nl> - - - a / src / java / org / apache / cassandra / db / WindowsFailedSnapshotTracker . java <nl> + + + b / src / java / org / apache / cassandra / db / WindowsFailedSnapshotTracker . java <nl> @ @ - 52 , 32 + 52 , 33 @ @ public class WindowsFailedSnapshotTracker <nl> { <nl> try <nl> { <nl> - BufferedReader reader = new BufferedReader ( new FileReader ( TODELETEFILE ) ) ; <nl> - String snapshotDirectory ; <nl> - while ( ( snapshotDirectory = reader . readLine ( ) ) ! = null ) <nl> + try ( BufferedReader reader = new BufferedReader ( new FileReader ( TODELETEFILE ) ) ) <nl> { <nl> - File f = new File ( snapshotDirectory ) ; <nl> + String snapshotDirectory ; <nl> + while ( ( snapshotDirectory = reader . readLine ( ) ) ! = null ) <nl> + { <nl> + File f = new File ( snapshotDirectory ) ; <nl> <nl> - / / Skip folders that aren ' t a subset of temp or a data folder . We don ' t want people to accidentally <nl> - / / delete something important by virtue of adding something invalid to the . toDelete file . <nl> - boolean validFolder = FileUtils . isSubDirectory ( new File ( System . getenv ( " TEMP " ) ) , f ) ; <nl> - for ( String s : DatabaseDescriptor . getAllDataFileLocations ( ) ) <nl> - validFolder | = FileUtils . isSubDirectory ( new File ( s ) , f ) ; <nl> + / / Skip folders that aren ' t a subset of temp or a data folder . We don ' t want people to accidentally <nl> + / / delete something important by virtue of adding something invalid to the . toDelete file . <nl> + boolean validFolder = FileUtils . isSubDirectory ( new File ( System . getenv ( " TEMP " ) ) , f ) ; <nl> + for ( String s : DatabaseDescriptor . getAllDataFileLocations ( ) ) <nl> + validFolder | = FileUtils . isSubDirectory ( new File ( s ) , f ) ; <nl> <nl> - if ( ! validFolder ) <nl> - { <nl> - logger . warn ( " Skipping invalid directory found in . toDelete : { } . Only % TEMP % or data file subdirectories are valid . " , f ) ; <nl> - continue ; <nl> - } <nl> + if ( ! validFolder ) <nl> + { <nl> + logger . warn ( " Skipping invalid directory found in . toDelete : { } . Only % TEMP % or data file subdirectories are valid . " , f ) ; <nl> + continue ; <nl> + } <nl> <nl> - / / Could be a non - existent directory if deletion worked on previous JVM shutdown . <nl> - if ( f . exists ( ) ) <nl> - { <nl> - logger . warn ( " Discovered obsolete snapshot . Deleting directory [ { } ] " , snapshotDirectory ) ; <nl> - FileUtils . deleteRecursive ( new File ( snapshotDirectory ) ) ; <nl> + / / Could be a non - existent directory if deletion worked on previous JVM shutdown . <nl> + if ( f . exists ( ) ) <nl> + { <nl> + logger . warn ( " Discovered obsolete snapshot . Deleting directory [ { } ] " , snapshotDirectory ) ; <nl> + FileUtils . deleteRecursive ( new File ( snapshotDirectory ) ) ; <nl> + } <nl> } <nl> } <nl> - reader . close ( ) ; <nl> <nl> / / Only delete the old . toDelete file if we succeed in deleting all our known bad snapshots . <nl> Files . delete ( Paths . get ( TODELETEFILE ) ) ; <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java <nl> index cb02a8c . . 98fb556 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java <nl> @ @ - 197 , 7 + 197 , 7 @ @ public class CommitLogReplayer <nl> } <nl> return end ; <nl> } <nl> - <nl> + <nl> abstract static class ReplayFilter <nl> { <nl> public abstract Iterable < ColumnFamily > filter ( Mutation mutation ) ; <nl> @ @ - 273 , 6 + 273 , 7 @ @ public class CommitLogReplayer <nl> } <nl> } <nl> <nl> + @ SuppressWarnings ( " resource " ) <nl> public void recover ( File file , boolean tolerateTruncation ) throws IOException <nl> { <nl> CommitLogDescriptor desc = CommitLogDescriptor . fromFileName ( file . getName ( ) ) ; <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> index 148c08a . . 3c088c2 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java <nl> @ @ - 117 , 9 + 117 , 9 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < <nl> } <nl> } <nl> <nl> - try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) ) <nl> + try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) ; <nl> + Session session = cluster . connect ( ) ) <nl> { <nl> - Session session = cluster . connect ( ) ; <nl> Metadata metadata = session . getCluster ( ) . getMetadata ( ) ; <nl> <nl> for ( TokenRange range : masterRangeNodes . keySet ( ) ) <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> index 14e24fb . . 84102a5 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java <nl> @ @ - 43 , 7 + 43 , 7 @ @ import org . apache . hadoop . util . Progressable ; <nl> / * * <nl> * The < code > CqlRecordWriter < / code > maps the output & lt ; key , value & gt ; <nl> * pairs to a Cassandra table . In particular , it applies the binded variables <nl> - * in the value to the prepared statement , which it associates with the key , and in <nl> + * in the value to the prepared statement , which it associates with the key , and in <nl> * turn the responsible endpoint . <nl> * <nl> * < p > <nl> @ @ - 112 , 11 + 112 , 11 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> this . queueSize = conf . getInt ( ColumnFamilyOutputFormat . QUEUE _ SIZE , 32 * FBUtilities . getAvailableProcessors ( ) ) ; <nl> batchThreshold = conf . getLong ( ColumnFamilyOutputFormat . BATCH _ THRESHOLD , 32 ) ; <nl> this . clients = new HashMap < > ( ) ; <nl> + String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; <nl> <nl> - try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ) <nl> + try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ; <nl> + Session client = cluster . connect ( keyspace ) ) <nl> { <nl> - String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; <nl> - Session client = cluster . connect ( keyspace ) ; <nl> ringCache = new NativeRingCache ( conf ) ; <nl> if ( client ! = null ) <nl> { <nl> @ @ - 179 , 7 + 179 , 7 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> if ( clientException ! = null ) <nl> throw clientException ; <nl> } <nl> - <nl> + <nl> / * * <nl> * If the key is to be associated with a valid value , a mutation is created <nl> * for it with the given table and columns . In the event the value <nl> @ @ - 225 , 6 + 225 , 20 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> HadoopCompat . progress ( context ) ; <nl> } <nl> <nl> + private static void closeSession ( Session session ) <nl> + { <nl> + / / Close the session to satisfy to avoid warnings for the resource not being closed <nl> + try <nl> + { <nl> + if ( session ! = null ) <nl> + session . close ( ) ; <nl> + } <nl> + catch ( Throwable t ) <nl> + { <nl> + logger . warn ( " Error closing connection " , t ) ; <nl> + } <nl> + } <nl> + <nl> / * * <nl> * A client that runs in a threadpool and connects to the list of endpoints for a particular <nl> * range . Bound variables for keys in that range are sent to this client via a queue . <nl> @ @ - 273 , 94 + 287 , 104 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> } <nl> } <nl> } <nl> - <nl> + <nl> / * * <nl> * Loops collecting cql binded variable values from the queue and sending to Cassandra <nl> * / <nl> + @ SuppressWarnings ( " resource " ) <nl> public void run ( ) <nl> { <nl> Session session = null ; <nl> - outer : <nl> - while ( run | | ! queue . isEmpty ( ) ) <nl> + try <nl> { <nl> - List < ByteBuffer > bindVariables ; <nl> - try <nl> + outer : <nl> + while ( run | | ! queue . isEmpty ( ) ) <nl> { <nl> - bindVariables = queue . take ( ) ; <nl> - } <nl> - catch ( InterruptedException e ) <nl> - { <nl> - / / re - check loop condition after interrupt <nl> - continue ; <nl> - } <nl> + List < ByteBuffer > bindVariables ; <nl> + try <nl> + { <nl> + bindVariables = queue . take ( ) ; <nl> + } <nl> + catch ( InterruptedException e ) <nl> + { <nl> + / / re - check loop condition after interrupt <nl> + continue ; <nl> + } <nl> <nl> - ListIterator < InetAddress > iter = endpoints . listIterator ( ) ; <nl> - while ( true ) <nl> - { <nl> - / / send the mutation to the last - used endpoint . first time through , this will NPE harmlessly . <nl> - if ( session ! = null ) <nl> + ListIterator < InetAddress > iter = endpoints . listIterator ( ) ; <nl> + while ( true ) <nl> { <nl> - try <nl> + / / send the mutation to the last - used endpoint . first time through , this will NPE harmlessly . <nl> + if ( session ! = null ) <nl> { <nl> - int i = 0 ; <nl> - PreparedStatement statement = preparedStatement ( session ) ; <nl> - while ( bindVariables ! = null ) <nl> + try <nl> { <nl> - BoundStatement boundStatement = new BoundStatement ( statement ) ; <nl> - for ( int columnPosition = 0 ; columnPosition < bindVariables . size ( ) ; columnPosition + + ) <nl> + int i = 0 ; <nl> + PreparedStatement statement = preparedStatement ( session ) ; <nl> + while ( bindVariables ! = null ) <nl> { <nl> - boundStatement . setBytesUnsafe ( columnPosition , bindVariables . get ( columnPosition ) ) ; <nl> + BoundStatement boundStatement = new BoundStatement ( statement ) ; <nl> + for ( int columnPosition = 0 ; columnPosition < bindVariables . size ( ) ; columnPosition + + ) <nl> + { <nl> + boundStatement . setBytesUnsafe ( columnPosition , bindVariables . get ( columnPosition ) ) ; <nl> + } <nl> + session . execute ( boundStatement ) ; <nl> + i + + ; <nl> + <nl> + if ( i > = batchThreshold ) <nl> + break ; <nl> + bindVariables = queue . poll ( ) ; <nl> } <nl> - session . execute ( boundStatement ) ; <nl> - i + + ; <nl> - <nl> - if ( i > = batchThreshold ) <nl> - break ; <nl> - bindVariables = queue . poll ( ) ; <nl> + break ; <nl> } <nl> - break ; <nl> + catch ( Exception e ) <nl> + { <nl> + closeInternal ( ) ; <nl> + if ( ! iter . hasNext ( ) ) <nl> + { <nl> + lastException = new IOException ( e ) ; <nl> + break outer ; <nl> + } <nl> + } <nl> + } <nl> + <nl> + / / attempt to connect to a different endpoint <nl> + try <nl> + { <nl> + InetAddress address = iter . next ( ) ; <nl> + String host = address . getHostName ( ) ; <nl> + cluster = CqlConfigHelper . getOutputCluster ( host , conf ) ; <nl> + closeSession ( session ) ; <nl> + session = cluster . connect ( ) ; <nl> } <nl> catch ( Exception e ) <nl> { <nl> + / / If connection died due to Interrupt , just try connecting to the endpoint again . <nl> + / / There are too many ways for the Thread . interrupted ( ) state to be cleared , so <nl> + / / we can ' t rely on that here . Until the java driver gives us a better way of knowing <nl> + / / that this exception came from an InterruptedException , this is the best solution . <nl> + if ( canRetryDriverConnection ( e ) ) <nl> + { <nl> + iter . previous ( ) ; <nl> + } <nl> closeInternal ( ) ; <nl> - if ( ! iter . hasNext ( ) ) <nl> + <nl> + / / Most exceptions mean something unexpected went wrong to that endpoint , so <nl> + / / we should try again to another . Other exceptions ( auth or invalid request ) are fatal . <nl> + if ( ( e instanceof AuthenticationException | | e instanceof InvalidQueryException ) | | ! iter . hasNext ( ) ) <nl> { <nl> lastException = new IOException ( e ) ; <nl> break outer ; <nl> } <nl> - } <nl> - } <nl> - <nl> - / / attempt to connect to a different endpoint <nl> - try <nl> - { <nl> - InetAddress address = iter . next ( ) ; <nl> - String host = address . getHostName ( ) ; <nl> - cluster = CqlConfigHelper . getOutputCluster ( host , conf ) ; <nl> - session = cluster . connect ( ) ; <nl> - } <nl> - catch ( Exception e ) <nl> - { <nl> - / / If connection died due to Interrupt , just try connecting to the endpoint again . <nl> - / / There are too many ways for the Thread . interrupted ( ) state to be cleared , so <nl> - / / we can ' t rely on that here . Until the java driver gives us a better way of knowing <nl> - / / that this exception came from an InterruptedException , this is the best solution . <nl> - if ( canRetryDriverConnection ( e ) ) <nl> - { <nl> - iter . previous ( ) ; <nl> - } <nl> - closeInternal ( ) ; <nl> - <nl> - / / Most exceptions mean something unexpected went wrong to that endpoint , so <nl> - / / we should try again to another . Other exceptions ( auth or invalid request ) are fatal . <nl> - if ( ( e instanceof AuthenticationException | | e instanceof InvalidQueryException ) | | ! iter . hasNext ( ) ) <nl> - { <nl> - lastException = new IOException ( e ) ; <nl> - break outer ; <nl> } <nl> } <nl> } <nl> } <nl> + finally <nl> + { <nl> + closeSession ( session ) ; <nl> + } <nl> + <nl> / / close all our connections once we are done . <nl> closeInternal ( ) ; <nl> } <nl> @ @ - 489 , 9 + 513 , 9 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf <nl> private void refreshEndpointMap ( ) <nl> { <nl> String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; <nl> - try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ) <nl> + try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ; <nl> + Session session = cluster . connect ( keyspace ) ) <nl> { <nl> - Session session = cluster . connect ( keyspace ) ; <nl> rangeMap = new HashMap < > ( ) ; <nl> metadata = session . getCluster ( ) . getMetadata ( ) ; <nl> Set < TokenRange > ranges = metadata . getTokenRanges ( ) ; <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java <nl> index 74058b1 . . 8831cf2 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java <nl> @ @ - 690 , 7 + 690 , 7 @ @ public class CqlNativeStorage extends LoadFunc implements StoreFuncInterface , Lo <nl> else <nl> throw new IOException ( " bulk _ insert _ statement is missing in input url parameter " ) ; <nl> if ( bulkTableAlias ! = null ) <nl> - CqlBulkOutputFormat . setTableAlias ( conf , bulkTableAlias , column _ family ) ; <nl> + CqlBulkOutputFormat . setTableAlias ( conf , bulkTableAlias , column _ family ) ; <nl> CqlBulkOutputFormat . setDeleteSourceOnSuccess ( conf , bulkDeleteSourceOnSuccess ) ; <nl> if ( bulkOutputLocation ! = null ) <nl> conf . set ( CqlBulkRecordWriter . OUTPUT _ LOCATION , bulkOutputLocation ) ; <nl> @ @ - 724 , 9 + 724 , 9 @ @ public class CqlNativeStorage extends LoadFunc implements StoreFuncInterface , Lo <nl> / / Only get the schema if we haven ' t already gotten it <nl> if ( ! properties . containsKey ( signature ) ) <nl> { <nl> - try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) , conf ) ) <nl> + try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) , conf ) ; <nl> + Session client = cluster . connect ( ) ) <nl> { <nl> - Session client = cluster . connect ( ) ; <nl> client . execute ( " USE " + keyspace ) ; <nl> <nl> / / compose the CfDef for the columfamily <nl> diff - - git a / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java b / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java <nl> index 442236d . . 9015b61 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java <nl> + + + b / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java <nl> @ @ - 48 , 15 + 48 , 36 @ @ public class ChecksummedRandomAccessReader extends RandomAccessReader <nl> this . file = file ; <nl> } <nl> <nl> + @ SuppressWarnings ( " resource " ) <nl> public static ChecksummedRandomAccessReader open ( File file , File crcFile ) throws IOException <nl> { <nl> try ( ChannelProxy channel = new ChannelProxy ( file ) ) <nl> { <nl> RandomAccessReader crcReader = RandomAccessReader . open ( crcFile ) ; <nl> - @ SuppressWarnings ( " resource " ) <nl> - DataIntegrityMetadata . ChecksumValidator validator = <nl> - new DataIntegrityMetadata . ChecksumValidator ( new Adler32 ( ) , crcReader , file . getPath ( ) ) ; <nl> - return new ChecksummedRandomAccessReader ( file , channel , validator ) ; <nl> + boolean closeCrcReader = true ; <nl> + try <nl> + { <nl> + DataIntegrityMetadata . ChecksumValidator validator = <nl> + new DataIntegrityMetadata . ChecksumValidator ( new Adler32 ( ) , crcReader , file . getPath ( ) ) ; <nl> + closeCrcReader = false ; <nl> + boolean closeValidator = true ; <nl> + try <nl> + { <nl> + ChecksummedRandomAccessReader retval = new ChecksummedRandomAccessReader ( file , channel , validator ) ; <nl> + closeValidator = false ; <nl> + return retval ; <nl> + } <nl> + finally <nl> + { <nl> + if ( closeValidator ) <nl> + validator . close ( ) ; <nl> + } <nl> + } <nl> + finally <nl> + { <nl> + if ( closeCrcReader ) <nl> + crcReader . close ( ) ; <nl> + } <nl> } <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / io / util / SegmentedFile . java b / src / java / org / apache / cassandra / io / util / SegmentedFile . java <nl> index 30707d8 . . 553cc0d 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / SegmentedFile . java <nl> + + + b / src / java / org / apache / cassandra / io / util / SegmentedFile . java <nl> @ @ - 179 , 6 + 179 , 7 @ @ public abstract class SegmentedFile extends SharedCloseableImpl <nl> return complete ( path , - 1L ) ; <nl> } <nl> <nl> + @ SuppressWarnings ( " resource " ) <nl> public SegmentedFile complete ( String path , long overrideLength ) <nl> { <nl> ChannelProxy channelCopy = getChannel ( path ) ; <nl> diff - - git a / src / java / org / apache / cassandra / net / IncomingTcpConnection . java b / src / java / org / apache / cassandra / net / IncomingTcpConnection . java <nl> index f6652b0 . . a972114 100644 <nl> - - - a / src / java / org / apache / cassandra / net / IncomingTcpConnection . java <nl> + + + b / src / java / org / apache / cassandra / net / IncomingTcpConnection . java <nl> @ @ - 108 , 7 + 108 , 7 @ @ public class IncomingTcpConnection extends Thread implements Closeable <nl> close ( ) ; <nl> } <nl> } <nl> - <nl> + <nl> @ Override <nl> public void close ( ) <nl> { <nl> @ @ - 164 , 6 + 164 , 7 @ @ public class IncomingTcpConnection extends Thread implements Closeable <nl> } <nl> else <nl> { <nl> + @ SuppressWarnings ( " resource " ) <nl> ReadableByteChannel channel = socket . getChannel ( ) ; <nl> in = new NIODataInputStream ( channel ! = null ? channel : Channels . newChannel ( socket . getInputStream ( ) ) , BUFFER _ SIZE ) ; <nl> }
NEAREST DIFF (one line): diff - - git a / drivers / py / cql / marshal . py b / drivers / py / cql / marshal . py <nl> index b3b61f2 . . 31fbd29 100644 <nl> - - - a / drivers / py / cql / marshal . py <nl> + + + b / drivers / py / cql / marshal . py <nl> @ @ - 76 , 7 + 76 , 7 @ @ def unmarshal ( bytestr , typestr ) : <nl> elif typestr = = " org . apache . cassandra . db . marshal . LexicalUUIDType " : <nl> return UUID ( bytes = bytestr ) <nl> elif typestr = = " org . apache . cassandra . db . marshal . TimeUUIDType " : <nl> - return UUID ( bytes = bytetr ) <nl> + return UUID ( bytes = bytestr ) <nl> else : <nl> return bytestr <nl>

TEST DIFF:
diff - - git a / src / java / org / apache / cassandra / cache / AutoSavingCache . java b / src / java / org / apache / cassandra / cache / AutoSavingCache . java 
 index c08925d . . 2c6820e 100644 
 - - - a / src / java / org / apache / cassandra / cache / AutoSavingCache . java 
 + + + b / src / java / org / apache / cassandra / cache / AutoSavingCache . java 
 @ @ - 318 , 6 + 318 , 7 @ @ public class AutoSavingCache < K extends CacheKey , V > extends InstrumentingCache < K 
 return info . forProgress ( keysWritten , Math . max ( keysWritten , keysEstimate ) ) ; 
 } 
 
 + @ SuppressWarnings ( " resource " ) 
 public void saveCache ( ) 
 { 
 logger . trace ( " Deleting old { } files . " , cacheType ) ; 
 diff - - git a / src / java / org / apache / cassandra / db / WindowsFailedSnapshotTracker . java b / src / java / org / apache / cassandra / db / WindowsFailedSnapshotTracker . java 
 index 9e6bb47 . . 7cc7893 100644 
 - - - a / src / java / org / apache / cassandra / db / WindowsFailedSnapshotTracker . java 
 + + + b / src / java / org / apache / cassandra / db / WindowsFailedSnapshotTracker . java 
 @ @ - 52 , 32 + 52 , 33 @ @ public class WindowsFailedSnapshotTracker 
 { 
 try 
 { 
 - BufferedReader reader = new BufferedReader ( new FileReader ( TODELETEFILE ) ) ; 
 - String snapshotDirectory ; 
 - while ( ( snapshotDirectory = reader . readLine ( ) ) ! = null ) 
 + try ( BufferedReader reader = new BufferedReader ( new FileReader ( TODELETEFILE ) ) ) 
 { 
 - File f = new File ( snapshotDirectory ) ; 
 + String snapshotDirectory ; 
 + while ( ( snapshotDirectory = reader . readLine ( ) ) ! = null ) 
 + { 
 + File f = new File ( snapshotDirectory ) ; 
 
 - / / Skip folders that aren ' t a subset of temp or a data folder . We don ' t want people to accidentally 
 - / / delete something important by virtue of adding something invalid to the . toDelete file . 
 - boolean validFolder = FileUtils . isSubDirectory ( new File ( System . getenv ( " TEMP " ) ) , f ) ; 
 - for ( String s : DatabaseDescriptor . getAllDataFileLocations ( ) ) 
 - validFolder | = FileUtils . isSubDirectory ( new File ( s ) , f ) ; 
 + / / Skip folders that aren ' t a subset of temp or a data folder . We don ' t want people to accidentally 
 + / / delete something important by virtue of adding something invalid to the . toDelete file . 
 + boolean validFolder = FileUtils . isSubDirectory ( new File ( System . getenv ( " TEMP " ) ) , f ) ; 
 + for ( String s : DatabaseDescriptor . getAllDataFileLocations ( ) ) 
 + validFolder | = FileUtils . isSubDirectory ( new File ( s ) , f ) ; 
 
 - if ( ! validFolder ) 
 - { 
 - logger . warn ( " Skipping invalid directory found in . toDelete : { } . Only % TEMP % or data file subdirectories are valid . " , f ) ; 
 - continue ; 
 - } 
 + if ( ! validFolder ) 
 + { 
 + logger . warn ( " Skipping invalid directory found in . toDelete : { } . Only % TEMP % or data file subdirectories are valid . " , f ) ; 
 + continue ; 
 + } 
 
 - / / Could be a non - existent directory if deletion worked on previous JVM shutdown . 
 - if ( f . exists ( ) ) 
 - { 
 - logger . warn ( " Discovered obsolete snapshot . Deleting directory [ { } ] " , snapshotDirectory ) ; 
 - FileUtils . deleteRecursive ( new File ( snapshotDirectory ) ) ; 
 + / / Could be a non - existent directory if deletion worked on previous JVM shutdown . 
 + if ( f . exists ( ) ) 
 + { 
 + logger . warn ( " Discovered obsolete snapshot . Deleting directory [ { } ] " , snapshotDirectory ) ; 
 + FileUtils . deleteRecursive ( new File ( snapshotDirectory ) ) ; 
 + } 
 } 
 } 
 - reader . close ( ) ; 
 
 / / Only delete the old . toDelete file if we succeed in deleting all our known bad snapshots . 
 Files . delete ( Paths . get ( TODELETEFILE ) ) ; 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java 
 index cb02a8c . . 98fb556 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogReplayer . java 
 @ @ - 197 , 7 + 197 , 7 @ @ public class CommitLogReplayer 
 } 
 return end ; 
 } 
 - 
 + 
 abstract static class ReplayFilter 
 { 
 public abstract Iterable < ColumnFamily > filter ( Mutation mutation ) ; 
 @ @ - 273 , 6 + 273 , 7 @ @ public class CommitLogReplayer 
 } 
 } 
 
 + @ SuppressWarnings ( " resource " ) 
 public void recover ( File file , boolean tolerateTruncation ) throws IOException 
 { 
 CommitLogDescriptor desc = CommitLogDescriptor . fromFileName ( file . getName ( ) ) ; 
 diff - - git a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 index 148c08a . . 3c088c2 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 + + + b / src / java / org / apache / cassandra / hadoop / AbstractColumnFamilyInputFormat . java 
 @ @ - 117 , 9 + 117 , 9 @ @ public abstract class AbstractColumnFamilyInputFormat < K , Y > extends InputFormat < 
 } 
 } 
 
 - try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) ) 
 + try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( " , " ) , conf ) ; 
 + Session session = cluster . connect ( ) ) 
 { 
 - Session session = cluster . connect ( ) ; 
 Metadata metadata = session . getCluster ( ) . getMetadata ( ) ; 
 
 for ( TokenRange range : masterRangeNodes . keySet ( ) ) 
 diff - - git a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 index 14e24fb . . 84102a5 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 + + + b / src / java / org / apache / cassandra / hadoop / cql3 / CqlRecordWriter . java 
 @ @ - 43 , 7 + 43 , 7 @ @ import org . apache . hadoop . util . Progressable ; 
 / * * 
 * The < code > CqlRecordWriter < / code > maps the output & lt ; key , value & gt ; 
 * pairs to a Cassandra table . In particular , it applies the binded variables 
 - * in the value to the prepared statement , which it associates with the key , and in 
 + * in the value to the prepared statement , which it associates with the key , and in 
 * turn the responsible endpoint . 
 * 
 * < p > 
 @ @ - 112 , 11 + 112 , 11 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 this . queueSize = conf . getInt ( ColumnFamilyOutputFormat . QUEUE _ SIZE , 32 * FBUtilities . getAvailableProcessors ( ) ) ; 
 batchThreshold = conf . getLong ( ColumnFamilyOutputFormat . BATCH _ THRESHOLD , 32 ) ; 
 this . clients = new HashMap < > ( ) ; 
 + String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; 
 
 - try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ) 
 + try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ; 
 + Session client = cluster . connect ( keyspace ) ) 
 { 
 - String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; 
 - Session client = cluster . connect ( keyspace ) ; 
 ringCache = new NativeRingCache ( conf ) ; 
 if ( client ! = null ) 
 { 
 @ @ - 179 , 7 + 179 , 7 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 if ( clientException ! = null ) 
 throw clientException ; 
 } 
 - 
 + 
 / * * 
 * If the key is to be associated with a valid value , a mutation is created 
 * for it with the given table and columns . In the event the value 
 @ @ - 225 , 6 + 225 , 20 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 HadoopCompat . progress ( context ) ; 
 } 
 
 + private static void closeSession ( Session session ) 
 + { 
 + / / Close the session to satisfy to avoid warnings for the resource not being closed 
 + try 
 + { 
 + if ( session ! = null ) 
 + session . close ( ) ; 
 + } 
 + catch ( Throwable t ) 
 + { 
 + logger . warn ( " Error closing connection " , t ) ; 
 + } 
 + } 
 + 
 / * * 
 * A client that runs in a threadpool and connects to the list of endpoints for a particular 
 * range . Bound variables for keys in that range are sent to this client via a queue . 
 @ @ - 273 , 94 + 287 , 104 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 } 
 } 
 } 
 - 
 + 
 / * * 
 * Loops collecting cql binded variable values from the queue and sending to Cassandra 
 * / 
 + @ SuppressWarnings ( " resource " ) 
 public void run ( ) 
 { 
 Session session = null ; 
 - outer : 
 - while ( run | | ! queue . isEmpty ( ) ) 
 + try 
 { 
 - List < ByteBuffer > bindVariables ; 
 - try 
 + outer : 
 + while ( run | | ! queue . isEmpty ( ) ) 
 { 
 - bindVariables = queue . take ( ) ; 
 - } 
 - catch ( InterruptedException e ) 
 - { 
 - / / re - check loop condition after interrupt 
 - continue ; 
 - } 
 + List < ByteBuffer > bindVariables ; 
 + try 
 + { 
 + bindVariables = queue . take ( ) ; 
 + } 
 + catch ( InterruptedException e ) 
 + { 
 + / / re - check loop condition after interrupt 
 + continue ; 
 + } 
 
 - ListIterator < InetAddress > iter = endpoints . listIterator ( ) ; 
 - while ( true ) 
 - { 
 - / / send the mutation to the last - used endpoint . first time through , this will NPE harmlessly . 
 - if ( session ! = null ) 
 + ListIterator < InetAddress > iter = endpoints . listIterator ( ) ; 
 + while ( true ) 
 { 
 - try 
 + / / send the mutation to the last - used endpoint . first time through , this will NPE harmlessly . 
 + if ( session ! = null ) 
 { 
 - int i = 0 ; 
 - PreparedStatement statement = preparedStatement ( session ) ; 
 - while ( bindVariables ! = null ) 
 + try 
 { 
 - BoundStatement boundStatement = new BoundStatement ( statement ) ; 
 - for ( int columnPosition = 0 ; columnPosition < bindVariables . size ( ) ; columnPosition + + ) 
 + int i = 0 ; 
 + PreparedStatement statement = preparedStatement ( session ) ; 
 + while ( bindVariables ! = null ) 
 { 
 - boundStatement . setBytesUnsafe ( columnPosition , bindVariables . get ( columnPosition ) ) ; 
 + BoundStatement boundStatement = new BoundStatement ( statement ) ; 
 + for ( int columnPosition = 0 ; columnPosition < bindVariables . size ( ) ; columnPosition + + ) 
 + { 
 + boundStatement . setBytesUnsafe ( columnPosition , bindVariables . get ( columnPosition ) ) ; 
 + } 
 + session . execute ( boundStatement ) ; 
 + i + + ; 
 + 
 + if ( i > = batchThreshold ) 
 + break ; 
 + bindVariables = queue . poll ( ) ; 
 } 
 - session . execute ( boundStatement ) ; 
 - i + + ; 
 - 
 - if ( i > = batchThreshold ) 
 - break ; 
 - bindVariables = queue . poll ( ) ; 
 + break ; 
 } 
 - break ; 
 + catch ( Exception e ) 
 + { 
 + closeInternal ( ) ; 
 + if ( ! iter . hasNext ( ) ) 
 + { 
 + lastException = new IOException ( e ) ; 
 + break outer ; 
 + } 
 + } 
 + } 
 + 
 + / / attempt to connect to a different endpoint 
 + try 
 + { 
 + InetAddress address = iter . next ( ) ; 
 + String host = address . getHostName ( ) ; 
 + cluster = CqlConfigHelper . getOutputCluster ( host , conf ) ; 
 + closeSession ( session ) ; 
 + session = cluster . connect ( ) ; 
 } 
 catch ( Exception e ) 
 { 
 + / / If connection died due to Interrupt , just try connecting to the endpoint again . 
 + / / There are too many ways for the Thread . interrupted ( ) state to be cleared , so 
 + / / we can ' t rely on that here . Until the java driver gives us a better way of knowing 
 + / / that this exception came from an InterruptedException , this is the best solution . 
 + if ( canRetryDriverConnection ( e ) ) 
 + { 
 + iter . previous ( ) ; 
 + } 
 closeInternal ( ) ; 
 - if ( ! iter . hasNext ( ) ) 
 + 
 + / / Most exceptions mean something unexpected went wrong to that endpoint , so 
 + / / we should try again to another . Other exceptions ( auth or invalid request ) are fatal . 
 + if ( ( e instanceof AuthenticationException | | e instanceof InvalidQueryException ) | | ! iter . hasNext ( ) ) 
 { 
 lastException = new IOException ( e ) ; 
 break outer ; 
 } 
 - } 
 - } 
 - 
 - / / attempt to connect to a different endpoint 
 - try 
 - { 
 - InetAddress address = iter . next ( ) ; 
 - String host = address . getHostName ( ) ; 
 - cluster = CqlConfigHelper . getOutputCluster ( host , conf ) ; 
 - session = cluster . connect ( ) ; 
 - } 
 - catch ( Exception e ) 
 - { 
 - / / If connection died due to Interrupt , just try connecting to the endpoint again . 
 - / / There are too many ways for the Thread . interrupted ( ) state to be cleared , so 
 - / / we can ' t rely on that here . Until the java driver gives us a better way of knowing 
 - / / that this exception came from an InterruptedException , this is the best solution . 
 - if ( canRetryDriverConnection ( e ) ) 
 - { 
 - iter . previous ( ) ; 
 - } 
 - closeInternal ( ) ; 
 - 
 - / / Most exceptions mean something unexpected went wrong to that endpoint , so 
 - / / we should try again to another . Other exceptions ( auth or invalid request ) are fatal . 
 - if ( ( e instanceof AuthenticationException | | e instanceof InvalidQueryException ) | | ! iter . hasNext ( ) ) 
 - { 
 - lastException = new IOException ( e ) ; 
 - break outer ; 
 } 
 } 
 } 
 } 
 + finally 
 + { 
 + closeSession ( session ) ; 
 + } 
 + 
 / / close all our connections once we are done . 
 closeInternal ( ) ; 
 } 
 @ @ - 489 , 9 + 513 , 9 @ @ class CqlRecordWriter extends RecordWriter < Map < String , ByteBuffer > , List < ByteBuf 
 private void refreshEndpointMap ( ) 
 { 
 String keyspace = ConfigHelper . getOutputKeyspace ( conf ) ; 
 - try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ) 
 + try ( Cluster cluster = CqlConfigHelper . getOutputCluster ( ConfigHelper . getOutputInitialAddress ( conf ) , conf ) ; 
 + Session session = cluster . connect ( keyspace ) ) 
 { 
 - Session session = cluster . connect ( keyspace ) ; 
 rangeMap = new HashMap < > ( ) ; 
 metadata = session . getCluster ( ) . getMetadata ( ) ; 
 Set < TokenRange > ranges = metadata . getTokenRanges ( ) ; 
 diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java 
 index 74058b1 . . 8831cf2 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java 
 + + + b / src / java / org / apache / cassandra / hadoop / pig / CqlNativeStorage . java 
 @ @ - 690 , 7 + 690 , 7 @ @ public class CqlNativeStorage extends LoadFunc implements StoreFuncInterface , Lo 
 else 
 throw new IOException ( " bulk _ insert _ statement is missing in input url parameter " ) ; 
 if ( bulkTableAlias ! = null ) 
 - CqlBulkOutputFormat . setTableAlias ( conf , bulkTableAlias , column _ family ) ; 
 + CqlBulkOutputFormat . setTableAlias ( conf , bulkTableAlias , column _ family ) ; 
 CqlBulkOutputFormat . setDeleteSourceOnSuccess ( conf , bulkDeleteSourceOnSuccess ) ; 
 if ( bulkOutputLocation ! = null ) 
 conf . set ( CqlBulkRecordWriter . OUTPUT _ LOCATION , bulkOutputLocation ) ; 
 @ @ - 724 , 9 + 724 , 9 @ @ public class CqlNativeStorage extends LoadFunc implements StoreFuncInterface , Lo 
 / / Only get the schema if we haven ' t already gotten it 
 if ( ! properties . containsKey ( signature ) ) 
 { 
 - try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) , conf ) ) 
 + try ( Cluster cluster = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) , conf ) ; 
 + Session client = cluster . connect ( ) ) 
 { 
 - Session client = cluster . connect ( ) ; 
 client . execute ( " USE " + keyspace ) ; 
 
 / / compose the CfDef for the columfamily 
 diff - - git a / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java b / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java 
 index 442236d . . 9015b61 100644 
 - - - a / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java 
 + + + b / src / java / org / apache / cassandra / io / util / ChecksummedRandomAccessReader . java 
 @ @ - 48 , 15 + 48 , 36 @ @ public class ChecksummedRandomAccessReader extends RandomAccessReader 
 this . file = file ; 
 } 
 
 + @ SuppressWarnings ( " resource " ) 
 public static ChecksummedRandomAccessReader open ( File file , File crcFile ) throws IOException 
 { 
 try ( ChannelProxy channel = new ChannelProxy ( file ) ) 
 { 
 RandomAccessReader crcReader = RandomAccessReader . open ( crcFile ) ; 
 - @ SuppressWarnings ( " resource " ) 
 - DataIntegrityMetadata . ChecksumValidator validator = 
 - new DataIntegrityMetadata . ChecksumValidator ( new Adler32 ( ) , crcReader , file . getPath ( ) ) ; 
 - return new ChecksummedRandomAccessReader ( file , channel , validator ) ; 
 + boolean closeCrcReader = true ; 
 + try 
 + { 
 + DataIntegrityMetadata . ChecksumValidator validator = 
 + new DataIntegrityMetadata . ChecksumValidator ( new Adler32 ( ) , crcReader , file . getPath ( ) ) ; 
 + closeCrcReader = false ; 
 + boolean closeValidator = true ; 
 + try 
 + { 
 + ChecksummedRandomAccessReader retval = new ChecksummedRandomAccessReader ( file , channel , validator ) ; 
 + closeValidator = false ; 
 + return retval ; 
 + } 
 + finally 
 + { 
 + if ( closeValidator ) 
 + validator . close ( ) ; 
 + } 
 + } 
 + finally 
 + { 
 + if ( closeCrcReader ) 
 + crcReader . close ( ) ; 
 + } 
 } 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / io / util / SegmentedFile . java b / src / java / org / apache / cassandra / io / util / SegmentedFile . java 
 index 30707d8 . . 553cc0d 100644 
 - - - a / src / java / org / apache / cassandra / io / util / SegmentedFile . java 
 + + + b / src / java / org / apache / cassandra / io / util / SegmentedFile . java 
 @ @ - 179 , 6 + 179 , 7 @ @ public abstract class SegmentedFile extends SharedCloseableImpl 
 return complete ( path , - 1L ) ; 
 } 
 
 + @ SuppressWarnings ( " resource " ) 
 public SegmentedFile complete ( String path , long overrideLength ) 
 { 
 ChannelProxy channelCopy = getChannel ( path ) ; 
 diff - - git a / src / java / org / apache / cassandra / net / IncomingTcpConnection . java b / src / java / org / apache / cassandra / net / IncomingTcpConnection . java 
 index f6652b0 . . a972114 100644 
 - - - a / src / java / org / apache / cassandra / net / IncomingTcpConnection . java 
 + + + b / src / java / org / apache / cassandra / net / IncomingTcpConnection . java 
 @ @ - 108 , 7 + 108 , 7 @ @ public class IncomingTcpConnection extends Thread implements Closeable 
 close ( ) ; 
 } 
 } 
 - 
 + 
 @ Override 
 public void close ( ) 
 { 
 @ @ - 164 , 6 + 164 , 7 @ @ public class IncomingTcpConnection extends Thread implements Closeable 
 } 
 else 
 { 
 + @ SuppressWarnings ( " resource " ) 
 ReadableByteChannel channel = socket . getChannel ( ) ; 
 in = new NIODataInputStream ( channel ! = null ? channel : Channels . newChannel ( socket . getInputStream ( ) ) , BUFFER _ SIZE ) ; 
 }

NEAREST DIFF:
diff - - git a / drivers / py / cql / marshal . py b / drivers / py / cql / marshal . py 
 index b3b61f2 . . 31fbd29 100644 
 - - - a / drivers / py / cql / marshal . py 
 + + + b / drivers / py / cql / marshal . py 
 @ @ - 76 , 7 + 76 , 7 @ @ def unmarshal ( bytestr , typestr ) : 
 elif typestr = = " org . apache . cassandra . db . marshal . LexicalUUIDType " : 
 return UUID ( bytes = bytestr ) 
 elif typestr = = " org . apache . cassandra . db . marshal . TimeUUIDType " : 
 - return UUID ( bytes = bytetr ) 
 + return UUID ( bytes = bytestr ) 
 else : 
 return bytestr 

