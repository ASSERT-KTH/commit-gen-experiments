BLEU SCORE: 0.00863611899608393

TEST MSG: Backport CASSANDRA - 8243 to 2 . 0 - Do more aggressive ttl expiration checks to be able to drop more sstables
GENERATED MSG: Make hint delivery asynchronous

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 3df91ce . . 22a9515 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 4 + 1 , 6 @ @ <nl> 2 . 0 . 15 : <nl> + * Do more agressive ttl expiration checks to be able to <nl> + drop more sstables ( CASSANDRA - 8243 ) <nl> * IncomingTcpConnection thread is not named ( CASSANDRA - 9262 ) <nl> * Close incoming connections when MessagingService is stopped ( CASSANDRA - 9238 ) <nl> * Avoid overflow when calculating max sstable size in LCS ( CASSANDRA - 9235 ) <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionController . java b / src / java / org / apache / cassandra / db / compaction / CompactionController . java <nl> index fba659d . . 7a4b7d9 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / CompactionController . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / CompactionController . java <nl> @ @ - 78 , 12 + 78 , 11 @ @ public class CompactionController <nl> * Finds expired sstables <nl> * <nl> * works something like this ; <nl> - * 1 . find " global " minTimestamp of overlapping sstables ( excluding the possibly droppable ones ) <nl> - * 2 . build a list of candidates to be dropped <nl> - * 3 . sort the candidate list , biggest maxTimestamp first in list <nl> - * 4 . check if the candidates to be dropped actually can be dropped ( maxTimestamp < global minTimestamp ) and it is included in the compaction <nl> - * - if not droppable , update global minTimestamp and remove from candidates <nl> - * 5 . return candidates . <nl> + * 1 . find " global " minTimestamp of overlapping sstables and compacting sstables containing any non - expired data <nl> + * 2 . build a list of fully expired candidates <nl> + * 3 . check if the candidates to be dropped actually can be dropped ( maxTimestamp < global minTimestamp ) <nl> + * - if not droppable , remove from candidates <nl> + * 4 . return candidates . <nl> * <nl> * @ param cfStore <nl> * @ param compacting we take the drop - candidates from this set , it is usually the sstables included in the compaction <nl> @ @ - 113 , 10 + 112 , 11 @ @ public class CompactionController <nl> minTimestamp = Math . min ( minTimestamp , candidate . getMinTimestamp ( ) ) ; <nl> } <nl> <nl> - / / we still need to keep candidates that might shadow something in a <nl> - / / non - candidate sstable . And if we remove a sstable from the candidates , we <nl> - / / must take it ' s timestamp into account ( hence the sorting below ) . <nl> Collections . sort ( candidates , SSTable . maxTimestampComparator ) ; <nl> + / / At this point , minTimestamp denotes the lowest timestamp of any relevant <nl> + / / SSTable that contains a constructive value . candidates contains all the <nl> + / / candidates with no constructive values . The ones out of these that have <nl> + / / ( getMaxTimestamp ( ) < minTimestamp ) serve no purpose anymore . <nl> <nl> Iterator < SSTableReader > iterator = candidates . iterator ( ) ; <nl> while ( iterator . hasNext ( ) ) <nl> @ @ - 124 , 7 + 124 , 6 @ @ public class CompactionController <nl> SSTableReader candidate = iterator . next ( ) ; <nl> if ( candidate . getMaxTimestamp ( ) > = minTimestamp ) <nl> { <nl> - minTimestamp = Math . min ( candidate . getMinTimestamp ( ) , minTimestamp ) ; <nl> iterator . remove ( ) ; <nl> } <nl> else <nl> diff - - git a / test / unit / org / apache / cassandra / db / compaction / TTLExpiryTest . java b / test / unit / org / apache / cassandra / db / compaction / TTLExpiryTest . java <nl> index 7666922 . . 3fad0ec 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / compaction / TTLExpiryTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / compaction / TTLExpiryTest . java <nl> @ @ - 22 , 7 + 22 , 11 @ @ package org . apache . cassandra . db . compaction ; <nl> <nl> <nl> <nl> + import java . util . Collections ; <nl> + import java . util . Set ; <nl> import java . util . concurrent . ExecutionException ; <nl> + <nl> + import com . google . common . collect . Sets ; <nl> import org . junit . Test ; <nl> import org . junit . runner . RunWith ; <nl> <nl> @ @ - 34 , 10 + 38 , 7 @ @ import org . apache . cassandra . db . DataRange ; <nl> import org . apache . cassandra . db . DecoratedKey ; <nl> import org . apache . cassandra . db . Keyspace ; <nl> import org . apache . cassandra . db . RowMutation ; <nl> - import org . apache . cassandra . db . RowPosition ; <nl> - import org . apache . cassandra . db . columniterator . IdentityQueryFilter ; <nl> import org . apache . cassandra . db . columniterator . OnDiskAtomIterator ; <nl> - import org . apache . cassandra . db . filter . QueryFilter ; <nl> import org . apache . cassandra . io . sstable . SSTableReader ; <nl> import org . apache . cassandra . io . sstable . SSTableScanner ; <nl> import org . apache . cassandra . utils . ByteBufferUtil ; <nl> @ @ - 149 , 4 + 150 , 51 @ @ public class TTLExpiryTest extends SchemaLoader <nl> assertEquals ( noTTLKey , iter . getKey ( ) ) ; <nl> } <nl> } <nl> + <nl> + @ Test <nl> + public void testAggressiveFullyExpired ( ) <nl> + { <nl> + String KEYSPACE1 = " Keyspace1 " ; <nl> + ColumnFamilyStore cfs = Keyspace . open ( " Keyspace1 " ) . getColumnFamilyStore ( " Standard1 " ) ; <nl> + cfs . disableAutoCompaction ( ) ; <nl> + cfs . metadata . gcGraceSeconds ( 0 ) ; <nl> + <nl> + DecoratedKey ttlKey = Util . dk ( " ttl " ) ; <nl> + RowMutation rm = new RowMutation ( " Keyspace1 " , ttlKey . key ) ; <nl> + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col1 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 1 , 1 ) ; <nl> + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col2 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 3 , 1 ) ; <nl> + rm . applyUnsafe ( ) ; <nl> + cfs . forceBlockingFlush ( ) ; <nl> + <nl> + rm = new RowMutation ( KEYSPACE1 , ttlKey . key ) ; <nl> + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col1 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 2 , 1 ) ; <nl> + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col2 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 5 , 1 ) ; <nl> + rm . applyUnsafe ( ) ; <nl> + cfs . forceBlockingFlush ( ) ; <nl> + <nl> + rm = new RowMutation ( KEYSPACE1 , ttlKey . key ) ; <nl> + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col1 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 4 , 1 ) ; <nl> + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " shadow " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 7 , 1 ) ; <nl> + rm . applyUnsafe ( ) ; <nl> + cfs . forceBlockingFlush ( ) ; <nl> + <nl> + rm = new RowMutation ( KEYSPACE1 , ttlKey . key ) ; <nl> + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " shadow " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 6 , 3 ) ; <nl> + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col2 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 8 , 1 ) ; <nl> + rm . applyUnsafe ( ) ; <nl> + cfs . forceBlockingFlush ( ) ; <nl> + <nl> + Set < SSTableReader > sstables = Sets . newHashSet ( cfs . getSSTables ( ) ) ; <nl> + int now = ( int ) ( System . currentTimeMillis ( ) / 1000 ) ; <nl> + int gcBefore = now + 2 ; <nl> + Set < SSTableReader > expired = CompactionController . getFullyExpiredSSTables ( <nl> + cfs , <nl> + sstables , <nl> + Collections . EMPTY _ SET , <nl> + gcBefore ) ; <nl> + assertEquals ( 2 , expired . size ( ) ) ; <nl> + <nl> + cfs . clearUnsafe ( ) ; <nl> + } <nl> + <nl> }
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java <nl> index 49d8eac . . 8f539a9 100644 <nl> - - - a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java <nl> + + + b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java <nl> @ @ - 83 , 8 + 83 , 6 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo <nl> <nl> private final static String DEFAULT _ INPUT _ FORMAT = " org . apache . cassandra . hadoop . ColumnFamilyInputFormat " ; <nl> private final static String DEFAULT _ OUTPUT _ FORMAT = " org . apache . cassandra . hadoop . ColumnFamilyOutputFormat " ; <nl> - private final static boolean DEFAULT _ WIDEROW _ INPUT = false ; <nl> - private final static boolean DEFAULT _ USE _ SECONDARY = false ; <nl> <nl> private final static String PARTITION _ FILTER _ SIGNATURE = " cassandra . partition . filter " ; <nl> <nl> @ @ - 106 , 8 + 104 , 8 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo <nl> private String inputFormatClass ; <nl> private String outputFormatClass ; <nl> private int limit ; <nl> - private boolean widerows ; <nl> - private boolean usePartitionFilter ; <nl> + private boolean widerows = false ; <nl> + private boolean usePartitionFilter = false ; <nl> / / wide row hacks <nl> private ByteBuffer lastKey ; <nl> private Map < ByteBuffer , IColumn > lastRow ; <nl> @ @ - 567 , 11 + 565 , 9 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo <nl> SlicePredicate predicate = new SlicePredicate ( ) . setSlice _ range ( range ) ; <nl> ConfigHelper . setInputSlicePredicate ( conf , predicate ) ; <nl> } <nl> - widerows = DEFAULT _ WIDEROW _ INPUT ; <nl> if ( System . getenv ( PIG _ WIDEROW _ INPUT ) ! = null ) <nl> - widerows = Boolean . valueOf ( System . getProperty ( PIG _ WIDEROW _ INPUT ) ) ; <nl> - usePartitionFilter = DEFAULT _ USE _ SECONDARY ; <nl> - if ( System . getenv ( ) ! = null ) <nl> + widerows = Boolean . valueOf ( System . getenv ( PIG _ WIDEROW _ INPUT ) ) ; <nl> + if ( System . getenv ( PIG _ USE _ SECONDARY ) ! = null ) <nl> usePartitionFilter = Boolean . valueOf ( System . getenv ( PIG _ USE _ SECONDARY ) ) ; <nl> <nl> if ( usePartitionFilter & & getIndexExpressions ( ) ! = null ) <nl> @ @ - 815 , 8 + 811 , 7 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo <nl> throw new IOException ( " PIG _ OUTPUT _ PARTITIONER or PIG _ PARTITIONER environment variable not set " ) ; <nl> <nl> / / we have to do this again here for the check in writeColumnsFromTuple <nl> - usePartitionFilter = DEFAULT _ USE _ SECONDARY ; <nl> - if ( System . getenv ( ) ! = null ) <nl> + if ( System . getenv ( PIG _ USE _ SECONDARY ) ! = null ) <nl> usePartitionFilter = Boolean . valueOf ( System . getenv ( PIG _ USE _ SECONDARY ) ) ; <nl> <nl> initSchema ( storeSignature ) ;

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 3df91ce . . 22a9515 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 4 + 1 , 6 @ @ 
 2 . 0 . 15 : 
 + * Do more agressive ttl expiration checks to be able to 
 + drop more sstables ( CASSANDRA - 8243 ) 
 * IncomingTcpConnection thread is not named ( CASSANDRA - 9262 ) 
 * Close incoming connections when MessagingService is stopped ( CASSANDRA - 9238 ) 
 * Avoid overflow when calculating max sstable size in LCS ( CASSANDRA - 9235 ) 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionController . java b / src / java / org / apache / cassandra / db / compaction / CompactionController . java 
 index fba659d . . 7a4b7d9 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / CompactionController . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / CompactionController . java 
 @ @ - 78 , 12 + 78 , 11 @ @ public class CompactionController 
 * Finds expired sstables 
 * 
 * works something like this ; 
 - * 1 . find " global " minTimestamp of overlapping sstables ( excluding the possibly droppable ones ) 
 - * 2 . build a list of candidates to be dropped 
 - * 3 . sort the candidate list , biggest maxTimestamp first in list 
 - * 4 . check if the candidates to be dropped actually can be dropped ( maxTimestamp < global minTimestamp ) and it is included in the compaction 
 - * - if not droppable , update global minTimestamp and remove from candidates 
 - * 5 . return candidates . 
 + * 1 . find " global " minTimestamp of overlapping sstables and compacting sstables containing any non - expired data 
 + * 2 . build a list of fully expired candidates 
 + * 3 . check if the candidates to be dropped actually can be dropped ( maxTimestamp < global minTimestamp ) 
 + * - if not droppable , remove from candidates 
 + * 4 . return candidates . 
 * 
 * @ param cfStore 
 * @ param compacting we take the drop - candidates from this set , it is usually the sstables included in the compaction 
 @ @ - 113 , 10 + 112 , 11 @ @ public class CompactionController 
 minTimestamp = Math . min ( minTimestamp , candidate . getMinTimestamp ( ) ) ; 
 } 
 
 - / / we still need to keep candidates that might shadow something in a 
 - / / non - candidate sstable . And if we remove a sstable from the candidates , we 
 - / / must take it ' s timestamp into account ( hence the sorting below ) . 
 Collections . sort ( candidates , SSTable . maxTimestampComparator ) ; 
 + / / At this point , minTimestamp denotes the lowest timestamp of any relevant 
 + / / SSTable that contains a constructive value . candidates contains all the 
 + / / candidates with no constructive values . The ones out of these that have 
 + / / ( getMaxTimestamp ( ) < minTimestamp ) serve no purpose anymore . 
 
 Iterator < SSTableReader > iterator = candidates . iterator ( ) ; 
 while ( iterator . hasNext ( ) ) 
 @ @ - 124 , 7 + 124 , 6 @ @ public class CompactionController 
 SSTableReader candidate = iterator . next ( ) ; 
 if ( candidate . getMaxTimestamp ( ) > = minTimestamp ) 
 { 
 - minTimestamp = Math . min ( candidate . getMinTimestamp ( ) , minTimestamp ) ; 
 iterator . remove ( ) ; 
 } 
 else 
 diff - - git a / test / unit / org / apache / cassandra / db / compaction / TTLExpiryTest . java b / test / unit / org / apache / cassandra / db / compaction / TTLExpiryTest . java 
 index 7666922 . . 3fad0ec 100644 
 - - - a / test / unit / org / apache / cassandra / db / compaction / TTLExpiryTest . java 
 + + + b / test / unit / org / apache / cassandra / db / compaction / TTLExpiryTest . java 
 @ @ - 22 , 7 + 22 , 11 @ @ package org . apache . cassandra . db . compaction ; 
 
 
 
 + import java . util . Collections ; 
 + import java . util . Set ; 
 import java . util . concurrent . ExecutionException ; 
 + 
 + import com . google . common . collect . Sets ; 
 import org . junit . Test ; 
 import org . junit . runner . RunWith ; 
 
 @ @ - 34 , 10 + 38 , 7 @ @ import org . apache . cassandra . db . DataRange ; 
 import org . apache . cassandra . db . DecoratedKey ; 
 import org . apache . cassandra . db . Keyspace ; 
 import org . apache . cassandra . db . RowMutation ; 
 - import org . apache . cassandra . db . RowPosition ; 
 - import org . apache . cassandra . db . columniterator . IdentityQueryFilter ; 
 import org . apache . cassandra . db . columniterator . OnDiskAtomIterator ; 
 - import org . apache . cassandra . db . filter . QueryFilter ; 
 import org . apache . cassandra . io . sstable . SSTableReader ; 
 import org . apache . cassandra . io . sstable . SSTableScanner ; 
 import org . apache . cassandra . utils . ByteBufferUtil ; 
 @ @ - 149 , 4 + 150 , 51 @ @ public class TTLExpiryTest extends SchemaLoader 
 assertEquals ( noTTLKey , iter . getKey ( ) ) ; 
 } 
 } 
 + 
 + @ Test 
 + public void testAggressiveFullyExpired ( ) 
 + { 
 + String KEYSPACE1 = " Keyspace1 " ; 
 + ColumnFamilyStore cfs = Keyspace . open ( " Keyspace1 " ) . getColumnFamilyStore ( " Standard1 " ) ; 
 + cfs . disableAutoCompaction ( ) ; 
 + cfs . metadata . gcGraceSeconds ( 0 ) ; 
 + 
 + DecoratedKey ttlKey = Util . dk ( " ttl " ) ; 
 + RowMutation rm = new RowMutation ( " Keyspace1 " , ttlKey . key ) ; 
 + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col1 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 1 , 1 ) ; 
 + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col2 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 3 , 1 ) ; 
 + rm . applyUnsafe ( ) ; 
 + cfs . forceBlockingFlush ( ) ; 
 + 
 + rm = new RowMutation ( KEYSPACE1 , ttlKey . key ) ; 
 + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col1 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 2 , 1 ) ; 
 + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col2 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 5 , 1 ) ; 
 + rm . applyUnsafe ( ) ; 
 + cfs . forceBlockingFlush ( ) ; 
 + 
 + rm = new RowMutation ( KEYSPACE1 , ttlKey . key ) ; 
 + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col1 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 4 , 1 ) ; 
 + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " shadow " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 7 , 1 ) ; 
 + rm . applyUnsafe ( ) ; 
 + cfs . forceBlockingFlush ( ) ; 
 + 
 + rm = new RowMutation ( KEYSPACE1 , ttlKey . key ) ; 
 + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " shadow " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 6 , 3 ) ; 
 + rm . add ( " Standard1 " , ByteBufferUtil . bytes ( " col2 " ) , ByteBufferUtil . EMPTY _ BYTE _ BUFFER , 8 , 1 ) ; 
 + rm . applyUnsafe ( ) ; 
 + cfs . forceBlockingFlush ( ) ; 
 + 
 + Set < SSTableReader > sstables = Sets . newHashSet ( cfs . getSSTables ( ) ) ; 
 + int now = ( int ) ( System . currentTimeMillis ( ) / 1000 ) ; 
 + int gcBefore = now + 2 ; 
 + Set < SSTableReader > expired = CompactionController . getFullyExpiredSSTables ( 
 + cfs , 
 + sstables , 
 + Collections . EMPTY _ SET , 
 + gcBefore ) ; 
 + assertEquals ( 2 , expired . size ( ) ) ; 
 + 
 + cfs . clearUnsafe ( ) ; 
 + } 
 + 
 }

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java 
 index 49d8eac . . 8f539a9 100644 
 - - - a / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java 
 + + + b / src / java / org / apache / cassandra / hadoop / pig / CassandraStorage . java 
 @ @ - 83 , 8 + 83 , 6 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo 
 
 private final static String DEFAULT _ INPUT _ FORMAT = " org . apache . cassandra . hadoop . ColumnFamilyInputFormat " ; 
 private final static String DEFAULT _ OUTPUT _ FORMAT = " org . apache . cassandra . hadoop . ColumnFamilyOutputFormat " ; 
 - private final static boolean DEFAULT _ WIDEROW _ INPUT = false ; 
 - private final static boolean DEFAULT _ USE _ SECONDARY = false ; 
 
 private final static String PARTITION _ FILTER _ SIGNATURE = " cassandra . partition . filter " ; 
 
 @ @ - 106 , 8 + 104 , 8 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo 
 private String inputFormatClass ; 
 private String outputFormatClass ; 
 private int limit ; 
 - private boolean widerows ; 
 - private boolean usePartitionFilter ; 
 + private boolean widerows = false ; 
 + private boolean usePartitionFilter = false ; 
 / / wide row hacks 
 private ByteBuffer lastKey ; 
 private Map < ByteBuffer , IColumn > lastRow ; 
 @ @ - 567 , 11 + 565 , 9 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo 
 SlicePredicate predicate = new SlicePredicate ( ) . setSlice _ range ( range ) ; 
 ConfigHelper . setInputSlicePredicate ( conf , predicate ) ; 
 } 
 - widerows = DEFAULT _ WIDEROW _ INPUT ; 
 if ( System . getenv ( PIG _ WIDEROW _ INPUT ) ! = null ) 
 - widerows = Boolean . valueOf ( System . getProperty ( PIG _ WIDEROW _ INPUT ) ) ; 
 - usePartitionFilter = DEFAULT _ USE _ SECONDARY ; 
 - if ( System . getenv ( ) ! = null ) 
 + widerows = Boolean . valueOf ( System . getenv ( PIG _ WIDEROW _ INPUT ) ) ; 
 + if ( System . getenv ( PIG _ USE _ SECONDARY ) ! = null ) 
 usePartitionFilter = Boolean . valueOf ( System . getenv ( PIG _ USE _ SECONDARY ) ) ; 
 
 if ( usePartitionFilter & & getIndexExpressions ( ) ! = null ) 
 @ @ - 815 , 8 + 811 , 7 @ @ public class CassandraStorage extends LoadFunc implements StoreFuncInterface , Lo 
 throw new IOException ( " PIG _ OUTPUT _ PARTITIONER or PIG _ PARTITIONER environment variable not set " ) ; 
 
 / / we have to do this again here for the check in writeColumnsFromTuple 
 - usePartitionFilter = DEFAULT _ USE _ SECONDARY ; 
 - if ( System . getenv ( ) ! = null ) 
 + if ( System . getenv ( PIG _ USE _ SECONDARY ) ! = null ) 
 usePartitionFilter = Boolean . valueOf ( System . getenv ( PIG _ USE _ SECONDARY ) ) ; 
 
 initSchema ( storeSignature ) ;
