BLEU SCORE: 0.037202894341240265

TEST MSG: CASSANDRA - 8049 follow up : only check versioning of * . db files at startup
GENERATED MSG: split commitlog header into separate file and add size checksum to mutations . patch by mdennis and jbellis for CASSANDRA - 1179

TEST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / service / StartupChecks . java b / src / java / org / apache / cassandra / service / StartupChecks . java <nl> index f9a1789 . . 9c8d071 100644 <nl> - - - a / src / java / org / apache / cassandra / service / StartupChecks . java <nl> + + + b / src / java / org / apache / cassandra / service / StartupChecks . java <nl> @ @ - 225 , 6 + 225 , 9 @ @ public class StartupChecks <nl> { <nl> public FileVisitResult visitFile ( Path file , BasicFileAttributes attrs ) throws IOException <nl> { <nl> + if ( ! file . toString ( ) . endsWith ( " . db " ) ) <nl> + return FileVisitResult . CONTINUE ; <nl> + <nl> try <nl> { <nl> if ( ! Descriptor . fromFilename ( file . toString ( ) ) . isCompatible ( ) ) <nl> diff - - git a / test / data / invalid - legacy - sstables / Keyspace1 / Standard1 / legacyleveled . json b / test / data / invalid - legacy - sstables / Keyspace1 / Standard1 / legacyleveled . json <nl> new file mode 100644 <nl> index 0000000 . . 1fc9c01 <nl> - - - / dev / null <nl> + + + b / test / data / invalid - legacy - sstables / Keyspace1 / Standard1 / legacyleveled . json <nl> @ @ - 0 , 0 + 1 , 27 @ @ <nl> + { <nl> + " generations " : [ { <nl> + " generation " : 0 , <nl> + " members " : [ 0 ] <nl> + } , { <nl> + " generation " : 1 , <nl> + " members " : [ 1 ] <nl> + } , { <nl> + " generation " : 2 , <nl> + " members " : [ 2 ] <nl> + } , { <nl> + " generation " : 3 , <nl> + " members " : [ ] <nl> + } , { <nl> + " generation " : 4 , <nl> + " members " : [ ] <nl> + } , { <nl> + " generation " : 5 , <nl> + " members " : [ ] <nl> + } , { <nl> + " generation " : 6 , <nl> + " members " : [ ] <nl> + } , { <nl> + " generation " : 7 , <nl> + " members " : [ ] <nl> + } ] <nl> + } <nl> \ No newline at end of file <nl> diff - - git a / test / unit / org / apache / cassandra / service / StartupChecksTest . java b / test / unit / org / apache / cassandra / service / StartupChecksTest . java <nl> index 834191a . . d32b1b1 100644 <nl> - - - a / test / unit / org / apache / cassandra / service / StartupChecksTest . java <nl> + + + b / test / unit / org / apache / cassandra / service / StartupChecksTest . java <nl> @ @ - 23 , 9 + 23 , 7 @ @ import java . nio . file . Files ; <nl> import java . nio . file . Path ; <nl> import java . nio . file . Paths ; <nl> <nl> - import org . junit . Before ; <nl> - import org . junit . BeforeClass ; <nl> - import org . junit . Test ; <nl> + import org . junit . * ; <nl> <nl> import org . apache . cassandra . SchemaLoader ; <nl> import org . apache . cassandra . config . DatabaseDescriptor ; <nl> @ @ - 33 , 6 + 31 , 7 @ @ import org . apache . cassandra . db . * ; <nl> import org . apache . cassandra . exceptions . StartupException ; <nl> import org . apache . cassandra . io . util . FileUtils ; <nl> <nl> + import static org . junit . Assert . assertFalse ; <nl> import static org . junit . Assert . assertTrue ; <nl> import static org . junit . Assert . fail ; <nl> <nl> @ @ - 40 , 6 + 39 , 7 @ @ public class StartupChecksTest <nl> { <nl> public static final String INVALID _ LEGACY _ SSTABLE _ ROOT _ PROP = " invalid - legacy - sstable - root " ; <nl> StartupChecks startupChecks ; <nl> + Path sstableDir ; <nl> <nl> @ BeforeClass <nl> public static void setupServer ( ) <nl> @ @ - 48 , 24 + 48 , 31 @ @ public class StartupChecksTest <nl> } <nl> <nl> @ Before <nl> - public void setup ( ) <nl> + public void setup ( ) throws IOException <nl> { <nl> for ( ColumnFamilyStore cfs : Keyspace . open ( SystemKeyspace . NAME ) . getColumnFamilyStores ( ) ) <nl> cfs . clearUnsafe ( ) ; <nl> for ( File dataDir : Directories . getKSChildDirectories ( SystemKeyspace . NAME ) ) <nl> FileUtils . deleteRecursive ( dataDir ) ; <nl> <nl> + File dataDir = new File ( DatabaseDescriptor . getAllDataFileLocations ( ) [ 0 ] ) ; <nl> + sstableDir = Paths . get ( dataDir . getAbsolutePath ( ) , " Keyspace1 " , " Standard1 " ) ; <nl> + Files . createDirectories ( sstableDir ) ; <nl> + <nl> startupChecks = new StartupChecks ( ) ; <nl> } <nl> <nl> + @ After <nl> + public void tearDown ( ) throws IOException <nl> + { <nl> + FileUtils . deleteRecursive ( sstableDir . toFile ( ) ) ; <nl> + } <nl> + <nl> @ Test <nl> public void failStartupIfInvalidSSTablesFound ( ) throws Exception <nl> { <nl> startupChecks = startupChecks . withTest ( StartupChecks . checkSSTablesFormat ) ; <nl> <nl> - File dataDir = new File ( DatabaseDescriptor . getAllDataFileLocations ( ) [ 0 ] ) ; <nl> - Path sstableDir = Paths . get ( dataDir . getAbsolutePath ( ) , " Keyspace1 " , " Standard1 " ) ; <nl> - Files . createDirectories ( sstableDir ) ; <nl> copyInvalidLegacySSTables ( sstableDir ) ; <nl> <nl> verifyFailure ( startupChecks , " Detected unreadable sstables " ) ; <nl> @ @ - 84 , 6 + 91 , 29 @ @ public class StartupChecksTest <nl> startupChecks . verify ( ) ; <nl> } <nl> <nl> + @ Test <nl> + public void compatibilityCheckIgnoresNonDbFiles ( ) throws Exception <nl> + { <nl> + startupChecks = startupChecks . withTest ( StartupChecks . checkSSTablesFormat ) ; <nl> + <nl> + copyLegacyNonSSTableFiles ( sstableDir ) ; <nl> + assertFalse ( sstableDir . toFile ( ) . listFiles ( ) . length = = 0 ) ; <nl> + <nl> + startupChecks . verify ( ) ; <nl> + } <nl> + <nl> + private void copyLegacyNonSSTableFiles ( Path targetDir ) throws IOException <nl> + { <nl> + <nl> + Path legacySSTableRoot = Paths . get ( System . getProperty ( INVALID _ LEGACY _ SSTABLE _ ROOT _ PROP ) , <nl> + " Keyspace1 " , <nl> + " Standard1 " ) ; <nl> + for ( String filename : new String [ ] { " Keyspace1 - Standard1 - ic - 0 - TOC . txt " , <nl> + " Keyspace1 - Standard1 - ic - 0 - Digest . sha1 " , <nl> + " legacyleveled . json " } ) <nl> + Files . copy ( Paths . get ( legacySSTableRoot . toString ( ) , filename ) , targetDir . resolve ( filename ) ) ; <nl> + } <nl> + <nl> private void copyInvalidLegacySSTables ( Path targetDir ) throws IOException <nl> { <nl> File legacySSTableRoot = Paths . get ( System . getProperty ( INVALID _ LEGACY _ SSTABLE _ ROOT _ PROP ) , <nl> @ @ - 94 , 7 + 124 , 6 @ @ public class StartupChecksTest <nl> <nl> } <nl> <nl> - <nl> private void verifyFailure ( StartupChecks tests , String message ) <nl> { <nl> try
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index f716eb1 . . a7e9313 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 30 , 6 + 30 , 8 @ @ dev <nl> * avoid reading large rows into memory during compaction ( CASSANDRA - 16 ) <nl> * added hadoop OutputFormat ( CASSANDRA - 1101 ) <nl> * efficient Streaming ( no more anticompaction ) ( CASSANDRA - 579 ) <nl> + * split commitlog header into separate file and add size checksum to <nl> + mutations ( CASSANDRA - 1179 ) <nl> <nl> <nl> 0 . 6 . 3 <nl> diff - - git a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> index 94db4a2 . . ba70d71 100644 <nl> - - - a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> + + + b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java <nl> @ @ - 880 , 7 + 880 , 7 @ @ public class DatabaseDescriptor <nl> return dataFileDirectory ; <nl> } <nl> <nl> - public static String getLogFileLocation ( ) <nl> + public static String getCommitLogLocation ( ) <nl> { <nl> return conf . commitlog _ directory ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLog . java b / src / java / org / apache / cassandra / db / commitlog / CommitLog . java <nl> index 27bcab2 . . 14f07ef 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLog . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLog . java <nl> @ @ - 155 , 9 + 155 , 8 @ @ public class CommitLog <nl> <nl> public static void recover ( ) throws IOException <nl> { <nl> - String directory = DatabaseDescriptor . getLogFileLocation ( ) ; <nl> - File file = new File ( directory ) ; <nl> - File [ ] files = file . listFiles ( new FilenameFilter ( ) <nl> + String directory = DatabaseDescriptor . getCommitLogLocation ( ) ; <nl> + File [ ] files = new File ( directory ) . listFiles ( new FilenameFilter ( ) <nl> { <nl> public boolean accept ( File dir , String name ) <nl> { <nl> @ @ - 170 , 7 + 169 , 11 @ @ public class CommitLog <nl> Arrays . sort ( files , new FileUtils . FileComparator ( ) ) ; <nl> logger . info ( " Replaying " + StringUtils . join ( files , " , " ) ) ; <nl> recover ( files ) ; <nl> - FileUtils . delete ( files ) ; <nl> + for ( File f : files ) <nl> + { <nl> + FileUtils . delete ( CommitLogHeader . getHeaderPathFromSegmentPath ( f . getAbsolutePath ( ) ) ) ; / / may not actually exist <nl> + FileUtils . deleteWithConfirm ( f ) ; <nl> + } <nl> logger . info ( " Log replay complete " ) ; <nl> } <nl> <nl> @ @ - 180 , 28 + 183 , 24 @ @ public class CommitLog <nl> final AtomicInteger counter = new AtomicInteger ( 0 ) ; <nl> for ( File file : clogs ) <nl> { <nl> + CommitLogHeader clHeader = null ; <nl> int bufferSize = ( int ) Math . min ( file . length ( ) , 32 * 1024 * 1024 ) ; <nl> BufferedRandomAccessFile reader = new BufferedRandomAccessFile ( file . getAbsolutePath ( ) , " r " , bufferSize ) ; <nl> <nl> - final CommitLogHeader clHeader ; <nl> + int replayPosition = 0 ; <nl> try <nl> { <nl> - clHeader = CommitLogHeader . readCommitLogHeader ( reader ) ; <nl> + clHeader = CommitLogHeader . readCommitLogHeader ( CommitLogHeader . getHeaderPathFromSegmentPath ( file . getAbsolutePath ( ) ) ) ; <nl> + replayPosition = clHeader . getReplayPosition ( ) ; <nl> } <nl> - catch ( EOFException eofe ) <nl> + catch ( IOException ioe ) <nl> { <nl> - logger . info ( " Attempted to recover an incomplete CommitLogHeader . Everything is ok , don ' t panic . " ) ; <nl> - continue ; <nl> + logger . info ( " Attempted to read an incomplete , missing or corrupt CommitLogHeader . Everything is ok , don ' t panic . CommitLog will be replayed from the beginning " , ioe ) ; <nl> } <nl> + reader . seek ( replayPosition ) ; <nl> <nl> - / * seek to the lowest position where any CF has non - flushed data * / <nl> - int lowPos = CommitLogHeader . getLowestPosition ( clHeader ) ; <nl> - if ( lowPos = = 0 ) <nl> - break ; <nl> - <nl> - reader . seek ( lowPos ) ; <nl> if ( logger . isDebugEnabled ( ) ) <nl> - logger . debug ( " Replaying " + file + " starting at " + lowPos ) ; <nl> + logger . debug ( " Replaying " + file + " starting at " + reader . getFilePointer ( ) ) ; <nl> <nl> / * read the logs populate RowMutation and apply * / <nl> while ( ! reader . isEOF ( ) ) <nl> @ @ - 211 , 29 + 210 , 36 @ @ public class CommitLog <nl> <nl> long claimedCRC32 ; <nl> byte [ ] bytes ; <nl> + <nl> + Checksum checksum = new CRC32 ( ) ; <nl> try <nl> { <nl> - bytes = new byte [ reader . readInt ( ) ] ; / / readInt can throw EOFException too <nl> + / / any of the reads may hit EOF <nl> + int size = reader . readInt ( ) ; <nl> + long claimedSizeChecksum = reader . readLong ( ) ; <nl> + checksum . update ( size ) ; <nl> + if ( checksum . getValue ( ) ! = claimedSizeChecksum | | size < = 0 ) <nl> + break ; / / entry wasn ' t synced correctly / fully . that ' s ok . <nl> + <nl> + bytes = new byte [ size ] ; <nl> reader . readFully ( bytes ) ; <nl> claimedCRC32 = reader . readLong ( ) ; <nl> } <nl> - catch ( EOFException e ) <nl> + catch ( EOFException eof ) <nl> { <nl> - / / last CL entry didn ' t get completely written . that ' s ok . <nl> - break ; <nl> + break ; / / last CL entry didn ' t get completely written . that ' s ok . <nl> } <nl> <nl> - ByteArrayInputStream bufIn = new ByteArrayInputStream ( bytes ) ; <nl> - Checksum checksum = new CRC32 ( ) ; <nl> checksum . update ( bytes , 0 , bytes . length ) ; <nl> if ( claimedCRC32 ! = checksum . getValue ( ) ) <nl> { <nl> - / / this part of the log must not have been fsynced . probably the rest is bad too , <nl> - / / but just in case there is no harm in trying them . <nl> + / / this entry must not have been fsynced . probably the rest is bad too , <nl> + / / but just in case there is no harm in trying them ( since we still read on an entry boundary ) <nl> continue ; <nl> } <nl> <nl> / * deserialize the commit log entry * / <nl> + ByteArrayInputStream bufIn = new ByteArrayInputStream ( bytes ) ; <nl> final RowMutation rm = RowMutation . serializer ( ) . deserialize ( new DataInputStream ( bufIn ) ) ; <nl> if ( logger . isDebugEnabled ( ) ) <nl> logger . debug ( String . format ( " replaying mutation for % s . % s : % s " , <nl> @ @ - 244 , 6 + 250 , 7 @ @ public class CommitLog <nl> tablesRecovered . add ( table ) ; <nl> final Collection < ColumnFamily > columnFamilies = new ArrayList < ColumnFamily > ( rm . getColumnFamilies ( ) ) ; <nl> final long entryLocation = reader . getFilePointer ( ) ; <nl> + final CommitLogHeader finalHeader = clHeader ; <nl> Runnable runnable = new WrappedRunnable ( ) <nl> { <nl> public void runMayThrow ( ) throws IOException <nl> @ @ - 259 , 7 + 266 , 7 @ @ public class CommitLog <nl> / / null means the cf has been dropped <nl> continue ; <nl> <nl> - if ( clHeader . isDirty ( columnFamily . id ( ) ) & & entryLocation > = clHeader . getPosition ( columnFamily . id ( ) ) ) <nl> + if ( finalHeader = = null | | ( finalHeader . isDirty ( columnFamily . id ( ) ) & & entryLocation > = finalHeader . getPosition ( columnFamily . id ( ) ) ) ) <nl> newRm . add ( columnFamily ) ; <nl> } <nl> if ( ! newRm . isEmpty ( ) ) <nl> @ @ - 424 , 6 + 431 , 7 @ @ public class CommitLog <nl> { <nl> logger . info ( " Discarding obsolete commit log : " + segment ) ; <nl> segment . close ( ) ; <nl> + DeletionService . submitDelete ( segment . getHeaderPath ( ) ) ; <nl> DeletionService . submitDelete ( segment . getPath ( ) ) ; <nl> / / usually this will be the first ( remaining ) segment , but not always , if segment A contains <nl> / / writes to a CF that is unflushed but is followed by segment B whose CFs are all flushed . <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java <nl> index 50a3806 . . c321696 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java <nl> @ @ - 19 , 39 + 19 , 30 @ @ <nl> package org . apache . cassandra . db . commitlog ; <nl> <nl> import java . io . * ; <nl> - import java . nio . ByteBuffer ; <nl> import java . util . Collections ; <nl> - import java . util . Comparator ; <nl> import java . util . HashMap ; <nl> import java . util . Map ; <nl> import java . util . zip . CRC32 ; <nl> import java . util . zip . Checksum ; <nl> <nl> import org . apache . cassandra . config . CFMetaData ; <nl> - import org . apache . cassandra . io . ICompactSerializer ; <nl> - import org . apache . cassandra . io . util . BufferedRandomAccessFile ; <nl> - import org . apache . cassandra . utils . Pair ; <nl> + import org . apache . cassandra . io . ICompactSerializer2 ; <nl> <nl> - class CommitLogHeader <nl> - { <nl> - static CommitLogHeaderSerializer serializer = new CommitLogHeaderSerializer ( ) ; <nl> + public class CommitLogHeader <nl> + { <nl> + public static String getHeaderPathFromSegment ( CommitLogSegment segment ) <nl> + { <nl> + return getHeaderPathFromSegmentPath ( segment . getPath ( ) ) ; <nl> + } <nl> <nl> - static int getLowestPosition ( CommitLogHeader clheader ) <nl> + public static String getHeaderPathFromSegmentPath ( String segmentPath ) <nl> { <nl> - return clheader . lastFlushedAt . size ( ) = = 0 ? 0 : Collections . min ( clheader . lastFlushedAt . values ( ) , new Comparator < Integer > ( ) { <nl> - public int compare ( Integer o1 , Integer o2 ) <nl> - { <nl> - if ( o1 = = 0 ) <nl> - return 1 ; <nl> - else if ( o2 = = 0 ) <nl> - return - 1 ; <nl> - else <nl> - return o1 - o2 ; <nl> - } <nl> - } ) ; <nl> + return segmentPath + " . header " ; <nl> } <nl> <nl> - private Map < Integer , Integer > lastFlushedAt ; / / position at which each CF was last flushed <nl> + public static CommitLogHeaderSerializer serializer = new CommitLogHeaderSerializer ( ) ; <nl> + <nl> + private Map < Integer , Integer > cfDirtiedAt ; / / position at which each CF was last flushed <nl> private final int cfCount ; / / we keep this in case cfcount changes in the interim ( size of lastFlushedAt is not a good indication ) . <nl> <nl> CommitLogHeader ( ) <nl> @ @ - 64 , 46 + 55 , 37 @ @ class CommitLogHeader <nl> * also builds an index of position to column family <nl> * Id . <nl> * / <nl> - private CommitLogHeader ( Map < Integer , Integer > lastFlushedAt , int cfCount ) <nl> + private CommitLogHeader ( Map < Integer , Integer > cfDirtiedAt , int cfCount ) <nl> { <nl> this . cfCount = cfCount ; <nl> - this . lastFlushedAt = lastFlushedAt ; <nl> - assert lastFlushedAt . size ( ) < = cfCount ; <nl> + this . cfDirtiedAt = cfDirtiedAt ; <nl> + assert cfDirtiedAt . size ( ) < = cfCount ; <nl> } <nl> <nl> boolean isDirty ( int cfId ) <nl> { <nl> - return lastFlushedAt . containsKey ( cfId ) ; <nl> + return cfDirtiedAt . containsKey ( cfId ) ; <nl> } <nl> <nl> int getPosition ( int index ) <nl> { <nl> - Integer x = lastFlushedAt . get ( index ) ; <nl> + Integer x = cfDirtiedAt . get ( index ) ; <nl> return x = = null ? 0 : x ; <nl> } <nl> <nl> void turnOn ( int cfId , long position ) <nl> { <nl> - lastFlushedAt . put ( cfId , ( int ) position ) ; <nl> + cfDirtiedAt . put ( cfId , ( int ) position ) ; <nl> } <nl> <nl> void turnOff ( int cfId ) <nl> { <nl> - lastFlushedAt . remove ( cfId ) ; <nl> + cfDirtiedAt . remove ( cfId ) ; <nl> } <nl> <nl> boolean isSafeToDelete ( ) throws IOException <nl> { <nl> - return lastFlushedAt . isEmpty ( ) ; <nl> - } <nl> - <nl> - byte [ ] toByteArray ( ) throws IOException <nl> - { <nl> - ByteArrayOutputStream bos = new ByteArrayOutputStream ( ) ; <nl> - DataOutputStream dos = new DataOutputStream ( bos ) ; <nl> - serializer . serialize ( this , dos ) ; <nl> - dos . flush ( ) ; <nl> - return bos . toByteArray ( ) ; <nl> + return cfDirtiedAt . isEmpty ( ) ; <nl> } <nl> <nl> / / we use cf ids . getting the cf names would be pretty pretty expensive . <nl> @ @ - 111 , 7 + 93 , 7 @ @ class CommitLogHeader <nl> { <nl> StringBuilder sb = new StringBuilder ( " " ) ; <nl> sb . append ( " CLH ( dirty + flushed = { " ) ; <nl> - for ( Map . Entry < Integer , Integer > entry : lastFlushedAt . entrySet ( ) ) <nl> + for ( Map . Entry < Integer , Integer > entry : cfDirtiedAt . entrySet ( ) ) <nl> { <nl> sb . append ( entry . getKey ( ) ) . append ( " : " ) . append ( entry . getValue ( ) ) . append ( " , " ) ; <nl> } <nl> @ @ - 122 , 36 + 104 , 67 @ @ class CommitLogHeader <nl> public String dirtyString ( ) <nl> { <nl> StringBuilder sb = new StringBuilder ( ) ; <nl> - for ( Map . Entry < Integer , Integer > entry : lastFlushedAt . entrySet ( ) ) <nl> + for ( Map . Entry < Integer , Integer > entry : cfDirtiedAt . entrySet ( ) ) <nl> sb . append ( entry . getKey ( ) ) . append ( " , " ) ; <nl> return sb . toString ( ) ; <nl> } <nl> <nl> - static CommitLogHeader readCommitLogHeader ( BufferedRandomAccessFile logReader ) throws IOException <nl> + static void writeCommitLogHeader ( CommitLogHeader header , String headerFile ) throws IOException <nl> + { <nl> + DataOutputStream out = null ; <nl> + try <nl> + { <nl> + / * <nl> + * FileOutputStream doesn ' t sync on flush / close . <nl> + * As headers are " optional " now there is no reason to sync it . <nl> + * This provides nearly double the performance of BRAF , more under heavey load . <nl> + * / <nl> + out = new DataOutputStream ( new FileOutputStream ( headerFile ) ) ; <nl> + serializer . serialize ( header , out ) ; <nl> + } <nl> + finally <nl> + { <nl> + if ( out ! = null ) <nl> + out . close ( ) ; <nl> + } <nl> + } <nl> + <nl> + static CommitLogHeader readCommitLogHeader ( String headerFile ) throws IOException <nl> + { <nl> + DataInputStream reader = null ; <nl> + try <nl> + { <nl> + reader = new DataInputStream ( new FileInputStream ( headerFile ) ) ; <nl> + return serializer . deserialize ( reader ) ; <nl> + } <nl> + finally <nl> + { <nl> + if ( reader ! = null ) <nl> + reader . close ( ) ; <nl> + } <nl> + } <nl> + <nl> + int getReplayPosition ( ) <nl> { <nl> - int statedSize = logReader . readInt ( ) ; <nl> - byte [ ] bytes = new byte [ statedSize ] ; <nl> - logReader . readFully ( bytes ) ; <nl> - ByteArrayInputStream byteStream = new ByteArrayInputStream ( bytes ) ; <nl> - return serializer . deserialize ( new DataInputStream ( byteStream ) ) ; <nl> + return cfDirtiedAt . isEmpty ( ) ? 0 : Collections . min ( cfDirtiedAt . values ( ) ) ; <nl> } <nl> <nl> - static class CommitLogHeaderSerializer implements ICompactSerializer < CommitLogHeader > <nl> + static class CommitLogHeaderSerializer implements ICompactSerializer2 < CommitLogHeader > <nl> { <nl> - public void serialize ( CommitLogHeader clHeader , DataOutputStream dos ) throws IOException <nl> + public void serialize ( CommitLogHeader clHeader , DataOutput dos ) throws IOException <nl> { <nl> - assert clHeader . lastFlushedAt . size ( ) < = clHeader . cfCount ; <nl> + assert clHeader . cfDirtiedAt . size ( ) < = clHeader . cfCount ; <nl> Checksum checksum = new CRC32 ( ) ; <nl> <nl> / / write the first checksum after the fixed - size part , so we won ' t read garbage lastFlushedAt data . <nl> dos . writeInt ( clHeader . cfCount ) ; / / 4 <nl> - dos . writeInt ( clHeader . lastFlushedAt . size ( ) ) ; / / 4 <nl> + dos . writeInt ( clHeader . cfDirtiedAt . size ( ) ) ; / / 4 <nl> checksum . update ( clHeader . cfCount ) ; <nl> - checksum . update ( clHeader . lastFlushedAt . size ( ) ) ; <nl> + checksum . update ( clHeader . cfDirtiedAt . size ( ) ) ; <nl> dos . writeLong ( checksum . getValue ( ) ) ; <nl> <nl> / / write the 2nd checksum after the lastflushedat map <nl> - for ( Map . Entry < Integer , Integer > entry : clHeader . lastFlushedAt . entrySet ( ) ) <nl> + for ( Map . Entry < Integer , Integer > entry : clHeader . cfDirtiedAt . entrySet ( ) ) <nl> { <nl> dos . writeInt ( entry . getKey ( ) ) ; / / 4 <nl> checksum . update ( entry . getKey ( ) ) ; <nl> @ @ - 161 , 14 + 174 , 14 @ @ class CommitLogHeader <nl> dos . writeLong ( checksum . getValue ( ) ) ; <nl> <nl> / / keep the size constant by padding for missing flushed - at entries . these do not affect checksum . <nl> - for ( int i = clHeader . lastFlushedAt . entrySet ( ) . size ( ) ; i < clHeader . cfCount ; i + + ) <nl> + for ( int i = clHeader . cfDirtiedAt . entrySet ( ) . size ( ) ; i < clHeader . cfCount ; i + + ) <nl> { <nl> dos . writeInt ( 0 ) ; <nl> dos . writeInt ( 0 ) ; <nl> } <nl> } <nl> <nl> - public CommitLogHeader deserialize ( DataInputStream dis ) throws IOException <nl> + public CommitLogHeader deserialize ( DataInput dis ) throws IOException <nl> { <nl> Checksum checksum = new CRC32 ( ) ; <nl> <nl> diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java <nl> index dcecaeb . . 94a9c44 100644 <nl> - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java <nl> + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java <nl> @ @ - 1 , 6 + 1 , 4 @ @ <nl> - package org . apache . cassandra . db . commitlog ; <nl> / * <nl> - * <nl> * Licensed to the Apache Software Foundation ( ASF ) under one <nl> * or more contributor license agreements . See the NOTICE file <nl> * distributed with this work for additional information <nl> @ @ - 20 , 6 + 18 , 7 @ @ package org . apache . cassandra . db . commitlog ; <nl> * <nl> * / <nl> <nl> + package org . apache . cassandra . db . commitlog ; <nl> <nl> import java . io . File ; <nl> import java . io . IOError ; <nl> @ @ - 27 , 15 + 26 , 13 @ @ import java . io . IOException ; <nl> import java . util . zip . CRC32 ; <nl> import java . util . zip . Checksum ; <nl> <nl> - import org . apache . cassandra . config . CFMetaData ; <nl> - <nl> import org . slf4j . Logger ; <nl> import org . slf4j . LoggerFactory ; <nl> <nl> + import org . apache . cassandra . config . CFMetaData ; <nl> import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . db . ColumnFamily ; <nl> import org . apache . cassandra . db . RowMutation ; <nl> - import org . apache . cassandra . db . Table ; <nl> import org . apache . cassandra . io . util . BufferedRandomAccessFile ; <nl> import org . apache . cassandra . io . util . DataOutputBuffer ; <nl> <nl> @ @ - 49 , 13 + 46 , 13 @ @ public class CommitLogSegment <nl> public CommitLogSegment ( ) <nl> { <nl> this . header = new CommitLogHeader ( ) ; <nl> - String logFile = DatabaseDescriptor . getLogFileLocation ( ) + File . separator + " CommitLog - " + System . currentTimeMillis ( ) + " . log " ; <nl> + String logFile = DatabaseDescriptor . getCommitLogLocation ( ) + File . separator + " CommitLog - " + System . currentTimeMillis ( ) + " . log " ; <nl> logger . info ( " Creating new commitlog segment " + logFile ) ; <nl> <nl> try <nl> { <nl> logWriter = createWriter ( logFile ) ; <nl> - writeCommitLogHeader ( header . toByteArray ( ) ) ; <nl> + writeHeader ( ) ; <nl> } <nl> catch ( IOException e ) <nl> { <nl> @ @ - 70 , 25 + 67 , 7 @ @ public class CommitLogSegment <nl> <nl> public void writeHeader ( ) throws IOException <nl> { <nl> - seekAndWriteCommitLogHeader ( header . toByteArray ( ) ) ; <nl> - } <nl> - <nl> - / * * writes header at the beginning of the file , then seeks back to current position * / <nl> - void seekAndWriteCommitLogHeader ( byte [ ] bytes ) throws IOException <nl> - { <nl> - long currentPos = logWriter . getFilePointer ( ) ; <nl> - logWriter . seek ( 0 ) ; <nl> - <nl> - writeCommitLogHeader ( bytes ) ; <nl> - <nl> - logWriter . seek ( currentPos ) ; <nl> - } <nl> - <nl> - private void writeCommitLogHeader ( byte [ ] bytes ) throws IOException <nl> - { <nl> - logWriter . writeInt ( bytes . length ) ; <nl> - logWriter . write ( bytes ) ; <nl> - logWriter . sync ( ) ; <nl> + CommitLogHeader . writeCommitLogHeader ( header , getHeaderPath ( ) ) ; <nl> } <nl> <nl> private static BufferedRandomAccessFile createWriter ( String file ) throws IOException <nl> @ @ - 121 , 29 + 100 , 30 @ @ public class CommitLogSegment <nl> if ( ! header . isDirty ( id ) ) <nl> { <nl> header . turnOn ( id , logWriter . getFilePointer ( ) ) ; <nl> - seekAndWriteCommitLogHeader ( header . toByteArray ( ) ) ; <nl> + writeHeader ( ) ; <nl> } <nl> } <nl> } <nl> <nl> - / / write mutation , w / checksum <nl> - Checksum checkum = new CRC32 ( ) ; <nl> + / / write mutation , w / checksum on the size and data <nl> + byte [ ] bytes ; <nl> + Checksum checksum = new CRC32 ( ) ; <nl> if ( serializedRow instanceof DataOutputBuffer ) <nl> { <nl> - DataOutputBuffer buffer = ( DataOutputBuffer ) serializedRow ; <nl> - logWriter . writeInt ( buffer . getLength ( ) ) ; <nl> - logWriter . write ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; <nl> - checkum . update ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; <nl> + bytes = ( ( DataOutputBuffer ) serializedRow ) . getData ( ) ; <nl> } <nl> else <nl> { <nl> assert serializedRow instanceof byte [ ] ; <nl> - byte [ ] bytes = ( byte [ ] ) serializedRow ; <nl> - logWriter . writeInt ( bytes . length ) ; <nl> - logWriter . write ( bytes ) ; <nl> - checkum . update ( bytes , 0 , bytes . length ) ; <nl> + bytes = ( byte [ ] ) serializedRow ; <nl> } <nl> - logWriter . writeLong ( checkum . getValue ( ) ) ; <nl> + <nl> + checksum . update ( bytes . length ) ; <nl> + logWriter . writeInt ( bytes . length ) ; <nl> + logWriter . writeLong ( checksum . getValue ( ) ) ; <nl> + logWriter . write ( bytes ) ; <nl> + checksum . update ( bytes , 0 , bytes . length ) ; <nl> + logWriter . writeLong ( checksum . getValue ( ) ) ; <nl> <nl> return cLogCtx ; <nl> } <nl> @ @ - 175 , 6 + 155 , 11 @ @ public class CommitLogSegment <nl> return logWriter . getPath ( ) ; <nl> } <nl> <nl> + public String getHeaderPath ( ) <nl> + { <nl> + return CommitLogHeader . getHeaderPathFromSegment ( this ) ; <nl> + } <nl> + <nl> public long length ( ) <nl> { <nl> try <nl> diff - - git a / test / unit / org / apache / cassandra / CleanupHelper . java b / test / unit / org / apache / cassandra / CleanupHelper . java <nl> index f783041 . . a908c28 100644 <nl> - - - a / test / unit / org / apache / cassandra / CleanupHelper . java <nl> + + + b / test / unit / org / apache / cassandra / CleanupHelper . java <nl> @ @ - 45 , 7 + 45 , 7 @ @ public class CleanupHelper extends SchemaLoader <nl> { <nl> / / clean up commitlog <nl> String [ ] directoryNames = { <nl> - DatabaseDescriptor . getLogFileLocation ( ) , <nl> + DatabaseDescriptor . getCommitLogLocation ( ) , <nl> } ; <nl> for ( String dirName : directoryNames ) <nl> { <nl> diff - - git a / test / unit / org / apache / cassandra / db / CommitLogTest . java b / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> index 905c7e1 . . 44a0161 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / CommitLogTest . java <nl> @ @ - 16 , 18 + 16 , 22 @ @ <nl> * specific language governing permissions and limitations <nl> * under the License . <nl> * / <nl> + <nl> package org . apache . cassandra . db ; <nl> <nl> + import java . io . * ; <nl> + import java . util . concurrent . ExecutionException ; <nl> + import java . util . zip . CRC32 ; <nl> + import java . util . zip . Checksum ; <nl> + <nl> + import org . junit . Test ; <nl> + <nl> import org . apache . cassandra . CleanupHelper ; <nl> import org . apache . cassandra . db . commitlog . CommitLog ; <nl> - import org . apache . cassandra . db . filter . QueryPath ; <nl> - import org . junit . Test ; <nl> <nl> - import java . io . File ; <nl> - import java . io . FileOutputStream ; <nl> - import java . io . IOException ; <nl> - import java . io . OutputStream ; <nl> - import java . util . concurrent . ExecutionException ; <nl> + import org . apache . cassandra . db . commitlog . CommitLogHeader ; <nl> + import org . apache . cassandra . db . filter . QueryPath ; <nl> + import org . apache . cassandra . utils . Pair ; <nl> <nl> public class CommitLogTest extends CleanupHelper <nl> { <nl> @ @ - 63 , 13 + 67 , 119 @ @ public class CommitLogTest extends CleanupHelper <nl> } <nl> <nl> @ Test <nl> - public void testRecoveryWithPartiallyWrittenHeader ( ) throws Exception <nl> + public void testRecoveryWithEmptyHeader ( ) throws Exception <nl> + { <nl> + testRecovery ( new byte [ 0 ] , new byte [ 10 ] ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testRecoveryWithShortHeader ( ) throws Exception <nl> + { <nl> + testRecovery ( new byte [ 2 ] , new byte [ 10 ] ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testRecoveryWithGarbageHeader ( ) throws Exception <nl> + { <nl> + byte [ ] garbage = new byte [ 100 ] ; <nl> + ( new java . util . Random ( ) ) . nextBytes ( garbage ) ; <nl> + testRecovery ( garbage , garbage ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testRecoveryWithEmptyLog ( ) throws Exception <nl> + { <nl> + CommitLog . recover ( new File [ ] { tmpFiles ( ) . right } ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testRecoveryWithShortLog ( ) throws Exception <nl> + { <nl> + / / force EOF while reading log <nl> + testRecoveryWithBadSizeArgument ( 100 , 10 ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testRecoveryWithShortSize ( ) throws Exception <nl> + { <nl> + testRecovery ( new byte [ 0 ] , new byte [ 2 ] ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testRecoveryWithShortCheckSum ( ) throws Exception <nl> + { <nl> + testRecovery ( new byte [ 0 ] , new byte [ 6 ] ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testRecoveryWithGarbageLog ( ) throws Exception <nl> + { <nl> + byte [ ] garbage = new byte [ 100 ] ; <nl> + ( new java . util . Random ( ) ) . nextBytes ( garbage ) ; <nl> + testRecovery ( new byte [ 0 ] , garbage ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testRecoveryWithBadSizeChecksum ( ) throws Exception <nl> + { <nl> + Checksum checksum = new CRC32 ( ) ; <nl> + checksum . update ( 100 ) ; <nl> + testRecoveryWithBadSizeArgument ( 100 , 100 , ~ checksum . getValue ( ) ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void testRecoveryWithZeroSegmentSizeArgument ( ) throws Exception <nl> + { <nl> + / / many different combinations of 4 bytes ( garbage ) will be read as zero by readInt ( ) <nl> + testRecoveryWithBadSizeArgument ( 0 , 10 ) ; / / zero size , but no EOF <nl> + } <nl> + <nl> + @ Test <nl> + public void testRecoveryWithNegativeSizeArgument ( ) throws Exception <nl> + { <nl> + / / garbage from a partial / bad flush could be read as a negative size even if there is no EOF <nl> + testRecoveryWithBadSizeArgument ( - 10 , 10 ) ; / / negative size , but no EOF <nl> + } <nl> + <nl> + protected void testRecoveryWithBadSizeArgument ( int size , int dataSize ) throws Exception <nl> + { <nl> + Checksum checksum = new CRC32 ( ) ; <nl> + checksum . update ( size ) ; <nl> + testRecoveryWithBadSizeArgument ( size , dataSize , checksum . getValue ( ) ) ; <nl> + } <nl> + <nl> + protected void testRecoveryWithBadSizeArgument ( int size , int dataSize , long checksum ) throws Exception <nl> + { <nl> + ByteArrayOutputStream out = new ByteArrayOutputStream ( ) ; <nl> + DataOutputStream dout = new DataOutputStream ( out ) ; <nl> + dout . writeInt ( size ) ; <nl> + dout . writeLong ( checksum ) ; <nl> + dout . write ( new byte [ dataSize ] ) ; <nl> + dout . close ( ) ; <nl> + testRecovery ( new byte [ 0 ] , out . toByteArray ( ) ) ; <nl> + } <nl> + <nl> + protected Pair < File , File > tmpFiles ( ) throws IOException <nl> + { <nl> + File logFile = File . createTempFile ( " testRecoveryWithPartiallyWrittenHeaderTestFile " , null ) ; <nl> + File headerFile = new File ( CommitLogHeader . getHeaderPathFromSegmentPath ( logFile . getAbsolutePath ( ) ) ) ; <nl> + logFile . deleteOnExit ( ) ; <nl> + headerFile . deleteOnExit ( ) ; <nl> + assert logFile . length ( ) = = 0 ; <nl> + assert headerFile . length ( ) = = 0 ; <nl> + return new Pair < File , File > ( headerFile , logFile ) ; <nl> + } <nl> + <nl> + protected void testRecovery ( byte [ ] headerData , byte [ ] logData ) throws Exception <nl> { <nl> - File tmpFile = File . createTempFile ( " testRecoveryWithPartiallyWrittenHeaderTestFile " , null ) ; <nl> - tmpFile . deleteOnExit ( ) ; <nl> - OutputStream out = new FileOutputStream ( tmpFile ) ; <nl> - out . write ( new byte [ 6 ] ) ; <nl> + Pair < File , File > tmpFiles = tmpFiles ( ) ; <nl> + File logFile = tmpFiles . right ; <nl> + File headerFile = tmpFiles . left ; <nl> + OutputStream lout = new FileOutputStream ( logFile ) ; <nl> + OutputStream hout = new FileOutputStream ( headerFile ) ; <nl> + lout . write ( logData ) ; <nl> + hout . write ( headerData ) ; <nl> / / statics make it annoying to test things correctly <nl> - CommitLog . instance ( ) . recover ( new File [ ] { tmpFile } ) ; / / CASSANDRA - 1119 throws on failure <nl> + CommitLog . recover ( new File [ ] { logFile } ) ; / / CASSANDRA - 1119 / CASSANDRA - 1179 throw on failure * / <nl> } <nl> } <nl> diff - - git a / test / unit / org / apache / cassandra / db / RecoveryManager3Test . java b / test / unit / org / apache / cassandra / db / RecoveryManager3Test . java <nl> new file mode 100644 <nl> index 0000000 . . 099a126 <nl> - - - / dev / null <nl> + + + b / test / unit / org / apache / cassandra / db / RecoveryManager3Test . java <nl> @ @ - 0 , 0 + 1 , 57 @ @ <nl> + package org . apache . cassandra . db ; <nl> + <nl> + import java . io . File ; <nl> + import java . io . IOException ; <nl> + import java . util . concurrent . ExecutionException ; <nl> + <nl> + import org . junit . Test ; <nl> + <nl> + import org . apache . cassandra . CleanupHelper ; <nl> + import org . apache . cassandra . Util ; <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + import org . apache . cassandra . db . commitlog . CommitLog ; <nl> + <nl> + import static org . apache . cassandra . Util . column ; <nl> + import static org . apache . cassandra . db . TableTest . assertColumns ; <nl> + <nl> + public class RecoveryManager3Test extends CleanupHelper <nl> + { <nl> + @ Test <nl> + public void testMissingHeader ( ) throws IOException , ExecutionException , InterruptedException <nl> + { <nl> + Table table1 = Table . open ( " Keyspace1 " ) ; <nl> + Table table2 = Table . open ( " Keyspace2 " ) ; <nl> + <nl> + RowMutation rm ; <nl> + DecoratedKey dk = Util . dk ( " keymulti " ) ; <nl> + ColumnFamily cf ; <nl> + <nl> + rm = new RowMutation ( " Keyspace1 " , dk . key ) ; <nl> + cf = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; <nl> + cf . addColumn ( column ( " col1 " , " val1 " , new TimestampClock ( 1L ) ) ) ; <nl> + rm . add ( cf ) ; <nl> + rm . apply ( ) ; <nl> + <nl> + rm = new RowMutation ( " Keyspace2 " , dk . key ) ; <nl> + cf = ColumnFamily . create ( " Keyspace2 " , " Standard3 " ) ; <nl> + cf . addColumn ( column ( " col2 " , " val2 " , new TimestampClock ( 1L ) ) ) ; <nl> + rm . add ( cf ) ; <nl> + rm . apply ( ) ; <nl> + <nl> + table1 . getColumnFamilyStore ( " Standard1 " ) . clearUnsafe ( ) ; <nl> + table2 . getColumnFamilyStore ( " Standard3 " ) . clearUnsafe ( ) ; <nl> + <nl> + / / nuke the header <nl> + for ( File file : new File ( DatabaseDescriptor . getCommitLogLocation ( ) ) . listFiles ( ) ) <nl> + { <nl> + if ( file . getName ( ) . endsWith ( " . header " ) ) <nl> + if ( ! file . delete ( ) ) <nl> + throw new AssertionError ( ) ; <nl> + } <nl> + <nl> + CommitLog . recover ( ) ; <nl> + <nl> + assertColumns ( Util . getColumnFamily ( table1 , dk , " Standard1 " ) , " col1 " ) ; <nl> + assertColumns ( Util . getColumnFamily ( table2 , dk , " Standard3 " ) , " col2 " ) ; <nl> + } <nl> + } <nl> diff - - git a / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java b / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java <nl> index 112c17f . . 843434f 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java <nl> @ @ - 34 , 8 + 34 , 7 @ @ import static org . apache . cassandra . db . TableTest . assertColumns ; <nl> public class RecoveryManagerTest extends CleanupHelper <nl> { <nl> @ Test <nl> - public void testNothing ( ) throws IOException { <nl> - / / TODO nothing to recover <nl> + public void testNothingToRecover ( ) throws IOException { <nl> CommitLog . recover ( ) ; <nl> } <nl> <nl> diff - - git a / test / unit / org / apache / cassandra / db / commitlog / CommitLogHeaderTest . java b / test / unit / org / apache / cassandra / db / commitlog / CommitLogHeaderTest . java <nl> index a431d0c . . 35b2ffe 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / commitlog / CommitLogHeaderTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / commitlog / CommitLogHeaderTest . java <nl> @ @ - 18 , 23 + 18 , 13 @ @ <nl> <nl> package org . apache . cassandra . db . commitlog ; <nl> <nl> - import com . google . common . collect . HashMultimap ; <nl> - import com . google . common . collect . Multimap ; <nl> - import org . apache . cassandra . SchemaLoader ; <nl> - import org . apache . cassandra . config . CFMetaData ; <nl> - import org . apache . cassandra . config . ConfigurationException ; <nl> - import org . apache . cassandra . config . DatabaseDescriptor ; <nl> - import org . apache . cassandra . utils . Pair ; <nl> - import org . junit . Before ; <nl> + import java . io . ByteArrayOutputStream ; <nl> + import java . io . DataOutputStream ; <nl> + import java . io . IOException ; <nl> + <nl> import org . junit . Test ; <nl> <nl> - import java . io . ByteArrayInputStream ; <nl> - import java . io . DataInputStream ; <nl> - import java . io . IOException ; <nl> - import java . util . Arrays ; <nl> - import java . util . Collection ; <nl> - import java . util . Collections ; <nl> - import java . util . Map ; <nl> + import org . apache . cassandra . SchemaLoader ; <nl> <nl> public class CommitLogHeaderTest extends SchemaLoader <nl> { <nl> @ @ - 43 , 41 + 33 , 42 @ @ public class CommitLogHeaderTest extends SchemaLoader <nl> public void testEmptyHeader ( ) <nl> { <nl> CommitLogHeader clh = new CommitLogHeader ( ) ; <nl> - assert CommitLogHeader . getLowestPosition ( clh ) = = 0 ; <nl> + assert clh . getReplayPosition ( ) = = 0 ; <nl> } <nl> <nl> @ Test <nl> public void lowestPositionWithZero ( ) <nl> { <nl> - / / zero should never be the lowest position unless all positions are zero . <nl> CommitLogHeader clh = new CommitLogHeader ( ) ; <nl> clh . turnOn ( 2 , 34 ) ; <nl> - assert CommitLogHeader . getLowestPosition ( clh ) = = 34 ; <nl> + assert clh . getReplayPosition ( ) = = 34 ; <nl> clh . turnOn ( 100 , 0 ) ; <nl> - assert CommitLogHeader . getLowestPosition ( clh ) = = 34 ; <nl> + assert clh . getReplayPosition ( ) = = 0 ; <nl> clh . turnOn ( 65 , 2 ) ; <nl> - assert CommitLogHeader . getLowestPosition ( clh ) = = 2 ; <nl> + assert clh . getReplayPosition ( ) = = 0 ; <nl> } <nl> <nl> @ Test <nl> public void lowestPositionEmpty ( ) <nl> { <nl> CommitLogHeader clh = new CommitLogHeader ( ) ; <nl> - assert CommitLogHeader . getLowestPosition ( clh ) = = 0 ; <nl> + assert clh . getReplayPosition ( ) = = 0 ; <nl> } <nl> <nl> @ Test <nl> public void constantSize ( ) throws IOException <nl> { <nl> - CommitLogHeader clh = new CommitLogHeader ( ) ; <nl> - clh . turnOn ( 2 , 34 ) ; <nl> - byte [ ] one = clh . toByteArray ( ) ; <nl> - <nl> - clh = new CommitLogHeader ( ) ; <nl> + CommitLogHeader clh0 = new CommitLogHeader ( ) ; <nl> + clh0 . turnOn ( 2 , 34 ) ; <nl> + ByteArrayOutputStream out0 = new ByteArrayOutputStream ( ) ; <nl> + CommitLogHeader . serializer . serialize ( clh0 , new DataOutputStream ( out0 ) ) ; <nl> + <nl> + CommitLogHeader clh1 = new CommitLogHeader ( ) ; <nl> for ( int i = 0 ; i < 5 ; i + + ) <nl> - clh . turnOn ( i , 1000 * i ) ; <nl> - byte [ ] two = clh . toByteArray ( ) ; <nl> - <nl> - assert one . length = = two . length ; <nl> + clh1 . turnOn ( i , 1000 * i ) ; <nl> + ByteArrayOutputStream out1 = new ByteArrayOutputStream ( ) ; <nl> + CommitLogHeader . serializer . serialize ( clh1 , new DataOutputStream ( out1 ) ) ; <nl> + <nl> + assert out0 . toByteArray ( ) . length = = out1 . toByteArray ( ) . length ; <nl> } <nl> }

TEST DIFF:
diff - - git a / src / java / org / apache / cassandra / service / StartupChecks . java b / src / java / org / apache / cassandra / service / StartupChecks . java 
 index f9a1789 . . 9c8d071 100644 
 - - - a / src / java / org / apache / cassandra / service / StartupChecks . java 
 + + + b / src / java / org / apache / cassandra / service / StartupChecks . java 
 @ @ - 225 , 6 + 225 , 9 @ @ public class StartupChecks 
 { 
 public FileVisitResult visitFile ( Path file , BasicFileAttributes attrs ) throws IOException 
 { 
 + if ( ! file . toString ( ) . endsWith ( " . db " ) ) 
 + return FileVisitResult . CONTINUE ; 
 + 
 try 
 { 
 if ( ! Descriptor . fromFilename ( file . toString ( ) ) . isCompatible ( ) ) 
 diff - - git a / test / data / invalid - legacy - sstables / Keyspace1 / Standard1 / legacyleveled . json b / test / data / invalid - legacy - sstables / Keyspace1 / Standard1 / legacyleveled . json 
 new file mode 100644 
 index 0000000 . . 1fc9c01 
 - - - / dev / null 
 + + + b / test / data / invalid - legacy - sstables / Keyspace1 / Standard1 / legacyleveled . json 
 @ @ - 0 , 0 + 1 , 27 @ @ 
 + { 
 + " generations " : [ { 
 + " generation " : 0 , 
 + " members " : [ 0 ] 
 + } , { 
 + " generation " : 1 , 
 + " members " : [ 1 ] 
 + } , { 
 + " generation " : 2 , 
 + " members " : [ 2 ] 
 + } , { 
 + " generation " : 3 , 
 + " members " : [ ] 
 + } , { 
 + " generation " : 4 , 
 + " members " : [ ] 
 + } , { 
 + " generation " : 5 , 
 + " members " : [ ] 
 + } , { 
 + " generation " : 6 , 
 + " members " : [ ] 
 + } , { 
 + " generation " : 7 , 
 + " members " : [ ] 
 + } ] 
 + } 
 \ No newline at end of file 
 diff - - git a / test / unit / org / apache / cassandra / service / StartupChecksTest . java b / test / unit / org / apache / cassandra / service / StartupChecksTest . java 
 index 834191a . . d32b1b1 100644 
 - - - a / test / unit / org / apache / cassandra / service / StartupChecksTest . java 
 + + + b / test / unit / org / apache / cassandra / service / StartupChecksTest . java 
 @ @ - 23 , 9 + 23 , 7 @ @ import java . nio . file . Files ; 
 import java . nio . file . Path ; 
 import java . nio . file . Paths ; 
 
 - import org . junit . Before ; 
 - import org . junit . BeforeClass ; 
 - import org . junit . Test ; 
 + import org . junit . * ; 
 
 import org . apache . cassandra . SchemaLoader ; 
 import org . apache . cassandra . config . DatabaseDescriptor ; 
 @ @ - 33 , 6 + 31 , 7 @ @ import org . apache . cassandra . db . * ; 
 import org . apache . cassandra . exceptions . StartupException ; 
 import org . apache . cassandra . io . util . FileUtils ; 
 
 + import static org . junit . Assert . assertFalse ; 
 import static org . junit . Assert . assertTrue ; 
 import static org . junit . Assert . fail ; 
 
 @ @ - 40 , 6 + 39 , 7 @ @ public class StartupChecksTest 
 { 
 public static final String INVALID _ LEGACY _ SSTABLE _ ROOT _ PROP = " invalid - legacy - sstable - root " ; 
 StartupChecks startupChecks ; 
 + Path sstableDir ; 
 
 @ BeforeClass 
 public static void setupServer ( ) 
 @ @ - 48 , 24 + 48 , 31 @ @ public class StartupChecksTest 
 } 
 
 @ Before 
 - public void setup ( ) 
 + public void setup ( ) throws IOException 
 { 
 for ( ColumnFamilyStore cfs : Keyspace . open ( SystemKeyspace . NAME ) . getColumnFamilyStores ( ) ) 
 cfs . clearUnsafe ( ) ; 
 for ( File dataDir : Directories . getKSChildDirectories ( SystemKeyspace . NAME ) ) 
 FileUtils . deleteRecursive ( dataDir ) ; 
 
 + File dataDir = new File ( DatabaseDescriptor . getAllDataFileLocations ( ) [ 0 ] ) ; 
 + sstableDir = Paths . get ( dataDir . getAbsolutePath ( ) , " Keyspace1 " , " Standard1 " ) ; 
 + Files . createDirectories ( sstableDir ) ; 
 + 
 startupChecks = new StartupChecks ( ) ; 
 } 
 
 + @ After 
 + public void tearDown ( ) throws IOException 
 + { 
 + FileUtils . deleteRecursive ( sstableDir . toFile ( ) ) ; 
 + } 
 + 
 @ Test 
 public void failStartupIfInvalidSSTablesFound ( ) throws Exception 
 { 
 startupChecks = startupChecks . withTest ( StartupChecks . checkSSTablesFormat ) ; 
 
 - File dataDir = new File ( DatabaseDescriptor . getAllDataFileLocations ( ) [ 0 ] ) ; 
 - Path sstableDir = Paths . get ( dataDir . getAbsolutePath ( ) , " Keyspace1 " , " Standard1 " ) ; 
 - Files . createDirectories ( sstableDir ) ; 
 copyInvalidLegacySSTables ( sstableDir ) ; 
 
 verifyFailure ( startupChecks , " Detected unreadable sstables " ) ; 
 @ @ - 84 , 6 + 91 , 29 @ @ public class StartupChecksTest 
 startupChecks . verify ( ) ; 
 } 
 
 + @ Test 
 + public void compatibilityCheckIgnoresNonDbFiles ( ) throws Exception 
 + { 
 + startupChecks = startupChecks . withTest ( StartupChecks . checkSSTablesFormat ) ; 
 + 
 + copyLegacyNonSSTableFiles ( sstableDir ) ; 
 + assertFalse ( sstableDir . toFile ( ) . listFiles ( ) . length = = 0 ) ; 
 + 
 + startupChecks . verify ( ) ; 
 + } 
 + 
 + private void copyLegacyNonSSTableFiles ( Path targetDir ) throws IOException 
 + { 
 + 
 + Path legacySSTableRoot = Paths . get ( System . getProperty ( INVALID _ LEGACY _ SSTABLE _ ROOT _ PROP ) , 
 + " Keyspace1 " , 
 + " Standard1 " ) ; 
 + for ( String filename : new String [ ] { " Keyspace1 - Standard1 - ic - 0 - TOC . txt " , 
 + " Keyspace1 - Standard1 - ic - 0 - Digest . sha1 " , 
 + " legacyleveled . json " } ) 
 + Files . copy ( Paths . get ( legacySSTableRoot . toString ( ) , filename ) , targetDir . resolve ( filename ) ) ; 
 + } 
 + 
 private void copyInvalidLegacySSTables ( Path targetDir ) throws IOException 
 { 
 File legacySSTableRoot = Paths . get ( System . getProperty ( INVALID _ LEGACY _ SSTABLE _ ROOT _ PROP ) , 
 @ @ - 94 , 7 + 124 , 6 @ @ public class StartupChecksTest 
 
 } 
 
 - 
 private void verifyFailure ( StartupChecks tests , String message ) 
 { 
 try

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index f716eb1 . . a7e9313 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 30 , 6 + 30 , 8 @ @ dev 
 * avoid reading large rows into memory during compaction ( CASSANDRA - 16 ) 
 * added hadoop OutputFormat ( CASSANDRA - 1101 ) 
 * efficient Streaming ( no more anticompaction ) ( CASSANDRA - 579 ) 
 + * split commitlog header into separate file and add size checksum to 
 + mutations ( CASSANDRA - 1179 ) 
 
 
 0 . 6 . 3 
 diff - - git a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 index 94db4a2 . . ba70d71 100644 
 - - - a / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 + + + b / src / java / org / apache / cassandra / config / DatabaseDescriptor . java 
 @ @ - 880 , 7 + 880 , 7 @ @ public class DatabaseDescriptor 
 return dataFileDirectory ; 
 } 
 
 - public static String getLogFileLocation ( ) 
 + public static String getCommitLogLocation ( ) 
 { 
 return conf . commitlog _ directory ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLog . java b / src / java / org / apache / cassandra / db / commitlog / CommitLog . java 
 index 27bcab2 . . 14f07ef 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLog . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLog . java 
 @ @ - 155 , 9 + 155 , 8 @ @ public class CommitLog 
 
 public static void recover ( ) throws IOException 
 { 
 - String directory = DatabaseDescriptor . getLogFileLocation ( ) ; 
 - File file = new File ( directory ) ; 
 - File [ ] files = file . listFiles ( new FilenameFilter ( ) 
 + String directory = DatabaseDescriptor . getCommitLogLocation ( ) ; 
 + File [ ] files = new File ( directory ) . listFiles ( new FilenameFilter ( ) 
 { 
 public boolean accept ( File dir , String name ) 
 { 
 @ @ - 170 , 7 + 169 , 11 @ @ public class CommitLog 
 Arrays . sort ( files , new FileUtils . FileComparator ( ) ) ; 
 logger . info ( " Replaying " + StringUtils . join ( files , " , " ) ) ; 
 recover ( files ) ; 
 - FileUtils . delete ( files ) ; 
 + for ( File f : files ) 
 + { 
 + FileUtils . delete ( CommitLogHeader . getHeaderPathFromSegmentPath ( f . getAbsolutePath ( ) ) ) ; / / may not actually exist 
 + FileUtils . deleteWithConfirm ( f ) ; 
 + } 
 logger . info ( " Log replay complete " ) ; 
 } 
 
 @ @ - 180 , 28 + 183 , 24 @ @ public class CommitLog 
 final AtomicInteger counter = new AtomicInteger ( 0 ) ; 
 for ( File file : clogs ) 
 { 
 + CommitLogHeader clHeader = null ; 
 int bufferSize = ( int ) Math . min ( file . length ( ) , 32 * 1024 * 1024 ) ; 
 BufferedRandomAccessFile reader = new BufferedRandomAccessFile ( file . getAbsolutePath ( ) , " r " , bufferSize ) ; 
 
 - final CommitLogHeader clHeader ; 
 + int replayPosition = 0 ; 
 try 
 { 
 - clHeader = CommitLogHeader . readCommitLogHeader ( reader ) ; 
 + clHeader = CommitLogHeader . readCommitLogHeader ( CommitLogHeader . getHeaderPathFromSegmentPath ( file . getAbsolutePath ( ) ) ) ; 
 + replayPosition = clHeader . getReplayPosition ( ) ; 
 } 
 - catch ( EOFException eofe ) 
 + catch ( IOException ioe ) 
 { 
 - logger . info ( " Attempted to recover an incomplete CommitLogHeader . Everything is ok , don ' t panic . " ) ; 
 - continue ; 
 + logger . info ( " Attempted to read an incomplete , missing or corrupt CommitLogHeader . Everything is ok , don ' t panic . CommitLog will be replayed from the beginning " , ioe ) ; 
 } 
 + reader . seek ( replayPosition ) ; 
 
 - / * seek to the lowest position where any CF has non - flushed data * / 
 - int lowPos = CommitLogHeader . getLowestPosition ( clHeader ) ; 
 - if ( lowPos = = 0 ) 
 - break ; 
 - 
 - reader . seek ( lowPos ) ; 
 if ( logger . isDebugEnabled ( ) ) 
 - logger . debug ( " Replaying " + file + " starting at " + lowPos ) ; 
 + logger . debug ( " Replaying " + file + " starting at " + reader . getFilePointer ( ) ) ; 
 
 / * read the logs populate RowMutation and apply * / 
 while ( ! reader . isEOF ( ) ) 
 @ @ - 211 , 29 + 210 , 36 @ @ public class CommitLog 
 
 long claimedCRC32 ; 
 byte [ ] bytes ; 
 + 
 + Checksum checksum = new CRC32 ( ) ; 
 try 
 { 
 - bytes = new byte [ reader . readInt ( ) ] ; / / readInt can throw EOFException too 
 + / / any of the reads may hit EOF 
 + int size = reader . readInt ( ) ; 
 + long claimedSizeChecksum = reader . readLong ( ) ; 
 + checksum . update ( size ) ; 
 + if ( checksum . getValue ( ) ! = claimedSizeChecksum | | size < = 0 ) 
 + break ; / / entry wasn ' t synced correctly / fully . that ' s ok . 
 + 
 + bytes = new byte [ size ] ; 
 reader . readFully ( bytes ) ; 
 claimedCRC32 = reader . readLong ( ) ; 
 } 
 - catch ( EOFException e ) 
 + catch ( EOFException eof ) 
 { 
 - / / last CL entry didn ' t get completely written . that ' s ok . 
 - break ; 
 + break ; / / last CL entry didn ' t get completely written . that ' s ok . 
 } 
 
 - ByteArrayInputStream bufIn = new ByteArrayInputStream ( bytes ) ; 
 - Checksum checksum = new CRC32 ( ) ; 
 checksum . update ( bytes , 0 , bytes . length ) ; 
 if ( claimedCRC32 ! = checksum . getValue ( ) ) 
 { 
 - / / this part of the log must not have been fsynced . probably the rest is bad too , 
 - / / but just in case there is no harm in trying them . 
 + / / this entry must not have been fsynced . probably the rest is bad too , 
 + / / but just in case there is no harm in trying them ( since we still read on an entry boundary ) 
 continue ; 
 } 
 
 / * deserialize the commit log entry * / 
 + ByteArrayInputStream bufIn = new ByteArrayInputStream ( bytes ) ; 
 final RowMutation rm = RowMutation . serializer ( ) . deserialize ( new DataInputStream ( bufIn ) ) ; 
 if ( logger . isDebugEnabled ( ) ) 
 logger . debug ( String . format ( " replaying mutation for % s . % s : % s " , 
 @ @ - 244 , 6 + 250 , 7 @ @ public class CommitLog 
 tablesRecovered . add ( table ) ; 
 final Collection < ColumnFamily > columnFamilies = new ArrayList < ColumnFamily > ( rm . getColumnFamilies ( ) ) ; 
 final long entryLocation = reader . getFilePointer ( ) ; 
 + final CommitLogHeader finalHeader = clHeader ; 
 Runnable runnable = new WrappedRunnable ( ) 
 { 
 public void runMayThrow ( ) throws IOException 
 @ @ - 259 , 7 + 266 , 7 @ @ public class CommitLog 
 / / null means the cf has been dropped 
 continue ; 
 
 - if ( clHeader . isDirty ( columnFamily . id ( ) ) & & entryLocation > = clHeader . getPosition ( columnFamily . id ( ) ) ) 
 + if ( finalHeader = = null | | ( finalHeader . isDirty ( columnFamily . id ( ) ) & & entryLocation > = finalHeader . getPosition ( columnFamily . id ( ) ) ) ) 
 newRm . add ( columnFamily ) ; 
 } 
 if ( ! newRm . isEmpty ( ) ) 
 @ @ - 424 , 6 + 431 , 7 @ @ public class CommitLog 
 { 
 logger . info ( " Discarding obsolete commit log : " + segment ) ; 
 segment . close ( ) ; 
 + DeletionService . submitDelete ( segment . getHeaderPath ( ) ) ; 
 DeletionService . submitDelete ( segment . getPath ( ) ) ; 
 / / usually this will be the first ( remaining ) segment , but not always , if segment A contains 
 / / writes to a CF that is unflushed but is followed by segment B whose CFs are all flushed . 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java 
 index 50a3806 . . c321696 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogHeader . java 
 @ @ - 19 , 39 + 19 , 30 @ @ 
 package org . apache . cassandra . db . commitlog ; 
 
 import java . io . * ; 
 - import java . nio . ByteBuffer ; 
 import java . util . Collections ; 
 - import java . util . Comparator ; 
 import java . util . HashMap ; 
 import java . util . Map ; 
 import java . util . zip . CRC32 ; 
 import java . util . zip . Checksum ; 
 
 import org . apache . cassandra . config . CFMetaData ; 
 - import org . apache . cassandra . io . ICompactSerializer ; 
 - import org . apache . cassandra . io . util . BufferedRandomAccessFile ; 
 - import org . apache . cassandra . utils . Pair ; 
 + import org . apache . cassandra . io . ICompactSerializer2 ; 
 
 - class CommitLogHeader 
 - { 
 - static CommitLogHeaderSerializer serializer = new CommitLogHeaderSerializer ( ) ; 
 + public class CommitLogHeader 
 + { 
 + public static String getHeaderPathFromSegment ( CommitLogSegment segment ) 
 + { 
 + return getHeaderPathFromSegmentPath ( segment . getPath ( ) ) ; 
 + } 
 
 - static int getLowestPosition ( CommitLogHeader clheader ) 
 + public static String getHeaderPathFromSegmentPath ( String segmentPath ) 
 { 
 - return clheader . lastFlushedAt . size ( ) = = 0 ? 0 : Collections . min ( clheader . lastFlushedAt . values ( ) , new Comparator < Integer > ( ) { 
 - public int compare ( Integer o1 , Integer o2 ) 
 - { 
 - if ( o1 = = 0 ) 
 - return 1 ; 
 - else if ( o2 = = 0 ) 
 - return - 1 ; 
 - else 
 - return o1 - o2 ; 
 - } 
 - } ) ; 
 + return segmentPath + " . header " ; 
 } 
 
 - private Map < Integer , Integer > lastFlushedAt ; / / position at which each CF was last flushed 
 + public static CommitLogHeaderSerializer serializer = new CommitLogHeaderSerializer ( ) ; 
 + 
 + private Map < Integer , Integer > cfDirtiedAt ; / / position at which each CF was last flushed 
 private final int cfCount ; / / we keep this in case cfcount changes in the interim ( size of lastFlushedAt is not a good indication ) . 
 
 CommitLogHeader ( ) 
 @ @ - 64 , 46 + 55 , 37 @ @ class CommitLogHeader 
 * also builds an index of position to column family 
 * Id . 
 * / 
 - private CommitLogHeader ( Map < Integer , Integer > lastFlushedAt , int cfCount ) 
 + private CommitLogHeader ( Map < Integer , Integer > cfDirtiedAt , int cfCount ) 
 { 
 this . cfCount = cfCount ; 
 - this . lastFlushedAt = lastFlushedAt ; 
 - assert lastFlushedAt . size ( ) < = cfCount ; 
 + this . cfDirtiedAt = cfDirtiedAt ; 
 + assert cfDirtiedAt . size ( ) < = cfCount ; 
 } 
 
 boolean isDirty ( int cfId ) 
 { 
 - return lastFlushedAt . containsKey ( cfId ) ; 
 + return cfDirtiedAt . containsKey ( cfId ) ; 
 } 
 
 int getPosition ( int index ) 
 { 
 - Integer x = lastFlushedAt . get ( index ) ; 
 + Integer x = cfDirtiedAt . get ( index ) ; 
 return x = = null ? 0 : x ; 
 } 
 
 void turnOn ( int cfId , long position ) 
 { 
 - lastFlushedAt . put ( cfId , ( int ) position ) ; 
 + cfDirtiedAt . put ( cfId , ( int ) position ) ; 
 } 
 
 void turnOff ( int cfId ) 
 { 
 - lastFlushedAt . remove ( cfId ) ; 
 + cfDirtiedAt . remove ( cfId ) ; 
 } 
 
 boolean isSafeToDelete ( ) throws IOException 
 { 
 - return lastFlushedAt . isEmpty ( ) ; 
 - } 
 - 
 - byte [ ] toByteArray ( ) throws IOException 
 - { 
 - ByteArrayOutputStream bos = new ByteArrayOutputStream ( ) ; 
 - DataOutputStream dos = new DataOutputStream ( bos ) ; 
 - serializer . serialize ( this , dos ) ; 
 - dos . flush ( ) ; 
 - return bos . toByteArray ( ) ; 
 + return cfDirtiedAt . isEmpty ( ) ; 
 } 
 
 / / we use cf ids . getting the cf names would be pretty pretty expensive . 
 @ @ - 111 , 7 + 93 , 7 @ @ class CommitLogHeader 
 { 
 StringBuilder sb = new StringBuilder ( " " ) ; 
 sb . append ( " CLH ( dirty + flushed = { " ) ; 
 - for ( Map . Entry < Integer , Integer > entry : lastFlushedAt . entrySet ( ) ) 
 + for ( Map . Entry < Integer , Integer > entry : cfDirtiedAt . entrySet ( ) ) 
 { 
 sb . append ( entry . getKey ( ) ) . append ( " : " ) . append ( entry . getValue ( ) ) . append ( " , " ) ; 
 } 
 @ @ - 122 , 36 + 104 , 67 @ @ class CommitLogHeader 
 public String dirtyString ( ) 
 { 
 StringBuilder sb = new StringBuilder ( ) ; 
 - for ( Map . Entry < Integer , Integer > entry : lastFlushedAt . entrySet ( ) ) 
 + for ( Map . Entry < Integer , Integer > entry : cfDirtiedAt . entrySet ( ) ) 
 sb . append ( entry . getKey ( ) ) . append ( " , " ) ; 
 return sb . toString ( ) ; 
 } 
 
 - static CommitLogHeader readCommitLogHeader ( BufferedRandomAccessFile logReader ) throws IOException 
 + static void writeCommitLogHeader ( CommitLogHeader header , String headerFile ) throws IOException 
 + { 
 + DataOutputStream out = null ; 
 + try 
 + { 
 + / * 
 + * FileOutputStream doesn ' t sync on flush / close . 
 + * As headers are " optional " now there is no reason to sync it . 
 + * This provides nearly double the performance of BRAF , more under heavey load . 
 + * / 
 + out = new DataOutputStream ( new FileOutputStream ( headerFile ) ) ; 
 + serializer . serialize ( header , out ) ; 
 + } 
 + finally 
 + { 
 + if ( out ! = null ) 
 + out . close ( ) ; 
 + } 
 + } 
 + 
 + static CommitLogHeader readCommitLogHeader ( String headerFile ) throws IOException 
 + { 
 + DataInputStream reader = null ; 
 + try 
 + { 
 + reader = new DataInputStream ( new FileInputStream ( headerFile ) ) ; 
 + return serializer . deserialize ( reader ) ; 
 + } 
 + finally 
 + { 
 + if ( reader ! = null ) 
 + reader . close ( ) ; 
 + } 
 + } 
 + 
 + int getReplayPosition ( ) 
 { 
 - int statedSize = logReader . readInt ( ) ; 
 - byte [ ] bytes = new byte [ statedSize ] ; 
 - logReader . readFully ( bytes ) ; 
 - ByteArrayInputStream byteStream = new ByteArrayInputStream ( bytes ) ; 
 - return serializer . deserialize ( new DataInputStream ( byteStream ) ) ; 
 + return cfDirtiedAt . isEmpty ( ) ? 0 : Collections . min ( cfDirtiedAt . values ( ) ) ; 
 } 
 
 - static class CommitLogHeaderSerializer implements ICompactSerializer < CommitLogHeader > 
 + static class CommitLogHeaderSerializer implements ICompactSerializer2 < CommitLogHeader > 
 { 
 - public void serialize ( CommitLogHeader clHeader , DataOutputStream dos ) throws IOException 
 + public void serialize ( CommitLogHeader clHeader , DataOutput dos ) throws IOException 
 { 
 - assert clHeader . lastFlushedAt . size ( ) < = clHeader . cfCount ; 
 + assert clHeader . cfDirtiedAt . size ( ) < = clHeader . cfCount ; 
 Checksum checksum = new CRC32 ( ) ; 
 
 / / write the first checksum after the fixed - size part , so we won ' t read garbage lastFlushedAt data . 
 dos . writeInt ( clHeader . cfCount ) ; / / 4 
 - dos . writeInt ( clHeader . lastFlushedAt . size ( ) ) ; / / 4 
 + dos . writeInt ( clHeader . cfDirtiedAt . size ( ) ) ; / / 4 
 checksum . update ( clHeader . cfCount ) ; 
 - checksum . update ( clHeader . lastFlushedAt . size ( ) ) ; 
 + checksum . update ( clHeader . cfDirtiedAt . size ( ) ) ; 
 dos . writeLong ( checksum . getValue ( ) ) ; 
 
 / / write the 2nd checksum after the lastflushedat map 
 - for ( Map . Entry < Integer , Integer > entry : clHeader . lastFlushedAt . entrySet ( ) ) 
 + for ( Map . Entry < Integer , Integer > entry : clHeader . cfDirtiedAt . entrySet ( ) ) 
 { 
 dos . writeInt ( entry . getKey ( ) ) ; / / 4 
 checksum . update ( entry . getKey ( ) ) ; 
 @ @ - 161 , 14 + 174 , 14 @ @ class CommitLogHeader 
 dos . writeLong ( checksum . getValue ( ) ) ; 
 
 / / keep the size constant by padding for missing flushed - at entries . these do not affect checksum . 
 - for ( int i = clHeader . lastFlushedAt . entrySet ( ) . size ( ) ; i < clHeader . cfCount ; i + + ) 
 + for ( int i = clHeader . cfDirtiedAt . entrySet ( ) . size ( ) ; i < clHeader . cfCount ; i + + ) 
 { 
 dos . writeInt ( 0 ) ; 
 dos . writeInt ( 0 ) ; 
 } 
 } 
 
 - public CommitLogHeader deserialize ( DataInputStream dis ) throws IOException 
 + public CommitLogHeader deserialize ( DataInput dis ) throws IOException 
 { 
 Checksum checksum = new CRC32 ( ) ; 
 
 diff - - git a / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java 
 index dcecaeb . . 94a9c44 100644 
 - - - a / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java 
 + + + b / src / java / org / apache / cassandra / db / commitlog / CommitLogSegment . java 
 @ @ - 1 , 6 + 1 , 4 @ @ 
 - package org . apache . cassandra . db . commitlog ; 
 / * 
 - * 
 * Licensed to the Apache Software Foundation ( ASF ) under one 
 * or more contributor license agreements . See the NOTICE file 
 * distributed with this work for additional information 
 @ @ - 20 , 6 + 18 , 7 @ @ package org . apache . cassandra . db . commitlog ; 
 * 
 * / 
 
 + package org . apache . cassandra . db . commitlog ; 
 
 import java . io . File ; 
 import java . io . IOError ; 
 @ @ - 27 , 15 + 26 , 13 @ @ import java . io . IOException ; 
 import java . util . zip . CRC32 ; 
 import java . util . zip . Checksum ; 
 
 - import org . apache . cassandra . config . CFMetaData ; 
 - 
 import org . slf4j . Logger ; 
 import org . slf4j . LoggerFactory ; 
 
 + import org . apache . cassandra . config . CFMetaData ; 
 import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . db . ColumnFamily ; 
 import org . apache . cassandra . db . RowMutation ; 
 - import org . apache . cassandra . db . Table ; 
 import org . apache . cassandra . io . util . BufferedRandomAccessFile ; 
 import org . apache . cassandra . io . util . DataOutputBuffer ; 
 
 @ @ - 49 , 13 + 46 , 13 @ @ public class CommitLogSegment 
 public CommitLogSegment ( ) 
 { 
 this . header = new CommitLogHeader ( ) ; 
 - String logFile = DatabaseDescriptor . getLogFileLocation ( ) + File . separator + " CommitLog - " + System . currentTimeMillis ( ) + " . log " ; 
 + String logFile = DatabaseDescriptor . getCommitLogLocation ( ) + File . separator + " CommitLog - " + System . currentTimeMillis ( ) + " . log " ; 
 logger . info ( " Creating new commitlog segment " + logFile ) ; 
 
 try 
 { 
 logWriter = createWriter ( logFile ) ; 
 - writeCommitLogHeader ( header . toByteArray ( ) ) ; 
 + writeHeader ( ) ; 
 } 
 catch ( IOException e ) 
 { 
 @ @ - 70 , 25 + 67 , 7 @ @ public class CommitLogSegment 
 
 public void writeHeader ( ) throws IOException 
 { 
 - seekAndWriteCommitLogHeader ( header . toByteArray ( ) ) ; 
 - } 
 - 
 - / * * writes header at the beginning of the file , then seeks back to current position * / 
 - void seekAndWriteCommitLogHeader ( byte [ ] bytes ) throws IOException 
 - { 
 - long currentPos = logWriter . getFilePointer ( ) ; 
 - logWriter . seek ( 0 ) ; 
 - 
 - writeCommitLogHeader ( bytes ) ; 
 - 
 - logWriter . seek ( currentPos ) ; 
 - } 
 - 
 - private void writeCommitLogHeader ( byte [ ] bytes ) throws IOException 
 - { 
 - logWriter . writeInt ( bytes . length ) ; 
 - logWriter . write ( bytes ) ; 
 - logWriter . sync ( ) ; 
 + CommitLogHeader . writeCommitLogHeader ( header , getHeaderPath ( ) ) ; 
 } 
 
 private static BufferedRandomAccessFile createWriter ( String file ) throws IOException 
 @ @ - 121 , 29 + 100 , 30 @ @ public class CommitLogSegment 
 if ( ! header . isDirty ( id ) ) 
 { 
 header . turnOn ( id , logWriter . getFilePointer ( ) ) ; 
 - seekAndWriteCommitLogHeader ( header . toByteArray ( ) ) ; 
 + writeHeader ( ) ; 
 } 
 } 
 } 
 
 - / / write mutation , w / checksum 
 - Checksum checkum = new CRC32 ( ) ; 
 + / / write mutation , w / checksum on the size and data 
 + byte [ ] bytes ; 
 + Checksum checksum = new CRC32 ( ) ; 
 if ( serializedRow instanceof DataOutputBuffer ) 
 { 
 - DataOutputBuffer buffer = ( DataOutputBuffer ) serializedRow ; 
 - logWriter . writeInt ( buffer . getLength ( ) ) ; 
 - logWriter . write ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; 
 - checkum . update ( buffer . getData ( ) , 0 , buffer . getLength ( ) ) ; 
 + bytes = ( ( DataOutputBuffer ) serializedRow ) . getData ( ) ; 
 } 
 else 
 { 
 assert serializedRow instanceof byte [ ] ; 
 - byte [ ] bytes = ( byte [ ] ) serializedRow ; 
 - logWriter . writeInt ( bytes . length ) ; 
 - logWriter . write ( bytes ) ; 
 - checkum . update ( bytes , 0 , bytes . length ) ; 
 + bytes = ( byte [ ] ) serializedRow ; 
 } 
 - logWriter . writeLong ( checkum . getValue ( ) ) ; 
 + 
 + checksum . update ( bytes . length ) ; 
 + logWriter . writeInt ( bytes . length ) ; 
 + logWriter . writeLong ( checksum . getValue ( ) ) ; 
 + logWriter . write ( bytes ) ; 
 + checksum . update ( bytes , 0 , bytes . length ) ; 
 + logWriter . writeLong ( checksum . getValue ( ) ) ; 
 
 return cLogCtx ; 
 } 
 @ @ - 175 , 6 + 155 , 11 @ @ public class CommitLogSegment 
 return logWriter . getPath ( ) ; 
 } 
 
 + public String getHeaderPath ( ) 
 + { 
 + return CommitLogHeader . getHeaderPathFromSegment ( this ) ; 
 + } 
 + 
 public long length ( ) 
 { 
 try 
 diff - - git a / test / unit / org / apache / cassandra / CleanupHelper . java b / test / unit / org / apache / cassandra / CleanupHelper . java 
 index f783041 . . a908c28 100644 
 - - - a / test / unit / org / apache / cassandra / CleanupHelper . java 
 + + + b / test / unit / org / apache / cassandra / CleanupHelper . java 
 @ @ - 45 , 7 + 45 , 7 @ @ public class CleanupHelper extends SchemaLoader 
 { 
 / / clean up commitlog 
 String [ ] directoryNames = { 
 - DatabaseDescriptor . getLogFileLocation ( ) , 
 + DatabaseDescriptor . getCommitLogLocation ( ) , 
 } ; 
 for ( String dirName : directoryNames ) 
 { 
 diff - - git a / test / unit / org / apache / cassandra / db / CommitLogTest . java b / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 index 905c7e1 . . 44a0161 100644 
 - - - a / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 + + + b / test / unit / org / apache / cassandra / db / CommitLogTest . java 
 @ @ - 16 , 18 + 16 , 22 @ @ 
 * specific language governing permissions and limitations 
 * under the License . 
 * / 
 + 
 package org . apache . cassandra . db ; 
 
 + import java . io . * ; 
 + import java . util . concurrent . ExecutionException ; 
 + import java . util . zip . CRC32 ; 
 + import java . util . zip . Checksum ; 
 + 
 + import org . junit . Test ; 
 + 
 import org . apache . cassandra . CleanupHelper ; 
 import org . apache . cassandra . db . commitlog . CommitLog ; 
 - import org . apache . cassandra . db . filter . QueryPath ; 
 - import org . junit . Test ; 
 
 - import java . io . File ; 
 - import java . io . FileOutputStream ; 
 - import java . io . IOException ; 
 - import java . io . OutputStream ; 
 - import java . util . concurrent . ExecutionException ; 
 + import org . apache . cassandra . db . commitlog . CommitLogHeader ; 
 + import org . apache . cassandra . db . filter . QueryPath ; 
 + import org . apache . cassandra . utils . Pair ; 
 
 public class CommitLogTest extends CleanupHelper 
 { 
 @ @ - 63 , 13 + 67 , 119 @ @ public class CommitLogTest extends CleanupHelper 
 } 
 
 @ Test 
 - public void testRecoveryWithPartiallyWrittenHeader ( ) throws Exception 
 + public void testRecoveryWithEmptyHeader ( ) throws Exception 
 + { 
 + testRecovery ( new byte [ 0 ] , new byte [ 10 ] ) ; 
 + } 
 + 
 + @ Test 
 + public void testRecoveryWithShortHeader ( ) throws Exception 
 + { 
 + testRecovery ( new byte [ 2 ] , new byte [ 10 ] ) ; 
 + } 
 + 
 + @ Test 
 + public void testRecoveryWithGarbageHeader ( ) throws Exception 
 + { 
 + byte [ ] garbage = new byte [ 100 ] ; 
 + ( new java . util . Random ( ) ) . nextBytes ( garbage ) ; 
 + testRecovery ( garbage , garbage ) ; 
 + } 
 + 
 + @ Test 
 + public void testRecoveryWithEmptyLog ( ) throws Exception 
 + { 
 + CommitLog . recover ( new File [ ] { tmpFiles ( ) . right } ) ; 
 + } 
 + 
 + @ Test 
 + public void testRecoveryWithShortLog ( ) throws Exception 
 + { 
 + / / force EOF while reading log 
 + testRecoveryWithBadSizeArgument ( 100 , 10 ) ; 
 + } 
 + 
 + @ Test 
 + public void testRecoveryWithShortSize ( ) throws Exception 
 + { 
 + testRecovery ( new byte [ 0 ] , new byte [ 2 ] ) ; 
 + } 
 + 
 + @ Test 
 + public void testRecoveryWithShortCheckSum ( ) throws Exception 
 + { 
 + testRecovery ( new byte [ 0 ] , new byte [ 6 ] ) ; 
 + } 
 + 
 + @ Test 
 + public void testRecoveryWithGarbageLog ( ) throws Exception 
 + { 
 + byte [ ] garbage = new byte [ 100 ] ; 
 + ( new java . util . Random ( ) ) . nextBytes ( garbage ) ; 
 + testRecovery ( new byte [ 0 ] , garbage ) ; 
 + } 
 + 
 + @ Test 
 + public void testRecoveryWithBadSizeChecksum ( ) throws Exception 
 + { 
 + Checksum checksum = new CRC32 ( ) ; 
 + checksum . update ( 100 ) ; 
 + testRecoveryWithBadSizeArgument ( 100 , 100 , ~ checksum . getValue ( ) ) ; 
 + } 
 + 
 + @ Test 
 + public void testRecoveryWithZeroSegmentSizeArgument ( ) throws Exception 
 + { 
 + / / many different combinations of 4 bytes ( garbage ) will be read as zero by readInt ( ) 
 + testRecoveryWithBadSizeArgument ( 0 , 10 ) ; / / zero size , but no EOF 
 + } 
 + 
 + @ Test 
 + public void testRecoveryWithNegativeSizeArgument ( ) throws Exception 
 + { 
 + / / garbage from a partial / bad flush could be read as a negative size even if there is no EOF 
 + testRecoveryWithBadSizeArgument ( - 10 , 10 ) ; / / negative size , but no EOF 
 + } 
 + 
 + protected void testRecoveryWithBadSizeArgument ( int size , int dataSize ) throws Exception 
 + { 
 + Checksum checksum = new CRC32 ( ) ; 
 + checksum . update ( size ) ; 
 + testRecoveryWithBadSizeArgument ( size , dataSize , checksum . getValue ( ) ) ; 
 + } 
 + 
 + protected void testRecoveryWithBadSizeArgument ( int size , int dataSize , long checksum ) throws Exception 
 + { 
 + ByteArrayOutputStream out = new ByteArrayOutputStream ( ) ; 
 + DataOutputStream dout = new DataOutputStream ( out ) ; 
 + dout . writeInt ( size ) ; 
 + dout . writeLong ( checksum ) ; 
 + dout . write ( new byte [ dataSize ] ) ; 
 + dout . close ( ) ; 
 + testRecovery ( new byte [ 0 ] , out . toByteArray ( ) ) ; 
 + } 
 + 
 + protected Pair < File , File > tmpFiles ( ) throws IOException 
 + { 
 + File logFile = File . createTempFile ( " testRecoveryWithPartiallyWrittenHeaderTestFile " , null ) ; 
 + File headerFile = new File ( CommitLogHeader . getHeaderPathFromSegmentPath ( logFile . getAbsolutePath ( ) ) ) ; 
 + logFile . deleteOnExit ( ) ; 
 + headerFile . deleteOnExit ( ) ; 
 + assert logFile . length ( ) = = 0 ; 
 + assert headerFile . length ( ) = = 0 ; 
 + return new Pair < File , File > ( headerFile , logFile ) ; 
 + } 
 + 
 + protected void testRecovery ( byte [ ] headerData , byte [ ] logData ) throws Exception 
 { 
 - File tmpFile = File . createTempFile ( " testRecoveryWithPartiallyWrittenHeaderTestFile " , null ) ; 
 - tmpFile . deleteOnExit ( ) ; 
 - OutputStream out = new FileOutputStream ( tmpFile ) ; 
 - out . write ( new byte [ 6 ] ) ; 
 + Pair < File , File > tmpFiles = tmpFiles ( ) ; 
 + File logFile = tmpFiles . right ; 
 + File headerFile = tmpFiles . left ; 
 + OutputStream lout = new FileOutputStream ( logFile ) ; 
 + OutputStream hout = new FileOutputStream ( headerFile ) ; 
 + lout . write ( logData ) ; 
 + hout . write ( headerData ) ; 
 / / statics make it annoying to test things correctly 
 - CommitLog . instance ( ) . recover ( new File [ ] { tmpFile } ) ; / / CASSANDRA - 1119 throws on failure 
 + CommitLog . recover ( new File [ ] { logFile } ) ; / / CASSANDRA - 1119 / CASSANDRA - 1179 throw on failure * / 
 } 
 } 
 diff - - git a / test / unit / org / apache / cassandra / db / RecoveryManager3Test . java b / test / unit / org / apache / cassandra / db / RecoveryManager3Test . java 
 new file mode 100644 
 index 0000000 . . 099a126 
 - - - / dev / null 
 + + + b / test / unit / org / apache / cassandra / db / RecoveryManager3Test . java 
 @ @ - 0 , 0 + 1 , 57 @ @ 
 + package org . apache . cassandra . db ; 
 + 
 + import java . io . File ; 
 + import java . io . IOException ; 
 + import java . util . concurrent . ExecutionException ; 
 + 
 + import org . junit . Test ; 
 + 
 + import org . apache . cassandra . CleanupHelper ; 
 + import org . apache . cassandra . Util ; 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 + import org . apache . cassandra . db . commitlog . CommitLog ; 
 + 
 + import static org . apache . cassandra . Util . column ; 
 + import static org . apache . cassandra . db . TableTest . assertColumns ; 
 + 
 + public class RecoveryManager3Test extends CleanupHelper 
 + { 
 + @ Test 
 + public void testMissingHeader ( ) throws IOException , ExecutionException , InterruptedException 
 + { 
 + Table table1 = Table . open ( " Keyspace1 " ) ; 
 + Table table2 = Table . open ( " Keyspace2 " ) ; 
 + 
 + RowMutation rm ; 
 + DecoratedKey dk = Util . dk ( " keymulti " ) ; 
 + ColumnFamily cf ; 
 + 
 + rm = new RowMutation ( " Keyspace1 " , dk . key ) ; 
 + cf = ColumnFamily . create ( " Keyspace1 " , " Standard1 " ) ; 
 + cf . addColumn ( column ( " col1 " , " val1 " , new TimestampClock ( 1L ) ) ) ; 
 + rm . add ( cf ) ; 
 + rm . apply ( ) ; 
 + 
 + rm = new RowMutation ( " Keyspace2 " , dk . key ) ; 
 + cf = ColumnFamily . create ( " Keyspace2 " , " Standard3 " ) ; 
 + cf . addColumn ( column ( " col2 " , " val2 " , new TimestampClock ( 1L ) ) ) ; 
 + rm . add ( cf ) ; 
 + rm . apply ( ) ; 
 + 
 + table1 . getColumnFamilyStore ( " Standard1 " ) . clearUnsafe ( ) ; 
 + table2 . getColumnFamilyStore ( " Standard3 " ) . clearUnsafe ( ) ; 
 + 
 + / / nuke the header 
 + for ( File file : new File ( DatabaseDescriptor . getCommitLogLocation ( ) ) . listFiles ( ) ) 
 + { 
 + if ( file . getName ( ) . endsWith ( " . header " ) ) 
 + if ( ! file . delete ( ) ) 
 + throw new AssertionError ( ) ; 
 + } 
 + 
 + CommitLog . recover ( ) ; 
 + 
 + assertColumns ( Util . getColumnFamily ( table1 , dk , " Standard1 " ) , " col1 " ) ; 
 + assertColumns ( Util . getColumnFamily ( table2 , dk , " Standard3 " ) , " col2 " ) ; 
 + } 
 + } 
 diff - - git a / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java b / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java 
 index 112c17f . . 843434f 100644 
 - - - a / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java 
 + + + b / test / unit / org / apache / cassandra / db / RecoveryManagerTest . java 
 @ @ - 34 , 8 + 34 , 7 @ @ import static org . apache . cassandra . db . TableTest . assertColumns ; 
 public class RecoveryManagerTest extends CleanupHelper 
 { 
 @ Test 
 - public void testNothing ( ) throws IOException { 
 - / / TODO nothing to recover 
 + public void testNothingToRecover ( ) throws IOException { 
 CommitLog . recover ( ) ; 
 } 
 
 diff - - git a / test / unit / org / apache / cassandra / db / commitlog / CommitLogHeaderTest . java b / test / unit / org / apache / cassandra / db / commitlog / CommitLogHeaderTest . java 
 index a431d0c . . 35b2ffe 100644 
 - - - a / test / unit / org / apache / cassandra / db / commitlog / CommitLogHeaderTest . java 
 + + + b / test / unit / org / apache / cassandra / db / commitlog / CommitLogHeaderTest . java 
 @ @ - 18 , 23 + 18 , 13 @ @ 
 
 package org . apache . cassandra . db . commitlog ; 
 
 - import com . google . common . collect . HashMultimap ; 
 - import com . google . common . collect . Multimap ; 
 - import org . apache . cassandra . SchemaLoader ; 
 - import org . apache . cassandra . config . CFMetaData ; 
 - import org . apache . cassandra . config . ConfigurationException ; 
 - import org . apache . cassandra . config . DatabaseDescriptor ; 
 - import org . apache . cassandra . utils . Pair ; 
 - import org . junit . Before ; 
 + import java . io . ByteArrayOutputStream ; 
 + import java . io . DataOutputStream ; 
 + import java . io . IOException ; 
 + 
 import org . junit . Test ; 
 
 - import java . io . ByteArrayInputStream ; 
 - import java . io . DataInputStream ; 
 - import java . io . IOException ; 
 - import java . util . Arrays ; 
 - import java . util . Collection ; 
 - import java . util . Collections ; 
 - import java . util . Map ; 
 + import org . apache . cassandra . SchemaLoader ; 
 
 public class CommitLogHeaderTest extends SchemaLoader 
 { 
 @ @ - 43 , 41 + 33 , 42 @ @ public class CommitLogHeaderTest extends SchemaLoader 
 public void testEmptyHeader ( ) 
 { 
 CommitLogHeader clh = new CommitLogHeader ( ) ; 
 - assert CommitLogHeader . getLowestPosition ( clh ) = = 0 ; 
 + assert clh . getReplayPosition ( ) = = 0 ; 
 } 
 
 @ Test 
 public void lowestPositionWithZero ( ) 
 { 
 - / / zero should never be the lowest position unless all positions are zero . 
 CommitLogHeader clh = new CommitLogHeader ( ) ; 
 clh . turnOn ( 2 , 34 ) ; 
 - assert CommitLogHeader . getLowestPosition ( clh ) = = 34 ; 
 + assert clh . getReplayPosition ( ) = = 34 ; 
 clh . turnOn ( 100 , 0 ) ; 
 - assert CommitLogHeader . getLowestPosition ( clh ) = = 34 ; 
 + assert clh . getReplayPosition ( ) = = 0 ; 
 clh . turnOn ( 65 , 2 ) ; 
 - assert CommitLogHeader . getLowestPosition ( clh ) = = 2 ; 
 + assert clh . getReplayPosition ( ) = = 0 ; 
 } 
 
 @ Test 
 public void lowestPositionEmpty ( ) 
 { 
 CommitLogHeader clh = new CommitLogHeader ( ) ; 
 - assert CommitLogHeader . getLowestPosition ( clh ) = = 0 ; 
 + assert clh . getReplayPosition ( ) = = 0 ; 
 } 
 
 @ Test 
 public void constantSize ( ) throws IOException 
 { 
 - CommitLogHeader clh = new CommitLogHeader ( ) ; 
 - clh . turnOn ( 2 , 34 ) ; 
 - byte [ ] one = clh . toByteArray ( ) ; 
 - 
 - clh = new CommitLogHeader ( ) ; 
 + CommitLogHeader clh0 = new CommitLogHeader ( ) ; 
 + clh0 . turnOn ( 2 , 34 ) ; 
 + ByteArrayOutputStream out0 = new ByteArrayOutputStream ( ) ; 
 + CommitLogHeader . serializer . serialize ( clh0 , new DataOutputStream ( out0 ) ) ; 
 + 
 + CommitLogHeader clh1 = new CommitLogHeader ( ) ; 
 for ( int i = 0 ; i < 5 ; i + + ) 
 - clh . turnOn ( i , 1000 * i ) ; 
 - byte [ ] two = clh . toByteArray ( ) ; 
 - 
 - assert one . length = = two . length ; 
 + clh1 . turnOn ( i , 1000 * i ) ; 
 + ByteArrayOutputStream out1 = new ByteArrayOutputStream ( ) ; 
 + CommitLogHeader . serializer . serialize ( clh1 , new DataOutputStream ( out1 ) ) ; 
 + 
 + assert out0 . toByteArray ( ) . length = = out1 . toByteArray ( ) . length ; 
 } 
 }
