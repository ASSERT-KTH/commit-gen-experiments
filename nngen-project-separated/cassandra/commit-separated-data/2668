BLEU SCORE: 0.01908840668714524

TEST MSG: Fix trigger mutations when base mutation list is immutable
GENERATED MSG: add basic hadoop support using Thrift , one split per node

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 39656ff . . 91037d1 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 1 , 5 + 1 , 6 @ @ <nl> 2 . 0 . 7 <nl> * Fix saving triggers to schema ( CASSANDRA - 6789 ) <nl> + * Fix trigger mutations when base mutation list is immutable ( CASSANDRA - 6790 ) <nl> <nl> <nl> 2 . 0 . 6 <nl> diff - - git a / src / java / org / apache / cassandra / service / StorageProxy . java b / src / java / org / apache / cassandra / service / StorageProxy . java <nl> index 14c1ce3 . . a6db9cd 100644 <nl> - - - a / src / java / org / apache / cassandra / service / StorageProxy . java <nl> + + + b / src / java / org / apache / cassandra / service / StorageProxy . java <nl> @ @ - 508 , 13 + 508 , 13 @ @ public class StorageProxy implements StorageProxyMBean <nl> } <nl> } <nl> <nl> - public static void mutateWithTriggers ( Collection < ? extends IMutation > mutations , ConsistencyLevel consistencyLevel , boolean mutateAtomically ) throws WriteTimeoutException , UnavailableException , <nl> - OverloadedException , InvalidRequestException <nl> + public static void mutateWithTriggers ( Collection < ? extends IMutation > mutations , ConsistencyLevel consistencyLevel , boolean mutateAtomically ) <nl> + throws WriteTimeoutException , UnavailableException , OverloadedException , InvalidRequestException <nl> { <nl> Collection < RowMutation > tmutations = TriggerExecutor . instance . execute ( mutations ) ; <nl> if ( mutateAtomically | | tmutations ! = null ) <nl> { <nl> - Collection < RowMutation > allMutations = ( Collection < RowMutation > ) mutations ; <nl> + Collection < RowMutation > allMutations = new ArrayList < > ( ( Collection < RowMutation > ) mutations ) ; <nl> if ( tmutations ! = null ) <nl> allMutations . addAll ( tmutations ) ; <nl> StorageProxy . mutateAtomically ( allMutations , consistencyLevel ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / triggers / TriggersTest . java b / test / unit / org / apache / cassandra / triggers / TriggersTest . java <nl> new file mode 100644 <nl> index 0000000 . . 6ca3880 <nl> - - - / dev / null <nl> + + + b / test / unit / org / apache / cassandra / triggers / TriggersTest . java <nl> @ @ - 0 , 0 + 1 , 179 @ @ <nl> + / * <nl> + * Licensed to the Apache Software Foundation ( ASF ) under one <nl> + * or more contributor license agreements . See the NOTICE file <nl> + * distributed with this work for additional information <nl> + * regarding copyright ownership . The ASF licenses this file <nl> + * to you under the Apache License , Version 2 . 0 ( the <nl> + * " License " ) ; you may not use this file except in compliance <nl> + * with the License . You may obtain a copy of the License at <nl> + * <nl> + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 <nl> + * <nl> + * Unless required by applicable law or agreed to in writing , software <nl> + * distributed under the License is distributed on an " AS IS " BASIS , <nl> + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . <nl> + * See the License for the specific language governing permissions and <nl> + * limitations under the License . <nl> + * / <nl> + package org . apache . cassandra . triggers ; <nl> + <nl> + import java . net . InetAddress ; <nl> + import java . nio . ByteBuffer ; <nl> + import java . util . Collection ; <nl> + import java . util . Collections ; <nl> + <nl> + import org . junit . AfterClass ; <nl> + import org . junit . Before ; <nl> + import org . junit . Test ; <nl> + <nl> + import org . apache . cassandra . SchemaLoader ; <nl> + import org . apache . cassandra . config . Schema ; <nl> + import org . apache . cassandra . cql3 . QueryProcessor ; <nl> + import org . apache . cassandra . cql3 . UntypedResultSet ; <nl> + import org . apache . cassandra . db . ArrayBackedSortedColumns ; <nl> + import org . apache . cassandra . db . Column ; <nl> + import org . apache . cassandra . db . ColumnFamily ; <nl> + import org . apache . cassandra . db . ConsistencyLevel ; <nl> + import org . apache . cassandra . db . RowMutation ; <nl> + import org . apache . cassandra . service . StorageService ; <nl> + import org . apache . cassandra . thrift . Cassandra ; <nl> + import org . apache . cassandra . thrift . ColumnOrSuperColumn ; <nl> + import org . apache . cassandra . thrift . ColumnParent ; <nl> + import org . apache . cassandra . thrift . Mutation ; <nl> + import org . apache . cassandra . thrift . TFramedTransportFactory ; <nl> + import org . apache . cassandra . thrift . ThriftServer ; <nl> + import org . apache . cassandra . utils . ByteBufferUtil ; <nl> + import org . apache . thrift . protocol . TBinaryProtocol ; <nl> + <nl> + import static org . junit . Assert . assertEquals ; <nl> + <nl> + public class TriggersTest extends SchemaLoader <nl> + { <nl> + private static boolean triggerCreated = false ; <nl> + private static ThriftServer thriftServer ; <nl> + <nl> + private static String ksName = " triggers _ test _ ks " ; <nl> + private static String cfName = " test _ table " ; <nl> + <nl> + @ Before <nl> + public void setup ( ) throws Exception <nl> + { <nl> + StorageService . instance . initServer ( 0 ) ; <nl> + if ( thriftServer = = null | | ! thriftServer . isRunning ( ) ) <nl> + { <nl> + thriftServer = new ThriftServer ( InetAddress . getLocalHost ( ) , 9170 ) ; <nl> + thriftServer . start ( ) ; <nl> + } <nl> + <nl> + String cql = String . format ( " CREATE KEYSPACE IF NOT EXISTS % s " + <nl> + " WITH REPLICATION = { ' class ' : ' SimpleStrategy ' , ' replication _ factor ' : 1 } " , <nl> + ksName ) ; <nl> + QueryProcessor . process ( cql , ConsistencyLevel . ONE ) ; <nl> + <nl> + cql = String . format ( " CREATE TABLE IF NOT EXISTS % s . % s ( k int , v1 int , v2 int , PRIMARY KEY ( k ) ) " , ksName , cfName ) ; <nl> + QueryProcessor . process ( cql , ConsistencyLevel . ONE ) ; <nl> + <nl> + / / no conditional execution of create trigger stmt yet <nl> + if ( ! triggerCreated ) <nl> + { <nl> + cql = String . format ( " CREATE TRIGGER trigger _ 1 ON % s . % s USING ' % s ' " , <nl> + ksName , cfName , TestTrigger . class . getName ( ) ) ; <nl> + QueryProcessor . process ( cql , ConsistencyLevel . ONE ) ; <nl> + triggerCreated = true ; <nl> + } <nl> + } <nl> + <nl> + @ AfterClass <nl> + public static void teardown ( ) <nl> + { <nl> + if ( thriftServer ! = null & & thriftServer . isRunning ( ) ) <nl> + { <nl> + thriftServer . stop ( ) ; <nl> + } <nl> + } <nl> + <nl> + @ Test <nl> + public void executeTriggerOnCqlInsert ( ) throws Exception <nl> + { <nl> + String cql = String . format ( " INSERT INTO % s . % s ( k , v1 ) VALUES ( 0 , 0 ) " , ksName , cfName ) ; <nl> + QueryProcessor . process ( cql , ConsistencyLevel . ONE ) ; <nl> + assertUpdateIsAugmented ( 0 ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void executeTriggerOnCqlBatchInsert ( ) throws Exception <nl> + { <nl> + String cql = String . format ( " BEGIN BATCH " + <nl> + " INSERT INTO % s . % s ( k , v1 ) VALUES ( 1 , 1 ) ; " + <nl> + " APPLY BATCH " , <nl> + ksName , cfName ) ; <nl> + QueryProcessor . process ( cql , ConsistencyLevel . ONE ) ; <nl> + assertUpdateIsAugmented ( 1 ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void executeTriggerOnThriftInsert ( ) throws Exception <nl> + { <nl> + Cassandra . Client client = new Cassandra . Client ( <nl> + new TBinaryProtocol ( <nl> + new TFramedTransportFactory ( ) . openTransport ( <nl> + InetAddress . getLocalHost ( ) . getHostName ( ) , 9170 ) ) ) ; <nl> + client . set _ keyspace ( ksName ) ; <nl> + client . insert ( ByteBufferUtil . bytes ( 2 ) , <nl> + new ColumnParent ( cfName ) , <nl> + getColumnForInsert ( " v1 " , 2 ) , <nl> + org . apache . cassandra . thrift . ConsistencyLevel . ONE ) ; <nl> + <nl> + assertUpdateIsAugmented ( 2 ) ; <nl> + } <nl> + <nl> + @ Test <nl> + public void executeTriggerOnThriftBatchUpdate ( ) throws Exception <nl> + { <nl> + Cassandra . Client client = new Cassandra . Client ( <nl> + new TBinaryProtocol ( <nl> + new TFramedTransportFactory ( ) . openTransport ( <nl> + InetAddress . getLocalHost ( ) . getHostName ( ) , 9170 ) ) ) ; <nl> + client . set _ keyspace ( ksName ) ; <nl> + Mutation mutation = new Mutation ( ) ; <nl> + ColumnOrSuperColumn cosc = new ColumnOrSuperColumn ( ) ; <nl> + cosc . setColumn ( getColumnForInsert ( " v1 " , 3 ) ) ; <nl> + mutation . setColumn _ or _ supercolumn ( cosc ) ; <nl> + client . batch _ mutate ( <nl> + Collections . singletonMap ( ByteBufferUtil . bytes ( 3 ) , <nl> + Collections . singletonMap ( cfName , <nl> + Collections . singletonList ( mutation ) ) ) , <nl> + org . apache . cassandra . thrift . ConsistencyLevel . ONE ) ; <nl> + <nl> + assertUpdateIsAugmented ( 3 ) ; <nl> + } <nl> + <nl> + private void assertUpdateIsAugmented ( int key ) <nl> + { <nl> + UntypedResultSet rs = QueryProcessor . processInternal ( <nl> + String . format ( " SELECT * FROM % s . % s WHERE k = % s " , ksName , cfName , key ) ) ; <nl> + assertEquals ( 999 , rs . one ( ) . getInt ( " v2 " ) ) ; <nl> + } <nl> + <nl> + private org . apache . cassandra . thrift . Column getColumnForInsert ( String columnName , int value ) <nl> + { <nl> + org . apache . cassandra . thrift . Column column = new org . apache . cassandra . thrift . Column ( ) ; <nl> + column . setName ( Schema . instance . getCFMetaData ( ksName , cfName ) . comparator . fromString ( columnName ) ) ; <nl> + column . setValue ( ByteBufferUtil . bytes ( value ) ) ; <nl> + column . setTimestamp ( System . currentTimeMillis ( ) ) ; <nl> + return column ; <nl> + } <nl> + <nl> + public static class TestTrigger implements ITrigger <nl> + { <nl> + public Collection < RowMutation > augment ( ByteBuffer key , ColumnFamily update ) <nl> + { <nl> + ColumnFamily extraUpdate = update . cloneMeShallow ( ArrayBackedSortedColumns . factory , false ) ; <nl> + extraUpdate . addColumn ( new Column ( update . metadata ( ) . comparator . fromString ( " v2 " ) , <nl> + ByteBufferUtil . bytes ( 999 ) ) ) ; <nl> + RowMutation rm = new RowMutation ( ksName , key ) ; <nl> + rm . add ( extraUpdate ) ; <nl> + return Collections . singletonList ( rm ) ; <nl> + } <nl> + } <nl> + }
NEAREST DIFF (one line): diff - - git a / ivy . xml b / ivy . xml <nl> index e6f1fbc . . b85534c 100644 <nl> - - - a / ivy . xml <nl> + + + b / ivy . xml <nl> @ @ - 19 , 6 + 19 , 8 @ @ <nl> < ivy - module version = " 2 . 0 " > <nl> < info organisation = " apache - cassandra " module = " cassandra " / > <nl> < dependencies > <nl> + < dependency org = " org . apache . mahout . hadoop " <nl> + name = " hadoop - core " rev = " 0 . 20 . 1 " / > <nl> < ! - - FIXME : paranamer and jackson can be dropped after we ' re depending <nl> on avro ( since it depends on them ) . - - > <nl> < dependency org = " com . thoughtworks . paranamer " <nl> diff - - git a / src / java / org / apache / cassandra / db / SuperColumn . java b / src / java / org / apache / cassandra / db / SuperColumn . java <nl> index c588930 . . 82438a7 100644 <nl> - - - a / src / java / org / apache / cassandra / db / SuperColumn . java <nl> + + + b / src / java / org / apache / cassandra / db / SuperColumn . java <nl> @ @ - 47 , 7 + 47 , 7 @ @ public class SuperColumn implements IColumn , IColumnContainer <nl> private AtomicInteger localDeletionTime = new AtomicInteger ( Integer . MIN _ VALUE ) ; <nl> 	 private AtomicLong markedForDeleteAt = new AtomicLong ( Long . MIN _ VALUE ) ; <nl> <nl> - SuperColumn ( byte [ ] name , AbstractType comparator ) <nl> + public SuperColumn ( byte [ ] name , AbstractType comparator ) <nl> { <nl> this ( name , new ConcurrentSkipListMap < byte [ ] , IColumn > ( comparator ) ) ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java <nl> new file mode 100644 <nl> index 0000000 . . 900c441 <nl> - - - / dev / null <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java <nl> @ @ - 0 , 0 + 1 , 112 @ @ <nl> + package org . apache . cassandra . hadoop ; <nl> + <nl> + import java . io . IOException ; <nl> + import java . util . ArrayList ; <nl> + import java . util . List ; <nl> + import java . util . Map ; <nl> + import java . util . SortedMap ; <nl> + <nl> + import org . apache . log4j . Logger ; <nl> + <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + import org . apache . cassandra . db . IColumn ; <nl> + import org . apache . cassandra . dht . IPartitioner ; <nl> + import org . apache . cassandra . service . StorageService ; <nl> + import org . apache . cassandra . thrift . * ; <nl> + import org . apache . hadoop . conf . Configuration ; <nl> + import org . apache . hadoop . mapreduce . * ; <nl> + import org . apache . thrift . TException ; <nl> + import org . apache . thrift . protocol . TBinaryProtocol ; <nl> + import org . apache . thrift . transport . TSocket ; <nl> + import org . apache . thrift . transport . TTransportException ; <nl> + <nl> + public class ColumnFamilyInputFormat extends InputFormat < String , SortedMap < byte [ ] , IColumn > > <nl> + { <nl> + private static final String KEYSPACE _ CONFIG = " cassandra . input . keyspace " ; <nl> + private static final String COLUMNFAMILY _ CONFIG = " cassandra . input . columnfamily " ; <nl> + <nl> + private static final Logger logger = Logger . getLogger ( StorageService . class ) ; <nl> + <nl> + private String keyspace ; <nl> + private String columnFamily ; <nl> + <nl> + public static void setColumnFamily ( Job job , String keyspace , String columnFamily ) <nl> + { <nl> + validateNotNullKeyspaceAndColumnFamily ( keyspace , columnFamily ) ; <nl> + try <nl> + { <nl> + ThriftValidation . validateColumnFamily ( keyspace , columnFamily ) ; <nl> + } <nl> + catch ( InvalidRequestException e ) <nl> + { <nl> + throw new RuntimeException ( e ) ; <nl> + } <nl> + Configuration conf = job . getConfiguration ( ) ; <nl> + conf . set ( KEYSPACE _ CONFIG , keyspace ) ; <nl> + conf . set ( COLUMNFAMILY _ CONFIG , columnFamily ) ; <nl> + } <nl> + <nl> + private static void validateNotNullKeyspaceAndColumnFamily ( String keyspace , String columnFamily ) <nl> + { <nl> + if ( keyspace = = null ) <nl> + { <nl> + throw new RuntimeException ( " you forgot to set the keyspace with setKeyspace ( ) " ) ; <nl> + } <nl> + if ( columnFamily = = null ) <nl> + { <nl> + throw new RuntimeException ( " you forgot to set the column family with setColumnFamily ( ) " ) ; <nl> + } <nl> + } <nl> + <nl> + public List < InputSplit > getSplits ( JobContext context ) throws IOException <nl> + { <nl> + Configuration conf = context . getConfiguration ( ) ; <nl> + keyspace = conf . get ( KEYSPACE _ CONFIG ) ; <nl> + columnFamily = conf . get ( COLUMNFAMILY _ CONFIG ) ; <nl> + validateNotNullKeyspaceAndColumnFamily ( keyspace , columnFamily ) ; <nl> + <nl> + List < TokenRange > map = getRangeMap ( ) ; <nl> + ArrayList < InputSplit > splits = new ArrayList < InputSplit > ( ) ; <nl> + for ( TokenRange entry : map ) <nl> + { <nl> + if ( logger . isDebugEnabled ( ) ) <nl> + logger . debug ( " split range is [ " + entry . start _ token + " , " + entry . end _ token + " ] " ) ; <nl> + String [ ] endpoints = entry . endpoints . toArray ( new String [ 0 ] ) ; <nl> + splits . add ( new ColumnFamilySplit ( keyspace , columnFamily , entry . start _ token , entry . end _ token , endpoints ) ) ; <nl> + } <nl> + <nl> + return splits ; <nl> + } <nl> + <nl> + private List < TokenRange > getRangeMap ( ) throws IOException <nl> + { <nl> + TSocket socket = new TSocket ( DatabaseDescriptor . getSeeds ( ) . iterator ( ) . next ( ) . getHostAddress ( ) , <nl> + DatabaseDescriptor . getThriftPort ( ) ) ; <nl> + TBinaryProtocol binaryProtocol = new TBinaryProtocol ( socket , false , false ) ; <nl> + Cassandra . Client client = new Cassandra . Client ( binaryProtocol ) ; <nl> + try <nl> + { <nl> + socket . open ( ) ; <nl> + } <nl> + catch ( TTransportException e ) <nl> + { <nl> + throw new IOException ( e ) ; <nl> + } <nl> + List < TokenRange > map ; <nl> + try <nl> + { <nl> + map = client . describe _ ring ( keyspace ) ; <nl> + } <nl> + catch ( TException e ) <nl> + { <nl> + throw new RuntimeException ( e ) ; <nl> + } <nl> + return map ; <nl> + } <nl> + <nl> + @ Override <nl> + public RecordReader < String , SortedMap < byte [ ] , IColumn > > createRecordReader ( InputSplit inputSplit , TaskAttemptContext taskAttemptContext ) throws IOException , InterruptedException <nl> + { <nl> + return new ColumnFamilyRecordReader ( ) ; <nl> + } <nl> + } <nl> \ No newline at end of file <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java <nl> new file mode 100644 <nl> index 0000000 . . f03a247 <nl> - - - / dev / null <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java <nl> @ @ - 0 , 0 + 1 , 196 @ @ <nl> + package org . apache . cassandra . hadoop ; <nl> + <nl> + import java . io . IOException ; <nl> + import java . net . InetAddress ; <nl> + import java . net . UnknownHostException ; <nl> + import java . util . List ; <nl> + import java . util . SortedMap ; <nl> + import java . util . TreeMap ; <nl> + <nl> + import org . apache . commons . lang . ArrayUtils ; <nl> + <nl> + import com . google . common . collect . AbstractIterator ; <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> + import org . apache . cassandra . db . * ; <nl> + import org . apache . cassandra . db . marshal . AbstractType ; <nl> + import org . apache . cassandra . thrift . * ; <nl> + import org . apache . cassandra . thrift . Column ; <nl> + import org . apache . cassandra . thrift . SuperColumn ; <nl> + import org . apache . cassandra . utils . Pair ; <nl> + import org . apache . hadoop . mapreduce . InputSplit ; <nl> + import org . apache . hadoop . mapreduce . RecordReader ; <nl> + import org . apache . hadoop . mapreduce . TaskAttemptContext ; <nl> + import org . apache . thrift . protocol . TBinaryProtocol ; <nl> + import org . apache . thrift . transport . TSocket ; <nl> + import org . apache . thrift . transport . TTransportException ; <nl> + <nl> + public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byte [ ] , IColumn > > <nl> + { <nl> + private static final int ROWS _ PER _ RANGE _ QUERY = 1024 ; <nl> + <nl> + private ColumnFamilySplit split ; <nl> + private RowIterator iter ; <nl> + private Pair < String , SortedMap < byte [ ] , IColumn > > currentRow ; <nl> + <nl> + public void close ( ) { } <nl> + <nl> + public String getCurrentKey ( ) <nl> + { <nl> + return currentRow . left ; <nl> + } <nl> + <nl> + public SortedMap < byte [ ] , IColumn > getCurrentValue ( ) <nl> + { <nl> + return currentRow . right ; <nl> + } <nl> + <nl> + public float getProgress ( ) <nl> + { <nl> + return ( ( float ) iter . rowsRead ( ) ) / iter . size ( ) ; <nl> + } <nl> + <nl> + public void initialize ( InputSplit split , TaskAttemptContext context ) throws IOException <nl> + { <nl> + this . split = ( ColumnFamilySplit ) split ; <nl> + iter = new RowIterator ( ) ; <nl> + } <nl> + <nl> + public boolean nextKeyValue ( ) throws IOException <nl> + { <nl> + if ( ! iter . hasNext ( ) ) <nl> + return false ; <nl> + currentRow = iter . next ( ) ; <nl> + return true ; <nl> + } <nl> + <nl> + private class RowIterator extends AbstractIterator < Pair < String , SortedMap < byte [ ] , IColumn > > > <nl> + { <nl> + <nl> + private List < KeySlice > rows ; <nl> + private int i = 0 ; <nl> + private AbstractType comparator = DatabaseDescriptor . getComparator ( split . getTable ( ) , split . getColumnFamily ( ) ) ; <nl> + <nl> + private void maybeInit ( ) <nl> + { <nl> + if ( rows ! = null ) <nl> + return ; <nl> + TSocket socket = new TSocket ( getLocation ( ) , <nl> + DatabaseDescriptor . getThriftPort ( ) ) ; <nl> + TBinaryProtocol binaryProtocol = new TBinaryProtocol ( socket , false , false ) ; <nl> + Cassandra . Client client = new Cassandra . Client ( binaryProtocol ) ; <nl> + try <nl> + { <nl> + socket . open ( ) ; <nl> + } <nl> + catch ( TTransportException e ) <nl> + { <nl> + throw new RuntimeException ( e ) ; <nl> + } <nl> + SliceRange sliceRange = new SliceRange ( ArrayUtils . EMPTY _ BYTE _ ARRAY , <nl> + ArrayUtils . EMPTY _ BYTE _ ARRAY , <nl> + false , <nl> + Integer . MAX _ VALUE ) ; <nl> + KeyRange keyRange = new KeyRange ( ROWS _ PER _ RANGE _ QUERY ) <nl> + . setStart _ token ( split . getStartToken ( ) ) <nl> + . setEnd _ token ( split . getEndToken ( ) ) ; <nl> + / / TODO " paging " large rows would be good <nl> + try <nl> + { <nl> + rows = client . get _ range _ slices ( split . getTable ( ) , <nl> + new ColumnParent ( split . getColumnFamily ( ) ) , <nl> + new SlicePredicate ( ) . setSlice _ range ( sliceRange ) , <nl> + keyRange , <nl> + ConsistencyLevel . ONE ) ; <nl> + } <nl> + catch ( Exception e ) <nl> + { <nl> + throw new RuntimeException ( e ) ; <nl> + } <nl> + } <nl> + <nl> + / / we don ' t use endpointsnitch since we are trying to support hadoop nodes that are <nl> + / / not necessarily on Cassandra machines , too . This should be adequate for single - DC clusters , at least . <nl> + private String getLocation ( ) <nl> + { <nl> + InetAddress [ ] localAddresses = new InetAddress [ 0 ] ; <nl> + try <nl> + { <nl> + localAddresses = InetAddress . getAllByName ( InetAddress . getLocalHost ( ) . getHostAddress ( ) ) ; <nl> + } <nl> + catch ( UnknownHostException e ) <nl> + { <nl> + throw new AssertionError ( e ) ; <nl> + } <nl> + for ( InetAddress address : localAddresses ) <nl> + { <nl> + for ( String location : split . getLocations ( ) ) <nl> + { <nl> + InetAddress locationAddress = null ; <nl> + try <nl> + { <nl> + locationAddress = InetAddress . getByName ( location ) ; <nl> + } <nl> + catch ( UnknownHostException e ) <nl> + { <nl> + throw new AssertionError ( e ) ; <nl> + } <nl> + if ( address . equals ( locationAddress ) ) <nl> + { <nl> + return location ; <nl> + } <nl> + } <nl> + } <nl> + return split . getLocations ( ) [ 0 ] ; <nl> + } <nl> + <nl> + public int size ( ) <nl> + { <nl> + maybeInit ( ) ; <nl> + return rows . size ( ) ; <nl> + } <nl> + <nl> + public int rowsRead ( ) <nl> + { <nl> + return i ; <nl> + } <nl> + <nl> + @ Override <nl> + protected Pair < String , SortedMap < byte [ ] , IColumn > > computeNext ( ) <nl> + { <nl> + maybeInit ( ) ; <nl> + if ( i = = rows . size ( ) ) <nl> + return endOfData ( ) ; <nl> + KeySlice ks = rows . get ( i + + ) ; <nl> + SortedMap < byte [ ] , IColumn > map = new TreeMap < byte [ ] , IColumn > ( comparator ) ; <nl> + for ( ColumnOrSuperColumn cosc : ks . columns ) <nl> + { <nl> + IColumn column = unthriftify ( cosc ) ; <nl> + map . put ( column . name ( ) , column ) ; <nl> + } <nl> + return new Pair < String , SortedMap < byte [ ] , IColumn > > ( ks . key , map ) ; <nl> + } <nl> + } <nl> + <nl> + private IColumn unthriftify ( ColumnOrSuperColumn cosc ) <nl> + { <nl> + if ( cosc . column = = null ) <nl> + return unthriftifySuper ( cosc . super _ column ) ; <nl> + return unthriftifySimple ( cosc . column ) ; <nl> + } <nl> + <nl> + private IColumn unthriftifySuper ( SuperColumn super _ column ) <nl> + { <nl> + AbstractType subComparator = DatabaseDescriptor . getSubComparator ( split . getTable ( ) , split . getColumnFamily ( ) ) ; <nl> + org . apache . cassandra . db . SuperColumn sc = new org . apache . cassandra . db . SuperColumn ( super _ column . name , subComparator ) ; <nl> + for ( Column column : super _ column . columns ) <nl> + { <nl> + sc . addColumn ( unthriftifySimple ( column ) ) ; <nl> + } <nl> + return sc ; <nl> + } <nl> + <nl> + private IColumn unthriftifySimple ( Column column ) <nl> + { <nl> + return new org . apache . cassandra . db . Column ( column . name , column . value , column . timestamp ) ; <nl> + } <nl> + } <nl> \ No newline at end of file <nl> diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilySplit . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilySplit . java <nl> new file mode 100644 <nl> index 0000000 . . 7a57511 <nl> - - - / dev / null <nl> + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilySplit . java <nl> @ @ - 0 , 0 + 1 , 102 @ @ <nl> + package org . apache . cassandra . hadoop ; <nl> + <nl> + import java . io . DataInput ; <nl> + import java . io . DataOutput ; <nl> + import java . io . IOException ; <nl> + <nl> + import org . apache . hadoop . io . Writable ; <nl> + import org . apache . hadoop . mapreduce . InputSplit ; <nl> + <nl> + public class ColumnFamilySplit extends InputSplit implements Writable <nl> + { <nl> + private String startToken ; <nl> + private String endToken ; <nl> + private String table ; <nl> + private String columnFamily ; <nl> + private String [ ] dataNodes ; <nl> + <nl> + public ColumnFamilySplit ( String table , String columnFamily , String startToken , String endToken , String [ ] dataNodes ) <nl> + { <nl> + assert startToken ! = null ; <nl> + assert endToken ! = null ; <nl> + this . startToken = startToken ; <nl> + this . endToken = endToken ; <nl> + this . columnFamily = columnFamily ; <nl> + this . table = table ; <nl> + this . dataNodes = dataNodes ; <nl> + } <nl> + <nl> + public String getStartToken ( ) <nl> + { <nl> + return startToken ; <nl> + } <nl> + <nl> + public String getEndToken ( ) <nl> + { <nl> + return endToken ; <nl> + } <nl> + <nl> + public String getTable ( ) <nl> + { <nl> + return table ; <nl> + } <nl> + <nl> + public String getColumnFamily ( ) <nl> + { <nl> + return columnFamily ; <nl> + } <nl> + <nl> + / / getLength and getLocations satisfy the InputSplit abstraction <nl> + <nl> + public long getLength ( ) <nl> + { <nl> + / / only used for sorting splits . we don ' t have the capability , yet . <nl> + return 0 ; <nl> + } <nl> + <nl> + public String [ ] getLocations ( ) <nl> + { <nl> + return dataNodes ; <nl> + } <nl> + <nl> + / / This should only be used by KeyspaceSplit . read ( ) ; <nl> + protected ColumnFamilySplit ( ) { } <nl> + <nl> + / / These three methods are for serializing and deserializing <nl> + / / KeyspaceSplits as needed by the Writable interface . <nl> + public void write ( DataOutput out ) throws IOException <nl> + { <nl> + out . writeUTF ( table ) ; <nl> + out . writeUTF ( columnFamily ) ; <nl> + out . writeUTF ( startToken ) ; <nl> + out . writeUTF ( endToken ) ; <nl> + <nl> + out . writeInt ( dataNodes . length ) ; <nl> + for ( String endPoint : dataNodes ) <nl> + { <nl> + out . writeUTF ( endPoint ) ; <nl> + } <nl> + } <nl> + <nl> + public void readFields ( DataInput in ) throws IOException <nl> + { <nl> + table = in . readUTF ( ) ; <nl> + columnFamily = in . readUTF ( ) ; <nl> + startToken = in . readUTF ( ) ; <nl> + endToken = in . readUTF ( ) ; <nl> + <nl> + int numOfEndPoints = in . readInt ( ) ; <nl> + dataNodes = new String [ numOfEndPoints ] ; <nl> + for ( int i = 0 ; i < numOfEndPoints ; i + + ) <nl> + { <nl> + dataNodes [ i ] = in . readUTF ( ) ; <nl> + } <nl> + } <nl> + <nl> + public static ColumnFamilySplit read ( DataInput in ) throws IOException <nl> + { <nl> + ColumnFamilySplit w = new ColumnFamilySplit ( ) ; <nl> + w . readFields ( in ) ; <nl> + return w ; <nl> + } <nl> + } <nl> \ No newline at end of file

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 39656ff . . 91037d1 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 1 , 5 + 1 , 6 @ @ 
 2 . 0 . 7 
 * Fix saving triggers to schema ( CASSANDRA - 6789 ) 
 + * Fix trigger mutations when base mutation list is immutable ( CASSANDRA - 6790 ) 
 
 
 2 . 0 . 6 
 diff - - git a / src / java / org / apache / cassandra / service / StorageProxy . java b / src / java / org / apache / cassandra / service / StorageProxy . java 
 index 14c1ce3 . . a6db9cd 100644 
 - - - a / src / java / org / apache / cassandra / service / StorageProxy . java 
 + + + b / src / java / org / apache / cassandra / service / StorageProxy . java 
 @ @ - 508 , 13 + 508 , 13 @ @ public class StorageProxy implements StorageProxyMBean 
 } 
 } 
 
 - public static void mutateWithTriggers ( Collection < ? extends IMutation > mutations , ConsistencyLevel consistencyLevel , boolean mutateAtomically ) throws WriteTimeoutException , UnavailableException , 
 - OverloadedException , InvalidRequestException 
 + public static void mutateWithTriggers ( Collection < ? extends IMutation > mutations , ConsistencyLevel consistencyLevel , boolean mutateAtomically ) 
 + throws WriteTimeoutException , UnavailableException , OverloadedException , InvalidRequestException 
 { 
 Collection < RowMutation > tmutations = TriggerExecutor . instance . execute ( mutations ) ; 
 if ( mutateAtomically | | tmutations ! = null ) 
 { 
 - Collection < RowMutation > allMutations = ( Collection < RowMutation > ) mutations ; 
 + Collection < RowMutation > allMutations = new ArrayList < > ( ( Collection < RowMutation > ) mutations ) ; 
 if ( tmutations ! = null ) 
 allMutations . addAll ( tmutations ) ; 
 StorageProxy . mutateAtomically ( allMutations , consistencyLevel ) ; 
 diff - - git a / test / unit / org / apache / cassandra / triggers / TriggersTest . java b / test / unit / org / apache / cassandra / triggers / TriggersTest . java 
 new file mode 100644 
 index 0000000 . . 6ca3880 
 - - - / dev / null 
 + + + b / test / unit / org / apache / cassandra / triggers / TriggersTest . java 
 @ @ - 0 , 0 + 1 , 179 @ @ 
 + / * 
 + * Licensed to the Apache Software Foundation ( ASF ) under one 
 + * or more contributor license agreements . See the NOTICE file 
 + * distributed with this work for additional information 
 + * regarding copyright ownership . The ASF licenses this file 
 + * to you under the Apache License , Version 2 . 0 ( the 
 + * " License " ) ; you may not use this file except in compliance 
 + * with the License . You may obtain a copy of the License at 
 + * 
 + * http : / / www . apache . org / licenses / LICENSE - 2 . 0 
 + * 
 + * Unless required by applicable law or agreed to in writing , software 
 + * distributed under the License is distributed on an " AS IS " BASIS , 
 + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . 
 + * See the License for the specific language governing permissions and 
 + * limitations under the License . 
 + * / 
 + package org . apache . cassandra . triggers ; 
 + 
 + import java . net . InetAddress ; 
 + import java . nio . ByteBuffer ; 
 + import java . util . Collection ; 
 + import java . util . Collections ; 
 + 
 + import org . junit . AfterClass ; 
 + import org . junit . Before ; 
 + import org . junit . Test ; 
 + 
 + import org . apache . cassandra . SchemaLoader ; 
 + import org . apache . cassandra . config . Schema ; 
 + import org . apache . cassandra . cql3 . QueryProcessor ; 
 + import org . apache . cassandra . cql3 . UntypedResultSet ; 
 + import org . apache . cassandra . db . ArrayBackedSortedColumns ; 
 + import org . apache . cassandra . db . Column ; 
 + import org . apache . cassandra . db . ColumnFamily ; 
 + import org . apache . cassandra . db . ConsistencyLevel ; 
 + import org . apache . cassandra . db . RowMutation ; 
 + import org . apache . cassandra . service . StorageService ; 
 + import org . apache . cassandra . thrift . Cassandra ; 
 + import org . apache . cassandra . thrift . ColumnOrSuperColumn ; 
 + import org . apache . cassandra . thrift . ColumnParent ; 
 + import org . apache . cassandra . thrift . Mutation ; 
 + import org . apache . cassandra . thrift . TFramedTransportFactory ; 
 + import org . apache . cassandra . thrift . ThriftServer ; 
 + import org . apache . cassandra . utils . ByteBufferUtil ; 
 + import org . apache . thrift . protocol . TBinaryProtocol ; 
 + 
 + import static org . junit . Assert . assertEquals ; 
 + 
 + public class TriggersTest extends SchemaLoader 
 + { 
 + private static boolean triggerCreated = false ; 
 + private static ThriftServer thriftServer ; 
 + 
 + private static String ksName = " triggers _ test _ ks " ; 
 + private static String cfName = " test _ table " ; 
 + 
 + @ Before 
 + public void setup ( ) throws Exception 
 + { 
 + StorageService . instance . initServer ( 0 ) ; 
 + if ( thriftServer = = null | | ! thriftServer . isRunning ( ) ) 
 + { 
 + thriftServer = new ThriftServer ( InetAddress . getLocalHost ( ) , 9170 ) ; 
 + thriftServer . start ( ) ; 
 + } 
 + 
 + String cql = String . format ( " CREATE KEYSPACE IF NOT EXISTS % s " + 
 + " WITH REPLICATION = { ' class ' : ' SimpleStrategy ' , ' replication _ factor ' : 1 } " , 
 + ksName ) ; 
 + QueryProcessor . process ( cql , ConsistencyLevel . ONE ) ; 
 + 
 + cql = String . format ( " CREATE TABLE IF NOT EXISTS % s . % s ( k int , v1 int , v2 int , PRIMARY KEY ( k ) ) " , ksName , cfName ) ; 
 + QueryProcessor . process ( cql , ConsistencyLevel . ONE ) ; 
 + 
 + / / no conditional execution of create trigger stmt yet 
 + if ( ! triggerCreated ) 
 + { 
 + cql = String . format ( " CREATE TRIGGER trigger _ 1 ON % s . % s USING ' % s ' " , 
 + ksName , cfName , TestTrigger . class . getName ( ) ) ; 
 + QueryProcessor . process ( cql , ConsistencyLevel . ONE ) ; 
 + triggerCreated = true ; 
 + } 
 + } 
 + 
 + @ AfterClass 
 + public static void teardown ( ) 
 + { 
 + if ( thriftServer ! = null & & thriftServer . isRunning ( ) ) 
 + { 
 + thriftServer . stop ( ) ; 
 + } 
 + } 
 + 
 + @ Test 
 + public void executeTriggerOnCqlInsert ( ) throws Exception 
 + { 
 + String cql = String . format ( " INSERT INTO % s . % s ( k , v1 ) VALUES ( 0 , 0 ) " , ksName , cfName ) ; 
 + QueryProcessor . process ( cql , ConsistencyLevel . ONE ) ; 
 + assertUpdateIsAugmented ( 0 ) ; 
 + } 
 + 
 + @ Test 
 + public void executeTriggerOnCqlBatchInsert ( ) throws Exception 
 + { 
 + String cql = String . format ( " BEGIN BATCH " + 
 + " INSERT INTO % s . % s ( k , v1 ) VALUES ( 1 , 1 ) ; " + 
 + " APPLY BATCH " , 
 + ksName , cfName ) ; 
 + QueryProcessor . process ( cql , ConsistencyLevel . ONE ) ; 
 + assertUpdateIsAugmented ( 1 ) ; 
 + } 
 + 
 + @ Test 
 + public void executeTriggerOnThriftInsert ( ) throws Exception 
 + { 
 + Cassandra . Client client = new Cassandra . Client ( 
 + new TBinaryProtocol ( 
 + new TFramedTransportFactory ( ) . openTransport ( 
 + InetAddress . getLocalHost ( ) . getHostName ( ) , 9170 ) ) ) ; 
 + client . set _ keyspace ( ksName ) ; 
 + client . insert ( ByteBufferUtil . bytes ( 2 ) , 
 + new ColumnParent ( cfName ) , 
 + getColumnForInsert ( " v1 " , 2 ) , 
 + org . apache . cassandra . thrift . ConsistencyLevel . ONE ) ; 
 + 
 + assertUpdateIsAugmented ( 2 ) ; 
 + } 
 + 
 + @ Test 
 + public void executeTriggerOnThriftBatchUpdate ( ) throws Exception 
 + { 
 + Cassandra . Client client = new Cassandra . Client ( 
 + new TBinaryProtocol ( 
 + new TFramedTransportFactory ( ) . openTransport ( 
 + InetAddress . getLocalHost ( ) . getHostName ( ) , 9170 ) ) ) ; 
 + client . set _ keyspace ( ksName ) ; 
 + Mutation mutation = new Mutation ( ) ; 
 + ColumnOrSuperColumn cosc = new ColumnOrSuperColumn ( ) ; 
 + cosc . setColumn ( getColumnForInsert ( " v1 " , 3 ) ) ; 
 + mutation . setColumn _ or _ supercolumn ( cosc ) ; 
 + client . batch _ mutate ( 
 + Collections . singletonMap ( ByteBufferUtil . bytes ( 3 ) , 
 + Collections . singletonMap ( cfName , 
 + Collections . singletonList ( mutation ) ) ) , 
 + org . apache . cassandra . thrift . ConsistencyLevel . ONE ) ; 
 + 
 + assertUpdateIsAugmented ( 3 ) ; 
 + } 
 + 
 + private void assertUpdateIsAugmented ( int key ) 
 + { 
 + UntypedResultSet rs = QueryProcessor . processInternal ( 
 + String . format ( " SELECT * FROM % s . % s WHERE k = % s " , ksName , cfName , key ) ) ; 
 + assertEquals ( 999 , rs . one ( ) . getInt ( " v2 " ) ) ; 
 + } 
 + 
 + private org . apache . cassandra . thrift . Column getColumnForInsert ( String columnName , int value ) 
 + { 
 + org . apache . cassandra . thrift . Column column = new org . apache . cassandra . thrift . Column ( ) ; 
 + column . setName ( Schema . instance . getCFMetaData ( ksName , cfName ) . comparator . fromString ( columnName ) ) ; 
 + column . setValue ( ByteBufferUtil . bytes ( value ) ) ; 
 + column . setTimestamp ( System . currentTimeMillis ( ) ) ; 
 + return column ; 
 + } 
 + 
 + public static class TestTrigger implements ITrigger 
 + { 
 + public Collection < RowMutation > augment ( ByteBuffer key , ColumnFamily update ) 
 + { 
 + ColumnFamily extraUpdate = update . cloneMeShallow ( ArrayBackedSortedColumns . factory , false ) ; 
 + extraUpdate . addColumn ( new Column ( update . metadata ( ) . comparator . fromString ( " v2 " ) , 
 + ByteBufferUtil . bytes ( 999 ) ) ) ; 
 + RowMutation rm = new RowMutation ( ksName , key ) ; 
 + rm . add ( extraUpdate ) ; 
 + return Collections . singletonList ( rm ) ; 
 + } 
 + } 
 + }

NEAREST DIFF:
diff - - git a / ivy . xml b / ivy . xml 
 index e6f1fbc . . b85534c 100644 
 - - - a / ivy . xml 
 + + + b / ivy . xml 
 @ @ - 19 , 6 + 19 , 8 @ @ 
 < ivy - module version = " 2 . 0 " > 
 < info organisation = " apache - cassandra " module = " cassandra " / > 
 < dependencies > 
 + < dependency org = " org . apache . mahout . hadoop " 
 + name = " hadoop - core " rev = " 0 . 20 . 1 " / > 
 < ! - - FIXME : paranamer and jackson can be dropped after we ' re depending 
 on avro ( since it depends on them ) . - - > 
 < dependency org = " com . thoughtworks . paranamer " 
 diff - - git a / src / java / org / apache / cassandra / db / SuperColumn . java b / src / java / org / apache / cassandra / db / SuperColumn . java 
 index c588930 . . 82438a7 100644 
 - - - a / src / java / org / apache / cassandra / db / SuperColumn . java 
 + + + b / src / java / org / apache / cassandra / db / SuperColumn . java 
 @ @ - 47 , 7 + 47 , 7 @ @ public class SuperColumn implements IColumn , IColumnContainer 
 private AtomicInteger localDeletionTime = new AtomicInteger ( Integer . MIN _ VALUE ) ; 
 	 private AtomicLong markedForDeleteAt = new AtomicLong ( Long . MIN _ VALUE ) ; 
 
 - SuperColumn ( byte [ ] name , AbstractType comparator ) 
 + public SuperColumn ( byte [ ] name , AbstractType comparator ) 
 { 
 this ( name , new ConcurrentSkipListMap < byte [ ] , IColumn > ( comparator ) ) ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java 
 new file mode 100644 
 index 0000000 . . 900c441 
 - - - / dev / null 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyInputFormat . java 
 @ @ - 0 , 0 + 1 , 112 @ @ 
 + package org . apache . cassandra . hadoop ; 
 + 
 + import java . io . IOException ; 
 + import java . util . ArrayList ; 
 + import java . util . List ; 
 + import java . util . Map ; 
 + import java . util . SortedMap ; 
 + 
 + import org . apache . log4j . Logger ; 
 + 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 + import org . apache . cassandra . db . IColumn ; 
 + import org . apache . cassandra . dht . IPartitioner ; 
 + import org . apache . cassandra . service . StorageService ; 
 + import org . apache . cassandra . thrift . * ; 
 + import org . apache . hadoop . conf . Configuration ; 
 + import org . apache . hadoop . mapreduce . * ; 
 + import org . apache . thrift . TException ; 
 + import org . apache . thrift . protocol . TBinaryProtocol ; 
 + import org . apache . thrift . transport . TSocket ; 
 + import org . apache . thrift . transport . TTransportException ; 
 + 
 + public class ColumnFamilyInputFormat extends InputFormat < String , SortedMap < byte [ ] , IColumn > > 
 + { 
 + private static final String KEYSPACE _ CONFIG = " cassandra . input . keyspace " ; 
 + private static final String COLUMNFAMILY _ CONFIG = " cassandra . input . columnfamily " ; 
 + 
 + private static final Logger logger = Logger . getLogger ( StorageService . class ) ; 
 + 
 + private String keyspace ; 
 + private String columnFamily ; 
 + 
 + public static void setColumnFamily ( Job job , String keyspace , String columnFamily ) 
 + { 
 + validateNotNullKeyspaceAndColumnFamily ( keyspace , columnFamily ) ; 
 + try 
 + { 
 + ThriftValidation . validateColumnFamily ( keyspace , columnFamily ) ; 
 + } 
 + catch ( InvalidRequestException e ) 
 + { 
 + throw new RuntimeException ( e ) ; 
 + } 
 + Configuration conf = job . getConfiguration ( ) ; 
 + conf . set ( KEYSPACE _ CONFIG , keyspace ) ; 
 + conf . set ( COLUMNFAMILY _ CONFIG , columnFamily ) ; 
 + } 
 + 
 + private static void validateNotNullKeyspaceAndColumnFamily ( String keyspace , String columnFamily ) 
 + { 
 + if ( keyspace = = null ) 
 + { 
 + throw new RuntimeException ( " you forgot to set the keyspace with setKeyspace ( ) " ) ; 
 + } 
 + if ( columnFamily = = null ) 
 + { 
 + throw new RuntimeException ( " you forgot to set the column family with setColumnFamily ( ) " ) ; 
 + } 
 + } 
 + 
 + public List < InputSplit > getSplits ( JobContext context ) throws IOException 
 + { 
 + Configuration conf = context . getConfiguration ( ) ; 
 + keyspace = conf . get ( KEYSPACE _ CONFIG ) ; 
 + columnFamily = conf . get ( COLUMNFAMILY _ CONFIG ) ; 
 + validateNotNullKeyspaceAndColumnFamily ( keyspace , columnFamily ) ; 
 + 
 + List < TokenRange > map = getRangeMap ( ) ; 
 + ArrayList < InputSplit > splits = new ArrayList < InputSplit > ( ) ; 
 + for ( TokenRange entry : map ) 
 + { 
 + if ( logger . isDebugEnabled ( ) ) 
 + logger . debug ( " split range is [ " + entry . start _ token + " , " + entry . end _ token + " ] " ) ; 
 + String [ ] endpoints = entry . endpoints . toArray ( new String [ 0 ] ) ; 
 + splits . add ( new ColumnFamilySplit ( keyspace , columnFamily , entry . start _ token , entry . end _ token , endpoints ) ) ; 
 + } 
 + 
 + return splits ; 
 + } 
 + 
 + private List < TokenRange > getRangeMap ( ) throws IOException 
 + { 
 + TSocket socket = new TSocket ( DatabaseDescriptor . getSeeds ( ) . iterator ( ) . next ( ) . getHostAddress ( ) , 
 + DatabaseDescriptor . getThriftPort ( ) ) ; 
 + TBinaryProtocol binaryProtocol = new TBinaryProtocol ( socket , false , false ) ; 
 + Cassandra . Client client = new Cassandra . Client ( binaryProtocol ) ; 
 + try 
 + { 
 + socket . open ( ) ; 
 + } 
 + catch ( TTransportException e ) 
 + { 
 + throw new IOException ( e ) ; 
 + } 
 + List < TokenRange > map ; 
 + try 
 + { 
 + map = client . describe _ ring ( keyspace ) ; 
 + } 
 + catch ( TException e ) 
 + { 
 + throw new RuntimeException ( e ) ; 
 + } 
 + return map ; 
 + } 
 + 
 + @ Override 
 + public RecordReader < String , SortedMap < byte [ ] , IColumn > > createRecordReader ( InputSplit inputSplit , TaskAttemptContext taskAttemptContext ) throws IOException , InterruptedException 
 + { 
 + return new ColumnFamilyRecordReader ( ) ; 
 + } 
 + } 
 \ No newline at end of file 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java 
 new file mode 100644 
 index 0000000 . . f03a247 
 - - - / dev / null 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilyRecordReader . java 
 @ @ - 0 , 0 + 1 , 196 @ @ 
 + package org . apache . cassandra . hadoop ; 
 + 
 + import java . io . IOException ; 
 + import java . net . InetAddress ; 
 + import java . net . UnknownHostException ; 
 + import java . util . List ; 
 + import java . util . SortedMap ; 
 + import java . util . TreeMap ; 
 + 
 + import org . apache . commons . lang . ArrayUtils ; 
 + 
 + import com . google . common . collect . AbstractIterator ; 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 + import org . apache . cassandra . db . * ; 
 + import org . apache . cassandra . db . marshal . AbstractType ; 
 + import org . apache . cassandra . thrift . * ; 
 + import org . apache . cassandra . thrift . Column ; 
 + import org . apache . cassandra . thrift . SuperColumn ; 
 + import org . apache . cassandra . utils . Pair ; 
 + import org . apache . hadoop . mapreduce . InputSplit ; 
 + import org . apache . hadoop . mapreduce . RecordReader ; 
 + import org . apache . hadoop . mapreduce . TaskAttemptContext ; 
 + import org . apache . thrift . protocol . TBinaryProtocol ; 
 + import org . apache . thrift . transport . TSocket ; 
 + import org . apache . thrift . transport . TTransportException ; 
 + 
 + public class ColumnFamilyRecordReader extends RecordReader < String , SortedMap < byte [ ] , IColumn > > 
 + { 
 + private static final int ROWS _ PER _ RANGE _ QUERY = 1024 ; 
 + 
 + private ColumnFamilySplit split ; 
 + private RowIterator iter ; 
 + private Pair < String , SortedMap < byte [ ] , IColumn > > currentRow ; 
 + 
 + public void close ( ) { } 
 + 
 + public String getCurrentKey ( ) 
 + { 
 + return currentRow . left ; 
 + } 
 + 
 + public SortedMap < byte [ ] , IColumn > getCurrentValue ( ) 
 + { 
 + return currentRow . right ; 
 + } 
 + 
 + public float getProgress ( ) 
 + { 
 + return ( ( float ) iter . rowsRead ( ) ) / iter . size ( ) ; 
 + } 
 + 
 + public void initialize ( InputSplit split , TaskAttemptContext context ) throws IOException 
 + { 
 + this . split = ( ColumnFamilySplit ) split ; 
 + iter = new RowIterator ( ) ; 
 + } 
 + 
 + public boolean nextKeyValue ( ) throws IOException 
 + { 
 + if ( ! iter . hasNext ( ) ) 
 + return false ; 
 + currentRow = iter . next ( ) ; 
 + return true ; 
 + } 
 + 
 + private class RowIterator extends AbstractIterator < Pair < String , SortedMap < byte [ ] , IColumn > > > 
 + { 
 + 
 + private List < KeySlice > rows ; 
 + private int i = 0 ; 
 + private AbstractType comparator = DatabaseDescriptor . getComparator ( split . getTable ( ) , split . getColumnFamily ( ) ) ; 
 + 
 + private void maybeInit ( ) 
 + { 
 + if ( rows ! = null ) 
 + return ; 
 + TSocket socket = new TSocket ( getLocation ( ) , 
 + DatabaseDescriptor . getThriftPort ( ) ) ; 
 + TBinaryProtocol binaryProtocol = new TBinaryProtocol ( socket , false , false ) ; 
 + Cassandra . Client client = new Cassandra . Client ( binaryProtocol ) ; 
 + try 
 + { 
 + socket . open ( ) ; 
 + } 
 + catch ( TTransportException e ) 
 + { 
 + throw new RuntimeException ( e ) ; 
 + } 
 + SliceRange sliceRange = new SliceRange ( ArrayUtils . EMPTY _ BYTE _ ARRAY , 
 + ArrayUtils . EMPTY _ BYTE _ ARRAY , 
 + false , 
 + Integer . MAX _ VALUE ) ; 
 + KeyRange keyRange = new KeyRange ( ROWS _ PER _ RANGE _ QUERY ) 
 + . setStart _ token ( split . getStartToken ( ) ) 
 + . setEnd _ token ( split . getEndToken ( ) ) ; 
 + / / TODO " paging " large rows would be good 
 + try 
 + { 
 + rows = client . get _ range _ slices ( split . getTable ( ) , 
 + new ColumnParent ( split . getColumnFamily ( ) ) , 
 + new SlicePredicate ( ) . setSlice _ range ( sliceRange ) , 
 + keyRange , 
 + ConsistencyLevel . ONE ) ; 
 + } 
 + catch ( Exception e ) 
 + { 
 + throw new RuntimeException ( e ) ; 
 + } 
 + } 
 + 
 + / / we don ' t use endpointsnitch since we are trying to support hadoop nodes that are 
 + / / not necessarily on Cassandra machines , too . This should be adequate for single - DC clusters , at least . 
 + private String getLocation ( ) 
 + { 
 + InetAddress [ ] localAddresses = new InetAddress [ 0 ] ; 
 + try 
 + { 
 + localAddresses = InetAddress . getAllByName ( InetAddress . getLocalHost ( ) . getHostAddress ( ) ) ; 
 + } 
 + catch ( UnknownHostException e ) 
 + { 
 + throw new AssertionError ( e ) ; 
 + } 
 + for ( InetAddress address : localAddresses ) 
 + { 
 + for ( String location : split . getLocations ( ) ) 
 + { 
 + InetAddress locationAddress = null ; 
 + try 
 + { 
 + locationAddress = InetAddress . getByName ( location ) ; 
 + } 
 + catch ( UnknownHostException e ) 
 + { 
 + throw new AssertionError ( e ) ; 
 + } 
 + if ( address . equals ( locationAddress ) ) 
 + { 
 + return location ; 
 + } 
 + } 
 + } 
 + return split . getLocations ( ) [ 0 ] ; 
 + } 
 + 
 + public int size ( ) 
 + { 
 + maybeInit ( ) ; 
 + return rows . size ( ) ; 
 + } 
 + 
 + public int rowsRead ( ) 
 + { 
 + return i ; 
 + } 
 + 
 + @ Override 
 + protected Pair < String , SortedMap < byte [ ] , IColumn > > computeNext ( ) 
 + { 
 + maybeInit ( ) ; 
 + if ( i = = rows . size ( ) ) 
 + return endOfData ( ) ; 
 + KeySlice ks = rows . get ( i + + ) ; 
 + SortedMap < byte [ ] , IColumn > map = new TreeMap < byte [ ] , IColumn > ( comparator ) ; 
 + for ( ColumnOrSuperColumn cosc : ks . columns ) 
 + { 
 + IColumn column = unthriftify ( cosc ) ; 
 + map . put ( column . name ( ) , column ) ; 
 + } 
 + return new Pair < String , SortedMap < byte [ ] , IColumn > > ( ks . key , map ) ; 
 + } 
 + } 
 + 
 + private IColumn unthriftify ( ColumnOrSuperColumn cosc ) 
 + { 
 + if ( cosc . column = = null ) 
 + return unthriftifySuper ( cosc . super _ column ) ; 
 + return unthriftifySimple ( cosc . column ) ; 
 + } 
 + 
 + private IColumn unthriftifySuper ( SuperColumn super _ column ) 
 + { 
 + AbstractType subComparator = DatabaseDescriptor . getSubComparator ( split . getTable ( ) , split . getColumnFamily ( ) ) ; 
 + org . apache . cassandra . db . SuperColumn sc = new org . apache . cassandra . db . SuperColumn ( super _ column . name , subComparator ) ; 
 + for ( Column column : super _ column . columns ) 
 + { 
 + sc . addColumn ( unthriftifySimple ( column ) ) ; 
 + } 
 + return sc ; 
 + } 
 + 
 + private IColumn unthriftifySimple ( Column column ) 
 + { 
 + return new org . apache . cassandra . db . Column ( column . name , column . value , column . timestamp ) ; 
 + } 
 + } 
 \ No newline at end of file 
 diff - - git a / src / java / org / apache / cassandra / hadoop / ColumnFamilySplit . java b / src / java / org / apache / cassandra / hadoop / ColumnFamilySplit . java 
 new file mode 100644 
 index 0000000 . . 7a57511 
 - - - / dev / null 
 + + + b / src / java / org / apache / cassandra / hadoop / ColumnFamilySplit . java 
 @ @ - 0 , 0 + 1 , 102 @ @ 
 + package org . apache . cassandra . hadoop ; 
 + 
 + import java . io . DataInput ; 
 + import java . io . DataOutput ; 
 + import java . io . IOException ; 
 + 
 + import org . apache . hadoop . io . Writable ; 
 + import org . apache . hadoop . mapreduce . InputSplit ; 
 + 
 + public class ColumnFamilySplit extends InputSplit implements Writable 
 + { 
 + private String startToken ; 
 + private String endToken ; 
 + private String table ; 
 + private String columnFamily ; 
 + private String [ ] dataNodes ; 
 + 
 + public ColumnFamilySplit ( String table , String columnFamily , String startToken , String endToken , String [ ] dataNodes ) 
 + { 
 + assert startToken ! = null ; 
 + assert endToken ! = null ; 
 + this . startToken = startToken ; 
 + this . endToken = endToken ; 
 + this . columnFamily = columnFamily ; 
 + this . table = table ; 
 + this . dataNodes = dataNodes ; 
 + } 
 + 
 + public String getStartToken ( ) 
 + { 
 + return startToken ; 
 + } 
 + 
 + public String getEndToken ( ) 
 + { 
 + return endToken ; 
 + } 
 + 
 + public String getTable ( ) 
 + { 
 + return table ; 
 + } 
 + 
 + public String getColumnFamily ( ) 
 + { 
 + return columnFamily ; 
 + } 
 + 
 + / / getLength and getLocations satisfy the InputSplit abstraction 
 + 
 + public long getLength ( ) 
 + { 
 + / / only used for sorting splits . we don ' t have the capability , yet . 
 + return 0 ; 
 + } 
 + 
 + public String [ ] getLocations ( ) 
 + { 
 + return dataNodes ; 
 + } 
 + 
 + / / This should only be used by KeyspaceSplit . read ( ) ; 
 + protected ColumnFamilySplit ( ) { } 
 + 
 + / / These three methods are for serializing and deserializing 
 + / / KeyspaceSplits as needed by the Writable interface . 
 + public void write ( DataOutput out ) throws IOException 
 + { 
 + out . writeUTF ( table ) ; 
 + out . writeUTF ( columnFamily ) ; 
 + out . writeUTF ( startToken ) ; 
 + out . writeUTF ( endToken ) ; 
 + 
 + out . writeInt ( dataNodes . length ) ; 
 + for ( String endPoint : dataNodes ) 
 + { 
 + out . writeUTF ( endPoint ) ; 
 + } 
 + } 
 + 
 + public void readFields ( DataInput in ) throws IOException 
 + { 
 + table = in . readUTF ( ) ; 
 + columnFamily = in . readUTF ( ) ; 
 + startToken = in . readUTF ( ) ; 
 + endToken = in . readUTF ( ) ; 
 + 
 + int numOfEndPoints = in . readInt ( ) ; 
 + dataNodes = new String [ numOfEndPoints ] ; 
 + for ( int i = 0 ; i < numOfEndPoints ; i + + ) 
 + { 
 + dataNodes [ i ] = in . readUTF ( ) ; 
 + } 
 + } 
 + 
 + public static ColumnFamilySplit read ( DataInput in ) throws IOException 
 + { 
 + ColumnFamilySplit w = new ColumnFamilySplit ( ) ; 
 + w . readFields ( in ) ; 
 + return w ; 
 + } 
 + } 
 \ No newline at end of file
