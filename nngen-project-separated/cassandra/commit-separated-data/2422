BLEU SCORE: 0.02955926526467443

TEST MSG: Fix availability validation for LOCAL _ ONE CL
GENERATED MSG: Make the CL native protocol code match the on in 2 . 0

TEST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index 619c219 . . ba3a675 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 25 , 6 + 25 , 7 @ @ <nl> * reduce garbage creation in calculatePendingRanges ( CASSANDRA - 7191 ) <nl> * exit CQLSH with error status code if script fails ( CASSANDRA - 6344 ) <nl> * Fix bug with some IN queries missig results ( CASSANDRA - 7105 ) <nl> + * Fix availability validation for LOCAL _ ONE CL ( CASSANDRA - 7319 ) <nl> <nl> <nl> 1 . 2 . 16 <nl> diff - - git a / src / java / org / apache / cassandra / db / ConsistencyLevel . java b / src / java / org / apache / cassandra / db / ConsistencyLevel . java <nl> index 3737c73 . . e65b61f 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ConsistencyLevel . java <nl> + + + b / src / java / org / apache / cassandra / db / ConsistencyLevel . java <nl> @ @ - 249 , 6 + 249 , 10 @ @ public enum ConsistencyLevel <nl> case ANY : <nl> / / local hint is acceptable , and local node is always live <nl> break ; <nl> + case LOCAL _ ONE : <nl> + if ( countLocalEndpoints ( liveEndpoints ) = = 0 ) <nl> + throw new UnavailableException ( this , 1 , 0 ) ; <nl> + break ; <nl> case LOCAL _ QUORUM : <nl> int localLive = countLocalEndpoints ( liveEndpoints ) ; <nl> if ( localLive < blockFor )
NEAREST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / db / CollationController . java b / src / java / org / apache / cassandra / db / CollationController . java <nl> index 758d523 . . 9896fde 100644 <nl> - - - a / src / java / org / apache / cassandra / db / CollationController . java <nl> + + + b / src / java / org / apache / cassandra / db / CollationController . java <nl> @ @ - 27 , 7 + 27 , 6 @ @ import org . apache . cassandra . db . compaction . SizeTieredCompactionStrategy ; <nl> import org . apache . cassandra . db . filter . NamesQueryFilter ; <nl> import org . apache . cassandra . db . filter . QueryFilter ; <nl> import org . apache . cassandra . db . marshal . CounterColumnType ; <nl> - import org . apache . cassandra . io . sstable . SSTable ; <nl> import org . apache . cassandra . io . sstable . SSTableReader ; <nl> import org . apache . cassandra . io . util . FileUtils ; <nl> import org . apache . cassandra . tracing . Tracing ; <nl> @ @ - 99 , 7 + 98 , 7 @ @ public class CollationController <nl> QueryFilter reducedFilter = new QueryFilter ( filter . key , filter . cfName , namesFilter . withUpdatedColumns ( filterColumns ) , filter . timestamp ) ; <nl> <nl> / * add the SSTables on disk * / <nl> - Collections . sort ( view . sstables , SSTable . maxTimestampComparator ) ; <nl> + Collections . sort ( view . sstables , SSTableReader . maxTimestampComparator ) ; <nl> <nl> / / read sorted sstables <nl> long mostRecentRowTombstone = Long . MIN _ VALUE ; <nl> @ @ - 219 , 7 + 218 , 7 @ @ public class CollationController <nl> * In othere words , iterating in maxTimestamp order allow to do our mostRecentTombstone elimination <nl> * in one pass , and minimize the number of sstables for which we read a rowTombstone . <nl> * / <nl> - Collections . sort ( view . sstables , SSTable . maxTimestampComparator ) ; <nl> + Collections . sort ( view . sstables , SSTableReader . maxTimestampComparator ) ; <nl> List < SSTableReader > skippedSSTables = null ; <nl> long mostRecentRowTombstone = Long . MIN _ VALUE ; <nl> long minTimestamp = Long . MAX _ VALUE ; <nl> diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> index 73704d4 . . 1b8a1bf 100644 <nl> - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java <nl> @ @ - 422 , 7 + 422 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> Descriptor desc = sstableFiles . getKey ( ) ; <nl> Set < Component > components = sstableFiles . getValue ( ) ; <nl> <nl> - if ( components . contains ( Component . COMPACTED _ MARKER ) | | desc . temporary ) <nl> + if ( desc . temporary ) <nl> { <nl> SSTable . delete ( desc , components ) ; <nl> continue ; <nl> @ @ - 1010 , 7 + 1010 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean <nl> { <nl> if ( operation ! = OperationType . CLEANUP | | isIndex ( ) ) <nl> { <nl> - return SSTable . getTotalBytes ( sstables ) ; <nl> + return SSTableReader . getTotalBytes ( sstables ) ; <nl> } <nl> <nl> / / cleanup size estimation only counts bytes for keys local to this node <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionController . java b / src / java / org / apache / cassandra / db / compaction / CompactionController . java <nl> index 201cd0a . . dc7730c 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / CompactionController . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / CompactionController . java <nl> @ @ - 30 , 7 + 30 , 6 @ @ import org . slf4j . LoggerFactory ; <nl> import org . apache . cassandra . db . ColumnFamilyStore ; <nl> import org . apache . cassandra . db . DataTracker ; <nl> import org . apache . cassandra . db . DecoratedKey ; <nl> - import org . apache . cassandra . io . sstable . SSTable ; <nl> import org . apache . cassandra . io . sstable . SSTableIdentityIterator ; <nl> import org . apache . cassandra . io . sstable . SSTableReader ; <nl> import org . apache . cassandra . utils . AlwaysPresentFilter ; <nl> @ @ - 118 , 7 + 117 , 7 @ @ public class CompactionController <nl> / / we still need to keep candidates that might shadow something in a <nl> / / non - candidate sstable . And if we remove a sstable from the candidates , we <nl> / / must take it ' s timestamp into account ( hence the sorting below ) . <nl> - Collections . sort ( candidates , SSTable . maxTimestampComparator ) ; <nl> + Collections . sort ( candidates , SSTableReader . maxTimestampComparator ) ; <nl> <nl> Iterator < SSTableReader > iterator = candidates . iterator ( ) ; <nl> while ( iterator . hasNext ( ) ) <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionTask . java b / src / java / org / apache / cassandra / db / compaction / CompactionTask . java <nl> index bc419ad . . f4cc500 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / CompactionTask . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / CompactionTask . java <nl> @ @ - 118 , 7 + 118 , 7 @ @ public class CompactionTask extends AbstractCompactionTask <nl> long totalkeysWritten = 0 ; <nl> <nl> long estimatedTotalKeys = Math . max ( cfs . metadata . getIndexInterval ( ) , SSTableReader . getApproximateKeyCount ( actuallyCompact , cfs . metadata ) ) ; <nl> - long estimatedSSTables = Math . max ( 1 , SSTable . getTotalBytes ( actuallyCompact ) / strategy . getMaxSSTableSize ( ) ) ; <nl> + long estimatedSSTables = Math . max ( 1 , SSTableReader . getTotalBytes ( actuallyCompact ) / strategy . getMaxSSTableSize ( ) ) ; <nl> long keysPerSSTable = ( long ) Math . ceil ( ( double ) estimatedTotalKeys / estimatedSSTables ) ; <nl> if ( logger . isDebugEnabled ( ) ) <nl> logger . debug ( " Expected bloom filter size : { } " , keysPerSSTable ) ; <nl> @ @ - 244 , 8 + 244 , 8 @ @ public class CompactionTask extends AbstractCompactionTask <nl> <nl> / / log a bunch of statistics about the result and save to system table compaction _ history <nl> long dTime = TimeUnit . NANOSECONDS . toMillis ( System . nanoTime ( ) - start ) ; <nl> - long startsize = SSTable . getTotalBytes ( toCompact ) ; <nl> - long endsize = SSTable . getTotalBytes ( sstables ) ; <nl> + long startsize = SSTableReader . getTotalBytes ( toCompact ) ; <nl> + long endsize = SSTableReader . getTotalBytes ( sstables ) ; <nl> double ratio = ( double ) endsize / ( double ) startsize ; <nl> <nl> StringBuilder builder = new StringBuilder ( ) ; <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java <nl> index e992003 . . 3eb5e91 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java <nl> @ @ - 32 , 7 + 32 , 6 @ @ import org . apache . cassandra . db . columniterator . OnDiskAtomIterator ; <nl> import org . apache . cassandra . dht . Range ; <nl> import org . apache . cassandra . dht . Token ; <nl> import org . apache . cassandra . exceptions . ConfigurationException ; <nl> - import org . apache . cassandra . io . sstable . SSTable ; <nl> import org . apache . cassandra . io . sstable . SSTableReader ; <nl> import org . apache . cassandra . notifications . INotification ; <nl> import org . apache . cassandra . notifications . INotificationConsumer ; <nl> @ @ - 234 , 7 + 233 , 7 @ @ public class LeveledCompactionStrategy extends AbstractCompactionStrategy implem <nl> } <nl> <nl> totalLength = length ; <nl> - Collections . sort ( this . sstables , SSTable . sstableComparator ) ; <nl> + Collections . sort ( this . sstables , SSTableReader . sstableComparator ) ; <nl> sstableIterator = this . sstables . iterator ( ) ; <nl> assert sstableIterator . hasNext ( ) ; / / caller should check intersecting first <nl> currentScanner = sstableIterator . next ( ) . getScanner ( range , CompactionManager . instance . getRateLimiter ( ) ) ; <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java b / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java <nl> index 63ad2e4 . . 2b79493 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java <nl> @ @ - 151 , 13 + 151 , 13 @ @ public class LeveledManifest <nl> minLevel = Math . min ( minLevel , ssTableReader . getSSTableLevel ( ) ) ; <nl> add ( ssTableReader ) ; <nl> } <nl> - lastCompactedKeys [ minLevel ] = SSTable . sstableOrdering . max ( added ) . last ; <nl> + lastCompactedKeys [ minLevel ] = SSTableReader . sstableOrdering . max ( added ) . last ; <nl> } <nl> <nl> public synchronized void repairOverlappingSSTables ( int level ) <nl> { <nl> SSTableReader previous = null ; <nl> - Collections . sort ( generations [ level ] , SSTable . sstableComparator ) ; <nl> + Collections . sort ( generations [ level ] , SSTableReader . sstableComparator ) ; <nl> List < SSTableReader > outOfOrderSSTables = new ArrayList < SSTableReader > ( ) ; <nl> for ( SSTableReader current : generations [ level ] ) <nl> { <nl> @ @ - 264 , 7 + 264 , 7 @ @ public class LeveledManifest <nl> / / we want to calculate score excluding compacting ones <nl> Set < SSTableReader > sstablesInLevel = Sets . newHashSet ( sstables ) ; <nl> Set < SSTableReader > remaining = Sets . difference ( sstablesInLevel , cfs . getDataTracker ( ) . getCompacting ( ) ) ; <nl> - double score = ( double ) SSTableReader . getTotalBytes ( remaining ) / ( double ) maxBytesForLevel ( i ) ; <nl> + double score = ( double ) SSTableReader . getTotalBytes ( remaining ) / ( double ) maxBytesForLevel ( i ) ; <nl> logger . debug ( " Compaction score for level { } is { } " , i , score ) ; <nl> <nl> if ( score > 1 . 001 ) <nl> @ @ - 454 , 7 + 454 , 7 @ @ public class LeveledManifest <nl> } <nl> <nl> / / leave everything in L0 if we didn ' t end up with a full sstable ' s worth of data <nl> - if ( SSTable . getTotalBytes ( candidates ) > maxSSTableSizeInBytes ) <nl> + if ( SSTableReader . getTotalBytes ( candidates ) > maxSSTableSizeInBytes ) <nl> { <nl> / / add sstables from L1 that overlap candidates <nl> / / if the overlapping ones are already busy in a compaction , leave it out . <nl> @ @ - 468 , 7 + 468 , 7 @ @ public class LeveledManifest <nl> } <nl> <nl> / / for non - L0 compactions , pick up where we left off last time <nl> - Collections . sort ( generations [ level ] , SSTable . sstableComparator ) ; <nl> + Collections . sort ( generations [ level ] , SSTableReader . sstableComparator ) ; <nl> int start = 0 ; / / handles case where the prior compaction touched the very last range <nl> for ( int i = 0 ; i < generations [ level ] . size ( ) ; i + + ) <nl> { <nl> @ @ - 499 , 7 + 499 , 7 @ @ public class LeveledManifest <nl> private List < SSTableReader > ageSortedSSTables ( Collection < SSTableReader > candidates ) <nl> { <nl> List < SSTableReader > ageSortedCandidates = new ArrayList < SSTableReader > ( candidates ) ; <nl> - Collections . sort ( ageSortedCandidates , SSTable . maxTimestampComparator ) ; <nl> + Collections . sort ( ageSortedCandidates , SSTableReader . maxTimestampComparator ) ; <nl> return ageSortedCandidates ; <nl> } <nl> <nl> @ @ - 557 , 7 + 557 , 7 @ @ public class LeveledManifest <nl> } <nl> <nl> int newLevel ; <nl> - if ( minimumLevel = = 0 & & minimumLevel = = maximumLevel & & SSTable . getTotalBytes ( sstables ) < maxSSTableSizeInBytes ) <nl> + if ( minimumLevel = = 0 & & minimumLevel = = maximumLevel & & SSTableReader . getTotalBytes ( sstables ) < maxSSTableSizeInBytes ) <nl> { <nl> newLevel = 0 ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / db / compaction / Upgrader . java b / src / java / org / apache / cassandra / db / compaction / Upgrader . java <nl> index b98c2ae . . 383ff00 100644 <nl> - - - a / src / java / org / apache / cassandra / db / compaction / Upgrader . java <nl> + + + b / src / java / org / apache / cassandra / db / compaction / Upgrader . java <nl> @ @ - 25 , 13 + 25 , 6 @ @ import com . google . common . base . Throwables ; <nl> <nl> import org . apache . cassandra . db . ColumnFamilyStore ; <nl> import org . apache . cassandra . db . DecoratedKey ; <nl> - import org . apache . cassandra . db . compaction . AbstractCompactedRow ; <nl> - import org . apache . cassandra . db . compaction . AbstractCompactionStrategy ; <nl> - import org . apache . cassandra . db . compaction . AbstractCompactionIterable ; <nl> - import org . apache . cassandra . db . compaction . CompactionIterable ; <nl> - import org . apache . cassandra . db . compaction . CompactionController ; <nl> - import org . apache . cassandra . db . compaction . CompactionTask ; <nl> - import org . apache . cassandra . db . compaction . OperationType ; <nl> import org . apache . cassandra . io . sstable . * ; <nl> import org . apache . cassandra . utils . CloseableIterator ; <nl> import org . apache . cassandra . utils . OutputHandler ; <nl> @ @ - 63 , 7 + 56 , 7 @ @ public class Upgrader <nl> <nl> this . strategy = cfs . getCompactionStrategy ( ) ; <nl> long estimatedTotalKeys = Math . max ( cfs . metadata . getIndexInterval ( ) , SSTableReader . getApproximateKeyCount ( toUpgrade , cfs . metadata ) ) ; <nl> - long estimatedSSTables = Math . max ( 1 , SSTable . getTotalBytes ( this . toUpgrade ) / strategy . getMaxSSTableSize ( ) ) ; <nl> + long estimatedSSTables = Math . max ( 1 , SSTableReader . getTotalBytes ( this . toUpgrade ) / strategy . getMaxSSTableSize ( ) ) ; <nl> this . estimatedRows = ( long ) Math . ceil ( ( double ) estimatedTotalKeys / estimatedSSTables ) ; <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / Component . java b / src / java / org / apache / cassandra / io / sstable / Component . java <nl> index 599e0ba . . 4635251 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / Component . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / Component . java <nl> @ @ - 43 , 10 + 43 , 6 @ @ public class Component <nl> PRIMARY _ INDEX ( " Index . db " ) , <nl> / / serialized bloom filter for the row keys in the sstable <nl> FILTER ( " Filter . db " ) , <nl> - / / 0 - length file that is created when an sstable is ready to be deleted <nl> - / / @ deprecated : deletion of compacted file is based on the lineag information stored in the compacted sstabl <nl> - / / metadata . This ensure we can guarantee never using a sstable and some of its parents , even in case of failure . <nl> - COMPACTED _ MARKER ( " Compacted " ) , <nl> / / file to hold information about uncompressed data length , chunk offsets etc . <nl> COMPRESSION _ INFO ( " CompressionInfo . db " ) , <nl> / / statistical metadata about the content of the sstable <nl> @ @ - 81 , 7 + 77 , 6 @ @ public class Component <nl> public final static Component DATA = new Component ( Type . DATA ) ; <nl> public final static Component PRIMARY _ INDEX = new Component ( Type . PRIMARY _ INDEX ) ; <nl> public final static Component FILTER = new Component ( Type . FILTER ) ; <nl> - public final static Component COMPACTED _ MARKER = new Component ( Type . COMPACTED _ MARKER ) ; <nl> public final static Component COMPRESSION _ INFO = new Component ( Type . COMPRESSION _ INFO ) ; <nl> public final static Component STATS = new Component ( Type . STATS ) ; <nl> public final static Component DIGEST = new Component ( Type . DIGEST ) ; <nl> @ @ - 133 , 7 + 128 , 6 @ @ public class Component <nl> case DATA : component = Component . DATA ; break ; <nl> case PRIMARY _ INDEX : component = Component . PRIMARY _ INDEX ; break ; <nl> case FILTER : component = Component . FILTER ; break ; <nl> - case COMPACTED _ MARKER : component = Component . COMPACTED _ MARKER ; break ; <nl> case COMPRESSION _ INFO : component = Component . COMPRESSION _ INFO ; break ; <nl> case STATS : component = Component . STATS ; break ; <nl> case DIGEST : component = Component . DIGEST ; break ; <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> index f4f7ee5 . . 0c36f62 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java <nl> @ @ - 35 , 7 + 35 , 7 @ @ public class KeyIterator extends AbstractIterator < DecoratedKey > implements Close <nl> <nl> public KeyIterator ( Descriptor desc ) <nl> { <nl> - File path = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; <nl> + File path = new File ( desc . filenameFor ( Component . PRIMARY _ INDEX ) ) ; <nl> in = RandomAccessReader . open ( path ) ; <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTable . java b / src / java / org / apache / cassandra / io / sstable / SSTable . java <nl> index e469b33 . . c13c423 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTable . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTable . java <nl> @ @ - 24 , 7 + 24 , 6 @ @ import java . util . concurrent . CopyOnWriteArraySet ; <nl> <nl> import com . google . common . base . Predicates ; <nl> import com . google . common . collect . Collections2 ; <nl> - import com . google . common . collect . Ordering ; <nl> import com . google . common . collect . Sets ; <nl> import com . google . common . io . Files ; <nl> import org . slf4j . Logger ; <nl> @ @ - 57 , 27 + 56 , 10 @ @ public abstract class SSTable <nl> { <nl> static final Logger logger = LoggerFactory . getLogger ( SSTable . class ) ; <nl> <nl> - / / TODO : replace with ' Component ' objects <nl> - public static final String COMPONENT _ DATA = Component . Type . DATA . repr ; <nl> - public static final String COMPONENT _ INDEX = Component . Type . PRIMARY _ INDEX . repr ; <nl> - public static final String COMPONENT _ FILTER = Component . Type . FILTER . repr ; <nl> - public static final String COMPONENT _ STATS = Component . Type . STATS . repr ; <nl> - public static final String COMPONENT _ DIGEST = Component . Type . DIGEST . repr ; <nl> - <nl> public static final String TEMPFILE _ MARKER = " tmp " ; <nl> <nl> public static final int TOMBSTONE _ HISTOGRAM _ BIN _ SIZE = 100 ; <nl> <nl> - public static final Comparator < SSTableReader > maxTimestampComparator = new Comparator < SSTableReader > ( ) <nl> - { <nl> - public int compare ( SSTableReader o1 , SSTableReader o2 ) <nl> - { <nl> - long ts1 = o1 . getMaxTimestamp ( ) ; <nl> - long ts2 = o2 . getMaxTimestamp ( ) ; <nl> - return ( ts1 > ts2 ? - 1 : ( ts1 = = ts2 ? 0 : 1 ) ) ; <nl> - } <nl> - } ; <nl> - <nl> public final Descriptor descriptor ; <nl> protected final Set < Component > components ; <nl> public final CFMetaData metadata ; <nl> @ @ - 101 , 26 + 83 , 13 @ @ public abstract class SSTable <nl> assert partitioner ! = null ; <nl> <nl> this . descriptor = descriptor ; <nl> - Set < Component > dataComponents = new HashSet < Component > ( components ) ; <nl> - for ( Component component : components ) <nl> - assert component . type ! = Component . Type . COMPACTED _ MARKER ; <nl> - <nl> + Set < Component > dataComponents = new HashSet < > ( components ) ; <nl> this . compression = dataComponents . contains ( Component . COMPRESSION _ INFO ) ; <nl> - this . components = new CopyOnWriteArraySet < Component > ( dataComponents ) ; <nl> + this . components = new CopyOnWriteArraySet < > ( dataComponents ) ; <nl> this . metadata = metadata ; <nl> this . partitioner = partitioner ; <nl> } <nl> <nl> - public static final Comparator < SSTableReader > sstableComparator = new Comparator < SSTableReader > ( ) <nl> - { <nl> - public int compare ( SSTableReader o1 , SSTableReader o2 ) <nl> - { <nl> - return o1 . first . compareTo ( o2 . first ) ; <nl> - } <nl> - } ; <nl> - <nl> - public static final Ordering < SSTableReader > sstableOrdering = Ordering . from ( sstableComparator ) ; <nl> - <nl> / * * <nl> * We use a ReferenceQueue to manage deleting files that have been compacted <nl> * and for which no more SSTable references exist . But this is not guaranteed <nl> @ @ - 139 , 15 + 108 , 11 @ @ public abstract class SSTable <nl> FileUtils . deleteWithConfirm ( desc . filenameFor ( Component . DATA ) ) ; <nl> for ( Component component : components ) <nl> { <nl> - if ( component . equals ( Component . DATA ) | | component . equals ( Component . COMPACTED _ MARKER ) | | component . equals ( Component . SUMMARY ) ) <nl> + if ( component . equals ( Component . DATA ) | | component . equals ( Component . SUMMARY ) ) <nl> continue ; <nl> <nl> FileUtils . deleteWithConfirm ( desc . filenameFor ( component ) ) ; <nl> } <nl> - / / remove the COMPACTED _ MARKER component last if it exists <nl> - / / Note : newly created sstable should not have a marker , but we keep this for now to make sure <nl> - / / we don ' t leave older marker around <nl> - FileUtils . delete ( desc . filenameFor ( Component . COMPACTED _ MARKER ) ) ; <nl> FileUtils . delete ( desc . filenameFor ( Component . SUMMARY ) ) ; <nl> <nl> logger . debug ( " Deleted { } " , desc ) ; <nl> @ @ - 167 , 12 + 132 , 12 @ @ public abstract class SSTable <nl> <nl> public String getFilename ( ) <nl> { <nl> - return descriptor . filenameFor ( COMPONENT _ DATA ) ; <nl> + return descriptor . filenameFor ( Component . DATA ) ; <nl> } <nl> <nl> public String getIndexFilename ( ) <nl> { <nl> - return descriptor . filenameFor ( COMPONENT _ INDEX ) ; <nl> + return descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ; <nl> } <nl> <nl> public String getColumnFamilyName ( ) <nl> @ @ - 262 , 16 + 227 , 6 @ @ public abstract class SSTable <nl> return estimatedRows ; <nl> } <nl> <nl> - public static long getTotalBytes ( Iterable < SSTableReader > sstables ) <nl> - { <nl> - long sum = 0 ; <nl> - for ( SSTableReader sstable : sstables ) <nl> - { <nl> - sum + = sstable . onDiskLength ( ) ; <nl> - } <nl> - return sum ; <nl> - } <nl> - <nl> public long bytesOnDisk ( ) <nl> { <nl> long bytes = 0 ; <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableMetadata . java b / src / java / org / apache / cassandra / io / sstable / SSTableMetadata . java <nl> index 140e08b . . 8ddfdd7 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableMetadata . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableMetadata . java <nl> @ @ - 435 , 7 + 435 , 7 @ @ public class SSTableMetadata <nl> public Pair < SSTableMetadata , Set < Integer > > deserialize ( Descriptor descriptor , boolean loadSSTableLevel ) throws IOException <nl> { <nl> logger . debug ( " Load metadata for { } " , descriptor ) ; <nl> - File statsFile = new File ( descriptor . filenameFor ( SSTable . COMPONENT _ STATS ) ) ; <nl> + File statsFile = new File ( descriptor . filenameFor ( Component . STATS ) ) ; <nl> if ( ! statsFile . exists ( ) ) <nl> { <nl> logger . debug ( " No sstable stats for { } " , descriptor ) ; <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> index c5b61d9 . . 7fd9ca6 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java <nl> @ @ - 29 , 6 + 29 , 7 @ @ import java . util . concurrent . atomic . AtomicInteger ; <nl> import java . util . concurrent . atomic . AtomicLong ; <nl> <nl> import com . google . common . annotations . VisibleForTesting ; <nl> + import com . google . common . collect . Ordering ; <nl> import com . google . common . primitives . Longs ; <nl> import com . google . common . util . concurrent . RateLimiter ; <nl> import org . slf4j . Logger ; <nl> @ @ - 67 , 8 + 68 , 28 @ @ public class SSTableReader extends SSTable implements Closeable <nl> private static final ScheduledThreadPoolExecutor syncExecutor = new ScheduledThreadPoolExecutor ( 1 ) ; <nl> private static final RateLimiter meterSyncThrottle = RateLimiter . create ( 100 . 0 ) ; <nl> <nl> + public static final Comparator < SSTableReader > maxTimestampComparator = new Comparator < SSTableReader > ( ) <nl> + { <nl> + public int compare ( SSTableReader o1 , SSTableReader o2 ) <nl> + { <nl> + long ts1 = o1 . getMaxTimestamp ( ) ; <nl> + long ts2 = o2 . getMaxTimestamp ( ) ; <nl> + return ( ts1 > ts2 ? - 1 : ( ts1 = = ts2 ? 0 : 1 ) ) ; <nl> + } <nl> + } ; <nl> + <nl> + public static final Comparator < SSTableReader > sstableComparator = new Comparator < SSTableReader > ( ) <nl> + { <nl> + public int compare ( SSTableReader o1 , SSTableReader o2 ) <nl> + { <nl> + return o1 . first . compareTo ( o2 . first ) ; <nl> + } <nl> + } ; <nl> + <nl> + public static final Ordering < SSTableReader > sstableOrdering = Ordering . from ( sstableComparator ) ; <nl> + <nl> / * * <nl> - * maxDataAge is a timestamp in local server time ( e . g . System . currentTimeMilli ) which represents an uppper bound <nl> + * maxDataAge is a timestamp in local server time ( e . g . System . currentTimeMilli ) which represents an upper bound <nl> * to the newest piece of data stored in the sstable . In other words , this sstable does not contain items created <nl> * later than maxDataAge . <nl> * <nl> @ @ - 168 , 7 + 189 , 7 @ @ public class SSTableReader extends SSTable implements Closeable <nl> SegmentedFile . Builder dbuilder = sstable . compression <nl> ? new CompressedSegmentedFile . Builder ( ) <nl> : new BufferedSegmentedFile . Builder ( ) ; <nl> - if ( ! loadSummary ( sstable , ibuilder , dbuilder , sstable . metadata ) ) <nl> + if ( ! sstable . loadSummary ( ibuilder , dbuilder , sstable . metadata ) ) <nl> sstable . buildSummary ( false , ibuilder , dbuilder , false ) ; <nl> sstable . ifile = ibuilder . complete ( sstable . descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) ; <nl> sstable . dfile = dbuilder . complete ( sstable . descriptor . filenameFor ( Component . DATA ) ) ; <nl> @ @ - 218 , 7 + 239 , 7 @ @ public class SSTableReader extends SSTable implements Closeable <nl> assert components . contains ( Component . DATA ) : " Data component is missing for sstable " + descriptor ; <nl> assert components . contains ( Component . PRIMARY _ INDEX ) : " Primary index component is missing for sstable " + descriptor ; <nl> <nl> - logger . info ( " Opening { } ( { } bytes ) " , descriptor , new File ( descriptor . filenameFor ( COMPONENT _ DATA ) ) . length ( ) ) ; <nl> + logger . info ( " Opening { } ( { } bytes ) " , descriptor , new File ( descriptor . filenameFor ( Component . DATA ) ) . length ( ) ) ; <nl> <nl> SSTableMetadata sstableMetadata = SSTableMetadata . serializer . deserialize ( descriptor ) . left ; <nl> <nl> @ @ - 247 , 7 + 268 , 7 @ @ public class SSTableReader extends SSTable implements Closeable <nl> final CFMetaData metadata , <nl> final IPartitioner partitioner ) <nl> { <nl> - final Collection < SSTableReader > sstables = new LinkedBlockingQueue < SSTableReader > ( ) ; <nl> + final Collection < SSTableReader > sstables = new LinkedBlockingQueue < > ( ) ; <nl> <nl> ExecutorService executor = DebuggableThreadPoolExecutor . createWithFixedPoolSize ( " SSTableBatchOpen " , FBUtilities . getAvailableProcessors ( ) ) ; <nl> for ( final Map . Entry < Descriptor , Set < Component > > entry : entries ) <nl> @ @ - 365 , 6 + 386 , 16 @ @ public class SSTableReader extends SSTable implements Closeable <nl> this . bf = bloomFilter ; <nl> } <nl> <nl> + public static long getTotalBytes ( Iterable < SSTableReader > sstables ) <nl> + { <nl> + long sum = 0 ; <nl> + for ( SSTableReader sstable : sstables ) <nl> + { <nl> + sum + = sstable . onDiskLength ( ) ; <nl> + } <nl> + return sum ; <nl> + } <nl> + <nl> / * * <nl> * Clean up all opened resources . <nl> * <nl> @ @ - 441 , 18 + 472 , 18 @ @ public class SSTableReader extends SSTable implements Closeable <nl> ? SegmentedFile . getCompressedBuilder ( ) <nl> : SegmentedFile . getBuilder ( DatabaseDescriptor . getDiskAccessMode ( ) ) ; <nl> <nl> - boolean summaryLoaded = loadSummary ( this , ibuilder , dbuilder , metadata ) ; <nl> + boolean summaryLoaded = loadSummary ( ibuilder , dbuilder , metadata ) ; <nl> if ( recreateBloomFilter | | ! summaryLoaded ) <nl> buildSummary ( recreateBloomFilter , ibuilder , dbuilder , summaryLoaded ) ; <nl> <nl> ifile = ibuilder . complete ( descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) ; <nl> dfile = dbuilder . complete ( descriptor . filenameFor ( Component . DATA ) ) ; <nl> if ( saveSummaryIfCreated & & ( recreateBloomFilter | | ! summaryLoaded ) ) / / save summary information to disk <nl> - saveSummary ( this , ibuilder , dbuilder ) ; <nl> + saveSummary ( ibuilder , dbuilder ) ; <nl> } <nl> <nl> - private void buildSummary ( boolean recreateBloomFilter , SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder , boolean summaryLoaded ) throws IOException <nl> - { <nl> + private void buildSummary ( boolean recreateBloomFilter , SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder , boolean summaryLoaded ) throws IOException <nl> + { <nl> / / we read the positions in a BRAF so we don ' t have to worry about an entry spanning a mmap boundary . <nl> RandomAccessReader primaryIndex = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) ) ; <nl> <nl> @ @ - 505 , 27 + 536 , 27 @ @ public class SSTableReader extends SSTable implements Closeable <nl> last = getMinimalKey ( last ) ; <nl> } <nl> <nl> - public static boolean loadSummary ( SSTableReader reader , SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder , CFMetaData metadata ) <nl> + public boolean loadSummary ( SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder , CFMetaData metadata ) <nl> { <nl> - File summariesFile = new File ( reader . descriptor . filenameFor ( Component . SUMMARY ) ) ; <nl> - if ( ! reader . descriptor . version . offHeapSummaries | | ! summariesFile . exists ( ) ) <nl> + File summariesFile = new File ( descriptor . filenameFor ( Component . SUMMARY ) ) ; <nl> + if ( ! descriptor . version . offHeapSummaries | | ! summariesFile . exists ( ) ) <nl> return false ; <nl> <nl> DataInputStream iStream = null ; <nl> try <nl> { <nl> iStream = new DataInputStream ( new FileInputStream ( summariesFile ) ) ; <nl> - reader . indexSummary = IndexSummary . serializer . deserialize ( iStream , reader . partitioner ) ; <nl> - if ( reader . indexSummary . getIndexInterval ( ) ! = metadata . getIndexInterval ( ) ) <nl> + indexSummary = IndexSummary . serializer . deserialize ( iStream , partitioner ) ; <nl> + if ( indexSummary . getIndexInterval ( ) ! = metadata . getIndexInterval ( ) ) <nl> { <nl> iStream . close ( ) ; <nl> logger . debug ( " Cannot read the saved summary for { } because Index Interval changed from { } to { } . " , <nl> - reader . toString ( ) , reader . indexSummary . getIndexInterval ( ) , metadata . getIndexInterval ( ) ) ; <nl> + toString ( ) , indexSummary . getIndexInterval ( ) , metadata . getIndexInterval ( ) ) ; <nl> FileUtils . deleteWithConfirm ( summariesFile ) ; <nl> return false ; <nl> } <nl> - reader . first = reader . partitioner . decorateKey ( ByteBufferUtil . readWithLength ( iStream ) ) ; <nl> - reader . last = reader . partitioner . decorateKey ( ByteBufferUtil . readWithLength ( iStream ) ) ; <nl> + first = partitioner . decorateKey ( ByteBufferUtil . readWithLength ( iStream ) ) ; <nl> + last = partitioner . decorateKey ( ByteBufferUtil . readWithLength ( iStream ) ) ; <nl> ibuilder . deserializeBounds ( iStream ) ; <nl> dbuilder . deserializeBounds ( iStream ) ; <nl> } <nl> @ @ - 544 , 9 + 575 , 9 @ @ public class SSTableReader extends SSTable implements Closeable <nl> return true ; <nl> } <nl> <nl> - public static void saveSummary ( SSTableReader reader , SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder ) <nl> + public void saveSummary ( SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder ) <nl> { <nl> - File summariesFile = new File ( reader . descriptor . filenameFor ( Component . SUMMARY ) ) ; <nl> + File summariesFile = new File ( descriptor . filenameFor ( Component . SUMMARY ) ) ; <nl> if ( summariesFile . exists ( ) ) <nl> summariesFile . delete ( ) ; <nl> <nl> @ @ - 554 , 9 + 585 , 9 @ @ public class SSTableReader extends SSTable implements Closeable <nl> try <nl> { <nl> oStream = new DataOutputStream ( new FileOutputStream ( summariesFile ) ) ; <nl> - IndexSummary . serializer . serialize ( reader . indexSummary , oStream ) ; <nl> - ByteBufferUtil . writeWithLength ( reader . first . key , oStream ) ; <nl> - ByteBufferUtil . writeWithLength ( reader . last . key , oStream ) ; <nl> + IndexSummary . serializer . serialize ( indexSummary , oStream ) ; <nl> + ByteBufferUtil . writeWithLength ( first . key , oStream ) ; <nl> + ByteBufferUtil . writeWithLength ( last . key , oStream ) ; <nl> ibuilder . serializeBounds ( oStream ) ; <nl> dbuilder . serializeBounds ( oStream ) ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> index 70c0b42 . . b5d50cf 100644 <nl> - - - a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> + + + b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java <nl> @ @ - 313 , 8 + 313 , 8 @ @ public class SSTableWriter extends SSTable <nl> SSTableMetadata sstableMetadata = p . right ; <nl> <nl> / / finalize in - memory state for the reader <nl> - SegmentedFile ifile = iwriter . builder . complete ( newdesc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; <nl> - SegmentedFile dfile = dbuilder . complete ( newdesc . filenameFor ( SSTable . COMPONENT _ DATA ) ) ; <nl> + SegmentedFile ifile = iwriter . builder . complete ( newdesc . filenameFor ( Component . PRIMARY _ INDEX ) ) ; <nl> + SegmentedFile dfile = dbuilder . complete ( newdesc . filenameFor ( Component . DATA ) ) ; <nl> SSTableReader sstable = SSTableReader . internalOpen ( newdesc , <nl> components , <nl> metadata , <nl> @ @ - 328 , 7 + 328 , 7 @ @ public class SSTableWriter extends SSTable <nl> sstable . first = getMinimalKey ( first ) ; <nl> sstable . last = getMinimalKey ( last ) ; <nl> / / try to save the summaries to disk <nl> - SSTableReader . saveSummary ( sstable , iwriter . builder , dbuilder ) ; <nl> + sstable . saveSummary ( iwriter . builder , dbuilder ) ; <nl> iwriter = null ; <nl> dbuilder = null ; <nl> return sstable ; <nl> @ @ - 355 , 7 + 355 , 7 @ @ public class SSTableWriter extends SSTable <nl> <nl> private static void writeMetadata ( Descriptor desc , SSTableMetadata sstableMetadata , Set < Integer > ancestors ) <nl> { <nl> - SequentialWriter out = SequentialWriter . open ( new File ( desc . filenameFor ( SSTable . COMPONENT _ STATS ) ) , true ) ; <nl> + SequentialWriter out = SequentialWriter . open ( new File ( desc . filenameFor ( Component . STATS ) ) , true ) ; <nl> try <nl> { <nl> SSTableMetadata . serializer . serialize ( sstableMetadata , ancestors , out . stream ) ; <nl> @ @ - 411 , 7 + 411 , 7 @ @ public class SSTableWriter extends SSTable <nl> <nl> IndexWriter ( long keyCount ) <nl> { <nl> - indexFile = SequentialWriter . open ( new File ( descriptor . filenameFor ( SSTable . COMPONENT _ INDEX ) ) , <nl> + indexFile = SequentialWriter . open ( new File ( descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) , <nl> ! metadata . populateIoCacheOnFlush ( ) ) ; <nl> builder = SegmentedFile . getBuilder ( DatabaseDescriptor . getIndexAccessMode ( ) ) ; <nl> summary = new IndexSummaryBuilder ( keyCount , metadata . getIndexInterval ( ) ) ; <nl> @ @ - 446 , 7 + 446 , 7 @ @ public class SSTableWriter extends SSTable <nl> { <nl> if ( components . contains ( Component . FILTER ) ) <nl> { <nl> - String path = descriptor . filenameFor ( SSTable . COMPONENT _ FILTER ) ; <nl> + String path = descriptor . filenameFor ( Component . FILTER ) ; <nl> try <nl> { <nl> / / bloom filter <nl> diff - - git a / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java b / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java <nl> index f334d08 . . 0606941 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java <nl> + + + b / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java <nl> @ @ - 145 , 10 + 145 , 10 @ @ public class DataIntegrityMetadata <nl> byte [ ] bytes = digest . digest ( ) ; <nl> if ( bytes = = null ) <nl> return ; <nl> - SequentialWriter out = SequentialWriter . open ( new File ( descriptor . filenameFor ( SSTable . COMPONENT _ DIGEST ) ) , true ) ; <nl> + SequentialWriter out = SequentialWriter . open ( new File ( descriptor . filenameFor ( Component . DIGEST ) ) , true ) ; <nl> / / Writting output compatible with sha1sum <nl> Descriptor newdesc = descriptor . asTemporary ( false ) ; <nl> - String [ ] tmp = newdesc . filenameFor ( SSTable . COMPONENT _ DATA ) . split ( Pattern . quote ( File . separator ) ) ; <nl> + String [ ] tmp = newdesc . filenameFor ( Component . DATA ) . split ( Pattern . quote ( File . separator ) ) ; <nl> String dataFileName = tmp [ tmp . length - 1 ] ; <nl> try <nl> { <nl> diff - - git a / test / long / org / apache / cassandra / db / compaction / LongLeveledCompactionStrategyTest . java b / test / long / org / apache / cassandra / db / compaction / LongLeveledCompactionStrategyTest . java <nl> index c6b6eb0 . . 0eb44d0 100644 <nl> - - - a / test / long / org / apache / cassandra / db / compaction / LongLeveledCompactionStrategyTest . java <nl> + + + b / test / long / org / apache / cassandra / db / compaction / LongLeveledCompactionStrategyTest . java <nl> @ @ - 31 , 7 + 31 , 6 @ @ import org . apache . cassandra . db . ColumnFamilyStore ; <nl> import org . apache . cassandra . db . DecoratedKey ; <nl> import org . apache . cassandra . db . RowMutation ; <nl> import org . apache . cassandra . db . Keyspace ; <nl> - import org . apache . cassandra . io . sstable . SSTable ; <nl> import org . apache . cassandra . io . sstable . SSTableReader ; <nl> import org . apache . cassandra . utils . ByteBufferUtil ; <nl> import org . apache . cassandra . utils . FBUtilities ; <nl> @ @ - 106 , 7 + 105 , 7 @ @ public class LongLeveledCompactionStrategyTest extends SchemaLoader <nl> { <nl> List < SSTableReader > sstables = manifest . getLevel ( level ) ; <nl> / / score check <nl> - assert ( double ) SSTable . getTotalBytes ( sstables ) / manifest . maxBytesForLevel ( level ) < 1 . 00 ; <nl> + assert ( double ) SSTableReader . getTotalBytes ( sstables ) / manifest . maxBytesForLevel ( level ) < 1 . 00 ; <nl> / / overlap check for levels greater than 0 <nl> if ( level > 0 ) <nl> { <nl> diff - - git a / test / unit / org / apache / cassandra / db / ColumnFamilyStoreTest . java b / test / unit / org / apache / cassandra / db / ColumnFamilyStoreTest . java <nl> index 32bc7df . . 7f7d5c9 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / ColumnFamilyStoreTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / ColumnFamilyStoreTest . java <nl> @ @ - 938 , 7 + 938 , 7 @ @ public class ColumnFamilyStoreTest extends SchemaLoader <nl> cfs . clearUnsafe ( ) ; <nl> assertEquals ( 0 , cfs . getSSTables ( ) . size ( ) ) ; <nl> <nl> - new File ( ssTables . iterator ( ) . next ( ) . descriptor . filenameFor ( SSTable . COMPONENT _ STATS ) ) . delete ( ) ; <nl> + new File ( ssTables . iterator ( ) . next ( ) . descriptor . filenameFor ( Component . STATS ) ) . delete ( ) ; <nl> cfs . loadNewSSTables ( ) ; <nl> <nl> / / Add another column with a lower timestamp <nl> diff - - git a / test / unit / org / apache / cassandra / db / compaction / LeveledCompactionStrategyTest . java b / test / unit / org / apache / cassandra / db / compaction / LeveledCompactionStrategyTest . java <nl> index b60f6d9 . . a008de1 100644 <nl> - - - a / test / unit / org / apache / cassandra / db / compaction / LeveledCompactionStrategyTest . java <nl> + + + b / test / unit / org / apache / cassandra / db / compaction / LeveledCompactionStrategyTest . java <nl> @ @ - 20 , 9 + 20 , 7 @ @ package org . apache . cassandra . db . compaction ; <nl> import java . nio . ByteBuffer ; <nl> import java . util . Arrays ; <nl> import java . util . Collection ; <nl> - import java . util . HashSet ; <nl> import java . util . List ; <nl> - import java . util . Set ; <nl> import java . util . concurrent . ExecutionException ; <nl> import java . util . UUID ; <nl> <nl> @ @ - 39 , 7 + 37 , 6 @ @ import org . apache . cassandra . db . Keyspace ; <nl> import org . apache . cassandra . dht . Range ; <nl> import org . apache . cassandra . dht . Token ; <nl> import org . apache . cassandra . io . sstable . Component ; <nl> - import org . apache . cassandra . io . sstable . SSTable ; <nl> import org . apache . cassandra . io . sstable . SSTableReader ; <nl> import org . apache . cassandra . repair . RepairJobDesc ; <nl> import org . apache . cassandra . repair . Validator ; <nl> @ @ - 145 , 7 + 142 , 7 @ @ public class LeveledCompactionStrategyTest extends SchemaLoader <nl> scanner . next ( ) ; <nl> <nl> / / scanner . getCurrentPosition should be equal to total bytes of L1 sstables <nl> - assert scanner . getCurrentPosition ( ) = = SSTable . getTotalBytes ( sstables ) ; <nl> + assert scanner . getCurrentPosition ( ) = = SSTableReader . getTotalBytes ( sstables ) ; <nl> } <nl> <nl> @ Test <nl> diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableReaderTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableReaderTest . java <nl> index 36d8fbe . . b771e72 100644 <nl> - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableReaderTest . java <nl> + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableReaderTest . java <nl> @ @ - 278 , 7 + 278 , 7 @ @ public class SSTableReaderTest extends SchemaLoader <nl> SegmentedFile . Builder dbuilder = sstable . compression <nl> ? SegmentedFile . getCompressedBuilder ( ) <nl> : SegmentedFile . getBuilder ( DatabaseDescriptor . getDiskAccessMode ( ) ) ; <nl> - SSTableReader . saveSummary ( sstable , ibuilder , dbuilder ) ; <nl> + sstable . saveSummary ( ibuilder , dbuilder ) ; <nl> <nl> SSTableReader reopened = SSTableReader . open ( sstable . descriptor ) ; <nl> assert reopened . first . token instanceof LocalToken ;

TEST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index 619c219 . . ba3a675 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 25 , 6 + 25 , 7 @ @ 
 * reduce garbage creation in calculatePendingRanges ( CASSANDRA - 7191 ) 
 * exit CQLSH with error status code if script fails ( CASSANDRA - 6344 ) 
 * Fix bug with some IN queries missig results ( CASSANDRA - 7105 ) 
 + * Fix availability validation for LOCAL _ ONE CL ( CASSANDRA - 7319 ) 
 
 
 1 . 2 . 16 
 diff - - git a / src / java / org / apache / cassandra / db / ConsistencyLevel . java b / src / java / org / apache / cassandra / db / ConsistencyLevel . java 
 index 3737c73 . . e65b61f 100644 
 - - - a / src / java / org / apache / cassandra / db / ConsistencyLevel . java 
 + + + b / src / java / org / apache / cassandra / db / ConsistencyLevel . java 
 @ @ - 249 , 6 + 249 , 10 @ @ public enum ConsistencyLevel 
 case ANY : 
 / / local hint is acceptable , and local node is always live 
 break ; 
 + case LOCAL _ ONE : 
 + if ( countLocalEndpoints ( liveEndpoints ) = = 0 ) 
 + throw new UnavailableException ( this , 1 , 0 ) ; 
 + break ; 
 case LOCAL _ QUORUM : 
 int localLive = countLocalEndpoints ( liveEndpoints ) ; 
 if ( localLive < blockFor )

NEAREST DIFF:
diff - - git a / src / java / org / apache / cassandra / db / CollationController . java b / src / java / org / apache / cassandra / db / CollationController . java 
 index 758d523 . . 9896fde 100644 
 - - - a / src / java / org / apache / cassandra / db / CollationController . java 
 + + + b / src / java / org / apache / cassandra / db / CollationController . java 
 @ @ - 27 , 7 + 27 , 6 @ @ import org . apache . cassandra . db . compaction . SizeTieredCompactionStrategy ; 
 import org . apache . cassandra . db . filter . NamesQueryFilter ; 
 import org . apache . cassandra . db . filter . QueryFilter ; 
 import org . apache . cassandra . db . marshal . CounterColumnType ; 
 - import org . apache . cassandra . io . sstable . SSTable ; 
 import org . apache . cassandra . io . sstable . SSTableReader ; 
 import org . apache . cassandra . io . util . FileUtils ; 
 import org . apache . cassandra . tracing . Tracing ; 
 @ @ - 99 , 7 + 98 , 7 @ @ public class CollationController 
 QueryFilter reducedFilter = new QueryFilter ( filter . key , filter . cfName , namesFilter . withUpdatedColumns ( filterColumns ) , filter . timestamp ) ; 
 
 / * add the SSTables on disk * / 
 - Collections . sort ( view . sstables , SSTable . maxTimestampComparator ) ; 
 + Collections . sort ( view . sstables , SSTableReader . maxTimestampComparator ) ; 
 
 / / read sorted sstables 
 long mostRecentRowTombstone = Long . MIN _ VALUE ; 
 @ @ - 219 , 7 + 218 , 7 @ @ public class CollationController 
 * In othere words , iterating in maxTimestamp order allow to do our mostRecentTombstone elimination 
 * in one pass , and minimize the number of sstables for which we read a rowTombstone . 
 * / 
 - Collections . sort ( view . sstables , SSTable . maxTimestampComparator ) ; 
 + Collections . sort ( view . sstables , SSTableReader . maxTimestampComparator ) ; 
 List < SSTableReader > skippedSSTables = null ; 
 long mostRecentRowTombstone = Long . MIN _ VALUE ; 
 long minTimestamp = Long . MAX _ VALUE ; 
 diff - - git a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 index 73704d4 . . 1b8a1bf 100644 
 - - - a / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 + + + b / src / java / org / apache / cassandra / db / ColumnFamilyStore . java 
 @ @ - 422 , 7 + 422 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 Descriptor desc = sstableFiles . getKey ( ) ; 
 Set < Component > components = sstableFiles . getValue ( ) ; 
 
 - if ( components . contains ( Component . COMPACTED _ MARKER ) | | desc . temporary ) 
 + if ( desc . temporary ) 
 { 
 SSTable . delete ( desc , components ) ; 
 continue ; 
 @ @ - 1010 , 7 + 1010 , 7 @ @ public class ColumnFamilyStore implements ColumnFamilyStoreMBean 
 { 
 if ( operation ! = OperationType . CLEANUP | | isIndex ( ) ) 
 { 
 - return SSTable . getTotalBytes ( sstables ) ; 
 + return SSTableReader . getTotalBytes ( sstables ) ; 
 } 
 
 / / cleanup size estimation only counts bytes for keys local to this node 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionController . java b / src / java / org / apache / cassandra / db / compaction / CompactionController . java 
 index 201cd0a . . dc7730c 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / CompactionController . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / CompactionController . java 
 @ @ - 30 , 7 + 30 , 6 @ @ import org . slf4j . LoggerFactory ; 
 import org . apache . cassandra . db . ColumnFamilyStore ; 
 import org . apache . cassandra . db . DataTracker ; 
 import org . apache . cassandra . db . DecoratedKey ; 
 - import org . apache . cassandra . io . sstable . SSTable ; 
 import org . apache . cassandra . io . sstable . SSTableIdentityIterator ; 
 import org . apache . cassandra . io . sstable . SSTableReader ; 
 import org . apache . cassandra . utils . AlwaysPresentFilter ; 
 @ @ - 118 , 7 + 117 , 7 @ @ public class CompactionController 
 / / we still need to keep candidates that might shadow something in a 
 / / non - candidate sstable . And if we remove a sstable from the candidates , we 
 / / must take it ' s timestamp into account ( hence the sorting below ) . 
 - Collections . sort ( candidates , SSTable . maxTimestampComparator ) ; 
 + Collections . sort ( candidates , SSTableReader . maxTimestampComparator ) ; 
 
 Iterator < SSTableReader > iterator = candidates . iterator ( ) ; 
 while ( iterator . hasNext ( ) ) 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / CompactionTask . java b / src / java / org / apache / cassandra / db / compaction / CompactionTask . java 
 index bc419ad . . f4cc500 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / CompactionTask . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / CompactionTask . java 
 @ @ - 118 , 7 + 118 , 7 @ @ public class CompactionTask extends AbstractCompactionTask 
 long totalkeysWritten = 0 ; 
 
 long estimatedTotalKeys = Math . max ( cfs . metadata . getIndexInterval ( ) , SSTableReader . getApproximateKeyCount ( actuallyCompact , cfs . metadata ) ) ; 
 - long estimatedSSTables = Math . max ( 1 , SSTable . getTotalBytes ( actuallyCompact ) / strategy . getMaxSSTableSize ( ) ) ; 
 + long estimatedSSTables = Math . max ( 1 , SSTableReader . getTotalBytes ( actuallyCompact ) / strategy . getMaxSSTableSize ( ) ) ; 
 long keysPerSSTable = ( long ) Math . ceil ( ( double ) estimatedTotalKeys / estimatedSSTables ) ; 
 if ( logger . isDebugEnabled ( ) ) 
 logger . debug ( " Expected bloom filter size : { } " , keysPerSSTable ) ; 
 @ @ - 244 , 8 + 244 , 8 @ @ public class CompactionTask extends AbstractCompactionTask 
 
 / / log a bunch of statistics about the result and save to system table compaction _ history 
 long dTime = TimeUnit . NANOSECONDS . toMillis ( System . nanoTime ( ) - start ) ; 
 - long startsize = SSTable . getTotalBytes ( toCompact ) ; 
 - long endsize = SSTable . getTotalBytes ( sstables ) ; 
 + long startsize = SSTableReader . getTotalBytes ( toCompact ) ; 
 + long endsize = SSTableReader . getTotalBytes ( sstables ) ; 
 double ratio = ( double ) endsize / ( double ) startsize ; 
 
 StringBuilder builder = new StringBuilder ( ) ; 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java 
 index e992003 . . 3eb5e91 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / LeveledCompactionStrategy . java 
 @ @ - 32 , 7 + 32 , 6 @ @ import org . apache . cassandra . db . columniterator . OnDiskAtomIterator ; 
 import org . apache . cassandra . dht . Range ; 
 import org . apache . cassandra . dht . Token ; 
 import org . apache . cassandra . exceptions . ConfigurationException ; 
 - import org . apache . cassandra . io . sstable . SSTable ; 
 import org . apache . cassandra . io . sstable . SSTableReader ; 
 import org . apache . cassandra . notifications . INotification ; 
 import org . apache . cassandra . notifications . INotificationConsumer ; 
 @ @ - 234 , 7 + 233 , 7 @ @ public class LeveledCompactionStrategy extends AbstractCompactionStrategy implem 
 } 
 
 totalLength = length ; 
 - Collections . sort ( this . sstables , SSTable . sstableComparator ) ; 
 + Collections . sort ( this . sstables , SSTableReader . sstableComparator ) ; 
 sstableIterator = this . sstables . iterator ( ) ; 
 assert sstableIterator . hasNext ( ) ; / / caller should check intersecting first 
 currentScanner = sstableIterator . next ( ) . getScanner ( range , CompactionManager . instance . getRateLimiter ( ) ) ; 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java b / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java 
 index 63ad2e4 . . 2b79493 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / LeveledManifest . java 
 @ @ - 151 , 13 + 151 , 13 @ @ public class LeveledManifest 
 minLevel = Math . min ( minLevel , ssTableReader . getSSTableLevel ( ) ) ; 
 add ( ssTableReader ) ; 
 } 
 - lastCompactedKeys [ minLevel ] = SSTable . sstableOrdering . max ( added ) . last ; 
 + lastCompactedKeys [ minLevel ] = SSTableReader . sstableOrdering . max ( added ) . last ; 
 } 
 
 public synchronized void repairOverlappingSSTables ( int level ) 
 { 
 SSTableReader previous = null ; 
 - Collections . sort ( generations [ level ] , SSTable . sstableComparator ) ; 
 + Collections . sort ( generations [ level ] , SSTableReader . sstableComparator ) ; 
 List < SSTableReader > outOfOrderSSTables = new ArrayList < SSTableReader > ( ) ; 
 for ( SSTableReader current : generations [ level ] ) 
 { 
 @ @ - 264 , 7 + 264 , 7 @ @ public class LeveledManifest 
 / / we want to calculate score excluding compacting ones 
 Set < SSTableReader > sstablesInLevel = Sets . newHashSet ( sstables ) ; 
 Set < SSTableReader > remaining = Sets . difference ( sstablesInLevel , cfs . getDataTracker ( ) . getCompacting ( ) ) ; 
 - double score = ( double ) SSTableReader . getTotalBytes ( remaining ) / ( double ) maxBytesForLevel ( i ) ; 
 + double score = ( double ) SSTableReader . getTotalBytes ( remaining ) / ( double ) maxBytesForLevel ( i ) ; 
 logger . debug ( " Compaction score for level { } is { } " , i , score ) ; 
 
 if ( score > 1 . 001 ) 
 @ @ - 454 , 7 + 454 , 7 @ @ public class LeveledManifest 
 } 
 
 / / leave everything in L0 if we didn ' t end up with a full sstable ' s worth of data 
 - if ( SSTable . getTotalBytes ( candidates ) > maxSSTableSizeInBytes ) 
 + if ( SSTableReader . getTotalBytes ( candidates ) > maxSSTableSizeInBytes ) 
 { 
 / / add sstables from L1 that overlap candidates 
 / / if the overlapping ones are already busy in a compaction , leave it out . 
 @ @ - 468 , 7 + 468 , 7 @ @ public class LeveledManifest 
 } 
 
 / / for non - L0 compactions , pick up where we left off last time 
 - Collections . sort ( generations [ level ] , SSTable . sstableComparator ) ; 
 + Collections . sort ( generations [ level ] , SSTableReader . sstableComparator ) ; 
 int start = 0 ; / / handles case where the prior compaction touched the very last range 
 for ( int i = 0 ; i < generations [ level ] . size ( ) ; i + + ) 
 { 
 @ @ - 499 , 7 + 499 , 7 @ @ public class LeveledManifest 
 private List < SSTableReader > ageSortedSSTables ( Collection < SSTableReader > candidates ) 
 { 
 List < SSTableReader > ageSortedCandidates = new ArrayList < SSTableReader > ( candidates ) ; 
 - Collections . sort ( ageSortedCandidates , SSTable . maxTimestampComparator ) ; 
 + Collections . sort ( ageSortedCandidates , SSTableReader . maxTimestampComparator ) ; 
 return ageSortedCandidates ; 
 } 
 
 @ @ - 557 , 7 + 557 , 7 @ @ public class LeveledManifest 
 } 
 
 int newLevel ; 
 - if ( minimumLevel = = 0 & & minimumLevel = = maximumLevel & & SSTable . getTotalBytes ( sstables ) < maxSSTableSizeInBytes ) 
 + if ( minimumLevel = = 0 & & minimumLevel = = maximumLevel & & SSTableReader . getTotalBytes ( sstables ) < maxSSTableSizeInBytes ) 
 { 
 newLevel = 0 ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / db / compaction / Upgrader . java b / src / java / org / apache / cassandra / db / compaction / Upgrader . java 
 index b98c2ae . . 383ff00 100644 
 - - - a / src / java / org / apache / cassandra / db / compaction / Upgrader . java 
 + + + b / src / java / org / apache / cassandra / db / compaction / Upgrader . java 
 @ @ - 25 , 13 + 25 , 6 @ @ import com . google . common . base . Throwables ; 
 
 import org . apache . cassandra . db . ColumnFamilyStore ; 
 import org . apache . cassandra . db . DecoratedKey ; 
 - import org . apache . cassandra . db . compaction . AbstractCompactedRow ; 
 - import org . apache . cassandra . db . compaction . AbstractCompactionStrategy ; 
 - import org . apache . cassandra . db . compaction . AbstractCompactionIterable ; 
 - import org . apache . cassandra . db . compaction . CompactionIterable ; 
 - import org . apache . cassandra . db . compaction . CompactionController ; 
 - import org . apache . cassandra . db . compaction . CompactionTask ; 
 - import org . apache . cassandra . db . compaction . OperationType ; 
 import org . apache . cassandra . io . sstable . * ; 
 import org . apache . cassandra . utils . CloseableIterator ; 
 import org . apache . cassandra . utils . OutputHandler ; 
 @ @ - 63 , 7 + 56 , 7 @ @ public class Upgrader 
 
 this . strategy = cfs . getCompactionStrategy ( ) ; 
 long estimatedTotalKeys = Math . max ( cfs . metadata . getIndexInterval ( ) , SSTableReader . getApproximateKeyCount ( toUpgrade , cfs . metadata ) ) ; 
 - long estimatedSSTables = Math . max ( 1 , SSTable . getTotalBytes ( this . toUpgrade ) / strategy . getMaxSSTableSize ( ) ) ; 
 + long estimatedSSTables = Math . max ( 1 , SSTableReader . getTotalBytes ( this . toUpgrade ) / strategy . getMaxSSTableSize ( ) ) ; 
 this . estimatedRows = ( long ) Math . ceil ( ( double ) estimatedTotalKeys / estimatedSSTables ) ; 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / Component . java b / src / java / org / apache / cassandra / io / sstable / Component . java 
 index 599e0ba . . 4635251 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / Component . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / Component . java 
 @ @ - 43 , 10 + 43 , 6 @ @ public class Component 
 PRIMARY _ INDEX ( " Index . db " ) , 
 / / serialized bloom filter for the row keys in the sstable 
 FILTER ( " Filter . db " ) , 
 - / / 0 - length file that is created when an sstable is ready to be deleted 
 - / / @ deprecated : deletion of compacted file is based on the lineag information stored in the compacted sstabl 
 - / / metadata . This ensure we can guarantee never using a sstable and some of its parents , even in case of failure . 
 - COMPACTED _ MARKER ( " Compacted " ) , 
 / / file to hold information about uncompressed data length , chunk offsets etc . 
 COMPRESSION _ INFO ( " CompressionInfo . db " ) , 
 / / statistical metadata about the content of the sstable 
 @ @ - 81 , 7 + 77 , 6 @ @ public class Component 
 public final static Component DATA = new Component ( Type . DATA ) ; 
 public final static Component PRIMARY _ INDEX = new Component ( Type . PRIMARY _ INDEX ) ; 
 public final static Component FILTER = new Component ( Type . FILTER ) ; 
 - public final static Component COMPACTED _ MARKER = new Component ( Type . COMPACTED _ MARKER ) ; 
 public final static Component COMPRESSION _ INFO = new Component ( Type . COMPRESSION _ INFO ) ; 
 public final static Component STATS = new Component ( Type . STATS ) ; 
 public final static Component DIGEST = new Component ( Type . DIGEST ) ; 
 @ @ - 133 , 7 + 128 , 6 @ @ public class Component 
 case DATA : component = Component . DATA ; break ; 
 case PRIMARY _ INDEX : component = Component . PRIMARY _ INDEX ; break ; 
 case FILTER : component = Component . FILTER ; break ; 
 - case COMPACTED _ MARKER : component = Component . COMPACTED _ MARKER ; break ; 
 case COMPRESSION _ INFO : component = Component . COMPRESSION _ INFO ; break ; 
 case STATS : component = Component . STATS ; break ; 
 case DIGEST : component = Component . DIGEST ; break ; 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 index f4f7ee5 . . 0c36f62 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / KeyIterator . java 
 @ @ - 35 , 7 + 35 , 7 @ @ public class KeyIterator extends AbstractIterator < DecoratedKey > implements Close 
 
 public KeyIterator ( Descriptor desc ) 
 { 
 - File path = new File ( desc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; 
 + File path = new File ( desc . filenameFor ( Component . PRIMARY _ INDEX ) ) ; 
 in = RandomAccessReader . open ( path ) ; 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTable . java b / src / java / org / apache / cassandra / io / sstable / SSTable . java 
 index e469b33 . . c13c423 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTable . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTable . java 
 @ @ - 24 , 7 + 24 , 6 @ @ import java . util . concurrent . CopyOnWriteArraySet ; 
 
 import com . google . common . base . Predicates ; 
 import com . google . common . collect . Collections2 ; 
 - import com . google . common . collect . Ordering ; 
 import com . google . common . collect . Sets ; 
 import com . google . common . io . Files ; 
 import org . slf4j . Logger ; 
 @ @ - 57 , 27 + 56 , 10 @ @ public abstract class SSTable 
 { 
 static final Logger logger = LoggerFactory . getLogger ( SSTable . class ) ; 
 
 - / / TODO : replace with ' Component ' objects 
 - public static final String COMPONENT _ DATA = Component . Type . DATA . repr ; 
 - public static final String COMPONENT _ INDEX = Component . Type . PRIMARY _ INDEX . repr ; 
 - public static final String COMPONENT _ FILTER = Component . Type . FILTER . repr ; 
 - public static final String COMPONENT _ STATS = Component . Type . STATS . repr ; 
 - public static final String COMPONENT _ DIGEST = Component . Type . DIGEST . repr ; 
 - 
 public static final String TEMPFILE _ MARKER = " tmp " ; 
 
 public static final int TOMBSTONE _ HISTOGRAM _ BIN _ SIZE = 100 ; 
 
 - public static final Comparator < SSTableReader > maxTimestampComparator = new Comparator < SSTableReader > ( ) 
 - { 
 - public int compare ( SSTableReader o1 , SSTableReader o2 ) 
 - { 
 - long ts1 = o1 . getMaxTimestamp ( ) ; 
 - long ts2 = o2 . getMaxTimestamp ( ) ; 
 - return ( ts1 > ts2 ? - 1 : ( ts1 = = ts2 ? 0 : 1 ) ) ; 
 - } 
 - } ; 
 - 
 public final Descriptor descriptor ; 
 protected final Set < Component > components ; 
 public final CFMetaData metadata ; 
 @ @ - 101 , 26 + 83 , 13 @ @ public abstract class SSTable 
 assert partitioner ! = null ; 
 
 this . descriptor = descriptor ; 
 - Set < Component > dataComponents = new HashSet < Component > ( components ) ; 
 - for ( Component component : components ) 
 - assert component . type ! = Component . Type . COMPACTED _ MARKER ; 
 - 
 + Set < Component > dataComponents = new HashSet < > ( components ) ; 
 this . compression = dataComponents . contains ( Component . COMPRESSION _ INFO ) ; 
 - this . components = new CopyOnWriteArraySet < Component > ( dataComponents ) ; 
 + this . components = new CopyOnWriteArraySet < > ( dataComponents ) ; 
 this . metadata = metadata ; 
 this . partitioner = partitioner ; 
 } 
 
 - public static final Comparator < SSTableReader > sstableComparator = new Comparator < SSTableReader > ( ) 
 - { 
 - public int compare ( SSTableReader o1 , SSTableReader o2 ) 
 - { 
 - return o1 . first . compareTo ( o2 . first ) ; 
 - } 
 - } ; 
 - 
 - public static final Ordering < SSTableReader > sstableOrdering = Ordering . from ( sstableComparator ) ; 
 - 
 / * * 
 * We use a ReferenceQueue to manage deleting files that have been compacted 
 * and for which no more SSTable references exist . But this is not guaranteed 
 @ @ - 139 , 15 + 108 , 11 @ @ public abstract class SSTable 
 FileUtils . deleteWithConfirm ( desc . filenameFor ( Component . DATA ) ) ; 
 for ( Component component : components ) 
 { 
 - if ( component . equals ( Component . DATA ) | | component . equals ( Component . COMPACTED _ MARKER ) | | component . equals ( Component . SUMMARY ) ) 
 + if ( component . equals ( Component . DATA ) | | component . equals ( Component . SUMMARY ) ) 
 continue ; 
 
 FileUtils . deleteWithConfirm ( desc . filenameFor ( component ) ) ; 
 } 
 - / / remove the COMPACTED _ MARKER component last if it exists 
 - / / Note : newly created sstable should not have a marker , but we keep this for now to make sure 
 - / / we don ' t leave older marker around 
 - FileUtils . delete ( desc . filenameFor ( Component . COMPACTED _ MARKER ) ) ; 
 FileUtils . delete ( desc . filenameFor ( Component . SUMMARY ) ) ; 
 
 logger . debug ( " Deleted { } " , desc ) ; 
 @ @ - 167 , 12 + 132 , 12 @ @ public abstract class SSTable 
 
 public String getFilename ( ) 
 { 
 - return descriptor . filenameFor ( COMPONENT _ DATA ) ; 
 + return descriptor . filenameFor ( Component . DATA ) ; 
 } 
 
 public String getIndexFilename ( ) 
 { 
 - return descriptor . filenameFor ( COMPONENT _ INDEX ) ; 
 + return descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ; 
 } 
 
 public String getColumnFamilyName ( ) 
 @ @ - 262 , 16 + 227 , 6 @ @ public abstract class SSTable 
 return estimatedRows ; 
 } 
 
 - public static long getTotalBytes ( Iterable < SSTableReader > sstables ) 
 - { 
 - long sum = 0 ; 
 - for ( SSTableReader sstable : sstables ) 
 - { 
 - sum + = sstable . onDiskLength ( ) ; 
 - } 
 - return sum ; 
 - } 
 - 
 public long bytesOnDisk ( ) 
 { 
 long bytes = 0 ; 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableMetadata . java b / src / java / org / apache / cassandra / io / sstable / SSTableMetadata . java 
 index 140e08b . . 8ddfdd7 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableMetadata . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableMetadata . java 
 @ @ - 435 , 7 + 435 , 7 @ @ public class SSTableMetadata 
 public Pair < SSTableMetadata , Set < Integer > > deserialize ( Descriptor descriptor , boolean loadSSTableLevel ) throws IOException 
 { 
 logger . debug ( " Load metadata for { } " , descriptor ) ; 
 - File statsFile = new File ( descriptor . filenameFor ( SSTable . COMPONENT _ STATS ) ) ; 
 + File statsFile = new File ( descriptor . filenameFor ( Component . STATS ) ) ; 
 if ( ! statsFile . exists ( ) ) 
 { 
 logger . debug ( " No sstable stats for { } " , descriptor ) ; 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 index c5b61d9 . . 7fd9ca6 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableReader . java 
 @ @ - 29 , 6 + 29 , 7 @ @ import java . util . concurrent . atomic . AtomicInteger ; 
 import java . util . concurrent . atomic . AtomicLong ; 
 
 import com . google . common . annotations . VisibleForTesting ; 
 + import com . google . common . collect . Ordering ; 
 import com . google . common . primitives . Longs ; 
 import com . google . common . util . concurrent . RateLimiter ; 
 import org . slf4j . Logger ; 
 @ @ - 67 , 8 + 68 , 28 @ @ public class SSTableReader extends SSTable implements Closeable 
 private static final ScheduledThreadPoolExecutor syncExecutor = new ScheduledThreadPoolExecutor ( 1 ) ; 
 private static final RateLimiter meterSyncThrottle = RateLimiter . create ( 100 . 0 ) ; 
 
 + public static final Comparator < SSTableReader > maxTimestampComparator = new Comparator < SSTableReader > ( ) 
 + { 
 + public int compare ( SSTableReader o1 , SSTableReader o2 ) 
 + { 
 + long ts1 = o1 . getMaxTimestamp ( ) ; 
 + long ts2 = o2 . getMaxTimestamp ( ) ; 
 + return ( ts1 > ts2 ? - 1 : ( ts1 = = ts2 ? 0 : 1 ) ) ; 
 + } 
 + } ; 
 + 
 + public static final Comparator < SSTableReader > sstableComparator = new Comparator < SSTableReader > ( ) 
 + { 
 + public int compare ( SSTableReader o1 , SSTableReader o2 ) 
 + { 
 + return o1 . first . compareTo ( o2 . first ) ; 
 + } 
 + } ; 
 + 
 + public static final Ordering < SSTableReader > sstableOrdering = Ordering . from ( sstableComparator ) ; 
 + 
 / * * 
 - * maxDataAge is a timestamp in local server time ( e . g . System . currentTimeMilli ) which represents an uppper bound 
 + * maxDataAge is a timestamp in local server time ( e . g . System . currentTimeMilli ) which represents an upper bound 
 * to the newest piece of data stored in the sstable . In other words , this sstable does not contain items created 
 * later than maxDataAge . 
 * 
 @ @ - 168 , 7 + 189 , 7 @ @ public class SSTableReader extends SSTable implements Closeable 
 SegmentedFile . Builder dbuilder = sstable . compression 
 ? new CompressedSegmentedFile . Builder ( ) 
 : new BufferedSegmentedFile . Builder ( ) ; 
 - if ( ! loadSummary ( sstable , ibuilder , dbuilder , sstable . metadata ) ) 
 + if ( ! sstable . loadSummary ( ibuilder , dbuilder , sstable . metadata ) ) 
 sstable . buildSummary ( false , ibuilder , dbuilder , false ) ; 
 sstable . ifile = ibuilder . complete ( sstable . descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) ; 
 sstable . dfile = dbuilder . complete ( sstable . descriptor . filenameFor ( Component . DATA ) ) ; 
 @ @ - 218 , 7 + 239 , 7 @ @ public class SSTableReader extends SSTable implements Closeable 
 assert components . contains ( Component . DATA ) : " Data component is missing for sstable " + descriptor ; 
 assert components . contains ( Component . PRIMARY _ INDEX ) : " Primary index component is missing for sstable " + descriptor ; 
 
 - logger . info ( " Opening { } ( { } bytes ) " , descriptor , new File ( descriptor . filenameFor ( COMPONENT _ DATA ) ) . length ( ) ) ; 
 + logger . info ( " Opening { } ( { } bytes ) " , descriptor , new File ( descriptor . filenameFor ( Component . DATA ) ) . length ( ) ) ; 
 
 SSTableMetadata sstableMetadata = SSTableMetadata . serializer . deserialize ( descriptor ) . left ; 
 
 @ @ - 247 , 7 + 268 , 7 @ @ public class SSTableReader extends SSTable implements Closeable 
 final CFMetaData metadata , 
 final IPartitioner partitioner ) 
 { 
 - final Collection < SSTableReader > sstables = new LinkedBlockingQueue < SSTableReader > ( ) ; 
 + final Collection < SSTableReader > sstables = new LinkedBlockingQueue < > ( ) ; 
 
 ExecutorService executor = DebuggableThreadPoolExecutor . createWithFixedPoolSize ( " SSTableBatchOpen " , FBUtilities . getAvailableProcessors ( ) ) ; 
 for ( final Map . Entry < Descriptor , Set < Component > > entry : entries ) 
 @ @ - 365 , 6 + 386 , 16 @ @ public class SSTableReader extends SSTable implements Closeable 
 this . bf = bloomFilter ; 
 } 
 
 + public static long getTotalBytes ( Iterable < SSTableReader > sstables ) 
 + { 
 + long sum = 0 ; 
 + for ( SSTableReader sstable : sstables ) 
 + { 
 + sum + = sstable . onDiskLength ( ) ; 
 + } 
 + return sum ; 
 + } 
 + 
 / * * 
 * Clean up all opened resources . 
 * 
 @ @ - 441 , 18 + 472 , 18 @ @ public class SSTableReader extends SSTable implements Closeable 
 ? SegmentedFile . getCompressedBuilder ( ) 
 : SegmentedFile . getBuilder ( DatabaseDescriptor . getDiskAccessMode ( ) ) ; 
 
 - boolean summaryLoaded = loadSummary ( this , ibuilder , dbuilder , metadata ) ; 
 + boolean summaryLoaded = loadSummary ( ibuilder , dbuilder , metadata ) ; 
 if ( recreateBloomFilter | | ! summaryLoaded ) 
 buildSummary ( recreateBloomFilter , ibuilder , dbuilder , summaryLoaded ) ; 
 
 ifile = ibuilder . complete ( descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) ; 
 dfile = dbuilder . complete ( descriptor . filenameFor ( Component . DATA ) ) ; 
 if ( saveSummaryIfCreated & & ( recreateBloomFilter | | ! summaryLoaded ) ) / / save summary information to disk 
 - saveSummary ( this , ibuilder , dbuilder ) ; 
 + saveSummary ( ibuilder , dbuilder ) ; 
 } 
 
 - private void buildSummary ( boolean recreateBloomFilter , SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder , boolean summaryLoaded ) throws IOException 
 - { 
 + private void buildSummary ( boolean recreateBloomFilter , SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder , boolean summaryLoaded ) throws IOException 
 + { 
 / / we read the positions in a BRAF so we don ' t have to worry about an entry spanning a mmap boundary . 
 RandomAccessReader primaryIndex = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) ) ; 
 
 @ @ - 505 , 27 + 536 , 27 @ @ public class SSTableReader extends SSTable implements Closeable 
 last = getMinimalKey ( last ) ; 
 } 
 
 - public static boolean loadSummary ( SSTableReader reader , SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder , CFMetaData metadata ) 
 + public boolean loadSummary ( SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder , CFMetaData metadata ) 
 { 
 - File summariesFile = new File ( reader . descriptor . filenameFor ( Component . SUMMARY ) ) ; 
 - if ( ! reader . descriptor . version . offHeapSummaries | | ! summariesFile . exists ( ) ) 
 + File summariesFile = new File ( descriptor . filenameFor ( Component . SUMMARY ) ) ; 
 + if ( ! descriptor . version . offHeapSummaries | | ! summariesFile . exists ( ) ) 
 return false ; 
 
 DataInputStream iStream = null ; 
 try 
 { 
 iStream = new DataInputStream ( new FileInputStream ( summariesFile ) ) ; 
 - reader . indexSummary = IndexSummary . serializer . deserialize ( iStream , reader . partitioner ) ; 
 - if ( reader . indexSummary . getIndexInterval ( ) ! = metadata . getIndexInterval ( ) ) 
 + indexSummary = IndexSummary . serializer . deserialize ( iStream , partitioner ) ; 
 + if ( indexSummary . getIndexInterval ( ) ! = metadata . getIndexInterval ( ) ) 
 { 
 iStream . close ( ) ; 
 logger . debug ( " Cannot read the saved summary for { } because Index Interval changed from { } to { } . " , 
 - reader . toString ( ) , reader . indexSummary . getIndexInterval ( ) , metadata . getIndexInterval ( ) ) ; 
 + toString ( ) , indexSummary . getIndexInterval ( ) , metadata . getIndexInterval ( ) ) ; 
 FileUtils . deleteWithConfirm ( summariesFile ) ; 
 return false ; 
 } 
 - reader . first = reader . partitioner . decorateKey ( ByteBufferUtil . readWithLength ( iStream ) ) ; 
 - reader . last = reader . partitioner . decorateKey ( ByteBufferUtil . readWithLength ( iStream ) ) ; 
 + first = partitioner . decorateKey ( ByteBufferUtil . readWithLength ( iStream ) ) ; 
 + last = partitioner . decorateKey ( ByteBufferUtil . readWithLength ( iStream ) ) ; 
 ibuilder . deserializeBounds ( iStream ) ; 
 dbuilder . deserializeBounds ( iStream ) ; 
 } 
 @ @ - 544 , 9 + 575 , 9 @ @ public class SSTableReader extends SSTable implements Closeable 
 return true ; 
 } 
 
 - public static void saveSummary ( SSTableReader reader , SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder ) 
 + public void saveSummary ( SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder ) 
 { 
 - File summariesFile = new File ( reader . descriptor . filenameFor ( Component . SUMMARY ) ) ; 
 + File summariesFile = new File ( descriptor . filenameFor ( Component . SUMMARY ) ) ; 
 if ( summariesFile . exists ( ) ) 
 summariesFile . delete ( ) ; 
 
 @ @ - 554 , 9 + 585 , 9 @ @ public class SSTableReader extends SSTable implements Closeable 
 try 
 { 
 oStream = new DataOutputStream ( new FileOutputStream ( summariesFile ) ) ; 
 - IndexSummary . serializer . serialize ( reader . indexSummary , oStream ) ; 
 - ByteBufferUtil . writeWithLength ( reader . first . key , oStream ) ; 
 - ByteBufferUtil . writeWithLength ( reader . last . key , oStream ) ; 
 + IndexSummary . serializer . serialize ( indexSummary , oStream ) ; 
 + ByteBufferUtil . writeWithLength ( first . key , oStream ) ; 
 + ByteBufferUtil . writeWithLength ( last . key , oStream ) ; 
 ibuilder . serializeBounds ( oStream ) ; 
 dbuilder . serializeBounds ( oStream ) ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 index 70c0b42 . . b5d50cf 100644 
 - - - a / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 + + + b / src / java / org / apache / cassandra / io / sstable / SSTableWriter . java 
 @ @ - 313 , 8 + 313 , 8 @ @ public class SSTableWriter extends SSTable 
 SSTableMetadata sstableMetadata = p . right ; 
 
 / / finalize in - memory state for the reader 
 - SegmentedFile ifile = iwriter . builder . complete ( newdesc . filenameFor ( SSTable . COMPONENT _ INDEX ) ) ; 
 - SegmentedFile dfile = dbuilder . complete ( newdesc . filenameFor ( SSTable . COMPONENT _ DATA ) ) ; 
 + SegmentedFile ifile = iwriter . builder . complete ( newdesc . filenameFor ( Component . PRIMARY _ INDEX ) ) ; 
 + SegmentedFile dfile = dbuilder . complete ( newdesc . filenameFor ( Component . DATA ) ) ; 
 SSTableReader sstable = SSTableReader . internalOpen ( newdesc , 
 components , 
 metadata , 
 @ @ - 328 , 7 + 328 , 7 @ @ public class SSTableWriter extends SSTable 
 sstable . first = getMinimalKey ( first ) ; 
 sstable . last = getMinimalKey ( last ) ; 
 / / try to save the summaries to disk 
 - SSTableReader . saveSummary ( sstable , iwriter . builder , dbuilder ) ; 
 + sstable . saveSummary ( iwriter . builder , dbuilder ) ; 
 iwriter = null ; 
 dbuilder = null ; 
 return sstable ; 
 @ @ - 355 , 7 + 355 , 7 @ @ public class SSTableWriter extends SSTable 
 
 private static void writeMetadata ( Descriptor desc , SSTableMetadata sstableMetadata , Set < Integer > ancestors ) 
 { 
 - SequentialWriter out = SequentialWriter . open ( new File ( desc . filenameFor ( SSTable . COMPONENT _ STATS ) ) , true ) ; 
 + SequentialWriter out = SequentialWriter . open ( new File ( desc . filenameFor ( Component . STATS ) ) , true ) ; 
 try 
 { 
 SSTableMetadata . serializer . serialize ( sstableMetadata , ancestors , out . stream ) ; 
 @ @ - 411 , 7 + 411 , 7 @ @ public class SSTableWriter extends SSTable 
 
 IndexWriter ( long keyCount ) 
 { 
 - indexFile = SequentialWriter . open ( new File ( descriptor . filenameFor ( SSTable . COMPONENT _ INDEX ) ) , 
 + indexFile = SequentialWriter . open ( new File ( descriptor . filenameFor ( Component . PRIMARY _ INDEX ) ) , 
 ! metadata . populateIoCacheOnFlush ( ) ) ; 
 builder = SegmentedFile . getBuilder ( DatabaseDescriptor . getIndexAccessMode ( ) ) ; 
 summary = new IndexSummaryBuilder ( keyCount , metadata . getIndexInterval ( ) ) ; 
 @ @ - 446 , 7 + 446 , 7 @ @ public class SSTableWriter extends SSTable 
 { 
 if ( components . contains ( Component . FILTER ) ) 
 { 
 - String path = descriptor . filenameFor ( SSTable . COMPONENT _ FILTER ) ; 
 + String path = descriptor . filenameFor ( Component . FILTER ) ; 
 try 
 { 
 / / bloom filter 
 diff - - git a / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java b / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java 
 index f334d08 . . 0606941 100644 
 - - - a / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java 
 + + + b / src / java / org / apache / cassandra / io / util / DataIntegrityMetadata . java 
 @ @ - 145 , 10 + 145 , 10 @ @ public class DataIntegrityMetadata 
 byte [ ] bytes = digest . digest ( ) ; 
 if ( bytes = = null ) 
 return ; 
 - SequentialWriter out = SequentialWriter . open ( new File ( descriptor . filenameFor ( SSTable . COMPONENT _ DIGEST ) ) , true ) ; 
 + SequentialWriter out = SequentialWriter . open ( new File ( descriptor . filenameFor ( Component . DIGEST ) ) , true ) ; 
 / / Writting output compatible with sha1sum 
 Descriptor newdesc = descriptor . asTemporary ( false ) ; 
 - String [ ] tmp = newdesc . filenameFor ( SSTable . COMPONENT _ DATA ) . split ( Pattern . quote ( File . separator ) ) ; 
 + String [ ] tmp = newdesc . filenameFor ( Component . DATA ) . split ( Pattern . quote ( File . separator ) ) ; 
 String dataFileName = tmp [ tmp . length - 1 ] ; 
 try 
 { 
 diff - - git a / test / long / org / apache / cassandra / db / compaction / LongLeveledCompactionStrategyTest . java b / test / long / org / apache / cassandra / db / compaction / LongLeveledCompactionStrategyTest . java 
 index c6b6eb0 . . 0eb44d0 100644 
 - - - a / test / long / org / apache / cassandra / db / compaction / LongLeveledCompactionStrategyTest . java 
 + + + b / test / long / org / apache / cassandra / db / compaction / LongLeveledCompactionStrategyTest . java 
 @ @ - 31 , 7 + 31 , 6 @ @ import org . apache . cassandra . db . ColumnFamilyStore ; 
 import org . apache . cassandra . db . DecoratedKey ; 
 import org . apache . cassandra . db . RowMutation ; 
 import org . apache . cassandra . db . Keyspace ; 
 - import org . apache . cassandra . io . sstable . SSTable ; 
 import org . apache . cassandra . io . sstable . SSTableReader ; 
 import org . apache . cassandra . utils . ByteBufferUtil ; 
 import org . apache . cassandra . utils . FBUtilities ; 
 @ @ - 106 , 7 + 105 , 7 @ @ public class LongLeveledCompactionStrategyTest extends SchemaLoader 
 { 
 List < SSTableReader > sstables = manifest . getLevel ( level ) ; 
 / / score check 
 - assert ( double ) SSTable . getTotalBytes ( sstables ) / manifest . maxBytesForLevel ( level ) < 1 . 00 ; 
 + assert ( double ) SSTableReader . getTotalBytes ( sstables ) / manifest . maxBytesForLevel ( level ) < 1 . 00 ; 
 / / overlap check for levels greater than 0 
 if ( level > 0 ) 
 { 
 diff - - git a / test / unit / org / apache / cassandra / db / ColumnFamilyStoreTest . java b / test / unit / org / apache / cassandra / db / ColumnFamilyStoreTest . java 
 index 32bc7df . . 7f7d5c9 100644 
 - - - a / test / unit / org / apache / cassandra / db / ColumnFamilyStoreTest . java 
 + + + b / test / unit / org / apache / cassandra / db / ColumnFamilyStoreTest . java 
 @ @ - 938 , 7 + 938 , 7 @ @ public class ColumnFamilyStoreTest extends SchemaLoader 
 cfs . clearUnsafe ( ) ; 
 assertEquals ( 0 , cfs . getSSTables ( ) . size ( ) ) ; 
 
 - new File ( ssTables . iterator ( ) . next ( ) . descriptor . filenameFor ( SSTable . COMPONENT _ STATS ) ) . delete ( ) ; 
 + new File ( ssTables . iterator ( ) . next ( ) . descriptor . filenameFor ( Component . STATS ) ) . delete ( ) ; 
 cfs . loadNewSSTables ( ) ; 
 
 / / Add another column with a lower timestamp 
 diff - - git a / test / unit / org / apache / cassandra / db / compaction / LeveledCompactionStrategyTest . java b / test / unit / org / apache / cassandra / db / compaction / LeveledCompactionStrategyTest . java 
 index b60f6d9 . . a008de1 100644 
 - - - a / test / unit / org / apache / cassandra / db / compaction / LeveledCompactionStrategyTest . java 
 + + + b / test / unit / org / apache / cassandra / db / compaction / LeveledCompactionStrategyTest . java 
 @ @ - 20 , 9 + 20 , 7 @ @ package org . apache . cassandra . db . compaction ; 
 import java . nio . ByteBuffer ; 
 import java . util . Arrays ; 
 import java . util . Collection ; 
 - import java . util . HashSet ; 
 import java . util . List ; 
 - import java . util . Set ; 
 import java . util . concurrent . ExecutionException ; 
 import java . util . UUID ; 
 
 @ @ - 39 , 7 + 37 , 6 @ @ import org . apache . cassandra . db . Keyspace ; 
 import org . apache . cassandra . dht . Range ; 
 import org . apache . cassandra . dht . Token ; 
 import org . apache . cassandra . io . sstable . Component ; 
 - import org . apache . cassandra . io . sstable . SSTable ; 
 import org . apache . cassandra . io . sstable . SSTableReader ; 
 import org . apache . cassandra . repair . RepairJobDesc ; 
 import org . apache . cassandra . repair . Validator ; 
 @ @ - 145 , 7 + 142 , 7 @ @ public class LeveledCompactionStrategyTest extends SchemaLoader 
 scanner . next ( ) ; 
 
 / / scanner . getCurrentPosition should be equal to total bytes of L1 sstables 
 - assert scanner . getCurrentPosition ( ) = = SSTable . getTotalBytes ( sstables ) ; 
 + assert scanner . getCurrentPosition ( ) = = SSTableReader . getTotalBytes ( sstables ) ; 
 } 
 
 @ Test 
 diff - - git a / test / unit / org / apache / cassandra / io / sstable / SSTableReaderTest . java b / test / unit / org / apache / cassandra / io / sstable / SSTableReaderTest . java 
 index 36d8fbe . . b771e72 100644 
 - - - a / test / unit / org / apache / cassandra / io / sstable / SSTableReaderTest . java 
 + + + b / test / unit / org / apache / cassandra / io / sstable / SSTableReaderTest . java 
 @ @ - 278 , 7 + 278 , 7 @ @ public class SSTableReaderTest extends SchemaLoader 
 SegmentedFile . Builder dbuilder = sstable . compression 
 ? SegmentedFile . getCompressedBuilder ( ) 
 : SegmentedFile . getBuilder ( DatabaseDescriptor . getDiskAccessMode ( ) ) ; 
 - SSTableReader . saveSummary ( sstable , ibuilder , dbuilder ) ; 
 + sstable . saveSummary ( ibuilder , dbuilder ) ; 
 
 SSTableReader reopened = SSTableReader . open ( sstable . descriptor ) ; 
 assert reopened . first . token instanceof LocalToken ;
