BLEU SCORE: 0.040583489434387374

TEST MSG: Fix regression with compressed reader performance
GENERATED MSG: merge from 0 . 7

TEST DIFF (one line): diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java <nl> index edf8c68 . . 1febe37 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java <nl> @ @ - 42 , 17 + 42 , 23 @ @ import org . apache . cassandra . utils . FBUtilities ; <nl> * / <nl> public class CompressedRandomAccessReader extends RandomAccessReader <nl> { <nl> - private static final boolean useMmap = DatabaseDescriptor . getDiskAccessMode ( ) = = Config . DiskAccessMode . mmap ; <nl> - <nl> public static CompressedRandomAccessReader open ( ChannelProxy channel , CompressionMetadata metadata ) <nl> { <nl> - return open ( channel , metadata , null ) ; <nl> + try <nl> + { <nl> + return new CompressedRandomAccessReader ( channel , metadata , null ) ; <nl> + } <nl> + catch ( FileNotFoundException e ) <nl> + { <nl> + throw new RuntimeException ( e ) ; <nl> + } <nl> } <nl> - public static CompressedRandomAccessReader open ( ChannelProxy channel , CompressionMetadata metadata , CompressedPoolingSegmentedFile owner ) <nl> + <nl> + public static CompressedRandomAccessReader open ( ICompressedFile file ) <nl> { <nl> try <nl> { <nl> - return new CompressedRandomAccessReader ( channel , metadata , owner ) ; <nl> + return new CompressedRandomAccessReader ( file . channel ( ) , file . getMetadata ( ) , file ) ; <nl> } <nl> catch ( FileNotFoundException e ) <nl> { <nl> @ @ - 60 , 9 + 66 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> } <nl> } <nl> <nl> - <nl> - private TreeMap < Long , MappedByteBuffer > chunkSegments ; <nl> - private int MAX _ SEGMENT _ SIZE = Integer . MAX _ VALUE ; <nl> + private final TreeMap < Long , MappedByteBuffer > chunkSegments ; <nl> <nl> private final CompressionMetadata metadata ; <nl> <nl> @ @ - 75 , 61 + 79 , 24 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> / / raw checksum bytes <nl> private ByteBuffer checksumBytes ; <nl> <nl> - protected CompressedRandomAccessReader ( ChannelProxy channel , CompressionMetadata metadata , PoolingSegmentedFile owner ) throws FileNotFoundException <nl> + protected CompressedRandomAccessReader ( ChannelProxy channel , CompressionMetadata metadata , ICompressedFile file ) throws FileNotFoundException <nl> { <nl> - super ( channel , metadata . chunkLength ( ) , metadata . compressedFileLength , metadata . compressor ( ) . useDirectOutputByteBuffers ( ) , owner ) ; <nl> + super ( channel , metadata . chunkLength ( ) , metadata . compressedFileLength , metadata . compressor ( ) . useDirectOutputByteBuffers ( ) , file instanceof PoolingSegmentedFile ? ( PoolingSegmentedFile ) file : null ) ; <nl> this . metadata = metadata ; <nl> checksum = new Adler32 ( ) ; <nl> <nl> - if ( ! useMmap ) <nl> + chunkSegments = file = = null ? null : file . chunkSegments ( ) ; <nl> + if ( chunkSegments = = null ) <nl> { <nl> - compressed = ByteBuffer . wrap ( new byte [ metadata . compressor ( ) . initialCompressedBufferLength ( metadata . chunkLength ( ) ) ] ) ; <nl> + compressed = super . allocateBuffer ( metadata . compressor ( ) . initialCompressedBufferLength ( metadata . chunkLength ( ) ) , metadata . compressor ( ) . useDirectOutputByteBuffers ( ) ) ; <nl> checksumBytes = ByteBuffer . wrap ( new byte [ 4 ] ) ; <nl> } <nl> - else <nl> - { <nl> - try <nl> - { <nl> - createMappedSegments ( ) ; <nl> - } <nl> - catch ( IOException e ) <nl> - { <nl> - throw new IOError ( e ) ; <nl> - } <nl> - } <nl> - } <nl> - <nl> - private void createMappedSegments ( ) throws IOException <nl> - { <nl> - chunkSegments = new TreeMap < > ( ) ; <nl> - long offset = 0 ; <nl> - long lastSegmentOffset = 0 ; <nl> - long segmentSize = 0 ; <nl> - <nl> - while ( offset < metadata . dataLength ) <nl> - { <nl> - CompressionMetadata . Chunk chunk = metadata . chunkFor ( offset ) ; <nl> - <nl> - / / Reached a new mmap boundary <nl> - if ( segmentSize + chunk . length + 4 > MAX _ SEGMENT _ SIZE ) <nl> - { <nl> - chunkSegments . put ( lastSegmentOffset , channel . map ( FileChannel . MapMode . READ _ ONLY , lastSegmentOffset , segmentSize ) ) ; <nl> - lastSegmentOffset + = segmentSize ; <nl> - segmentSize = 0 ; <nl> - } <nl> - <nl> - segmentSize + = chunk . length + 4 ; / / checksum <nl> - offset + = metadata . chunkLength ( ) ; <nl> - } <nl> - <nl> - if ( segmentSize > 0 ) <nl> - chunkSegments . put ( lastSegmentOffset , channel . map ( FileChannel . MapMode . READ _ ONLY , lastSegmentOffset , segmentSize ) ) ; <nl> } <nl> <nl> protected ByteBuffer allocateBuffer ( int bufferSize , boolean useDirect ) <nl> { <nl> assert Integer . bitCount ( bufferSize ) = = 1 ; <nl> - return useMmap & & useDirect <nl> + return useDirect <nl> ? ByteBuffer . allocateDirect ( bufferSize ) <nl> : ByteBuffer . allocate ( bufferSize ) ; <nl> } <nl> @ @ - 138 , 16 + 105 , 9 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> public void deallocate ( ) <nl> { <nl> super . deallocate ( ) ; <nl> - <nl> - if ( chunkSegments ! = null ) <nl> - { <nl> - for ( Map . Entry < Long , MappedByteBuffer > entry : chunkSegments . entrySet ( ) ) <nl> - { <nl> - FileUtils . clean ( entry . getValue ( ) ) ; <nl> - } <nl> - } <nl> - <nl> - chunkSegments = null ; <nl> + if ( compressed ! = null ) <nl> + FileUtils . clean ( compressed ) ; <nl> + compressed = null ; <nl> } <nl> <nl> private void reBufferStandard ( ) <nl> @ @ - 175 , 7 + 135 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> int decompressedBytes ; <nl> try <nl> { <nl> - decompressedBytes = metadata . compressor ( ) . uncompress ( compressed . array ( ) , 0 , chunk . length , buffer . array ( ) , 0 ) ; <nl> + decompressedBytes = metadata . compressor ( ) . uncompress ( compressed , buffer ) ; <nl> buffer . limit ( decompressedBytes ) ; <nl> } <nl> catch ( IOException e ) <nl> @ @ - 186 , 8 + 146 , 8 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> <nl> if ( metadata . parameters . getCrcCheckChance ( ) > ThreadLocalRandom . current ( ) . nextDouble ( ) ) <nl> { <nl> - <nl> - checksum . update ( compressed . array ( ) , 0 , chunk . length ) ; <nl> + compressed . position ( 0 ) ; <nl> + FBUtilities . directCheckSum ( checksum , compressed ) ; <nl> <nl> if ( checksum ( chunk ) ! = ( int ) checksum . getValue ( ) ) <nl> throw new CorruptBlockException ( getPath ( ) , chunk ) ; <nl> @ @ - 226 , 7 + 186 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> Map . Entry < Long , MappedByteBuffer > entry = chunkSegments . floorEntry ( chunk . offset ) ; <nl> long segmentOffset = entry . getKey ( ) ; <nl> int chunkOffset = Ints . checkedCast ( chunk . offset - segmentOffset ) ; <nl> - MappedByteBuffer compressedChunk = entry . getValue ( ) ; <nl> + ByteBuffer compressedChunk = entry . getValue ( ) . duplicate ( ) ; <nl> <nl> compressedChunk . position ( chunkOffset ) ; <nl> compressedChunk . limit ( chunkOffset + chunk . length ) ; <nl> @ @ - 284 , 7 + 244 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> @ Override <nl> protected void reBuffer ( ) <nl> { <nl> - if ( useMmap ) <nl> + if ( chunkSegments ! = null ) <nl> { <nl> reBufferMmap ( ) ; <nl> } <nl> @ @ - 305 , 7 + 265 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader <nl> <nl> public int getTotalBufferSize ( ) <nl> { <nl> - return super . getTotalBufferSize ( ) + ( useMmap ? 0 : compressed . capacity ( ) ) ; <nl> + return super . getTotalBufferSize ( ) + ( chunkSegments ! = null ? 0 : compressed . capacity ( ) ) ; <nl> } <nl> <nl> @ Override <nl> diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedThrottledReader . java b / src / java / org / apache / cassandra / io / compress / CompressedThrottledReader . java <nl> index 63727d8 . . a29129c 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / CompressedThrottledReader . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / CompressedThrottledReader . java <nl> @ @ - 26 , 14 + 26 , 15 @ @ import java . io . FileNotFoundException ; <nl> import com . google . common . util . concurrent . RateLimiter ; <nl> <nl> import org . apache . cassandra . io . util . ChannelProxy ; <nl> + import org . apache . cassandra . io . util . ICompressedFile ; <nl> <nl> public class CompressedThrottledReader extends CompressedRandomAccessReader <nl> { <nl> private final RateLimiter limiter ; <nl> <nl> - public CompressedThrottledReader ( ChannelProxy channel , CompressionMetadata metadata , RateLimiter limiter ) throws FileNotFoundException <nl> + public CompressedThrottledReader ( ChannelProxy channel , CompressionMetadata metadata , ICompressedFile file , RateLimiter limiter ) throws FileNotFoundException <nl> { <nl> - super ( channel , metadata , null ) ; <nl> + super ( channel , metadata , file ) ; <nl> this . limiter = limiter ; <nl> } <nl> <nl> @ @ - 43 , 11 + 44 , 11 @ @ public class CompressedThrottledReader extends CompressedRandomAccessReader <nl> super . reBuffer ( ) ; <nl> } <nl> <nl> - public static CompressedThrottledReader open ( ChannelProxy channel , CompressionMetadata metadata , RateLimiter limiter ) <nl> + public static CompressedThrottledReader open ( ICompressedFile file , RateLimiter limiter ) <nl> { <nl> try <nl> { <nl> - return new CompressedThrottledReader ( channel , metadata , limiter ) ; <nl> + return new CompressedThrottledReader ( file . channel ( ) , file . getMetadata ( ) , file , limiter ) ; <nl> } <nl> catch ( FileNotFoundException e ) <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / io / compress / DeflateCompressor . java b / src / java / org / apache / cassandra / io / compress / DeflateCompressor . java <nl> index a88e4d2 . . 833c375 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / DeflateCompressor . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / DeflateCompressor . java <nl> @ @ - 122 , 13 + 122 , 14 @ @ public class DeflateCompressor implements ICompressor <nl> } <nl> } <nl> <nl> - public int uncompress ( ByteBuffer input _ , ByteBuffer output ) throws IOException <nl> + public int uncompress ( ByteBuffer input , ByteBuffer output ) throws IOException <nl> { <nl> if ( ! output . hasArray ( ) ) <nl> throw new IllegalArgumentException ( " DeflateCompressor doesn ' t work with direct byte buffers " ) ; <nl> <nl> - byte [ ] input = ByteBufferUtil . getArray ( input _ ) ; <nl> - return uncompress ( input , 0 , input . length , output . array ( ) , output . arrayOffset ( ) + output . position ( ) ) ; <nl> + if ( input . hasArray ( ) ) <nl> + return uncompress ( input . array ( ) , input . arrayOffset ( ) + input . position ( ) , input . remaining ( ) , output . array ( ) , output . arrayOffset ( ) + output . position ( ) ) ; <nl> + return uncompress ( ByteBufferUtil . getArray ( input ) , 0 , input . remaining ( ) , output . array ( ) , output . arrayOffset ( ) + output . position ( ) ) ; <nl> } <nl> <nl> public boolean useDirectOutputByteBuffers ( ) <nl> diff - - git a / src / java / org / apache / cassandra / io / compress / LZ4Compressor . java b / src / java / org / apache / cassandra / io / compress / LZ4Compressor . java <nl> index ab10a00 . . 9d54048 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / LZ4Compressor . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / LZ4Compressor . java <nl> @ @ - 83 , 6 + 83 , 7 @ @ public class LZ4Compressor implements ICompressor <nl> | ( ( input [ inputOffset + 1 ] & 0xFF ) < < 8 ) <nl> | ( ( input [ inputOffset + 2 ] & 0xFF ) < < 16 ) <nl> | ( ( input [ inputOffset + 3 ] & 0xFF ) < < 24 ) ; <nl> + <nl> final int compressedLength ; <nl> try <nl> { <nl> @ @ - 104 , 6 + 105 , 9 @ @ public class LZ4Compressor implements ICompressor <nl> <nl> public int uncompress ( ByteBuffer input , ByteBuffer output ) throws IOException <nl> { <nl> + if ( input . hasArray ( ) & & output . hasArray ( ) ) <nl> + return uncompress ( input . array ( ) , input . arrayOffset ( ) + input . position ( ) , input . remaining ( ) , output . array ( ) , output . arrayOffset ( ) + output . position ( ) ) ; <nl> + <nl> int pos = input . position ( ) ; <nl> final int decompressedLength = ( input . get ( pos ) & 0xFF ) <nl> | ( ( input . get ( pos + 1 ) & 0xFF ) < < 8 ) <nl> @ @ - 132 , 7 + 136 , 7 @ @ public class LZ4Compressor implements ICompressor <nl> @ Override <nl> public boolean useDirectOutputByteBuffers ( ) <nl> { <nl> - return false ; <nl> + return true ; <nl> } <nl> <nl> public Set < String > supportedOptions ( ) <nl> diff - - git a / src / java / org / apache / cassandra / io / compress / SnappyCompressor . java b / src / java / org / apache / cassandra / io / compress / SnappyCompressor . java <nl> index d1f1f34 . . 04f676b 100644 <nl> - - - a / src / java / org / apache / cassandra / io / compress / SnappyCompressor . java <nl> + + + b / src / java / org / apache / cassandra / io / compress / SnappyCompressor . java <nl> @ @ - 96 , 6 + 96 , 8 @ @ public class SnappyCompressor implements ICompressor <nl> <nl> public int uncompress ( ByteBuffer input , ByteBuffer output ) throws IOException <nl> { <nl> + if ( input . hasArray ( ) & & output . hasArray ( ) ) <nl> + return Snappy . rawUncompress ( input . array ( ) , input . arrayOffset ( ) + input . position ( ) , input . remaining ( ) , output . array ( ) , output . arrayOffset ( ) + output . position ( ) ) ; <nl> return Snappy . uncompress ( input , output ) ; <nl> } <nl> <nl> diff - - git a / src / java / org / apache / cassandra / io / util / CompressedPoolingSegmentedFile . java b / src / java / org / apache / cassandra / io / util / CompressedPoolingSegmentedFile . java <nl> index 502c461 . . cb30131 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / CompressedPoolingSegmentedFile . java <nl> + + + b / src / java / org / apache / cassandra / io / util / CompressedPoolingSegmentedFile . java <nl> @ @ - 17 , 6 + 17 , 10 @ @ <nl> * / <nl> package org . apache . cassandra . io . util ; <nl> <nl> + import java . nio . MappedByteBuffer ; <nl> + import java . nio . channels . FileChannel ; <nl> + import java . util . TreeMap ; <nl> + <nl> import com . google . common . util . concurrent . RateLimiter ; <nl> <nl> import org . apache . cassandra . io . compress . CompressedRandomAccessReader ; <nl> @ @ - 27 , 31 + 31 , 56 @ @ import org . apache . cassandra . io . compress . CompressionMetadata ; <nl> public class CompressedPoolingSegmentedFile extends PoolingSegmentedFile implements ICompressedFile <nl> { <nl> public final CompressionMetadata metadata ; <nl> + private final TreeMap < Long , MappedByteBuffer > chunkSegments ; <nl> <nl> public CompressedPoolingSegmentedFile ( ChannelProxy channel , CompressionMetadata metadata ) <nl> { <nl> - super ( new Cleanup ( channel , metadata ) , channel , metadata . dataLength , metadata . compressedFileLength ) ; <nl> + this ( channel , metadata , CompressedSegmentedFile . createMappedSegments ( channel , metadata ) ) ; <nl> + } <nl> + <nl> + private CompressedPoolingSegmentedFile ( ChannelProxy channel , CompressionMetadata metadata , TreeMap < Long , MappedByteBuffer > chunkSegments ) <nl> + { <nl> + super ( new Cleanup ( channel , metadata , chunkSegments ) , channel , metadata . dataLength , metadata . compressedFileLength ) ; <nl> this . metadata = metadata ; <nl> + this . chunkSegments = chunkSegments ; <nl> } <nl> <nl> private CompressedPoolingSegmentedFile ( CompressedPoolingSegmentedFile copy ) <nl> { <nl> super ( copy ) ; <nl> this . metadata = copy . metadata ; <nl> + this . chunkSegments = copy . chunkSegments ; <nl> + } <nl> + <nl> + public ChannelProxy channel ( ) <nl> + { <nl> + return channel ; <nl> + } <nl> + <nl> + public TreeMap < Long , MappedByteBuffer > chunkSegments ( ) <nl> + { <nl> + return chunkSegments ; <nl> } <nl> <nl> protected static final class Cleanup extends PoolingSegmentedFile . Cleanup <nl> { <nl> final CompressionMetadata metadata ; <nl> - protected Cleanup ( ChannelProxy channel , CompressionMetadata metadata ) <nl> + final TreeMap < Long , MappedByteBuffer > chunkSegments ; <nl> + protected Cleanup ( ChannelProxy channel , CompressionMetadata metadata , TreeMap < Long , MappedByteBuffer > chunkSegments ) <nl> { <nl> super ( channel ) ; <nl> this . metadata = metadata ; <nl> + this . chunkSegments = chunkSegments ; <nl> } <nl> public void tidy ( ) <nl> { <nl> super . tidy ( ) ; <nl> metadata . close ( ) ; <nl> + if ( chunkSegments ! = null ) <nl> + { <nl> + for ( MappedByteBuffer segment : chunkSegments . values ( ) ) <nl> + FileUtils . clean ( segment ) ; <nl> + } <nl> } <nl> } <nl> <nl> @ @ - 82 , 17 + 111 , 17 @ @ public class CompressedPoolingSegmentedFile extends PoolingSegmentedFile impleme <nl> <nl> public RandomAccessReader createReader ( ) <nl> { <nl> - return CompressedRandomAccessReader . open ( channel , metadata , null ) ; <nl> + return CompressedRandomAccessReader . open ( this ) ; <nl> } <nl> <nl> public RandomAccessReader createThrottledReader ( RateLimiter limiter ) <nl> { <nl> - return CompressedThrottledReader . open ( channel , metadata , limiter ) ; <nl> + return CompressedThrottledReader . open ( this , limiter ) ; <nl> } <nl> <nl> protected RandomAccessReader createPooledReader ( ) <nl> { <nl> - return CompressedRandomAccessReader . open ( channel , metadata , this ) ; <nl> + return CompressedRandomAccessReader . open ( this ) ; <nl> } <nl> <nl> public CompressionMetadata getMetadata ( ) <nl> diff - - git a / src / java / org / apache / cassandra / io / util / CompressedSegmentedFile . java b / src / java / org / apache / cassandra / io / util / CompressedSegmentedFile . java <nl> index 5d2c897 . . caf4c22 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / CompressedSegmentedFile . java <nl> + + + b / src / java / org / apache / cassandra / io / util / CompressedSegmentedFile . java <nl> @ @ - 17 , 8 + 17 , 14 @ @ <nl> * / <nl> package org . apache . cassandra . io . util ; <nl> <nl> + import java . nio . MappedByteBuffer ; <nl> + import java . nio . channels . FileChannel ; <nl> + import java . util . TreeMap ; <nl> + <nl> import com . google . common . util . concurrent . RateLimiter ; <nl> <nl> + import org . apache . cassandra . config . Config ; <nl> + import org . apache . cassandra . config . DatabaseDescriptor ; <nl> import org . apache . cassandra . io . compress . CompressedRandomAccessReader ; <nl> import org . apache . cassandra . io . compress . CompressedSequentialWriter ; <nl> import org . apache . cassandra . io . compress . CompressedThrottledReader ; <nl> @ @ - 27 , 31 + 33 , 88 @ @ import org . apache . cassandra . io . compress . CompressionMetadata ; <nl> public class CompressedSegmentedFile extends SegmentedFile implements ICompressedFile <nl> { <nl> public final CompressionMetadata metadata ; <nl> + private static final boolean useMmap = DatabaseDescriptor . getDiskAccessMode ( ) = = Config . DiskAccessMode . mmap ; <nl> + private static int MAX _ SEGMENT _ SIZE = Integer . MAX _ VALUE ; <nl> + private final TreeMap < Long , MappedByteBuffer > chunkSegments ; <nl> <nl> public CompressedSegmentedFile ( ChannelProxy channel , CompressionMetadata metadata ) <nl> { <nl> - super ( new Cleanup ( channel , metadata ) , channel , metadata . dataLength , metadata . compressedFileLength ) ; <nl> + this ( channel , metadata , createMappedSegments ( channel , metadata ) ) ; <nl> + } <nl> + <nl> + public CompressedSegmentedFile ( ChannelProxy channel , CompressionMetadata metadata , TreeMap < Long , MappedByteBuffer > chunkSegments ) <nl> + { <nl> + super ( new Cleanup ( channel , metadata , chunkSegments ) , channel , metadata . dataLength , metadata . compressedFileLength ) ; <nl> this . metadata = metadata ; <nl> + this . chunkSegments = chunkSegments ; <nl> } <nl> <nl> private CompressedSegmentedFile ( CompressedSegmentedFile copy ) <nl> { <nl> super ( copy ) ; <nl> this . metadata = copy . metadata ; <nl> + this . chunkSegments = copy . chunkSegments ; <nl> + } <nl> + <nl> + public ChannelProxy channel ( ) <nl> + { <nl> + return channel ; <nl> + } <nl> + <nl> + public TreeMap < Long , MappedByteBuffer > chunkSegments ( ) <nl> + { <nl> + return chunkSegments ; <nl> + } <nl> + <nl> + static TreeMap < Long , MappedByteBuffer > createMappedSegments ( ChannelProxy channel , CompressionMetadata metadata ) <nl> + { <nl> + if ( ! useMmap ) <nl> + return null ; <nl> + TreeMap < Long , MappedByteBuffer > chunkSegments = new TreeMap < > ( ) ; <nl> + long offset = 0 ; <nl> + long lastSegmentOffset = 0 ; <nl> + long segmentSize = 0 ; <nl> + <nl> + while ( offset < metadata . dataLength ) <nl> + { <nl> + CompressionMetadata . Chunk chunk = metadata . chunkFor ( offset ) ; <nl> + <nl> + / / Reached a new mmap boundary <nl> + if ( segmentSize + chunk . length + 4 > MAX _ SEGMENT _ SIZE ) <nl> + { <nl> + chunkSegments . put ( lastSegmentOffset , channel . map ( FileChannel . MapMode . READ _ ONLY , lastSegmentOffset , segmentSize ) ) ; <nl> + lastSegmentOffset + = segmentSize ; <nl> + segmentSize = 0 ; <nl> + } <nl> + <nl> + segmentSize + = chunk . length + 4 ; / / checksum <nl> + offset + = metadata . chunkLength ( ) ; <nl> + } <nl> + <nl> + if ( segmentSize > 0 ) <nl> + chunkSegments . put ( lastSegmentOffset , channel . map ( FileChannel . MapMode . READ _ ONLY , lastSegmentOffset , segmentSize ) ) ; <nl> + return chunkSegments ; <nl> } <nl> <nl> private static final class Cleanup extends SegmentedFile . Cleanup <nl> { <nl> final CompressionMetadata metadata ; <nl> - protected Cleanup ( ChannelProxy channel , CompressionMetadata metadata ) <nl> + final TreeMap < Long , MappedByteBuffer > chunkSegments ; <nl> + protected Cleanup ( ChannelProxy channel , CompressionMetadata metadata , TreeMap < Long , MappedByteBuffer > chunkSegments ) <nl> { <nl> super ( channel ) ; <nl> this . metadata = metadata ; <nl> + this . chunkSegments = chunkSegments ; <nl> } <nl> public void tidy ( ) <nl> { <nl> super . tidy ( ) ; <nl> metadata . close ( ) ; <nl> + if ( chunkSegments ! = null ) <nl> + { <nl> + for ( MappedByteBuffer segment : chunkSegments . values ( ) ) <nl> + FileUtils . clean ( segment ) ; <nl> + } <nl> } <nl> } <nl> <nl> @ @ - 97 , 12 + 160 , 12 @ @ public class CompressedSegmentedFile extends SegmentedFile implements ICompresse <nl> <nl> public RandomAccessReader createReader ( ) <nl> { <nl> - return CompressedRandomAccessReader . open ( channel , metadata ) ; <nl> + return CompressedRandomAccessReader . open ( this ) ; <nl> } <nl> <nl> public RandomAccessReader createThrottledReader ( RateLimiter limiter ) <nl> { <nl> - return CompressedThrottledReader . open ( channel , metadata , limiter ) ; <nl> + return CompressedThrottledReader . open ( this , limiter ) ; <nl> } <nl> <nl> public CompressionMetadata getMetadata ( ) <nl> diff - - git a / src / java / org / apache / cassandra / io / util / ICompressedFile . java b / src / java / org / apache / cassandra / io / util / ICompressedFile . java <nl> index 3ca7718 . . ce7b22c 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / ICompressedFile . java <nl> + + + b / src / java / org / apache / cassandra / io / util / ICompressedFile . java <nl> @ @ - 17 , 9 + 17 , 14 @ @ <nl> * / <nl> package org . apache . cassandra . io . util ; <nl> <nl> + import java . nio . MappedByteBuffer ; <nl> + import java . util . TreeMap ; <nl> + <nl> import org . apache . cassandra . io . compress . CompressionMetadata ; <nl> <nl> public interface ICompressedFile <nl> { <nl> + public ChannelProxy channel ( ) ; <nl> public CompressionMetadata getMetadata ( ) ; <nl> + public TreeMap < Long , MappedByteBuffer > chunkSegments ( ) ; <nl> } <nl> diff - - git a / src / java / org / apache / cassandra / io / util / RandomAccessReader . java b / src / java / org / apache / cassandra / io / util / RandomAccessReader . java <nl> index 87ba677 . . 328095b 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / RandomAccessReader . java <nl> + + + b / src / java / org / apache / cassandra / io / util / RandomAccessReader . java <nl> @ @ - 65 , 8 + 65 , 8 @ @ public class RandomAccessReader extends AbstractDataInput implements FileDataInp <nl> { <nl> int size = ( int ) Math . min ( fileLength , bufferSize ) ; <nl> return useDirectBuffer <nl> - ? ByteBuffer . allocate ( size ) <nl> - : ByteBuffer . allocateDirect ( size ) ; <nl> + ? ByteBuffer . allocateDirect ( size ) <nl> + : ByteBuffer . allocate ( size ) ; <nl> } <nl> <nl> public static RandomAccessReader open ( ChannelProxy channel , long overrideSize , PoolingSegmentedFile owner ) <nl> diff - - git a / src / java / org / apache / cassandra / io / util / SegmentedFile . java b / src / java / org / apache / cassandra / io / util / SegmentedFile . java <nl> index 129d914 . . cb4d132 100644 <nl> - - - a / src / java / org / apache / cassandra / io / util / SegmentedFile . java <nl> + + + b / src / java / org / apache / cassandra / io / util / SegmentedFile . java <nl> @ @ - 133 , 7 + 133 , 7 @ @ public abstract class SegmentedFile extends SharedCloseableImpl <nl> * / <nl> public static Builder getBuilder ( Config . DiskAccessMode mode , boolean compressed ) <nl> { <nl> - return compressed ? new CompressedSegmentedFile . Builder ( null ) <nl> + return compressed ? new CompressedPoolingSegmentedFile . Builder ( null ) <nl> : mode = = Config . DiskAccessMode . mmap ? new MmappedSegmentedFile . Builder ( ) <nl> : new BufferedPoolingSegmentedFile . Builder ( ) ; <nl> }
NEAREST DIFF (one line): diff - - git a / CHANGES . txt b / CHANGES . txt <nl> index eec904a . . 982454c 100644 <nl> - - - a / CHANGES . txt <nl> + + + b / CHANGES . txt <nl> @ @ - 59 , 6 + 59 , 8 @ @ <nl> the old ones ( CASSANDRA - 1644 ) <nl> * upgrade to Thrift 0 . 5 ( CASSANDRA - 1367 ) <nl> * renamed CL . DCQUORUM to LOCAL _ QUORUM and DCQUORUMSYNC to EACH _ QUORUM <nl> + * cli truncate support ( CASSANDRA - 1653 ) <nl> + * update GC settings in cassandra . bat ( CASSANDRA - 1636 ) <nl> <nl> <nl> 0 . 7 - beta2 <nl> diff - - git a / src / java / org / apache / cassandra / cli / Cli . g b / src / java / org / apache / cassandra / cli / Cli . g <nl> index e59c608 . . a9229ee 100644 <nl> - - - a / src / java / org / apache / cassandra / cli / Cli . g <nl> + + + b / src / java / org / apache / cassandra / cli / Cli . g <nl> @ @ - 55 , 6 + 55 , 7 @ @ tokens { <nl> NODE _ UPDATE _ KEYSPACE ; <nl> NODE _ UPDATE _ COLUMN _ FAMILY ; <nl> NODE _ LIST ; <nl> + NODE _ TRUNCATE ; <nl> <nl> / / Internal Nodes . <nl> NODE _ COLUMN _ ACCESS ; <nl> @ @ - 123 , 6 + 124 , 7 @ @ statement <nl> | setStatement <nl> | showStatement <nl> | listStatement <nl> + | truncateStatement <nl> | - > ^ ( NODE _ NO _ OP ) <nl> ; <nl> <nl> @ @ - 174 , 6 + 176 , 8 @ @ helpStatement <nl> - > ^ ( NODE _ HELP NODE _ THRIFT _ COUNT ) <nl> | K _ HELP K _ LIST <nl> - > ^ ( NODE _ HELP NODE _ LIST ) <nl> + | K _ HELP K _ TRUNCATE <nl> + - > ^ ( NODE _ HELP NODE _ TRUNCATE ) <nl> | K _ HELP <nl> - > ^ ( NODE _ HELP ) <nl> | ' ? ' <nl> @ @ - 231 , 6 + 235 , 11 @ @ listStatement <nl> - > ^ ( NODE _ LIST columnFamily keyRangeExpr ? ^ ( NODE _ LIMIT $ limit ) ? ) <nl> ; <nl> <nl> + truncateStatement <nl> + : K _ TRUNCATE columnFamily <nl> + - > ^ ( NODE _ TRUNCATE columnFamily ) <nl> + ; <nl> + <nl> showClusterName <nl> : K _ SHOW K _ CLUSTER K _ NAME <nl> - > ^ ( NODE _ SHOW _ CLUSTER _ NAME ) <nl> @ @ - 430 , 6 + 439 , 7 @ @ K _ AND : ' AND ' ; <nl> K _ UPDATE : ' UPDATE ' ; <nl> K _ LIST : ' LIST ' ; <nl> K _ LIMIT : ' LIMIT ' ; <nl> + K _ TRUNCATE : ' TRUNCATE ' ; <nl> <nl> / / private syntactic rules <nl> fragment <nl> diff - - git a / src / java / org / apache / cassandra / cli / CliClient . java b / src / java / org / apache / cassandra / cli / CliClient . java <nl> index b6ace98 . . b63105a 100644 <nl> - - - a / src / java / org / apache / cassandra / cli / CliClient . java <nl> + + + b / src / java / org / apache / cassandra / cli / CliClient . java <nl> @ @ - 171 , 6 + 171 , 9 @ @ public class CliClient extends CliUserHelp <nl> case CliParser . NODE _ LIST : <nl> executeList ( tree ) ; <nl> break ; <nl> + case CliParser . NODE _ TRUNCATE : <nl> + executeTruncate ( tree . getChild ( 0 ) . getText ( ) ) ; <nl> + break ; <nl> case CliParser . NODE _ NO _ OP : <nl> / / comment lines come here ; they are treated as no ops . <nl> break ; <nl> @ @ - 943 , 6 + 946 , 30 @ @ public class CliClient extends CliUserHelp <nl> printSliceList ( columnFamilyDef , keySlices ) ; <nl> } <nl> <nl> + / / TRUNCATE < columnFamily > <nl> + private void executeTruncate ( String columnFamily ) <nl> + { <nl> + if ( ! CliMain . isConnected ( ) | | ! hasKeySpace ( ) ) <nl> + return ; <nl> + <nl> + / / getting CfDef , it will fail if there is no such column family in current keySpace . <nl> + CfDef cfDef = getCfDef ( columnFamily ) ; <nl> + <nl> + try <nl> + { <nl> + thriftClient . truncate ( cfDef . getName ( ) ) ; <nl> + sessionState . out . println ( columnFamily + " truncated . " ) ; <nl> + } <nl> + catch ( InvalidRequestException e ) <nl> + { <nl> + throw new RuntimeException ( e . getWhy ( ) ) ; <nl> + } <nl> + catch ( Exception e ) <nl> + { <nl> + throw new RuntimeException ( e . getMessage ( ) ) ; <nl> + } <nl> + } <nl> + <nl> / / SHOW API VERSION <nl> private void executeShowVersion ( ) throws TException <nl> { <nl> diff - - git a / src / java / org / apache / cassandra / cli / CliCompleter . java b / src / java / org / apache / cassandra / cli / CliCompleter . java <nl> index f541fe5 . . b033d8f 100644 <nl> - - - a / src / java / org / apache / cassandra / cli / CliCompleter . java <nl> + + + b / src / java / org / apache / cassandra / cli / CliCompleter . java <nl> @ @ - 55 , 12 + 55 , 16 @ @ public class CliCompleter extends SimpleCompletor <nl> " help set " , <nl> " help del " , <nl> " help count " , <nl> + " help list " , <nl> + " help truncate " <nl> } ; <nl> private static String [ ] keyspaceCommands = { <nl> " get " , <nl> " set " , <nl> " count " , <nl> - " del " <nl> + " del " , <nl> + " list " , <nl> + " truncate " <nl> } ; <nl> <nl> public CliCompleter ( ) <nl> diff - - git a / src / java / org / apache / cassandra / cli / CliUserHelp . java b / src / java / org / apache / cassandra / cli / CliUserHelp . java <nl> index 7c512e2 . . db6a779 100644 <nl> - - - a / src / java / org / apache / cassandra / cli / CliUserHelp . java <nl> + + + b / src / java / org / apache / cassandra / cli / CliUserHelp . java <nl> @ @ - 281 , 6 + 281 , 13 @ @ public class CliUserHelp { <nl> state . out . println ( " list Users [ j : ] limit 40 " ) ; <nl> break ; <nl> <nl> + case CliParser . NODE _ TRUNCATE : <nl> + state . out . println ( " truncate < column _ family > " ) ; <nl> + state . out . println ( " Truncate specified column family . \ n " ) ; <nl> + state . out . println ( " example : " ) ; <nl> + state . out . println ( " truncate Category " ) ; <nl> + break ; <nl> + <nl> default : <nl> state . out . println ( " ? " ) ; <nl> break ; <nl> @ @ - 324 , 7 + 331 , 8 @ @ public class CliUserHelp { <nl> state . out . println ( " del < cf > [ ' < key > ' ] [ ' < super > ' ] [ ' < col > ' ] Delete sub column . " ) ; <nl> state . out . println ( " count < cf > [ ' < key > ' ] Count columns in record . " ) ; <nl> state . out . println ( " count < cf > [ ' < key > ' ] [ ' < super > ' ] Count columns in a super column . " ) ; <nl> - state . out . println ( " list < cf > List all rows in the column family . " ) ; <nl> + state . out . println ( " truncate < column _ family > Truncate specified column family . " ) ; <nl> + state . out . println ( " list < cf > List all rows in the column family . " ) ; <nl> state . out . println ( " list < cf > [ < startKey > : ] " ) ; <nl> state . out . println ( " List rows in the column family beginning with < startKey > . " ) ; <nl> state . out . println ( " list < cf > [ < startKey > : < endKey > ] " ) ; <nl> diff - - git a / test / unit / org / apache / cassandra / cli / CliTest . java b / test / unit / org / apache / cassandra / cli / CliTest . java <nl> index 24a3de0 . . eddcafd 100644 <nl> - - - a / test / unit / org / apache / cassandra / cli / CliTest . java <nl> + + + b / test / unit / org / apache / cassandra / cli / CliTest . java <nl> @ @ - 52 , 6 + 52 , 7 @ @ public class CliTest extends TestCase <nl> " list CF3 [ h : ] " , <nl> " list CF3 limit 10 " , <nl> " list CF3 [ h : g ] limit 10 " , <nl> + " truncate CF1 " , <nl> " update keyspace TestKeySpace with placement _ strategy = ' org . apache . cassandra . locator . LocalStrategy ' " , <nl> " update keyspace TestKeySpace with replication _ factor = 1 and strategy _ options = [ { DC1 : 3 , DC2 : 4 , DC5 : 1 } ] " <nl> } ; <nl> @ @ - 101 , 6 + 102 , 10 @ @ public class CliTest extends TestCase <nl> assertTrue ( result . startsWith ( " = > ( column = " ) ) ; <nl> } <nl> } <nl> + else if ( statement . startsWith ( " truncate " ) ) <nl> + { <nl> + assertTrue ( result . contains ( " truncated . " ) ) ; <nl> + } <nl> <nl> outStream . reset ( ) ; / / reset stream so we have only output from next statement all the time <nl> errStream . reset ( ) ; / / no errors to the end user .

TEST DIFF:
diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java 
 index edf8c68 . . 1febe37 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java 
 + + + b / src / java / org / apache / cassandra / io / compress / CompressedRandomAccessReader . java 
 @ @ - 42 , 17 + 42 , 23 @ @ import org . apache . cassandra . utils . FBUtilities ; 
 * / 
 public class CompressedRandomAccessReader extends RandomAccessReader 
 { 
 - private static final boolean useMmap = DatabaseDescriptor . getDiskAccessMode ( ) = = Config . DiskAccessMode . mmap ; 
 - 
 public static CompressedRandomAccessReader open ( ChannelProxy channel , CompressionMetadata metadata ) 
 { 
 - return open ( channel , metadata , null ) ; 
 + try 
 + { 
 + return new CompressedRandomAccessReader ( channel , metadata , null ) ; 
 + } 
 + catch ( FileNotFoundException e ) 
 + { 
 + throw new RuntimeException ( e ) ; 
 + } 
 } 
 - public static CompressedRandomAccessReader open ( ChannelProxy channel , CompressionMetadata metadata , CompressedPoolingSegmentedFile owner ) 
 + 
 + public static CompressedRandomAccessReader open ( ICompressedFile file ) 
 { 
 try 
 { 
 - return new CompressedRandomAccessReader ( channel , metadata , owner ) ; 
 + return new CompressedRandomAccessReader ( file . channel ( ) , file . getMetadata ( ) , file ) ; 
 } 
 catch ( FileNotFoundException e ) 
 { 
 @ @ - 60 , 9 + 66 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 } 
 } 
 
 - 
 - private TreeMap < Long , MappedByteBuffer > chunkSegments ; 
 - private int MAX _ SEGMENT _ SIZE = Integer . MAX _ VALUE ; 
 + private final TreeMap < Long , MappedByteBuffer > chunkSegments ; 
 
 private final CompressionMetadata metadata ; 
 
 @ @ - 75 , 61 + 79 , 24 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 / / raw checksum bytes 
 private ByteBuffer checksumBytes ; 
 
 - protected CompressedRandomAccessReader ( ChannelProxy channel , CompressionMetadata metadata , PoolingSegmentedFile owner ) throws FileNotFoundException 
 + protected CompressedRandomAccessReader ( ChannelProxy channel , CompressionMetadata metadata , ICompressedFile file ) throws FileNotFoundException 
 { 
 - super ( channel , metadata . chunkLength ( ) , metadata . compressedFileLength , metadata . compressor ( ) . useDirectOutputByteBuffers ( ) , owner ) ; 
 + super ( channel , metadata . chunkLength ( ) , metadata . compressedFileLength , metadata . compressor ( ) . useDirectOutputByteBuffers ( ) , file instanceof PoolingSegmentedFile ? ( PoolingSegmentedFile ) file : null ) ; 
 this . metadata = metadata ; 
 checksum = new Adler32 ( ) ; 
 
 - if ( ! useMmap ) 
 + chunkSegments = file = = null ? null : file . chunkSegments ( ) ; 
 + if ( chunkSegments = = null ) 
 { 
 - compressed = ByteBuffer . wrap ( new byte [ metadata . compressor ( ) . initialCompressedBufferLength ( metadata . chunkLength ( ) ) ] ) ; 
 + compressed = super . allocateBuffer ( metadata . compressor ( ) . initialCompressedBufferLength ( metadata . chunkLength ( ) ) , metadata . compressor ( ) . useDirectOutputByteBuffers ( ) ) ; 
 checksumBytes = ByteBuffer . wrap ( new byte [ 4 ] ) ; 
 } 
 - else 
 - { 
 - try 
 - { 
 - createMappedSegments ( ) ; 
 - } 
 - catch ( IOException e ) 
 - { 
 - throw new IOError ( e ) ; 
 - } 
 - } 
 - } 
 - 
 - private void createMappedSegments ( ) throws IOException 
 - { 
 - chunkSegments = new TreeMap < > ( ) ; 
 - long offset = 0 ; 
 - long lastSegmentOffset = 0 ; 
 - long segmentSize = 0 ; 
 - 
 - while ( offset < metadata . dataLength ) 
 - { 
 - CompressionMetadata . Chunk chunk = metadata . chunkFor ( offset ) ; 
 - 
 - / / Reached a new mmap boundary 
 - if ( segmentSize + chunk . length + 4 > MAX _ SEGMENT _ SIZE ) 
 - { 
 - chunkSegments . put ( lastSegmentOffset , channel . map ( FileChannel . MapMode . READ _ ONLY , lastSegmentOffset , segmentSize ) ) ; 
 - lastSegmentOffset + = segmentSize ; 
 - segmentSize = 0 ; 
 - } 
 - 
 - segmentSize + = chunk . length + 4 ; / / checksum 
 - offset + = metadata . chunkLength ( ) ; 
 - } 
 - 
 - if ( segmentSize > 0 ) 
 - chunkSegments . put ( lastSegmentOffset , channel . map ( FileChannel . MapMode . READ _ ONLY , lastSegmentOffset , segmentSize ) ) ; 
 } 
 
 protected ByteBuffer allocateBuffer ( int bufferSize , boolean useDirect ) 
 { 
 assert Integer . bitCount ( bufferSize ) = = 1 ; 
 - return useMmap & & useDirect 
 + return useDirect 
 ? ByteBuffer . allocateDirect ( bufferSize ) 
 : ByteBuffer . allocate ( bufferSize ) ; 
 } 
 @ @ - 138 , 16 + 105 , 9 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 public void deallocate ( ) 
 { 
 super . deallocate ( ) ; 
 - 
 - if ( chunkSegments ! = null ) 
 - { 
 - for ( Map . Entry < Long , MappedByteBuffer > entry : chunkSegments . entrySet ( ) ) 
 - { 
 - FileUtils . clean ( entry . getValue ( ) ) ; 
 - } 
 - } 
 - 
 - chunkSegments = null ; 
 + if ( compressed ! = null ) 
 + FileUtils . clean ( compressed ) ; 
 + compressed = null ; 
 } 
 
 private void reBufferStandard ( ) 
 @ @ - 175 , 7 + 135 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 int decompressedBytes ; 
 try 
 { 
 - decompressedBytes = metadata . compressor ( ) . uncompress ( compressed . array ( ) , 0 , chunk . length , buffer . array ( ) , 0 ) ; 
 + decompressedBytes = metadata . compressor ( ) . uncompress ( compressed , buffer ) ; 
 buffer . limit ( decompressedBytes ) ; 
 } 
 catch ( IOException e ) 
 @ @ - 186 , 8 + 146 , 8 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 
 if ( metadata . parameters . getCrcCheckChance ( ) > ThreadLocalRandom . current ( ) . nextDouble ( ) ) 
 { 
 - 
 - checksum . update ( compressed . array ( ) , 0 , chunk . length ) ; 
 + compressed . position ( 0 ) ; 
 + FBUtilities . directCheckSum ( checksum , compressed ) ; 
 
 if ( checksum ( chunk ) ! = ( int ) checksum . getValue ( ) ) 
 throw new CorruptBlockException ( getPath ( ) , chunk ) ; 
 @ @ - 226 , 7 + 186 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 Map . Entry < Long , MappedByteBuffer > entry = chunkSegments . floorEntry ( chunk . offset ) ; 
 long segmentOffset = entry . getKey ( ) ; 
 int chunkOffset = Ints . checkedCast ( chunk . offset - segmentOffset ) ; 
 - MappedByteBuffer compressedChunk = entry . getValue ( ) ; 
 + ByteBuffer compressedChunk = entry . getValue ( ) . duplicate ( ) ; 
 
 compressedChunk . position ( chunkOffset ) ; 
 compressedChunk . limit ( chunkOffset + chunk . length ) ; 
 @ @ - 284 , 7 + 244 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 @ Override 
 protected void reBuffer ( ) 
 { 
 - if ( useMmap ) 
 + if ( chunkSegments ! = null ) 
 { 
 reBufferMmap ( ) ; 
 } 
 @ @ - 305 , 7 + 265 , 7 @ @ public class CompressedRandomAccessReader extends RandomAccessReader 
 
 public int getTotalBufferSize ( ) 
 { 
 - return super . getTotalBufferSize ( ) + ( useMmap ? 0 : compressed . capacity ( ) ) ; 
 + return super . getTotalBufferSize ( ) + ( chunkSegments ! = null ? 0 : compressed . capacity ( ) ) ; 
 } 
 
 @ Override 
 diff - - git a / src / java / org / apache / cassandra / io / compress / CompressedThrottledReader . java b / src / java / org / apache / cassandra / io / compress / CompressedThrottledReader . java 
 index 63727d8 . . a29129c 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / CompressedThrottledReader . java 
 + + + b / src / java / org / apache / cassandra / io / compress / CompressedThrottledReader . java 
 @ @ - 26 , 14 + 26 , 15 @ @ import java . io . FileNotFoundException ; 
 import com . google . common . util . concurrent . RateLimiter ; 
 
 import org . apache . cassandra . io . util . ChannelProxy ; 
 + import org . apache . cassandra . io . util . ICompressedFile ; 
 
 public class CompressedThrottledReader extends CompressedRandomAccessReader 
 { 
 private final RateLimiter limiter ; 
 
 - public CompressedThrottledReader ( ChannelProxy channel , CompressionMetadata metadata , RateLimiter limiter ) throws FileNotFoundException 
 + public CompressedThrottledReader ( ChannelProxy channel , CompressionMetadata metadata , ICompressedFile file , RateLimiter limiter ) throws FileNotFoundException 
 { 
 - super ( channel , metadata , null ) ; 
 + super ( channel , metadata , file ) ; 
 this . limiter = limiter ; 
 } 
 
 @ @ - 43 , 11 + 44 , 11 @ @ public class CompressedThrottledReader extends CompressedRandomAccessReader 
 super . reBuffer ( ) ; 
 } 
 
 - public static CompressedThrottledReader open ( ChannelProxy channel , CompressionMetadata metadata , RateLimiter limiter ) 
 + public static CompressedThrottledReader open ( ICompressedFile file , RateLimiter limiter ) 
 { 
 try 
 { 
 - return new CompressedThrottledReader ( channel , metadata , limiter ) ; 
 + return new CompressedThrottledReader ( file . channel ( ) , file . getMetadata ( ) , file , limiter ) ; 
 } 
 catch ( FileNotFoundException e ) 
 { 
 diff - - git a / src / java / org / apache / cassandra / io / compress / DeflateCompressor . java b / src / java / org / apache / cassandra / io / compress / DeflateCompressor . java 
 index a88e4d2 . . 833c375 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / DeflateCompressor . java 
 + + + b / src / java / org / apache / cassandra / io / compress / DeflateCompressor . java 
 @ @ - 122 , 13 + 122 , 14 @ @ public class DeflateCompressor implements ICompressor 
 } 
 } 
 
 - public int uncompress ( ByteBuffer input _ , ByteBuffer output ) throws IOException 
 + public int uncompress ( ByteBuffer input , ByteBuffer output ) throws IOException 
 { 
 if ( ! output . hasArray ( ) ) 
 throw new IllegalArgumentException ( " DeflateCompressor doesn ' t work with direct byte buffers " ) ; 
 
 - byte [ ] input = ByteBufferUtil . getArray ( input _ ) ; 
 - return uncompress ( input , 0 , input . length , output . array ( ) , output . arrayOffset ( ) + output . position ( ) ) ; 
 + if ( input . hasArray ( ) ) 
 + return uncompress ( input . array ( ) , input . arrayOffset ( ) + input . position ( ) , input . remaining ( ) , output . array ( ) , output . arrayOffset ( ) + output . position ( ) ) ; 
 + return uncompress ( ByteBufferUtil . getArray ( input ) , 0 , input . remaining ( ) , output . array ( ) , output . arrayOffset ( ) + output . position ( ) ) ; 
 } 
 
 public boolean useDirectOutputByteBuffers ( ) 
 diff - - git a / src / java / org / apache / cassandra / io / compress / LZ4Compressor . java b / src / java / org / apache / cassandra / io / compress / LZ4Compressor . java 
 index ab10a00 . . 9d54048 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / LZ4Compressor . java 
 + + + b / src / java / org / apache / cassandra / io / compress / LZ4Compressor . java 
 @ @ - 83 , 6 + 83 , 7 @ @ public class LZ4Compressor implements ICompressor 
 | ( ( input [ inputOffset + 1 ] & 0xFF ) < < 8 ) 
 | ( ( input [ inputOffset + 2 ] & 0xFF ) < < 16 ) 
 | ( ( input [ inputOffset + 3 ] & 0xFF ) < < 24 ) ; 
 + 
 final int compressedLength ; 
 try 
 { 
 @ @ - 104 , 6 + 105 , 9 @ @ public class LZ4Compressor implements ICompressor 
 
 public int uncompress ( ByteBuffer input , ByteBuffer output ) throws IOException 
 { 
 + if ( input . hasArray ( ) & & output . hasArray ( ) ) 
 + return uncompress ( input . array ( ) , input . arrayOffset ( ) + input . position ( ) , input . remaining ( ) , output . array ( ) , output . arrayOffset ( ) + output . position ( ) ) ; 
 + 
 int pos = input . position ( ) ; 
 final int decompressedLength = ( input . get ( pos ) & 0xFF ) 
 | ( ( input . get ( pos + 1 ) & 0xFF ) < < 8 ) 
 @ @ - 132 , 7 + 136 , 7 @ @ public class LZ4Compressor implements ICompressor 
 @ Override 
 public boolean useDirectOutputByteBuffers ( ) 
 { 
 - return false ; 
 + return true ; 
 } 
 
 public Set < String > supportedOptions ( ) 
 diff - - git a / src / java / org / apache / cassandra / io / compress / SnappyCompressor . java b / src / java / org / apache / cassandra / io / compress / SnappyCompressor . java 
 index d1f1f34 . . 04f676b 100644 
 - - - a / src / java / org / apache / cassandra / io / compress / SnappyCompressor . java 
 + + + b / src / java / org / apache / cassandra / io / compress / SnappyCompressor . java 
 @ @ - 96 , 6 + 96 , 8 @ @ public class SnappyCompressor implements ICompressor 
 
 public int uncompress ( ByteBuffer input , ByteBuffer output ) throws IOException 
 { 
 + if ( input . hasArray ( ) & & output . hasArray ( ) ) 
 + return Snappy . rawUncompress ( input . array ( ) , input . arrayOffset ( ) + input . position ( ) , input . remaining ( ) , output . array ( ) , output . arrayOffset ( ) + output . position ( ) ) ; 
 return Snappy . uncompress ( input , output ) ; 
 } 
 
 diff - - git a / src / java / org / apache / cassandra / io / util / CompressedPoolingSegmentedFile . java b / src / java / org / apache / cassandra / io / util / CompressedPoolingSegmentedFile . java 
 index 502c461 . . cb30131 100644 
 - - - a / src / java / org / apache / cassandra / io / util / CompressedPoolingSegmentedFile . java 
 + + + b / src / java / org / apache / cassandra / io / util / CompressedPoolingSegmentedFile . java 
 @ @ - 17 , 6 + 17 , 10 @ @ 
 * / 
 package org . apache . cassandra . io . util ; 
 
 + import java . nio . MappedByteBuffer ; 
 + import java . nio . channels . FileChannel ; 
 + import java . util . TreeMap ; 
 + 
 import com . google . common . util . concurrent . RateLimiter ; 
 
 import org . apache . cassandra . io . compress . CompressedRandomAccessReader ; 
 @ @ - 27 , 31 + 31 , 56 @ @ import org . apache . cassandra . io . compress . CompressionMetadata ; 
 public class CompressedPoolingSegmentedFile extends PoolingSegmentedFile implements ICompressedFile 
 { 
 public final CompressionMetadata metadata ; 
 + private final TreeMap < Long , MappedByteBuffer > chunkSegments ; 
 
 public CompressedPoolingSegmentedFile ( ChannelProxy channel , CompressionMetadata metadata ) 
 { 
 - super ( new Cleanup ( channel , metadata ) , channel , metadata . dataLength , metadata . compressedFileLength ) ; 
 + this ( channel , metadata , CompressedSegmentedFile . createMappedSegments ( channel , metadata ) ) ; 
 + } 
 + 
 + private CompressedPoolingSegmentedFile ( ChannelProxy channel , CompressionMetadata metadata , TreeMap < Long , MappedByteBuffer > chunkSegments ) 
 + { 
 + super ( new Cleanup ( channel , metadata , chunkSegments ) , channel , metadata . dataLength , metadata . compressedFileLength ) ; 
 this . metadata = metadata ; 
 + this . chunkSegments = chunkSegments ; 
 } 
 
 private CompressedPoolingSegmentedFile ( CompressedPoolingSegmentedFile copy ) 
 { 
 super ( copy ) ; 
 this . metadata = copy . metadata ; 
 + this . chunkSegments = copy . chunkSegments ; 
 + } 
 + 
 + public ChannelProxy channel ( ) 
 + { 
 + return channel ; 
 + } 
 + 
 + public TreeMap < Long , MappedByteBuffer > chunkSegments ( ) 
 + { 
 + return chunkSegments ; 
 } 
 
 protected static final class Cleanup extends PoolingSegmentedFile . Cleanup 
 { 
 final CompressionMetadata metadata ; 
 - protected Cleanup ( ChannelProxy channel , CompressionMetadata metadata ) 
 + final TreeMap < Long , MappedByteBuffer > chunkSegments ; 
 + protected Cleanup ( ChannelProxy channel , CompressionMetadata metadata , TreeMap < Long , MappedByteBuffer > chunkSegments ) 
 { 
 super ( channel ) ; 
 this . metadata = metadata ; 
 + this . chunkSegments = chunkSegments ; 
 } 
 public void tidy ( ) 
 { 
 super . tidy ( ) ; 
 metadata . close ( ) ; 
 + if ( chunkSegments ! = null ) 
 + { 
 + for ( MappedByteBuffer segment : chunkSegments . values ( ) ) 
 + FileUtils . clean ( segment ) ; 
 + } 
 } 
 } 
 
 @ @ - 82 , 17 + 111 , 17 @ @ public class CompressedPoolingSegmentedFile extends PoolingSegmentedFile impleme 
 
 public RandomAccessReader createReader ( ) 
 { 
 - return CompressedRandomAccessReader . open ( channel , metadata , null ) ; 
 + return CompressedRandomAccessReader . open ( this ) ; 
 } 
 
 public RandomAccessReader createThrottledReader ( RateLimiter limiter ) 
 { 
 - return CompressedThrottledReader . open ( channel , metadata , limiter ) ; 
 + return CompressedThrottledReader . open ( this , limiter ) ; 
 } 
 
 protected RandomAccessReader createPooledReader ( ) 
 { 
 - return CompressedRandomAccessReader . open ( channel , metadata , this ) ; 
 + return CompressedRandomAccessReader . open ( this ) ; 
 } 
 
 public CompressionMetadata getMetadata ( ) 
 diff - - git a / src / java / org / apache / cassandra / io / util / CompressedSegmentedFile . java b / src / java / org / apache / cassandra / io / util / CompressedSegmentedFile . java 
 index 5d2c897 . . caf4c22 100644 
 - - - a / src / java / org / apache / cassandra / io / util / CompressedSegmentedFile . java 
 + + + b / src / java / org / apache / cassandra / io / util / CompressedSegmentedFile . java 
 @ @ - 17 , 8 + 17 , 14 @ @ 
 * / 
 package org . apache . cassandra . io . util ; 
 
 + import java . nio . MappedByteBuffer ; 
 + import java . nio . channels . FileChannel ; 
 + import java . util . TreeMap ; 
 + 
 import com . google . common . util . concurrent . RateLimiter ; 
 
 + import org . apache . cassandra . config . Config ; 
 + import org . apache . cassandra . config . DatabaseDescriptor ; 
 import org . apache . cassandra . io . compress . CompressedRandomAccessReader ; 
 import org . apache . cassandra . io . compress . CompressedSequentialWriter ; 
 import org . apache . cassandra . io . compress . CompressedThrottledReader ; 
 @ @ - 27 , 31 + 33 , 88 @ @ import org . apache . cassandra . io . compress . CompressionMetadata ; 
 public class CompressedSegmentedFile extends SegmentedFile implements ICompressedFile 
 { 
 public final CompressionMetadata metadata ; 
 + private static final boolean useMmap = DatabaseDescriptor . getDiskAccessMode ( ) = = Config . DiskAccessMode . mmap ; 
 + private static int MAX _ SEGMENT _ SIZE = Integer . MAX _ VALUE ; 
 + private final TreeMap < Long , MappedByteBuffer > chunkSegments ; 
 
 public CompressedSegmentedFile ( ChannelProxy channel , CompressionMetadata metadata ) 
 { 
 - super ( new Cleanup ( channel , metadata ) , channel , metadata . dataLength , metadata . compressedFileLength ) ; 
 + this ( channel , metadata , createMappedSegments ( channel , metadata ) ) ; 
 + } 
 + 
 + public CompressedSegmentedFile ( ChannelProxy channel , CompressionMetadata metadata , TreeMap < Long , MappedByteBuffer > chunkSegments ) 
 + { 
 + super ( new Cleanup ( channel , metadata , chunkSegments ) , channel , metadata . dataLength , metadata . compressedFileLength ) ; 
 this . metadata = metadata ; 
 + this . chunkSegments = chunkSegments ; 
 } 
 
 private CompressedSegmentedFile ( CompressedSegmentedFile copy ) 
 { 
 super ( copy ) ; 
 this . metadata = copy . metadata ; 
 + this . chunkSegments = copy . chunkSegments ; 
 + } 
 + 
 + public ChannelProxy channel ( ) 
 + { 
 + return channel ; 
 + } 
 + 
 + public TreeMap < Long , MappedByteBuffer > chunkSegments ( ) 
 + { 
 + return chunkSegments ; 
 + } 
 + 
 + static TreeMap < Long , MappedByteBuffer > createMappedSegments ( ChannelProxy channel , CompressionMetadata metadata ) 
 + { 
 + if ( ! useMmap ) 
 + return null ; 
 + TreeMap < Long , MappedByteBuffer > chunkSegments = new TreeMap < > ( ) ; 
 + long offset = 0 ; 
 + long lastSegmentOffset = 0 ; 
 + long segmentSize = 0 ; 
 + 
 + while ( offset < metadata . dataLength ) 
 + { 
 + CompressionMetadata . Chunk chunk = metadata . chunkFor ( offset ) ; 
 + 
 + / / Reached a new mmap boundary 
 + if ( segmentSize + chunk . length + 4 > MAX _ SEGMENT _ SIZE ) 
 + { 
 + chunkSegments . put ( lastSegmentOffset , channel . map ( FileChannel . MapMode . READ _ ONLY , lastSegmentOffset , segmentSize ) ) ; 
 + lastSegmentOffset + = segmentSize ; 
 + segmentSize = 0 ; 
 + } 
 + 
 + segmentSize + = chunk . length + 4 ; / / checksum 
 + offset + = metadata . chunkLength ( ) ; 
 + } 
 + 
 + if ( segmentSize > 0 ) 
 + chunkSegments . put ( lastSegmentOffset , channel . map ( FileChannel . MapMode . READ _ ONLY , lastSegmentOffset , segmentSize ) ) ; 
 + return chunkSegments ; 
 } 
 
 private static final class Cleanup extends SegmentedFile . Cleanup 
 { 
 final CompressionMetadata metadata ; 
 - protected Cleanup ( ChannelProxy channel , CompressionMetadata metadata ) 
 + final TreeMap < Long , MappedByteBuffer > chunkSegments ; 
 + protected Cleanup ( ChannelProxy channel , CompressionMetadata metadata , TreeMap < Long , MappedByteBuffer > chunkSegments ) 
 { 
 super ( channel ) ; 
 this . metadata = metadata ; 
 + this . chunkSegments = chunkSegments ; 
 } 
 public void tidy ( ) 
 { 
 super . tidy ( ) ; 
 metadata . close ( ) ; 
 + if ( chunkSegments ! = null ) 
 + { 
 + for ( MappedByteBuffer segment : chunkSegments . values ( ) ) 
 + FileUtils . clean ( segment ) ; 
 + } 
 } 
 } 
 
 @ @ - 97 , 12 + 160 , 12 @ @ public class CompressedSegmentedFile extends SegmentedFile implements ICompresse 
 
 public RandomAccessReader createReader ( ) 
 { 
 - return CompressedRandomAccessReader . open ( channel , metadata ) ; 
 + return CompressedRandomAccessReader . open ( this ) ; 
 } 
 
 public RandomAccessReader createThrottledReader ( RateLimiter limiter ) 
 { 
 - return CompressedThrottledReader . open ( channel , metadata , limiter ) ; 
 + return CompressedThrottledReader . open ( this , limiter ) ; 
 } 
 
 public CompressionMetadata getMetadata ( ) 
 diff - - git a / src / java / org / apache / cassandra / io / util / ICompressedFile . java b / src / java / org / apache / cassandra / io / util / ICompressedFile . java 
 index 3ca7718 . . ce7b22c 100644 
 - - - a / src / java / org / apache / cassandra / io / util / ICompressedFile . java 
 + + + b / src / java / org / apache / cassandra / io / util / ICompressedFile . java 
 @ @ - 17 , 9 + 17 , 14 @ @ 
 * / 
 package org . apache . cassandra . io . util ; 
 
 + import java . nio . MappedByteBuffer ; 
 + import java . util . TreeMap ; 
 + 
 import org . apache . cassandra . io . compress . CompressionMetadata ; 
 
 public interface ICompressedFile 
 { 
 + public ChannelProxy channel ( ) ; 
 public CompressionMetadata getMetadata ( ) ; 
 + public TreeMap < Long , MappedByteBuffer > chunkSegments ( ) ; 
 } 
 diff - - git a / src / java / org / apache / cassandra / io / util / RandomAccessReader . java b / src / java / org / apache / cassandra / io / util / RandomAccessReader . java 
 index 87ba677 . . 328095b 100644 
 - - - a / src / java / org / apache / cassandra / io / util / RandomAccessReader . java 
 + + + b / src / java / org / apache / cassandra / io / util / RandomAccessReader . java 
 @ @ - 65 , 8 + 65 , 8 @ @ public class RandomAccessReader extends AbstractDataInput implements FileDataInp 
 { 
 int size = ( int ) Math . min ( fileLength , bufferSize ) ; 
 return useDirectBuffer 
 - ? ByteBuffer . allocate ( size ) 
 - : ByteBuffer . allocateDirect ( size ) ; 
 + ? ByteBuffer . allocateDirect ( size ) 
 + : ByteBuffer . allocate ( size ) ; 
 } 
 
 public static RandomAccessReader open ( ChannelProxy channel , long overrideSize , PoolingSegmentedFile owner ) 
 diff - - git a / src / java / org / apache / cassandra / io / util / SegmentedFile . java b / src / java / org / apache / cassandra / io / util / SegmentedFile . java 
 index 129d914 . . cb4d132 100644 
 - - - a / src / java / org / apache / cassandra / io / util / SegmentedFile . java 
 + + + b / src / java / org / apache / cassandra / io / util / SegmentedFile . java 
 @ @ - 133 , 7 + 133 , 7 @ @ public abstract class SegmentedFile extends SharedCloseableImpl 
 * / 
 public static Builder getBuilder ( Config . DiskAccessMode mode , boolean compressed ) 
 { 
 - return compressed ? new CompressedSegmentedFile . Builder ( null ) 
 + return compressed ? new CompressedPoolingSegmentedFile . Builder ( null ) 
 : mode = = Config . DiskAccessMode . mmap ? new MmappedSegmentedFile . Builder ( ) 
 : new BufferedPoolingSegmentedFile . Builder ( ) ; 
 }

NEAREST DIFF:
diff - - git a / CHANGES . txt b / CHANGES . txt 
 index eec904a . . 982454c 100644 
 - - - a / CHANGES . txt 
 + + + b / CHANGES . txt 
 @ @ - 59 , 6 + 59 , 8 @ @ 
 the old ones ( CASSANDRA - 1644 ) 
 * upgrade to Thrift 0 . 5 ( CASSANDRA - 1367 ) 
 * renamed CL . DCQUORUM to LOCAL _ QUORUM and DCQUORUMSYNC to EACH _ QUORUM 
 + * cli truncate support ( CASSANDRA - 1653 ) 
 + * update GC settings in cassandra . bat ( CASSANDRA - 1636 ) 
 
 
 0 . 7 - beta2 
 diff - - git a / src / java / org / apache / cassandra / cli / Cli . g b / src / java / org / apache / cassandra / cli / Cli . g 
 index e59c608 . . a9229ee 100644 
 - - - a / src / java / org / apache / cassandra / cli / Cli . g 
 + + + b / src / java / org / apache / cassandra / cli / Cli . g 
 @ @ - 55 , 6 + 55 , 7 @ @ tokens { 
 NODE _ UPDATE _ KEYSPACE ; 
 NODE _ UPDATE _ COLUMN _ FAMILY ; 
 NODE _ LIST ; 
 + NODE _ TRUNCATE ; 
 
 / / Internal Nodes . 
 NODE _ COLUMN _ ACCESS ; 
 @ @ - 123 , 6 + 124 , 7 @ @ statement 
 | setStatement 
 | showStatement 
 | listStatement 
 + | truncateStatement 
 | - > ^ ( NODE _ NO _ OP ) 
 ; 
 
 @ @ - 174 , 6 + 176 , 8 @ @ helpStatement 
 - > ^ ( NODE _ HELP NODE _ THRIFT _ COUNT ) 
 | K _ HELP K _ LIST 
 - > ^ ( NODE _ HELP NODE _ LIST ) 
 + | K _ HELP K _ TRUNCATE 
 + - > ^ ( NODE _ HELP NODE _ TRUNCATE ) 
 | K _ HELP 
 - > ^ ( NODE _ HELP ) 
 | ' ? ' 
 @ @ - 231 , 6 + 235 , 11 @ @ listStatement 
 - > ^ ( NODE _ LIST columnFamily keyRangeExpr ? ^ ( NODE _ LIMIT $ limit ) ? ) 
 ; 
 
 + truncateStatement 
 + : K _ TRUNCATE columnFamily 
 + - > ^ ( NODE _ TRUNCATE columnFamily ) 
 + ; 
 + 
 showClusterName 
 : K _ SHOW K _ CLUSTER K _ NAME 
 - > ^ ( NODE _ SHOW _ CLUSTER _ NAME ) 
 @ @ - 430 , 6 + 439 , 7 @ @ K _ AND : ' AND ' ; 
 K _ UPDATE : ' UPDATE ' ; 
 K _ LIST : ' LIST ' ; 
 K _ LIMIT : ' LIMIT ' ; 
 + K _ TRUNCATE : ' TRUNCATE ' ; 
 
 / / private syntactic rules 
 fragment 
 diff - - git a / src / java / org / apache / cassandra / cli / CliClient . java b / src / java / org / apache / cassandra / cli / CliClient . java 
 index b6ace98 . . b63105a 100644 
 - - - a / src / java / org / apache / cassandra / cli / CliClient . java 
 + + + b / src / java / org / apache / cassandra / cli / CliClient . java 
 @ @ - 171 , 6 + 171 , 9 @ @ public class CliClient extends CliUserHelp 
 case CliParser . NODE _ LIST : 
 executeList ( tree ) ; 
 break ; 
 + case CliParser . NODE _ TRUNCATE : 
 + executeTruncate ( tree . getChild ( 0 ) . getText ( ) ) ; 
 + break ; 
 case CliParser . NODE _ NO _ OP : 
 / / comment lines come here ; they are treated as no ops . 
 break ; 
 @ @ - 943 , 6 + 946 , 30 @ @ public class CliClient extends CliUserHelp 
 printSliceList ( columnFamilyDef , keySlices ) ; 
 } 
 
 + / / TRUNCATE < columnFamily > 
 + private void executeTruncate ( String columnFamily ) 
 + { 
 + if ( ! CliMain . isConnected ( ) | | ! hasKeySpace ( ) ) 
 + return ; 
 + 
 + / / getting CfDef , it will fail if there is no such column family in current keySpace . 
 + CfDef cfDef = getCfDef ( columnFamily ) ; 
 + 
 + try 
 + { 
 + thriftClient . truncate ( cfDef . getName ( ) ) ; 
 + sessionState . out . println ( columnFamily + " truncated . " ) ; 
 + } 
 + catch ( InvalidRequestException e ) 
 + { 
 + throw new RuntimeException ( e . getWhy ( ) ) ; 
 + } 
 + catch ( Exception e ) 
 + { 
 + throw new RuntimeException ( e . getMessage ( ) ) ; 
 + } 
 + } 
 + 
 / / SHOW API VERSION 
 private void executeShowVersion ( ) throws TException 
 { 
 diff - - git a / src / java / org / apache / cassandra / cli / CliCompleter . java b / src / java / org / apache / cassandra / cli / CliCompleter . java 
 index f541fe5 . . b033d8f 100644 
 - - - a / src / java / org / apache / cassandra / cli / CliCompleter . java 
 + + + b / src / java / org / apache / cassandra / cli / CliCompleter . java 
 @ @ - 55 , 12 + 55 , 16 @ @ public class CliCompleter extends SimpleCompletor 
 " help set " , 
 " help del " , 
 " help count " , 
 + " help list " , 
 + " help truncate " 
 } ; 
 private static String [ ] keyspaceCommands = { 
 " get " , 
 " set " , 
 " count " , 
 - " del " 
 + " del " , 
 + " list " , 
 + " truncate " 
 } ; 
 
 public CliCompleter ( ) 
 diff - - git a / src / java / org / apache / cassandra / cli / CliUserHelp . java b / src / java / org / apache / cassandra / cli / CliUserHelp . java 
 index 7c512e2 . . db6a779 100644 
 - - - a / src / java / org / apache / cassandra / cli / CliUserHelp . java 
 + + + b / src / java / org / apache / cassandra / cli / CliUserHelp . java 
 @ @ - 281 , 6 + 281 , 13 @ @ public class CliUserHelp { 
 state . out . println ( " list Users [ j : ] limit 40 " ) ; 
 break ; 
 
 + case CliParser . NODE _ TRUNCATE : 
 + state . out . println ( " truncate < column _ family > " ) ; 
 + state . out . println ( " Truncate specified column family . \ n " ) ; 
 + state . out . println ( " example : " ) ; 
 + state . out . println ( " truncate Category " ) ; 
 + break ; 
 + 
 default : 
 state . out . println ( " ? " ) ; 
 break ; 
 @ @ - 324 , 7 + 331 , 8 @ @ public class CliUserHelp { 
 state . out . println ( " del < cf > [ ' < key > ' ] [ ' < super > ' ] [ ' < col > ' ] Delete sub column . " ) ; 
 state . out . println ( " count < cf > [ ' < key > ' ] Count columns in record . " ) ; 
 state . out . println ( " count < cf > [ ' < key > ' ] [ ' < super > ' ] Count columns in a super column . " ) ; 
 - state . out . println ( " list < cf > List all rows in the column family . " ) ; 
 + state . out . println ( " truncate < column _ family > Truncate specified column family . " ) ; 
 + state . out . println ( " list < cf > List all rows in the column family . " ) ; 
 state . out . println ( " list < cf > [ < startKey > : ] " ) ; 
 state . out . println ( " List rows in the column family beginning with < startKey > . " ) ; 
 state . out . println ( " list < cf > [ < startKey > : < endKey > ] " ) ; 
 diff - - git a / test / unit / org / apache / cassandra / cli / CliTest . java b / test / unit / org / apache / cassandra / cli / CliTest . java 
 index 24a3de0 . . eddcafd 100644 
 - - - a / test / unit / org / apache / cassandra / cli / CliTest . java 
 + + + b / test / unit / org / apache / cassandra / cli / CliTest . java 
 @ @ - 52 , 6 + 52 , 7 @ @ public class CliTest extends TestCase 
 " list CF3 [ h : ] " , 
 " list CF3 limit 10 " , 
 " list CF3 [ h : g ] limit 10 " , 
 + " truncate CF1 " , 
 " update keyspace TestKeySpace with placement _ strategy = ' org . apache . cassandra . locator . LocalStrategy ' " , 
 " update keyspace TestKeySpace with replication _ factor = 1 and strategy _ options = [ { DC1 : 3 , DC2 : 4 , DC5 : 1 } ] " 
 } ; 
 @ @ - 101 , 6 + 102 , 10 @ @ public class CliTest extends TestCase 
 assertTrue ( result . startsWith ( " = > ( column = " ) ) ; 
 } 
 } 
 + else if ( statement . startsWith ( " truncate " ) ) 
 + { 
 + assertTrue ( result . contains ( " truncated . " ) ) ; 
 + } 
 
 outStream . reset ( ) ; / / reset stream so we have only output from next statement all the time 
 errStream . reset ( ) ; / / no errors to the end user .
